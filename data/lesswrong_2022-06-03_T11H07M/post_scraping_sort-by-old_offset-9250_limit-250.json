{"results": [{"createdAt": null, "postedAt": "2013-07-10T14:33:14.244Z", "modifiedAt": null, "url": null, "title": "[LINK] Hypothesis about the mechanism for storing long-term memory", "slug": "link-hypothesis-about-the-mechanism-for-storing-long-term", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:57.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andy_McKenzie", "createdAt": "2009-02-28T21:46:45.283Z", "isAdmin": false, "displayName": "Andy_McKenzie"}, "userId": "7PFnr3J3uCGSfjnZJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H3Qv7ZjKPbjRsJ72r/link-hypothesis-about-the-mechanism-for-storing-long-term", "pageUrlRelative": "/posts/H3Qv7ZjKPbjRsJ72r/link-hypothesis-about-the-mechanism-for-storing-long-term", "linkUrl": "https://www.lesswrong.com/posts/H3Qv7ZjKPbjRsJ72r/link-hypothesis-about-the-mechanism-for-storing-long-term", "postedAtFormatted": "Wednesday, July 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Hypothesis%20about%20the%20mechanism%20for%20storing%20long-term%20memory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Hypothesis%20about%20the%20mechanism%20for%20storing%20long-term%20memory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH3Qv7ZjKPbjRsJ72r%2Flink-hypothesis-about-the-mechanism-for-storing-long-term%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Hypothesis%20about%20the%20mechanism%20for%20storing%20long-term%20memory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH3Qv7ZjKPbjRsJ72r%2Flink-hypothesis-about-the-mechanism-for-storing-long-term", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH3Qv7ZjKPbjRsJ72r%2Flink-hypothesis-about-the-mechanism-for-storing-long-term", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 305, "htmlBody": "<div>As proposed by Roger Tsien, open access <a href=\"http://www.pnas.org/content/early/2013/07/05/1310158110.abstract\">here</a>. The first paragraph offers a potentially biased but still useful overview of the state of the field:&nbsp;</div>\n<div><br /></div>\n<blockquote>\n<div>A major problem in understanding memory is how it can be very long-lasting and stable from early childhood until death, despite massive interruptions in brain state as extreme as prolonged comas. Current prominent candidates for molecular substrates for long-term memory storage have focused on macromolecules such as <a href=\"http://en.wikipedia.org/wiki/Ca2%2B/calmodulin-dependent_protein_kinase\">calmodulin-dependent protein kinase II </a>(CaMKII) coupled with the <a href=\"https://en.wikipedia.org/wiki/NMDA_receptor\">NMDA receptor</a> and <a href=\"http://en.wikipedia.org/wiki/Protein_phosphatase_2\">protein phosphatase 2A</a> (2), <a href=\"https://en.wikipedia.org/wiki/Protein_kinase_M_zeta/Protein_kinase_C_zeta\">protein kinase M zeta</a> (PKM&zeta;) (3), and <a href=\"http://en.wikipedia.org/wiki/CPEB\">cytoplasmic polyadenylation element binding protein</a> (CPEB) (4), all of which are inside postsynaptic spines. To retain information despite metabolic turnover, all such candidates need to have some sort of bistable switch (e.g., state of phosphorylation or prion conformation) and a mechanism by which older copies of the molecule pass on their status to newer copies to preserve the information. A major problem is that individual intracellular molecules typically last at most a few days before being turned over. Therefore, the information would have to survive being copied tens of thousands of times in a long-lived human, despite metabolic interruptions. Such robust \ufb01delity would be extremely dif\ufb01cult to engineer. Even dynamic computer memory with sophisticated refresh and error correction circuits cannot cope with even a momentary hiccup in its power supply. Instead, long-term information storage in both computers and human civilizations requires writing the information onto physically stable storage media (e.g., magnetic disks, clay tablets, or acid-free paper), which do not require frequent energy-dependent recopying. Aside from some nuclear pore constituents, all of the known really long-lived proteins are insoluble [extracellular matrix] components such as crystallin, elastin, collagen, and proteoglycans (5), which gain stability by extensive cross-linkage and remoteness from intracellular degradative machinery, such as proteasomes, lysosomes, and autophagy.</div>\n<div><br /></div>\n</blockquote>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H3Qv7ZjKPbjRsJ72r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.2612838687210269e-06, "legacy": true, "legacyId": "23274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-10T23:00:38.541Z", "modifiedAt": null, "url": null, "title": "Svante Arrhenius's Prediction of Climate Change", "slug": "svante-arrhenius-s-prediction-of-climate-change", "viewCount": null, "lastCommentedAt": "2019-01-08T16:13:26.806Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NAaXDoidWStxz8DSz/svante-arrhenius-s-prediction-of-climate-change", "pageUrlRelative": "/posts/NAaXDoidWStxz8DSz/svante-arrhenius-s-prediction-of-climate-change", "linkUrl": "https://www.lesswrong.com/posts/NAaXDoidWStxz8DSz/svante-arrhenius-s-prediction-of-climate-change", "postedAtFormatted": "Wednesday, July 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Svante%20Arrhenius's%20Prediction%20of%20Climate%20Change&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASvante%20Arrhenius's%20Prediction%20of%20Climate%20Change%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAaXDoidWStxz8DSz%2Fsvante-arrhenius-s-prediction-of-climate-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Svante%20Arrhenius's%20Prediction%20of%20Climate%20Change%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAaXDoidWStxz8DSz%2Fsvante-arrhenius-s-prediction-of-climate-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAaXDoidWStxz8DSz%2Fsvante-arrhenius-s-prediction-of-climate-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 435, "htmlBody": "<div>\n<p>In&nbsp;<a href=\"/lw/8f9/intelligence_explosion_analysis_draft_introduction/\">Intelligence Explosion analysis draft: introduction</a>, Luke Muehlhauser and&nbsp;Anna Salamon wrote</p>\n<blockquote>\n<p class=\"MsoNormal\">Svante Arrhenius' (1896) models of climate change lacked modern climate theory and data but, by making reasonable extrapolations from what was known of physics, still managed to predict (within 2&deg;C) how much warming would result from a doubling of CO2 in the atmosphere (Crawford 1997).</p>\n</blockquote>\n<p class=\"MsoNormal\">As a part of the project&nbsp;<a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a>, I've summarized my initial impressions of Arrhenius's predictions and the impact that they might have had. The object level material is all draw from Wikipedia, and I have not vetted it.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<ul>\n<li>Arrhenius's chemistry was sound: the equation for how the Earth's temperature varies as a function of concentration of carbon dioxide is the same equation used today.</li>\n<li>For the most part, Arrhenius didn't model how increased carbon concentrations would impact other factors that influence the Earth's temperature. I don't know if this is because he wasn't aware of these, because he thought that they were sufficiently small to ignore, or because he didn't try to. </li>\n<li>Knut &Aring;ngstr&ouml;m criticized Arrhenius's claim on scientific grounds, giving a different model which predicted no climate change from increased carbon concentrations. My surface impression is that Arrhenius was a much more accomplished scientist than Knut &Aring;ngstr&ouml;m was. To the extent that this is true, I think that &Aring;ngstr&ouml;m's view should be heavily discounted, but I haven't investigated further. </li>\n<li>While Arrhenius recognized that the use of fossil fuels could increase atmospheric concentrations, he underestimated how fast carbon emissions would increase (by a huge margin) because he didn't recognize how widespread fossil fuel use would become. </li>\n<li>People later thought that Arrhenius's prediction that atmospheric carbon would increase was wrong, because they thought that oceans would serve as great carbon sinks. It would be interesting to look into whether they had good reasons for thinking this at the time. </li>\n<li>Arrhenius predicted that global warming would have positive humanitarian impacts on balance, global warming now appears to have negative humanitarian impacts on balance.&nbsp;</li>\n</ul>\n<p>Taking this all together, based on my surface impressions, I think that this case study gives evidence against attempting to predict the far future being useful:</p>\n<ul>\n<li>To the extent that Arrhenius was right, he was largely ignored.&nbsp;</li>\n</ul>\n<ul>\n<li>Arrhenius could have been wrong (the countervailing theories could have been right), but this warrants further investigation.&nbsp;</li>\n</ul>\n<ul>\n<li>If, in the subsequent years, people had started burning fossil fuels more with a view toward giving rise to positive humanitarian impacts, they would have exacerbated the <a href=\"/lw/hi1/potential_impacts_of_climate_change/\">potential negative impacts of climate change</a>.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"frcrRgCk9PDbEScua": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NAaXDoidWStxz8DSz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "23275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dEkSus8QzcCyEKNa6", "t7gKX9Av5zggsQsYr", "9qgQWEauLGuPwuYPh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-10T23:20:03.920Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 77-78", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-77-78", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2sP9zgjnotPEGwubM/meetup-durham-rtlw-hpmor-discussion-ch-77-78", "pageUrlRelative": "/posts/2sP9zgjnotPEGwubM/meetup-durham-rtlw-hpmor-discussion-ch-77-78", "linkUrl": "https://www.lesswrong.com/posts/2sP9zgjnotPEGwubM/meetup-durham-rtlw-hpmor-discussion-ch-77-78", "postedAtFormatted": "Wednesday, July 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2077-78&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2077-78%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2sP9zgjnotPEGwubM%2Fmeetup-durham-rtlw-hpmor-discussion-ch-77-78%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2077-78%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2sP9zgjnotPEGwubM%2Fmeetup-durham-rtlw-hpmor-discussion-ch-77-78", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2sP9zgjnotPEGwubM%2Fmeetup-durham-rtlw-hpmor-discussion-ch-77-78", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/oi'>Durham/RTLW HPMoR discussion, ch. 77-78</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 July 2013 12:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Saturday:  food truck food, coffee from Cocoa Cinnamon, beer from Fullsteam, and discussion from HPMoR!</p>\n\n<p>If you have the time &amp; inclination to do the reading in advance, please notice and note any discussion-worthy points of interest.  If not, please come hang out anyway!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/oi'>Durham/RTLW HPMoR discussion, ch. 77-78</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2sP9zgjnotPEGwubM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.2617012803481351e-06, "legacy": true, "legacyId": "23276", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_78\">Discussion article for the meetup : <a href=\"/meetups/oi\">Durham/RTLW HPMoR discussion, ch. 77-78</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 July 2013 12:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Saturday:  food truck food, coffee from Cocoa Cinnamon, beer from Fullsteam, and discussion from HPMoR!</p>\n\n<p>If you have the time &amp; inclination to do the reading in advance, please notice and note any discussion-worthy points of interest.  If not, please come hang out anyway!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_781\">Discussion article for the meetup : <a href=\"/meetups/oi\">Durham/RTLW HPMoR discussion, ch. 77-78</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 77-78", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_78", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 77-78", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_781", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-11T07:17:20.355Z", "modifiedAt": null, "url": null, "title": "Seed Study: Polyphasic Sleep in Ten Steps", "slug": "seed-study-polyphasic-sleep-in-ten-steps", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:31.894Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cMnjqk8iEzycbLBDA/seed-study-polyphasic-sleep-in-ten-steps", "pageUrlRelative": "/posts/cMnjqk8iEzycbLBDA/seed-study-polyphasic-sleep-in-ten-steps", "linkUrl": "https://www.lesswrong.com/posts/cMnjqk8iEzycbLBDA/seed-study-polyphasic-sleep-in-ten-steps", "postedAtFormatted": "Thursday, July 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seed%20Study%3A%20Polyphasic%20Sleep%20in%20Ten%20Steps&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeed%20Study%3A%20Polyphasic%20Sleep%20in%20Ten%20Steps%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMnjqk8iEzycbLBDA%2Fseed-study-polyphasic-sleep-in-ten-steps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seed%20Study%3A%20Polyphasic%20Sleep%20in%20Ten%20Steps%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMnjqk8iEzycbLBDA%2Fseed-study-polyphasic-sleep-in-ten-steps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMnjqk8iEzycbLBDA%2Fseed-study-polyphasic-sleep-in-ten-steps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1073, "htmlBody": "<p><img style=\"margin-top: 5px; margin-bottom: 5px; margin-left: 15px; margin-right: 15px; float: right;\" src=\"http://cutearoo.com/wp-content/uploads/2011/03/Sleepy-Mouse.jpg\" alt=\"\" width=\"225\" height=\"183\" />(Update on this project now available&nbsp;<a href=\"/lw/ip6/polyphasic_sleep_seed_study_reprise/\">here</a>.)</p>\n<p><br />A handful of Bay Area folks will be going polyphasic over the next month. By that, I mean we'll be adopting a sleep schedule that gets us 4 extra hours of productive work or play time per day, or two whole months per year. (Or a decade over 60 years.)</p>\n<p>If you want to tell me about why it's a bad idea, feel free to post comments. I don't plan to use this space to sell you on polyphasic sleep. That might be another post, depending on how this goes.</p>\n<p>I'm going to be collecting some very simple data <strong><a href=\"https://docs.google.com/forms/d/1zr99Nzz4HVRZNNw22A1gDq_GAcp_53b1e7bEPeQ0HM0/viewform\">through this here form</a></strong>. I invite you to join us!</p>\n<p>This will be hard. It will hurt. You'll probably need a buddy to follow you around and keep you awake. If you don't have a lot of self-discipline, I don't recommend even trying.</p>\n<p>Still with me? If you want in by the time you're done reading this, message me (or comment below) with your name so I know who you are. Here's the plan.</p>\n<p style=\"padding-left: 30px;\">1. &nbsp; Stop using caffeine <strong>right now</strong>. If you try to maintain a caffeine addition during this process, you will fail. I promise.</p>\n<p style=\"padding-left: 30px;\">2. &nbsp; Data collection began on July 10th. Start submitting daily reports at any point as soon as you want to participate, especially if you can begin in the next couple of days and then stick to our schedule. Fill out the <a href=\"https://docs.google.com/forms/d/1zr99Nzz4HVRZNNw22A1gDq_GAcp_53b1e7bEPeQ0HM0/viewform\">above form</a> once every 24hrs (whenever it's convenient) until August 10th.</p>\n<p style=\"padding-left: 30px;\">3. &nbsp; Pick a time to take a 20min nap each day from Monday, July 15th through Sunday, July 21st. You probably won't actually sleep during this time, but you can use it for mindfulness meditation if you stay awake. The goal is to practice napping. This is important.</p>\n<p style=\"padding-left: 30px;\">4. &nbsp; On Monday, July 22nd, begin fasting immediately after lunch.</p>\n<p style=\"padding-left: 30px;\">5. &nbsp; On the night of Monday, July 22nd, skip sleep. No naps, then an all-nighter. <strong>This is the official adaptation start date.</strong> The idea is to make you sleep deprived so your naps the next day are more likely to take.</p>\n<p style=\"padding-left: 30px;\">6. &nbsp; Eat breakfast on the morning of Tuesday, July 23rd. This should be the first time you've eaten anything since Monday lunch.</p>\n<p style=\"padding-left: 30px;\">7. &nbsp; Starting on the morning of Tuesday, July 23rd, take a 20min nap every 2hrs (for a total of 12 naps per day). <strong>Do not</strong> oversleep. Use an obnoxious alarm or whatever other means necessary. \"Nap\" counts as lying down trying to sleep; take your naps on a strict schedule regardless of how long you successfully sleep.</p>\n<p style=\"padding-left: 30px;\">8. &nbsp; Start to cut your naps down toward 6 a day as quickly as you can without it hurting too much. Beginning to dream during your naps is a good indicator that you're ready for this part.*</p>\n<p style=\"padding-left: 30px;\">9. &nbsp; Once you're down to one nap every 4 hours, you're on what's known as the Uberman schedule.</p>\n<p style=\"padding-left: 30px;\">10. &nbsp;Matt Fallshaw informs me that the next part is a little tricky.</p>\n<p style=\"padding-left: 60px;\">10.1. &nbsp; If you managed to reach the Uberman feeling good, you'll probably start getting really tired again shortly thereafter. This flavor of tired will be different from what you've suffered for the past week, and by that flavor you will know that you have hit SWS deprivation. If this is what happens to you, the new kind of sleepy is your cue to transition straight to the Everyman 3 schedule, which means a 3 hour block of core sleep plus three 20 minute naps spaced evenly throughout the day. And that's it!</p>\n<p style=\"padding-left: 60px;\">10.2. &nbsp; If you're unlucky, you'll not quite have reached Uberman in the space of a week--that is, you'll still be hanging on to some extra naps on July 30th. Then you'll be wolloped by a new bout of sleepiness. This flavor of tired will be different from the last. If it's is tolerable, drop straight to full Uberman and try to hold out for at least 24hrs, then convert to the Everyman 3. If the new flavor of tired is intolerable, convert to E3 as soon as the new tired hits, and expect the next week or so to be tougher on you than on the lucky ones.</p>\n<p><img style=\"vertical-align: middle; margin-top: 8px; margin-bottom: 8px; margin-left: 10px; margin-right: 10px;\" src=\"http://images.lesswrong.com/t3_hyr_0.png\" alt=\"\" width=\"475\" height=\"200\" /></p>\n<p>Why are we doing this weird naptation adaptation plan thing instead of just going straight for the Everyman 3? Mostly because Matthew Fallshaw said to. If you know Matt, that's enough. In case you don't: It takes people about a month to adapt to the Everyman 3, but only about a week to adapt to the Uberman. The Uberman forces your body to learn to get its REM and SWS in those tiny 20 minute naps. If you're still giving it core sleep time, your body won't take the fullest possible advantage of naps right away.</p>\n<p>If you think you can keep the Uberman schedule indefinitely, go for it! But keep me informed about it so I know what's up with my data.</p>\n<p>*A clarification from Matt: \"<span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Drop naps as quickly as you can while remaining functional. The most important part of this period is that you don't sleep for longer than 20 minutes at a time, but the earlier you can get to a pure Uberman schedule the better. Take naps as you need them (with at least 40 minutes awake and moving around between naps) while pushing towards Uberman. The longer you can maintain pure Uberman before introducing a longer core sleep block the further along you'll be to a full adaptation.\"</span></p>\n<p>ETA: I chose the psychomotor vigilance task (which you'll find if you check out the form I linked to) because the specific thing I'm trying to do here is distinguish polyphasic from chronic partial sleep restriction. If people on polyphasic return to their monophasic PVT baseline after a couple of weeks, especially if they stay there for a long time, that's clear evidence that the polyphasers are not experiencing the same physiological phenomenon as people suffering from chronic partial sleep restriction, which is what I'm actually concerned about. One of the only really well established facts in the literature on partial sleep restriction is that people who are deprived of a couple of hours of sleep a night get worse at the PVT as a function of time. If it's the case that the polyphasers will end up with memory problems, attention problems, and related issues, the simplest explanation is that they're suffering from a kind of chronic partial sleep restriction. I hope that clears some things up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HLoxy2feb2PYqooom": 1, "AodfCFefLAuwDyj7Z": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cMnjqk8iEzycbLBDA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 46, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "23283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 135, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QvZ6w64JugewNiccS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-11T12:57:55.405Z", "modifiedAt": null, "url": null, "title": "Meetup : [Moscow] The Meet up", "slug": "meetup-moscow-the-meet-up", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vfmeN337qtmoJ38LJ/meetup-moscow-the-meet-up", "pageUrlRelative": "/posts/vfmeN337qtmoJ38LJ/meetup-moscow-the-meet-up", "linkUrl": "https://www.lesswrong.com/posts/vfmeN337qtmoJ38LJ/meetup-moscow-the-meet-up", "postedAtFormatted": "Thursday, July 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BMoscow%5D%20The%20Meet%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BMoscow%5D%20The%20Meet%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfmeN337qtmoJ38LJ%2Fmeetup-moscow-the-meet-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BMoscow%5D%20The%20Meet%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfmeN337qtmoJ38LJ%2Fmeetup-moscow-the-meet-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfmeN337qtmoJ38LJ%2Fmeetup-moscow-the-meet-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/oj'>[Moscow] The Meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 July 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall. And we will also check the entrance at 16:10, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality exercises.</p></li>\n<li><p>Prediction market.</p></li>\n<li><p>Game session: the Liar's dice or the Resistance.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html?utm_source=lesswrong.com&amp;utm_medium=meetup_notice&amp;utm_term=link_for_reports+20130721_meet_up&amp;utm_content=meetup_20130721&amp;utm_campaign=moscow_meetups\">here, in Russian</a>, now with more photos!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/oj'>[Moscow] The Meet up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vfmeN337qtmoJ38LJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2623497627466537e-06, "legacy": true, "legacyId": "23284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Moscow__The_Meet_up\">Discussion article for the meetup : <a href=\"/meetups/oj\">[Moscow] The Meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 July 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall. And we will also check the entrance at 16:10, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality exercises.</p></li>\n<li><p>Prediction market.</p></li>\n<li><p>Game session: the Liar's dice or the Resistance.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html?utm_source=lesswrong.com&amp;utm_medium=meetup_notice&amp;utm_term=link_for_reports+20130721_meet_up&amp;utm_content=meetup_20130721&amp;utm_campaign=moscow_meetups\">here, in Russian</a>, now with more photos!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Moscow__The_Meet_up1\">Discussion article for the meetup : <a href=\"/meetups/oj\">[Moscow] The Meet up</a></h2>", "sections": [{"title": "Discussion article for the meetup : [Moscow] The Meet up", "anchor": "Discussion_article_for_the_meetup____Moscow__The_Meet_up", "level": 1}, {"title": "Discussion article for the meetup : [Moscow] The Meet up", "anchor": "Discussion_article_for_the_meetup____Moscow__The_Meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-11T16:53:53.270Z", "modifiedAt": null, "url": null, "title": "Less Wrong London New Arrival Integration Task Force", "slug": "less-wrong-london-new-arrival-integration-task-force", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:57.083Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6bweLn9Xv9po5Bdzs/less-wrong-london-new-arrival-integration-task-force", "pageUrlRelative": "/posts/6bweLn9Xv9po5Bdzs/less-wrong-london-new-arrival-integration-task-force", "linkUrl": "https://www.lesswrong.com/posts/6bweLn9Xv9po5Bdzs/less-wrong-london-new-arrival-integration-task-force", "postedAtFormatted": "Thursday, July 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20London%20New%20Arrival%20Integration%20Task%20Force&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20London%20New%20Arrival%20Integration%20Task%20Force%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bweLn9Xv9po5Bdzs%2Fless-wrong-london-new-arrival-integration-task-force%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20London%20New%20Arrival%20Integration%20Task%20Force%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bweLn9Xv9po5Bdzs%2Fless-wrong-london-new-arrival-integration-task-force", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bweLn9Xv9po5Bdzs%2Fless-wrong-london-new-arrival-integration-task-force", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>After considering how many of us have arrived here within recent memory, the London Less Wrong community is explicitly offering itself as a resource for those moving to the city.  We all know how awkward and daunting it can be to get settled somewhere new, and we'd like to help newcomers hit the ground running.  It seems that our most effective recruitment strategy at the moment is \"wait for existing Less Wrong readers to move here\", so it's a worthwhile offer to make.</p>\n<p>If any LessWrongers are moving to the Greater London area, let us know.  Either message me via the site or join our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google Group</a>.  Tell us where/when you're moving, what your circumstances are, and what sort of things you like to do.  We will try to proactively invite you to events and activities we think you'll enjoy, as well as providing you with useful local knowledge if you want it.</p>\n<p>We will also try and make some time available for you if there's anything you need another person's help with.  If assembling your Ikea bookshelf is a two-person job, we'll see if we can scrape together a couple of people for a couple of hours to help you put it together. &nbsp;If you need help setting up your wireless router, we'll see if someone with the relevant skills is available to give you a hand. &nbsp;We can't promise any specific type of help, but we're always happy to be asked.</p>\n<p>Big cities can often feel quite impersonal, so if you're planning on moving here, or even just thinking about it, let us know, and we'll see what we can do to make it a little more welcoming.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6bweLn9Xv9po5Bdzs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 44, "extendedScore": null, "score": 1.2625369693597139e-06, "legacy": true, "legacyId": "23285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-11T17:48:27.030Z", "modifiedAt": null, "url": null, "title": "[META] Open threads (and repository threads) are underutilized.", "slug": "meta-open-threads-and-repository-threads-are-underutilized", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:58.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P8Qc7te5qcEnpD3nv/meta-open-threads-and-repository-threads-are-underutilized", "pageUrlRelative": "/posts/P8Qc7te5qcEnpD3nv/meta-open-threads-and-repository-threads-are-underutilized", "linkUrl": "https://www.lesswrong.com/posts/P8Qc7te5qcEnpD3nv/meta-open-threads-and-repository-threads-are-underutilized", "postedAtFormatted": "Thursday, July 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Open%20threads%20(and%20repository%20threads)%20are%20underutilized.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Open%20threads%20(and%20repository%20threads)%20are%20underutilized.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP8Qc7te5qcEnpD3nv%2Fmeta-open-threads-and-repository-threads-are-underutilized%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Open%20threads%20(and%20repository%20threads)%20are%20underutilized.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP8Qc7te5qcEnpD3nv%2Fmeta-open-threads-and-repository-threads-are-underutilized", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP8Qc7te5qcEnpD3nv%2Fmeta-open-threads-and-repository-threads-are-underutilized", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 400, "htmlBody": "<p><a href=\"/lw/hyf/link_xkcd_comic_1236_seashells_and_bayes_theorem/9cf4?context=1#comments\">Recently</a>, issues with the way open threads currently work were brought up. Open threads aren't very visible and get crowded with comments quickly. This causes people to post things that belong in open threads in r/discussion, to not post in open threads more than a few days old, or to ignore/be unaware of new comments in open threads. I think we can do better.</p>\n<p>Some possible solutions that were pointed out, or that I thought of are:</p>\n<ul>\n<li>Put the most recent open thread at the top of the 'Recent Comments' sidebar.</li>\n<li>Having open threads more often.</li>\n<li>Put a link to it on the main page.</li>\n<li>Make a new subreddit for open threads.</li>\n<li>Create a new medium for open threads.</li>\n</ul>\n<p>Note that not all of these are orthogonal.</p>\n<p>Having them more often has the advantage of being especially easy to implement. Adding new links seems to be relatively easy to implement as well. As far as I know, making a new subreddit isn't too difficult, but making a new medium would probably be a waste of development resources.</p>\n<p>Personally, I like the idea of having a new subreddit for open threads. It&nbsp; would increase visibility, not get overcrowded, and have the right atmosphere for a casual open thread. My evidence for believing this comes from being familiar the way Reddit works. It seems like there is some resistance to creating new subreddits here, so I don't expect this to be implemented. I would like to see the reasoning for this attitude, if it indeed exists.</p>\n<p>&nbsp;</p>\n<p>There are similar issues for the repository threads. For repositories, having them more often defeats the purpose of having one place for a certain type of idea, and a different subreddit doesn't seem right either. Giving them their own wiki pages might be a better medium, with new threads to encourage new ideas every once in a while. The main problem for this is the <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconvenience</a> of going to the wiki, and logging in. It would be nice if there was a unified log-in for this part of the site and the wiki, but I realize this may be technically difficult. I might organize a wiki page for some of the repositories myself if people think this is a good idea but no one else feels like doing it (depends on if I feel like doing it too :p ).</p>\n<p>Thoughts?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P8Qc7te5qcEnpD3nv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 29, "extendedScore": null, "score": 1.262580264601266e-06, "legacy": true, "legacyId": "23286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-11T23:16:50.358Z", "modifiedAt": null, "url": null, "title": "The WASH-1400 Reactor Safety Study", "slug": "the-wash-1400-reactor-safety-study", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:01.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zXXCJmHcC2LLiXYaa/the-wash-1400-reactor-safety-study", "pageUrlRelative": "/posts/zXXCJmHcC2LLiXYaa/the-wash-1400-reactor-safety-study", "linkUrl": "https://www.lesswrong.com/posts/zXXCJmHcC2LLiXYaa/the-wash-1400-reactor-safety-study", "postedAtFormatted": "Thursday, July 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20WASH-1400%20Reactor%20Safety%20Study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20WASH-1400%20Reactor%20Safety%20Study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzXXCJmHcC2LLiXYaa%2Fthe-wash-1400-reactor-safety-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20WASH-1400%20Reactor%20Safety%20Study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzXXCJmHcC2LLiXYaa%2Fthe-wash-1400-reactor-safety-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzXXCJmHcC2LLiXYaa%2Fthe-wash-1400-reactor-safety-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 411, "htmlBody": "<p>In&nbsp;<a href=\"/lw/8f9/intelligence_explosion_analysis_draft_introduction/\">Intelligence Explosion analysis draft: introduction</a>, Luke Muehlhauser and&nbsp;Anna Salamon wrote</p>\n<blockquote>\n<p class=\"MsoNormal\">Norman Rasmussen's (1975) analysis of the safety of nuclear power plants, written before any nuclear accidents had occurred, correctly predicted several details of the Three Mile Island incident that previous experts had not (McGrayne 2011, 180).</p>\n</blockquote>\n<p class=\"MsoNormal\">I investigated this further a part of the project&nbsp;<a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a>. The analysis that Muehlhauser and Salamon reference is&nbsp;<a href=\"http://en.wikipedia.org/wiki/WASH-1400\">WASH-1400, 'The Reactor Safety Study'</a>, which was produced for the Nuclear Regulatory Commission.</p>\n<p class=\"MsoNormal\">Upon investigating, I formed the impression that this case study little relevance to the question of whether we can know what to do about AI:</p>\n<ul>\n<li>The issue was one that a lot of people were already concerned about.</li>\n<li>The issue was highly domain specific.</li>\n<li>Rather than there being a few salient predictions, there were a huge number of small predictions.</li>\n</ul>\n<p class=\"MsoNormal\">One way in which the situation is relevant is that risk of nuclear power plant accidents was not adequately addressed, but that's only one of many examples people not adequately addressing risks.&nbsp;</p>\n<p class=\"MsoNormal\">A few details below.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<ul>\n<li>The report predicted one core damage accident per 20,000 years of nuclear power plant operation.<br /><br />As a point of comparison, at the time of the Three Mile Island incident, only 500 years of nuclear power plant operation had occurred. This could be a fluke. The Chernobyl accident occurred only 6 years later. If the cause was the same, that strongly suggests that the Three Mile Island incident was not a fluke.&nbsp;<br /><br />However, the report was based on reactor designs that didn't include the Three Mile Island type.</li>\n<li>The report did discuss tidal waves as a potential cause for nuclear disaster, anticipating the recent disaster in Japan. But the report is 21 volumes long, and so this (weak) prediction could have been cherry picked retrospectively.</li>\n<li>The report is considered to be obsolete, and its main lasting value seems to have been pioneering use of&nbsp;<a href=\"http://en.wikipedia.org/wiki/Probabilistic_Risk_Assessment\">probabilistic risk assessment</a>.</li>\n</ul>\n<div>A summary of the situation seems to be: \"People were concerned about nuclear power plants being dangerous. The Nuclear Regulatory Commission commissioned a report analyzing the risks. The report was really long, and didn't address key relevant factors. Other scientists thought that the report understated the risks. The report was quickly recognized to have major flaws and became obsolete.\"</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zXXCJmHcC2LLiXYaa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 1.2628408953379772e-06, "legacy": true, "legacyId": "23287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dEkSus8QzcCyEKNa6", "t7gKX9Av5zggsQsYr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-12T04:12:33.453Z", "modifiedAt": null, "url": null, "title": "Meetup : [Columbus] Upcoming Workshop Topics", "slug": "meetup-columbus-upcoming-workshop-topics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hXMZYv6Bz6kYgSvmJ/meetup-columbus-upcoming-workshop-topics", "pageUrlRelative": "/posts/hXMZYv6Bz6kYgSvmJ/meetup-columbus-upcoming-workshop-topics", "linkUrl": "https://www.lesswrong.com/posts/hXMZYv6Bz6kYgSvmJ/meetup-columbus-upcoming-workshop-topics", "postedAtFormatted": "Friday, July 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BColumbus%5D%20Upcoming%20Workshop%20Topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BColumbus%5D%20Upcoming%20Workshop%20Topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXMZYv6Bz6kYgSvmJ%2Fmeetup-columbus-upcoming-workshop-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BColumbus%5D%20Upcoming%20Workshop%20Topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXMZYv6Bz6kYgSvmJ%2Fmeetup-columbus-upcoming-workshop-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXMZYv6Bz6kYgSvmJ%2Fmeetup-columbus-upcoming-workshop-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 389, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/ok\">Upcoming Workshop Topics</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 July 2013, 7:30 pm</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Columbus, OH</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Columbus, OH has a thriving rationality scene. If you live nearby, you should come check us out. PM me here, join the facebook page, or RSVP to the meetup.com site for more info!</p>\n<p><strong>Logistics:</strong> We run workshops on the first, third, and fifth Mondays of the month at the HCCO conference room off Henderson. Workshops start at 7:30pm, and go until about 9:30.</p>\n<p><strong>Demographics:</strong> Most workshops draw about 25 people. We have a wide range of ages, centering around 30 years old. Decent gender balance (1:2). Very casual atmosphere.</p>\n<p><strong>Links:</strong> Our Facebook page- https://www.facebook.com/groups/563449250347654/ &nbsp;&nbsp;</p>\n<p>Meetup page for the next meetup- <a rel=\"nofollow\" href=\"http://www.meetup.com/HumanistOhio/events/128040042/\">http://www.meetup.com/HumanistOhio/events/128040042/</a></p>\n<p>&nbsp;</p>\n<p><strong>UPCOMING TOPICS</strong></p>\n<p><em>July 15- Community Engagement- Gleb Tsipursky</em></p>\n<p>-Rationality is about the science of reasoning, and acting effectively to meet your goals. This week's workshop is on Community Engagement, and will be taught by Gleb Tsipursky, a professor of history at OSU.</p>\n<p>Most of our activities in Rationality focus on improving our own thinking. This workshop broadens our lens to consider how the ideas of rationality can be applied fruitfully to analyze and improve our communities. We will look at three levels of our community: our local, everyday communities; our economic system; and our political system. Our goal will be to come away with a broadened understanding of some of the irrationalities in our communities and some ideas for what we can do to help improve the situation.</p>\n<p><em>July 29- Social Activity TBA &nbsp; </em></p>\n<p><em>August 5- Popular Psych Studies, and Where They Go Wrong- Don Sutterfield &nbsp; </em></p>\n<p><em>August 19- Intro to Bayes- Eric Huff &nbsp; </em></p>\n<p><em>Sept 2- Labor Day- Possible Social Gathering TBA &nbsp; </em></p>\n<p><em>Sept 16- Budgeting/Finances- Jeff Dubin &nbsp; </em></p>\n<p><em>Sept 30- tentative- Effective Altruism- Elissa Caffery Fleming</em></p>\n<p><em>Oct 7-&nbsp;</em><em>TBA- Mike Riggs &nbsp;</em></p>\n<p><strong>OCTOBER 11-13 MEGA MEETUP: TOPICS</strong></p>\n<p><em>Down the Rabbit Hole: Magic as Psychic Entertainment -Jack Strauss</em></p>\n<p>-Magician/Mentalist Jack Strauss will present a stage act as a psychic entertainer. Afterwards, there will be a sit-down talkback with the audience. Topics will be determined by audience questions and may include: ethics of performing on stage with a psychic persona, psychology of deception, techniques of cold reading, etc. The only topic off the table will be the specifics of how the act you just saw is performed.</p>\n<p><em>Jesse Galef- Defense Against the Dark Arts &nbsp; </em></p>\n<p><em>Eric Huff- Applications of Models &nbsp; </em></p>\n<p><em>Demitri Muna- Instrumental Improv workshop</em></p>\n<p><em><br /></em></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/ok\">Upcoming Workshop Topics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hXMZYv6Bz6kYgSvmJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "23292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Upcoming_Workshop_Topics\">Discussion article for the meetup : <a href=\"/meetups/ok\">Upcoming Workshop Topics</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 July 2013, 7:30 pm</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Columbus, OH</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Columbus, OH has a thriving rationality scene. If you live nearby, you should come check us out. PM me here, join the facebook page, or RSVP to the meetup.com site for more info!</p>\n<p><strong>Logistics:</strong> We run workshops on the first, third, and fifth Mondays of the month at the HCCO conference room off Henderson. Workshops start at 7:30pm, and go until about 9:30.</p>\n<p><strong>Demographics:</strong> Most workshops draw about 25 people. We have a wide range of ages, centering around 30 years old. Decent gender balance (1:2). Very casual atmosphere.</p>\n<p><strong>Links:</strong> Our Facebook page- https://www.facebook.com/groups/563449250347654/ &nbsp;&nbsp;</p>\n<p>Meetup page for the next meetup- <a rel=\"nofollow\" href=\"http://www.meetup.com/HumanistOhio/events/128040042/\">http://www.meetup.com/HumanistOhio/events/128040042/</a></p>\n<p>&nbsp;</p>\n<p><strong id=\"UPCOMING_TOPICS\">UPCOMING TOPICS</strong></p>\n<p><em>July 15- Community Engagement- Gleb Tsipursky</em></p>\n<p>-Rationality is about the science of reasoning, and acting effectively to meet your goals. This week's workshop is on Community Engagement, and will be taught by Gleb Tsipursky, a professor of history at OSU.</p>\n<p>Most of our activities in Rationality focus on improving our own thinking. This workshop broadens our lens to consider how the ideas of rationality can be applied fruitfully to analyze and improve our communities. We will look at three levels of our community: our local, everyday communities; our economic system; and our political system. Our goal will be to come away with a broadened understanding of some of the irrationalities in our communities and some ideas for what we can do to help improve the situation.</p>\n<p><em>July 29- Social Activity TBA &nbsp; </em></p>\n<p><em>August 5- Popular Psych Studies, and Where They Go Wrong- Don Sutterfield &nbsp; </em></p>\n<p><em>August 19- Intro to Bayes- Eric Huff &nbsp; </em></p>\n<p><em>Sept 2- Labor Day- Possible Social Gathering TBA &nbsp; </em></p>\n<p><em>Sept 16- Budgeting/Finances- Jeff Dubin &nbsp; </em></p>\n<p><em>Sept 30- tentative- Effective Altruism- Elissa Caffery Fleming</em></p>\n<p><em>Oct 7-&nbsp;</em><em>TBA- Mike Riggs &nbsp;</em></p>\n<p><strong id=\"OCTOBER_11_13_MEGA_MEETUP__TOPICS\">OCTOBER 11-13 MEGA MEETUP: TOPICS</strong></p>\n<p><em>Down the Rabbit Hole: Magic as Psychic Entertainment -Jack Strauss</em></p>\n<p>-Magician/Mentalist Jack Strauss will present a stage act as a psychic entertainer. Afterwards, there will be a sit-down talkback with the audience. Topics will be determined by audience questions and may include: ethics of performing on stage with a psychic persona, psychology of deception, techniques of cold reading, etc. The only topic off the table will be the specifics of how the act you just saw is performed.</p>\n<p><em>Jesse Galef- Defense Against the Dark Arts &nbsp; </em></p>\n<p><em>Eric Huff- Applications of Models &nbsp; </em></p>\n<p><em>Demitri Muna- Instrumental Improv workshop</em></p>\n<p><em><br></em></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Upcoming_Workshop_Topics1\">Discussion article for the meetup : <a href=\"/meetups/ok\">Upcoming Workshop Topics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Upcoming Workshop Topics", "anchor": "Discussion_article_for_the_meetup___Upcoming_Workshop_Topics", "level": 1}, {"title": "UPCOMING TOPICS", "anchor": "UPCOMING_TOPICS", "level": 2}, {"title": "OCTOBER 11-13 MEGA MEETUP: TOPICS", "anchor": "OCTOBER_11_13_MEGA_MEETUP__TOPICS", "level": 2}, {"title": "Discussion article for the meetup : Upcoming Workshop Topics", "anchor": "Discussion_article_for_the_meetup___Upcoming_Workshop_Topics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-12T05:39:02.814Z", "modifiedAt": null, "url": null, "title": "[LINK] If correlation doesn\u2019t imply causation, then what does?", "slug": "link-if-correlation-doesn-t-imply-causation-then-what-does", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:37.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Strilanc", "createdAt": "2012-07-25T07:36:05.321Z", "isAdmin": false, "displayName": "Strilanc"}, "userId": "nBwAhG4QXmGpxLB2F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g2TyspdMPDSXu59qo/link-if-correlation-doesn-t-imply-causation-then-what-does", "pageUrlRelative": "/posts/g2TyspdMPDSXu59qo/link-if-correlation-doesn-t-imply-causation-then-what-does", "linkUrl": "https://www.lesswrong.com/posts/g2TyspdMPDSXu59qo/link-if-correlation-doesn-t-imply-causation-then-what-does", "postedAtFormatted": "Friday, July 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20If%20correlation%20doesn%E2%80%99t%20imply%20causation%2C%20then%20what%20does%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20If%20correlation%20doesn%E2%80%99t%20imply%20causation%2C%20then%20what%20does%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2TyspdMPDSXu59qo%2Flink-if-correlation-doesn-t-imply-causation-then-what-does%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20If%20correlation%20doesn%E2%80%99t%20imply%20causation%2C%20then%20what%20does%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2TyspdMPDSXu59qo%2Flink-if-correlation-doesn-t-imply-causation-then-what-does", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2TyspdMPDSXu59qo%2Flink-if-correlation-doesn-t-imply-causation-then-what-does", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>A post about how, for some causal models, causal relationships can be inferred without doing experiments that control one of the random variables.</p>\n<p><a href=\"http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/\">If correlation doesn&rsquo;t imply causation, then what does?</a></p>\n<blockquote>\n<p>To help address problems like the two example problems just discussed, Pearl introduced a <a href=\"http://ftp.cs.ucla.edu/pub/stat_ser/R212.pdf\">causal calculus</a>. In the remainder of this post, I will explain the rules of the causal calculus, and use them to analyse the smoking-cancer connection. We&rsquo;ll see that even without doing a randomized controlled experiment it&rsquo;s possible (with the aid of some reasonable assumptions) to <em>infer</em> what the outcome of a randomized controlled experiment would have been, using only relatively easily accessible experimental data, data that doesn&rsquo;t require experimental intervention to force people to smoke or not, but which can be obtained from purely observational studies.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g2TyspdMPDSXu59qo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 5, "extendedScore": null, "score": 1.2631443599760974e-06, "legacy": true, "legacyId": "23293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-12T07:37:07.297Z", "modifiedAt": null, "url": null, "title": "Cosmic expansion vs uploads economics?", "slug": "cosmic-expansion-vs-uploads-economics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.776Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gv8sa9j39Mx5eCRrb/cosmic-expansion-vs-uploads-economics", "pageUrlRelative": "/posts/gv8sa9j39Mx5eCRrb/cosmic-expansion-vs-uploads-economics", "linkUrl": "https://www.lesswrong.com/posts/gv8sa9j39Mx5eCRrb/cosmic-expansion-vs-uploads-economics", "postedAtFormatted": "Friday, July 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cosmic%20expansion%20vs%20uploads%20economics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACosmic%20expansion%20vs%20uploads%20economics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgv8sa9j39Mx5eCRrb%2Fcosmic-expansion-vs-uploads-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cosmic%20expansion%20vs%20uploads%20economics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgv8sa9j39Mx5eCRrb%2Fcosmic-expansion-vs-uploads-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgv8sa9j39Mx5eCRrb%2Fcosmic-expansion-vs-uploads-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>In a previous <a href=\"/lw/hll/to_reduce_astronomical_waste_take_your_time_then/\">post</a> (and the attendant&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/S0094576513001148\">paper</a>&nbsp;<a href=\"http://www.youtube.com/watch?v=mrUWkfeJABY\">and</a>&nbsp;<a href=\"http://www.youtube.com/watch?v=zQTfuI-9jIo\">talks</a>) I mentioned how easy it is to build a Dyson sphere around the sun (and start universal colonisation), given decent automation.</p>\n<p>Decent automation includes, of course, the copyable uploads that form the basis of Robin Hanson's <a href=\"http://www.youtube.com/watch?v=s2GIirg43sU\">upload economics</a> model. If uploads can gather vast new resources by Dysoning the sun using current or near future technology, this calls into question Robin's model that standard current economic assumptions can be extended to an uploads world.</p>\n<p>And Dysoning the sun is just one way uploads could be completely transformative. There are certainly other ways, that we cannot yet begin to imagine, that uploads could radically transform human society in short order, making all our continuity assumptions and our current models moot. It would be worth investigating these ways, keeping in mind that we will likely miss some important ones.</p>\n<p>Against this, though, is the general <a href=\"/lw/hvo/against_easy_superintelligence_the_unforeseen/\">unforeseen friction</a> argument. Uploads may be radically transformative, but probably on longer timescales than we'd expect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gv8sa9j39Mx5eCRrb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -7, "extendedScore": null, "score": 1.2632381347823174e-06, "legacy": true, "legacyId": "23153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["S4PGuNHgjNpnnnQvT", "rPBoLRxSJ88jrthEJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-12T15:37:17.572Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-42", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3PscmtNx4xRGAfwNa/weekly-lw-meetups-42", "pageUrlRelative": "/posts/3PscmtNx4xRGAfwNa/weekly-lw-meetups-42", "linkUrl": "https://www.lesswrong.com/posts/3PscmtNx4xRGAfwNa/weekly-lw-meetups-42", "postedAtFormatted": "Friday, July 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PscmtNx4xRGAfwNa%2Fweekly-lw-meetups-42%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PscmtNx4xRGAfwNa%2Fweekly-lw-meetups-42", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PscmtNx4xRGAfwNa%2Fweekly-lw-meetups-42", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 499, "htmlBody": "<p><strong>This summary was posted to LW Main on July 5th. The following week's summary is <a href=\"/lw/hzb/new_lw_meetup_zagreb/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/ob\">Atlanta: Intro to Meditation:&nbsp;<span class=\"date\">13 July 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/ns\">Brussels meetup with HEALES:&nbsp;<span class=\"date\">13 July 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/o3\">[Moscow] The Goals We Set:&nbsp;<span class=\"date\">07 July 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/o4\">[Munich] LW Munich Meetup in July:&nbsp;<span class=\"date\">06 July 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/o8\">[Vienna] LW Vienna Meetup #4:&nbsp;<span class=\"date\">13 July 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/oc\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">07 July 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">06 July 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/o6\">London Practical - Sunday 7th July:&nbsp;<span class=\"date\">07 July 2013 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong style=\"font-weight: bold;\"></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3PscmtNx4xRGAfwNa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.2636196135737276e-06, "legacy": true, "legacyId": "23205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CpM8mnuFSbxT584Wf", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-13T02:42:56.635Z", "modifiedAt": null, "url": null, "title": "\"Stupid\" questions thread", "slug": "stupid-questions-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:51.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gothgirl420666", "createdAt": "2013-01-06T19:35:18.030Z", "isAdmin": false, "displayName": "gothgirl420666"}, "userId": "P7J37T964Pxizzp9g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P42E5sSbCkDgxEqbn/stupid-questions-thread", "pageUrlRelative": "/posts/P42E5sSbCkDgxEqbn/stupid-questions-thread", "linkUrl": "https://www.lesswrong.com/posts/P42E5sSbCkDgxEqbn/stupid-questions-thread", "postedAtFormatted": "Saturday, July 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Stupid%22%20questions%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Stupid%22%20questions%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP42E5sSbCkDgxEqbn%2Fstupid-questions-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Stupid%22%20questions%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP42E5sSbCkDgxEqbn%2Fstupid-questions-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP42E5sSbCkDgxEqbn%2Fstupid-questions-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p>r/Fitness does a weekly <a href=\"http://www.reddit.com/r/Fitness/comments/xe4na/moronic_monday_your_weekly_stupid_questions_thread/\">\"Moronic Monday\"</a>, a judgment-free thread where people can ask questions that they would ordinarily feel embarrassed for not knowing the answer to. I thought this seemed like a useful thing to have here - after all, the concepts discussed on LessWrong are probably at least a little harder to grasp than those of weightlifting. Plus, I have a few stupid questions of my own, so it doesn't seem unreasonable that other people might as well.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P42E5sSbCkDgxEqbn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 62, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "23305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 854, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-13T11:09:16.790Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st ", "slug": "meetup-london-social-direct-sunlight-2-vitamin-boogaloo-july", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:02.674Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Leonhart", "createdAt": "2010-02-22T20:01:49.792Z", "isAdmin": false, "displayName": "Leonhart"}, "userId": "X5EZEkfccqyWXETHd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nBw485gkGcG6FraDQ/meetup-london-social-direct-sunlight-2-vitamin-boogaloo-july", "pageUrlRelative": "/posts/nBw485gkGcG6FraDQ/meetup-london-social-direct-sunlight-2-vitamin-boogaloo-july", "linkUrl": "https://www.lesswrong.com/posts/nBw485gkGcG6FraDQ/meetup-london-social-direct-sunlight-2-vitamin-boogaloo-july", "postedAtFormatted": "Saturday, July 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20-%20Direct%20Sunlight%202%2C%20Vitamin%20Boogaloo%20-%20July%2021st%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20-%20Direct%20Sunlight%202%2C%20Vitamin%20Boogaloo%20-%20July%2021st%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBw485gkGcG6FraDQ%2Fmeetup-london-social-direct-sunlight-2-vitamin-boogaloo-july%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20-%20Direct%20Sunlight%202%2C%20Vitamin%20Boogaloo%20-%20July%2021st%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBw485gkGcG6FraDQ%2Fmeetup-london-social-direct-sunlight-2-vitamin-boogaloo-july", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBw485gkGcG6FraDQ%2Fmeetup-london-social-direct-sunlight-2-vitamin-boogaloo-july", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ol'>London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 July 2013 11:54:28AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">St James Park, London, SW1H</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next London Social meetup will be taking place in St. James Park on Sunday July 21st from 2pm. Please join us all. Bring friends if you have made adequate backups.</p>\n\n<p>For further details - including where to meet if the weather doesn't get my memo - please see the last social meetup entry <a href=\"http://lesswrong.com/lw/hon/meetup_london_social_exposure_to_direct_sunlight/\">here</a>. I can be contacted on 07860 466862 that morning in case of confusion.</p>\n\n<p>There is no agenda or topic; although the whereabouts of Hermione's brain will probably be discussed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ol'>London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nBw485gkGcG6FraDQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.2645515712673438e-06, "legacy": true, "legacyId": "23319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social___Direct_Sunlight_2__Vitamin_Boogaloo___July_21st_\">Discussion article for the meetup : <a href=\"/meetups/ol\">London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 July 2013 11:54:28AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">St James Park, London, SW1H</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next London Social meetup will be taking place in St. James Park on Sunday July 21st from 2pm. Please join us all. Bring friends if you have made adequate backups.</p>\n\n<p>For further details - including where to meet if the weather doesn't get my memo - please see the last social meetup entry <a href=\"http://lesswrong.com/lw/hon/meetup_london_social_exposure_to_direct_sunlight/\">here</a>. I can be contacted on 07860 466862 that morning in case of confusion.</p>\n\n<p>There is no agenda or topic; although the whereabouts of Hermione's brain will probably be discussed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social___Direct_Sunlight_2__Vitamin_Boogaloo___July_21st_1\">Discussion article for the meetup : <a href=\"/meetups/ol\">London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st </a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st ", "anchor": "Discussion_article_for_the_meetup___London_Social___Direct_Sunlight_2__Vitamin_Boogaloo___July_21st_", "level": 1}, {"title": "Discussion article for the meetup : London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st ", "anchor": "Discussion_article_for_the_meetup___London_Social___Direct_Sunlight_2__Vitamin_Boogaloo___July_21st_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k6SATimbiHSdQbyXk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-13T15:21:14.903Z", "modifiedAt": null, "url": null, "title": "Some highlights from Nate Silver's \"The Signal and the Noise\"", "slug": "some-highlights-from-nate-silver-s-the-signal-and-the-noise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:01.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rGj2K8vu5qQCTWCar/some-highlights-from-nate-silver-s-the-signal-and-the-noise", "pageUrlRelative": "/posts/rGj2K8vu5qQCTWCar/some-highlights-from-nate-silver-s-the-signal-and-the-noise", "linkUrl": "https://www.lesswrong.com/posts/rGj2K8vu5qQCTWCar/some-highlights-from-nate-silver-s-the-signal-and-the-noise", "postedAtFormatted": "Saturday, July 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20highlights%20from%20Nate%20Silver's%20%22The%20Signal%20and%20the%20Noise%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20highlights%20from%20Nate%20Silver's%20%22The%20Signal%20and%20the%20Noise%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGj2K8vu5qQCTWCar%2Fsome-highlights-from-nate-silver-s-the-signal-and-the-noise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20highlights%20from%20Nate%20Silver's%20%22The%20Signal%20and%20the%20Noise%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGj2K8vu5qQCTWCar%2Fsome-highlights-from-nate-silver-s-the-signal-and-the-noise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGj2K8vu5qQCTWCar%2Fsome-highlights-from-nate-silver-s-the-signal-and-the-noise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1882, "htmlBody": "<p>As a part of my work for MIRI on the <a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a> project, I read Nate Silver's book&nbsp;<a href=\"http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X\">The Signal and the Noise: Why So Many Predictions Fail &mdash; but Some Don't</a>. I compiled a list of the takeaway points that I found most relevant to the project. I think that they might be of independent interest to the Less Wrong community, and so am posting them here.</p>\n<p>Because I've paraphrased Silver rather than quoting him, and because the summary is long, there may be places where I've&nbsp;inadvertently&nbsp;misrepresented Silver. A reader who's especially interested in a point should check the original text.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2>Main Points</h2>\n<ul>\n<li>The deluge of data available in the modern world has exacerbated the problem of people perceiving patterns where none exist, and overfitting predictive models to past data.</li>\n</ul>\n<ul>\n<li>Because of the risk of overfitting a model to past data, using a simple model can give more accurate results than using a refined model does.</li>\n</ul>\n<ul>\n<li>A major reason that predictions fail is that predictors often don't take model uncertainty into account. Looking at a situation from multiple different angles can be a guard against failure to give adequate weight to model uncertainty.&nbsp;</li>\n</ul>\n<ul>\n<li>Average different perspectives often yields better predictive results than using a single perspective.</li>\n</ul>\n<ul>\n<li>Humans have a very strong tendency toward being overconfident when making predictions.</li>\n</ul>\n<ul>\n<li>People make better predictions in domains where they have tight feedback loops to use to test their hypotheses.</li>\n</ul>\n<ul>\n<li>Sometimes people's failure to make good predictions is the result of perverse incentives.&nbsp;</li>\n</ul>\n<h2><strong style=\"font-family: arial;\">Chapter Summaries</strong></h2>\n<p><strong></strong></p>\n<p><strong>Introduction</strong><strong><br /></strong></p>\n<p>Increased access to information can do more harm than good. This is because the more information is available, the easier it is for people to cherry-pick information that supports their pre-existing positions, or to perceive patterns where there are none.<br />The invention of the printing press may have given rise to religious wars on account of facilitating the development of ideological agendas.</p>\n<p><strong>Chapter 1:&nbsp;</strong>The failure to predict the 2008 housing bubble and recession</p>\n<ul>\n<li>There was an issue of people failing to take into account&nbsp;model uncertainty. In particular,&nbsp;people shouldn't have taken the forecasted 0.12% default rate of mortgage securities at face value. This rate corresponded to the rating agencies giving mortgage securities AAA ratings, which are usually reserved only the world's most solvent governments and best-run businesses.</li>\n<li>Some of the actors involved failed to look at the situation from many different angles. For example, the fact that the increase in housing prices wasn't driven by a change in fundamentals seems to have been overlooked by some people.</li>\n<li>Each individual factor that contributed to the housing bubble, and to the recession, seems like a common occurrence (e.g. perverse incentives, inadequate regulation, ignoring of tail risk, and irrational behavior coming from consumers). The severity of the situation seems to have come from the factors all being present simultaneously (by chance). Any individual factor would ordinarily be offset by other safeguards built into our social institutions.</li>\n</ul>\n<p><strong>Chapter 2:</strong>&nbsp;Political Predictions</p>\n<ul>\n<li>Political pundits and political experts usually don't do much better than chance when forecasting political events, and usually do worse than crude statistical models.</li>\n<li>Averaging individual experts' forecasts gives better forecasts than the forecasts of the average individual, with the effect size being about 15-20%. </li>\n<li>There are some experts who do make predictions that are substantially more accurate than chance.</li>\n<li>The experts who do better tend to be multidisciplinary, pursue multiple approaches to forecasting at the same time, be willing to change their minds, offer&nbsp;<em>probabilistic</em>&nbsp;predictions, and rely more on observation than on theory.</li>\n<li>Making definitive predictions that fall into a pre-existing narrative is associated with political partisanship. It's negatively correlated with making accurate predictions, but positively correlated with getting media attention. So the most visible people may make systematically worse predictions than less visible people.</li>\n<li>The failure to predict the fall of the Soviet Union seems to have arisen from a failure to integrate multiple perspectives. There were some people who were aware of Gorbachev's progressiveness and other people who recognized the dysfunctionality of the Soviet Union's economy, but these groups were largely nonoverlapping.</li>\n<li>Nate Silver integrates poll data, historical track record of poll data, information about the economy and information about the demographics of states, in order to make predictions about political elections.</li>\n<li>There's an organization called the Cook Political Report that has a very impressive track record of making accurate predictions about how political elections will go.</li>\n</ul>\n<p><strong>Chapter 3:&nbsp;</strong>Baseball predictions</p>\n<ul>\n<li>Baseball statistics constitute a very rich collection of data, and people who aspire to predict how well players will play in the future &nbsp;have rapid feedback loops that allow them to repeatedly test the validity of their hypotheses.</li>\n<li>A simple model of how the performance of a baseball player varies with age outperformed a much more complicated model that attempted to form a more nuanced picture of how performance varies with age by dividing players into different classes. This may have been because the latter model was&nbsp;over-fitted&nbsp;to the existing data.</li>\n</ul>\n<p><strong>Chapter 4:&nbsp;</strong>Weather Predictions</p>\n<ul>\n<li>Weather forecasters have access to a large amount of data, which offers them rapid feedback loops that allow them to repeatedly test their hypotheses.</li>\n<li>The method of predicting what would happen under certain initial conditions for many different examples of initial conditions and then averaging over the results is&nbsp;tantalizing. It suggests the possibility of reducing uncertainty in situations that seem hopelessly complicated to analyze, by averaging over the predictions made under different assumptions.</li>\n<li>It's impressive that the weather experts are well calibrated.</li>\n<li>Local news networks sacrifice accuracy and honesty to optimize for viewer satisfaction.</li>\n<li>The integrated use of computer models and human judgment calls does notably better than computer models alone.</li>\n<li>The human input is getting better over time.</li>\n<li>Hurricane Katrina wasn't appropriately addressed because the local government didn't listen to the weather forecasters early enough, and the local people didn't take the hurricane warning sufficiently seriously.</li>\n</ul>\n<p><strong>Chapter 5:</strong>&nbsp;Earthquake predictions:</p>\n<ul>\n<li>The Gutenberg-Richter law predicts the frequency of earthquakes of a given magnitude in a given location. One can use the frequency of earthquakes of a given magnitude to predict the frequency of earthquakes of a higher magnitude (even without having many data points).</li>\n<li>Efforts to build models that offer more precise predictions than the Gutenberg-Richter law does have been unsuccessful, apparently owing to overfitting existing data, and have generally done worse than the Gutenberg-Richter law.</li>\n</ul>\n<p><strong>Chapter 6:&nbsp;</strong></p>\n<ul>\n<li>Communicating a prediction of the median case without giving a confidence interval can be very pernicious, because outcomes can be highly sensitive to error.</li>\n<li>Economists have a poor track record of predicting GDP growth. There's so much data pertaining to factors that might drive GDP growth that it's easy to perceive patterns that aren't real.</li>\n<li>The economy is always changing, and often past patterns don't predict the future patterns.</li>\n<li>Prediction markets for GDP growth might yield better predictions than economists' forecasts do. But existing prediction markets aren't very good.</li>\n</ul>\n<p><strong>Chapter 7:&nbsp;</strong>Disease Outbreaks</p>\n<ul>\n<li>Predictions can be self-fulfilling&nbsp;(e.g. in election primaries races) or self-canceling (e.g. when disease outbreaks are predicted, measures can be taken to prevent them, which can nullify the prediction).</li>\n</ul>\n<p><strong>Chapter 8:&nbsp;</strong>Bayes' Theorem</p>\n<ul>\n<li>When gauging the strength of a prediction, it's important to view the inside view in the context of the outside view. For example, most medical studies that claim 95% confidence aren't replicable, so one shouldn't take the 95% confidence figures at face value.</li>\n</ul>\n<p><strong>Chapter 9:&nbsp;</strong>Chess computers</p>\n<ul>\n<li>Our use of prediction heuristics makes us vulnerable to opponents who are aware of the heuristics that we're using and who can therefore act in unexpected ways that we're not prepared for.</li>\n</ul>\n<p><strong>Chapter 10:&nbsp;</strong>Poker</p>\n<ul>\n<li>Elite poker players use Bayesian reasoning to estimate the probability of a hand based on the cards on the table, contingent on opponents' behavior.</li>\n<li>Elite poker players also additional information, such as the fact that women tend to play more conservatively than men do, in order to refine their predictions about what cards the opponent has</li>\n<li>Often the 80%/20% rule applies to getting good at predictions relative to what's in principle possible. A relatively small amount of effort can result in large improvements. In competitive contexts such as poker, serious players have all already put this amount of effort in, so beating them requires further effort. But in arenas such as election results predictions, where not many people are trying hard, it's possible to do a lot better than most people do with relatively little effort.</li>\n</ul>\n<p><strong>Chapter 11:&nbsp;</strong>The stock market</p>\n<ul>\n<li>It's difficult to distinguish signal from noise when attempting to make predictions about the stock market.</li>\n<li>There are some systematic patterns in the stock market. For example, between 1965 and 1975, rises in stock prices one day were correlated with rises in stock prices the next day. But such patterns are rapidly exploited once people recognize them, and disappear.</li>\n<li>It's not so hard to predict a stock market bubble. One can look at the average price to earnings ratio across all stocks, and when it's sufficiently high, that's a signal that there's a bubble.</li>\n<li>It's hard to predict when a bubble is about to pop.</li>\n<li>Most investors are relatively shortsighted. This is especially the case because most investors are investing other people's money rather than their own.</li>\n<li>There are incentives not to short-sell stocks too much, both cultural and legal. This may give rise to a market inefficiency.</li>\n<li>An 1970 investment of $10k in S&amp;P 500 would have yielded $63k in profit in 2009, but if one adopted the strategy of pulling money out when the market dropped by 25% and putting it back in when it had recovered to 90% of its earlier price, the profit would only be $18k. Many investors behave in the latter fashion.</li>\n</ul>\n<p><strong>Chapter 12:&nbsp;</strong>Climate change</p>\n<ul>\n<li>There's a lot of uncertainty around climate change predictions: there's uncertainty about the climate models, uncertainty about the initial conditions, and uncertainty about society's ability to adapt.</li>\n<li>There may be global&nbsp;<em>cooling</em>&nbsp;coming from sulfer emissions</li>\n<li>The amount of uncertainty can easily justify focus on mitigating climate change, because the risk of the problem being worse than expected entails more potential negative consequences than the consequences in the median case.</li>\n<li>A simple regression analysis looking at the correlation between CO2 levels and temperature may give a better predictive model than more sophisticated climate models.</li>\n</ul>\n<p><strong>Chapter 13:&nbsp;</strong>Terrorism</p>\n<ul>\n<li>Governments often prepare for terrorist attacks, but often prepare for the wrong kinds of terrorist attacks, unaware of bigger threats.</li>\n<li>The September 11th scenario hadn't been considered and rejected, but rather, hadn't been considered at all.</li>\n<li>If one looks at number of terrorist attacks as a function of their magnitude, they seem to obey a power law.</li>\n<li>There are some reasons to be concerned about the possibility of a nuclear weapon terrorist attack, or bioterrorism, in the United States. Such an attack could kill over a million people</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2, "8daMDi9NEShyLqxth": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rGj2K8vu5qQCTWCar", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 35, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "23253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As a part of my work for MIRI on the <a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a> project, I read Nate Silver's book&nbsp;<a href=\"http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X\">The Signal and the Noise: Why So Many Predictions Fail \u2014 but Some Don't</a>. I compiled a list of the takeaway points that I found most relevant to the project. I think that they might be of independent interest to the Less Wrong community, and so am posting them here.</p>\n<p>Because I've paraphrased Silver rather than quoting him, and because the summary is long, there may be places where I've&nbsp;inadvertently&nbsp;misrepresented Silver. A reader who's especially interested in a point should check the original text.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Main_Points\">Main Points</h2>\n<ul>\n<li>The deluge of data available in the modern world has exacerbated the problem of people perceiving patterns where none exist, and overfitting predictive models to past data.</li>\n</ul>\n<ul>\n<li>Because of the risk of overfitting a model to past data, using a simple model can give more accurate results than using a refined model does.</li>\n</ul>\n<ul>\n<li>A major reason that predictions fail is that predictors often don't take model uncertainty into account. Looking at a situation from multiple different angles can be a guard against failure to give adequate weight to model uncertainty.&nbsp;</li>\n</ul>\n<ul>\n<li>Average different perspectives often yields better predictive results than using a single perspective.</li>\n</ul>\n<ul>\n<li>Humans have a very strong tendency toward being overconfident when making predictions.</li>\n</ul>\n<ul>\n<li>People make better predictions in domains where they have tight feedback loops to use to test their hypotheses.</li>\n</ul>\n<ul>\n<li>Sometimes people's failure to make good predictions is the result of perverse incentives.&nbsp;</li>\n</ul>\n<h2 id=\"Chapter_Summaries\"><strong style=\"font-family: arial;\">Chapter Summaries</strong></h2>\n<p><strong></strong></p>\n<p><strong>Introduction</strong><strong><br></strong></p>\n<p>Increased access to information can do more harm than good. This is because the more information is available, the easier it is for people to cherry-pick information that supports their pre-existing positions, or to perceive patterns where there are none.<br>The invention of the printing press may have given rise to religious wars on account of facilitating the development of ideological agendas.</p>\n<p><strong>Chapter 1:&nbsp;</strong>The failure to predict the 2008 housing bubble and recession</p>\n<ul>\n<li>There was an issue of people failing to take into account&nbsp;model uncertainty. In particular,&nbsp;people shouldn't have taken the forecasted 0.12% default rate of mortgage securities at face value. This rate corresponded to the rating agencies giving mortgage securities AAA ratings, which are usually reserved only the world's most solvent governments and best-run businesses.</li>\n<li>Some of the actors involved failed to look at the situation from many different angles. For example, the fact that the increase in housing prices wasn't driven by a change in fundamentals seems to have been overlooked by some people.</li>\n<li>Each individual factor that contributed to the housing bubble, and to the recession, seems like a common occurrence (e.g. perverse incentives, inadequate regulation, ignoring of tail risk, and irrational behavior coming from consumers). The severity of the situation seems to have come from the factors all being present simultaneously (by chance). Any individual factor would ordinarily be offset by other safeguards built into our social institutions.</li>\n</ul>\n<p><strong>Chapter 2:</strong>&nbsp;Political Predictions</p>\n<ul>\n<li>Political pundits and political experts usually don't do much better than chance when forecasting political events, and usually do worse than crude statistical models.</li>\n<li>Averaging individual experts' forecasts gives better forecasts than the forecasts of the average individual, with the effect size being about 15-20%. </li>\n<li>There are some experts who do make predictions that are substantially more accurate than chance.</li>\n<li>The experts who do better tend to be multidisciplinary, pursue multiple approaches to forecasting at the same time, be willing to change their minds, offer&nbsp;<em>probabilistic</em>&nbsp;predictions, and rely more on observation than on theory.</li>\n<li>Making definitive predictions that fall into a pre-existing narrative is associated with political partisanship. It's negatively correlated with making accurate predictions, but positively correlated with getting media attention. So the most visible people may make systematically worse predictions than less visible people.</li>\n<li>The failure to predict the fall of the Soviet Union seems to have arisen from a failure to integrate multiple perspectives. There were some people who were aware of Gorbachev's progressiveness and other people who recognized the dysfunctionality of the Soviet Union's economy, but these groups were largely nonoverlapping.</li>\n<li>Nate Silver integrates poll data, historical track record of poll data, information about the economy and information about the demographics of states, in order to make predictions about political elections.</li>\n<li>There's an organization called the Cook Political Report that has a very impressive track record of making accurate predictions about how political elections will go.</li>\n</ul>\n<p><strong>Chapter 3:&nbsp;</strong>Baseball predictions</p>\n<ul>\n<li>Baseball statistics constitute a very rich collection of data, and people who aspire to predict how well players will play in the future &nbsp;have rapid feedback loops that allow them to repeatedly test the validity of their hypotheses.</li>\n<li>A simple model of how the performance of a baseball player varies with age outperformed a much more complicated model that attempted to form a more nuanced picture of how performance varies with age by dividing players into different classes. This may have been because the latter model was&nbsp;over-fitted&nbsp;to the existing data.</li>\n</ul>\n<p><strong>Chapter 4:&nbsp;</strong>Weather Predictions</p>\n<ul>\n<li>Weather forecasters have access to a large amount of data, which offers them rapid feedback loops that allow them to repeatedly test their hypotheses.</li>\n<li>The method of predicting what would happen under certain initial conditions for many different examples of initial conditions and then averaging over the results is&nbsp;tantalizing. It suggests the possibility of reducing uncertainty in situations that seem hopelessly complicated to analyze, by averaging over the predictions made under different assumptions.</li>\n<li>It's impressive that the weather experts are well calibrated.</li>\n<li>Local news networks sacrifice accuracy and honesty to optimize for viewer satisfaction.</li>\n<li>The integrated use of computer models and human judgment calls does notably better than computer models alone.</li>\n<li>The human input is getting better over time.</li>\n<li>Hurricane Katrina wasn't appropriately addressed because the local government didn't listen to the weather forecasters early enough, and the local people didn't take the hurricane warning sufficiently seriously.</li>\n</ul>\n<p><strong>Chapter 5:</strong>&nbsp;Earthquake predictions:</p>\n<ul>\n<li>The Gutenberg-Richter law predicts the frequency of earthquakes of a given magnitude in a given location. One can use the frequency of earthquakes of a given magnitude to predict the frequency of earthquakes of a higher magnitude (even without having many data points).</li>\n<li>Efforts to build models that offer more precise predictions than the Gutenberg-Richter law does have been unsuccessful, apparently owing to overfitting existing data, and have generally done worse than the Gutenberg-Richter law.</li>\n</ul>\n<p><strong id=\"Chapter_6__\">Chapter 6:&nbsp;</strong></p>\n<ul>\n<li>Communicating a prediction of the median case without giving a confidence interval can be very pernicious, because outcomes can be highly sensitive to error.</li>\n<li>Economists have a poor track record of predicting GDP growth. There's so much data pertaining to factors that might drive GDP growth that it's easy to perceive patterns that aren't real.</li>\n<li>The economy is always changing, and often past patterns don't predict the future patterns.</li>\n<li>Prediction markets for GDP growth might yield better predictions than economists' forecasts do. But existing prediction markets aren't very good.</li>\n</ul>\n<p><strong>Chapter 7:&nbsp;</strong>Disease Outbreaks</p>\n<ul>\n<li>Predictions can be self-fulfilling&nbsp;(e.g. in election primaries races) or self-canceling (e.g. when disease outbreaks are predicted, measures can be taken to prevent them, which can nullify the prediction).</li>\n</ul>\n<p><strong>Chapter 8:&nbsp;</strong>Bayes' Theorem</p>\n<ul>\n<li>When gauging the strength of a prediction, it's important to view the inside view in the context of the outside view. For example, most medical studies that claim 95% confidence aren't replicable, so one shouldn't take the 95% confidence figures at face value.</li>\n</ul>\n<p><strong>Chapter 9:&nbsp;</strong>Chess computers</p>\n<ul>\n<li>Our use of prediction heuristics makes us vulnerable to opponents who are aware of the heuristics that we're using and who can therefore act in unexpected ways that we're not prepared for.</li>\n</ul>\n<p><strong>Chapter 10:&nbsp;</strong>Poker</p>\n<ul>\n<li>Elite poker players use Bayesian reasoning to estimate the probability of a hand based on the cards on the table, contingent on opponents' behavior.</li>\n<li>Elite poker players also additional information, such as the fact that women tend to play more conservatively than men do, in order to refine their predictions about what cards the opponent has</li>\n<li>Often the 80%/20% rule applies to getting good at predictions relative to what's in principle possible. A relatively small amount of effort can result in large improvements. In competitive contexts such as poker, serious players have all already put this amount of effort in, so beating them requires further effort. But in arenas such as election results predictions, where not many people are trying hard, it's possible to do a lot better than most people do with relatively little effort.</li>\n</ul>\n<p><strong>Chapter 11:&nbsp;</strong>The stock market</p>\n<ul>\n<li>It's difficult to distinguish signal from noise when attempting to make predictions about the stock market.</li>\n<li>There are some systematic patterns in the stock market. For example, between 1965 and 1975, rises in stock prices one day were correlated with rises in stock prices the next day. But such patterns are rapidly exploited once people recognize them, and disappear.</li>\n<li>It's not so hard to predict a stock market bubble. One can look at the average price to earnings ratio across all stocks, and when it's sufficiently high, that's a signal that there's a bubble.</li>\n<li>It's hard to predict when a bubble is about to pop.</li>\n<li>Most investors are relatively shortsighted. This is especially the case because most investors are investing other people's money rather than their own.</li>\n<li>There are incentives not to short-sell stocks too much, both cultural and legal. This may give rise to a market inefficiency.</li>\n<li>An 1970 investment of $10k in S&amp;P 500 would have yielded $63k in profit in 2009, but if one adopted the strategy of pulling money out when the market dropped by 25% and putting it back in when it had recovered to 90% of its earlier price, the profit would only be $18k. Many investors behave in the latter fashion.</li>\n</ul>\n<p><strong>Chapter 12:&nbsp;</strong>Climate change</p>\n<ul>\n<li>There's a lot of uncertainty around climate change predictions: there's uncertainty about the climate models, uncertainty about the initial conditions, and uncertainty about society's ability to adapt.</li>\n<li>There may be global&nbsp;<em>cooling</em>&nbsp;coming from sulfer emissions</li>\n<li>The amount of uncertainty can easily justify focus on mitigating climate change, because the risk of the problem being worse than expected entails more potential negative consequences than the consequences in the median case.</li>\n<li>A simple regression analysis looking at the correlation between CO2 levels and temperature may give a better predictive model than more sophisticated climate models.</li>\n</ul>\n<p><strong>Chapter 13:&nbsp;</strong>Terrorism</p>\n<ul>\n<li>Governments often prepare for terrorist attacks, but often prepare for the wrong kinds of terrorist attacks, unaware of bigger threats.</li>\n<li>The September 11th scenario hadn't been considered and rejected, but rather, hadn't been considered at all.</li>\n<li>If one looks at number of terrorist attacks as a function of their magnitude, they seem to obey a power law.</li>\n<li>There are some reasons to be concerned about the possibility of a nuclear weapon terrorist attack, or bioterrorism, in the United States. Such an attack could kill over a million people</li>\n</ul>", "sections": [{"title": "Main Points", "anchor": "Main_Points", "level": 1}, {"title": "Chapter Summaries", "anchor": "Chapter_Summaries", "level": 1}, {"title": "Chapter 6:\u00a0", "anchor": "Chapter_6__", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t7gKX9Av5zggsQsYr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-13T16:41:05.623Z", "modifiedAt": null, "url": null, "title": "To reduce astronomical waste: take your time, then go very fast", "slug": "to-reduce-astronomical-waste-take-your-time-then-go-very", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:06.918Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S4PGuNHgjNpnnnQvT/to-reduce-astronomical-waste-take-your-time-then-go-very", "pageUrlRelative": "/posts/S4PGuNHgjNpnnnQvT/to-reduce-astronomical-waste-take-your-time-then-go-very", "linkUrl": "https://www.lesswrong.com/posts/S4PGuNHgjNpnnnQvT/to-reduce-astronomical-waste-take-your-time-then-go-very", "postedAtFormatted": "Saturday, July 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20reduce%20astronomical%20waste%3A%20take%20your%20time%2C%20then%20go%20very%20fast&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20reduce%20astronomical%20waste%3A%20take%20your%20time%2C%20then%20go%20very%20fast%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4PGuNHgjNpnnnQvT%2Fto-reduce-astronomical-waste-take-your-time-then-go-very%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20reduce%20astronomical%20waste%3A%20take%20your%20time%2C%20then%20go%20very%20fast%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4PGuNHgjNpnnnQvT%2Fto-reduce-astronomical-waste-take-your-time-then-go-very", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4PGuNHgjNpnnnQvT%2Fto-reduce-astronomical-waste-take-your-time-then-go-very", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1003, "htmlBody": "<p>While we dither on the planet, are we losing resources in space? Nick Bostrom has an&nbsp;<a href=\"http://www.nickbostrom.com/astronomical/waste.html\">article</a> on astronomical waste, talking about the vast amounts of potentially useful energy that we're simply not using for anything:</p>\n<blockquote>\n<p>As I write these words, suns are illuminating and heating empty rooms, unused energy is being flushed down black holes, and our great common endowment of negentropy is being irreversibly degraded into entropy on a cosmic scale. These are resources that an advanced civilization could have used to create value-structures, such as sentient beings living worthwhile lives.</p>\n<p>The rate of this loss boggles the mind. One recent paper speculates, using loose theoretical considerations based on the rate of increase of entropy, that the loss of potential human lives in our own galactic supercluster is at least ~10<sup>46</sup> per century of delayed colonization.</p>\n</blockquote>\n<p>On top of that, galaxies are slipping away from us because of the exponentially&nbsp;<a href=\"https://en.wikipedia.org/wiki/Cosmological_constant\">accelerating expansion of the universe</a>&nbsp;(x axis in years since Big Bang, cosmic scale function arbitrarily set to 1 at the current day):</p>\n<p><img src=\"http://images.lesswrong.com/t3_hll_1.png?v=130f58d727ce66ccde1532538bff7d92\" alt=\"\" width=\"398\" height=\"254\" /></p>\n<p>At the rate things are going, we seem to be losing slightly more than one galaxy a year. One entire galaxy, with its hundreds of billions of stars, is slipping away from us each year, never to be interacted with again. This is many solar systems a second; poof! Before you've even had time to grasp that concept, we've lost millions of times more resources than humanity has even used.</p>\n<p>So it would seem that the answer to this desperate state of affairs is to rush thing: start expanding as soon as possible, greedily grab every hint of energy and negentropy before they vanish forever.</p>\n<p>Not so fast! Nick Bostrom's point was not that we should rush things, but that we should be very very careful:</p>\n<blockquote>\n<p>However, the lesson for utilitarians is not that we ought to maximize the pace of technological development, but rather that we ought to maximize its safety, i.e. the probability that colonization will eventually occur.<a id=\"more\"></a></p>\n</blockquote>\n<p>If we rush things and lose the whole universe, then we certainly don't come out ahead in this game.</p>\n<p>But let's ignore that; let's pretend that we've solved all risks, that we can expand safely, without fear of messing things up. Right. Full steam ahead to the starships, no?</p>\n<p>No. Firstly, though the losses are large in absolute terms, they are small in relative terms. Most of the energy of a star is contained in its mass. The light streaming through windows in empty rooms? A few specks of negentropy, that barely diminish the value of the huge hoard that is the stars' physical beings (which can be harvested by eg dropping them slowly into small black holes and feeding off the <a href=\"http://en.wikipedia.org/wiki/Hawking_radiation\">Hawking radiation</a>). And we lose a galaxy a year - but there are still billions out there. So waiting a while isn't a major problem, if we can gain something by doing so. Gain what? Well, maybe just going a tiny bit faster.</p>\n<p>In a <a href=\"http://www.sciencedirect.com/science/article/pii/S0094576513001148\">paper</a>&nbsp;published with Anders Sandberg, we looked at the ease and difficulty of intergalactic or universal expansion. It seems to be surprisingly easy (which has a lot of implications for the <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">Fermi Paradox</a>), given sufficient automation or AI. About six hours of the sun's energy would be enough to launch <a href=\"http://en.wikipedia.org/wiki/Von_Neumann_Probe\">self-replicating probes</a> to every reachable galaxy in the entire universe. We could get this energy by constructing a Dyson swarm around the sun, by, for instance, disassembling Mercury. This is the kind of task that would be well within the capacities of an decently automated manufacturing process. A video overview of the process can be found in this <a href=\"http://www.youtube.com/watch?v=mrUWkfeJABY\">talk</a>&nbsp;(and a longer exposition, with slightly older figures, can be found <a href=\"http://www.youtube.com/watch?v=zQTfuI-9jIo\">here</a>).</p>\n<p>How fast will those probes travel? This depends not on the acceleration phase (which can be done fine with quench guns or rail guns, or lasers into solar sales), but on the deceleration. The <a href=\"http://en.wikipedia.org/wiki/Relativistic_rocket#Relativistic_rocket_equation\">relativistic rocket equation</a> is vicious: it takes a lot of reaction mass to decelerate even a small payload. If fission power is used, decelerations from 50%c is about all that's reasonable. With fusion, we can push this to 80%c, while with matter-anti-matter reactions, we can get to 99%c. The top speed of 99%c is also obtainable if have more exotic ways of decelerating. This could be somehow using resources from the target galaxy (cunning <a href=\"http://en.wikipedia.org/wiki/Gravity_assist\">gravitational braking</a> or <a href=\"http://en.wikipedia.org/wiki/Bussard_ramjet\">Bussard ramjets</a> or something), or using the continuing expansion of the universe to bleed speed away (this is most practical for the most distant galaxies).</p>\n<p>At these three speeds (and at 100% c), we can reach a certain distance into the universe, in current comoving coordinates, as shown by this graph (the x axis is in years since the Big Bang, with the origin set at the current day):</p>\n<p><img src=\"http://images.lesswrong.com/t3_hll_0.png?v=8d5aa16c50580165426bd5357caa90fa\" alt=\"\" width=\"432\" height=\"365\" /></p>\n<p>The maximum reached at 99%c is about 4 GigaParsecs - not a unit often used is casual conversation! If we can reach these distances, we can claim this many galaxies, approximately:</p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">Speed</th><th style=\"cursor: text;\">Distance (Parsecs)<br /></th><th style=\"cursor: text;\"># of Galaxies<br /></th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">50%c<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">&nbsp;1.24\u0001x10<sup>9</sup></td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1.16\u0001x10<sup>8</sup></td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">80%c<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">2.33x\u000110<sup>9</sup></td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">7.62\u0001x10<sup>8</sup></td>\n</tr>\n<tr>\n<th>99%c</th>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\">4.09x\u000110<sup>9</sup></td>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\">4.13x\u000110<sup>9</sup></td>\n</tr>\n</tbody>\n</table>\n<p>These numbers don't change much if we delay. Even wasting a million years won't show up on these figure: it's a rounding error. Why is this?</p>\n<p>Well, a typical probe will be flying through space, at quasi-constant velocity, for several billion years. Gains in speed make an immense difference, as they compound over the whole duration of the trip; gains from an early launch, not so much. So if we have to wait a million years to squeeze an extra 0.1% of speed, we're still coming out ahead. So waiting for extra research is always sensible (apart from the closest galaxies). If we can get more efficient engines, more exotic ways of shielding the probe, or new methods for deceleration, the benefits will be immense.</p>\n<p>So, in conclusion: To efficiently colonise the universe, take your time. Do research. Think things over. Go to the pub. Saunter like an Egyptian. Write long letters to mum. Complain about the immorality of the youth of today. Watch dry paint stay dry.</p>\n<p>But when you do go... go very, very fast.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "csMv9MvvjYJyeHqoo": 1, "2JdCpTrNgBMNpJiyB": 2, "5f5c37ee1b5cdee568cfb2a8": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S4PGuNHgjNpnnnQvT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 69, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "22809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-14T04:47:48.167Z", "modifiedAt": null, "url": null, "title": "Writing Style and the Typical Mind Fallacy", "slug": "writing-style-and-the-typical-mind-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:32.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Q3MoE9YzFMxGZR7F/writing-style-and-the-typical-mind-fallacy", "pageUrlRelative": "/posts/7Q3MoE9YzFMxGZR7F/writing-style-and-the-typical-mind-fallacy", "linkUrl": "https://www.lesswrong.com/posts/7Q3MoE9YzFMxGZR7F/writing-style-and-the-typical-mind-fallacy", "postedAtFormatted": "Sunday, July 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Writing%20Style%20and%20the%20Typical%20Mind%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWriting%20Style%20and%20the%20Typical%20Mind%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Q3MoE9YzFMxGZR7F%2Fwriting-style-and-the-typical-mind-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Writing%20Style%20and%20the%20Typical%20Mind%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Q3MoE9YzFMxGZR7F%2Fwriting-style-and-the-typical-mind-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Q3MoE9YzFMxGZR7F%2Fwriting-style-and-the-typical-mind-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 355, "htmlBody": "<p>For a long time, Eliezer has been telling me I should write more like he does. I've mostly resisted, preferring instead to write like this:</p>\n<div><ol>\n<li>Explain the lesson of the post immediately, and outline the ideas clearly with lots of headings, subheadings, lists, etc.</li>\n<li>State the abstract points first, then give concrete examples.</li>\n<li>Provide lots of links and references to related work so that readers have the opportunity to read more detail about what I'm trying to say (in case it wasn't clear in a single sentence or paragraph), or read the same thing from a different angle (in case the metaphors and language I used weren't clear to that reader).</li>\n</ol>\n<div>Eliezer talks as though his style is simply&nbsp;<em>better writing</em>, while I've complained that I often can't even tell what his posts are <em>saying</em>.</div>\n</div>\n<div><br /></div>\n<div>I'm a bit embarrassed to admit that it wasn't until sometime last month that I realized that,&nbsp;<em>obviously</em>, different people prefer each style, and Eliezer and I were both falling prey to the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">typical mind fallacy</a>.</div>\n<div><br /></div>\n<p>&nbsp;</p>\n<p>At the recent <a href=\"http://www.effectivealtruismsummit.com/\">Effective Altruism Summit</a> I tried to figure out which personal features predicted writing style preference.</p>\n<p>One hypothesis was that people who read lots of fiction (like Eliezer) will tend to prefer Eliezer's story-like style, while those who read almost exclusively non-fiction (like me) will tend to prefer my \"just gimme the facts\" style. This hypothesis didn't hold up well on my non-scientific survey of ~10 LW-reading effective altruists.</p>\n<p>Another hypothesis was that most people would prefer Eliezer's more exciting posts, while people trained in the sciences or analytic philosophy (which insist on clear organization, definitions, references to related work, etc.) would prefer my posts. This hypothesis fared a bit better, but not by much.</p>\n<p>Another hypothesis was that people who had acquired an <a href=\"/lw/dxr/epiphany_addiction/\">epiphany addiction</a> would prefer Eliezer's style, whereas those who just want to <a href=\"http://metamodern.com/2009/05/17/how-to-understand-everything-and-why/\">learn everything</a>&nbsp;<a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">efficiently</a> would prefer my style. But I didn't test this.</p>\n<p>Another hypothesis that occurs to me is that people with short attention spans could prefer my more skimmable style. But I haven't tested this.</p>\n<p>Perhaps the community would like to propose some hypotheses, and test them with LW polling?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7mTviCYysGmLqiHai": 2, "GPhMyXoaHBLyzibxB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Q3MoE9YzFMxGZR7F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 42, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "23321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CZridws5zBzwfjgef", "37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-14T09:03:16.881Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup V.", "slug": "meetup-bratislava-meetup-v", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xPKNaD3jmwrnc3HEb/meetup-bratislava-meetup-v", "pageUrlRelative": "/posts/xPKNaD3jmwrnc3HEb/meetup-bratislava-meetup-v", "linkUrl": "https://www.lesswrong.com/posts/xPKNaD3jmwrnc3HEb/meetup-bratislava-meetup-v", "postedAtFormatted": "Sunday, July 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20V.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20V.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxPKNaD3jmwrnc3HEb%2Fmeetup-bratislava-meetup-v%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20V.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxPKNaD3jmwrnc3HEb%2Fmeetup-bratislava-meetup-v", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxPKNaD3jmwrnc3HEb%2Fmeetup-bratislava-meetup-v", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/om'>Bratislava Meetup V.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 July 2013 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Lanai, Dunajsk\u00e1 25, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we try a new place: Cafe Lanai. We will have a lecture (in Slovak language) about rationality in human relations and education of children.</p>\n\n<p>Stretneme sa o \u0161iestej v Lanai, pri kri\u017eovatke Dunajskej a Lazaretskej. Na stretnut\u00ed n\u00e1m redaktorka str\u00e1nky \"nanicmama.sme.sk\" porozpr\u00e1va o racionalite v medzi\u013eudsk\u00fdch vz\u0165ahoch a v\u00fdchove det\u00ed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/om'>Bratislava Meetup V.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xPKNaD3jmwrnc3HEb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.2655979053184303e-06, "legacy": true, "legacyId": "23323", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_V_\">Discussion article for the meetup : <a href=\"/meetups/om\">Bratislava Meetup V.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 July 2013 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Lanai, Dunajsk\u00e1 25, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we try a new place: Cafe Lanai. We will have a lecture (in Slovak language) about rationality in human relations and education of children.</p>\n\n<p>Stretneme sa o \u0161iestej v Lanai, pri kri\u017eovatke Dunajskej a Lazaretskej. Na stretnut\u00ed n\u00e1m redaktorka str\u00e1nky \"nanicmama.sme.sk\" porozpr\u00e1va o racionalite v medzi\u013eudsk\u00fdch vz\u0165ahoch a v\u00fdchove det\u00ed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_V_1\">Discussion article for the meetup : <a href=\"/meetups/om\">Bratislava Meetup V.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup V.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_V_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup V.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_V_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-14T23:11:09.455Z", "modifiedAt": null, "url": null, "title": "Notes/blog posts on two recent MIRI papers", "slug": "notes-blog-posts-on-two-recent-miri-papers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:09.586Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Quinn", "createdAt": "2011-12-14T06:27:22.790Z", "isAdmin": false, "displayName": "Quinn"}, "userId": "QeP9Ky3nrSL4niFiy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LQSchuqm3p6pPJ5Ec/notes-blog-posts-on-two-recent-miri-papers", "pageUrlRelative": "/posts/LQSchuqm3p6pPJ5Ec/notes-blog-posts-on-two-recent-miri-papers", "linkUrl": "https://www.lesswrong.com/posts/LQSchuqm3p6pPJ5Ec/notes-blog-posts-on-two-recent-miri-papers", "postedAtFormatted": "Sunday, July 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%2Fblog%20posts%20on%20two%20recent%20MIRI%20papers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%2Fblog%20posts%20on%20two%20recent%20MIRI%20papers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQSchuqm3p6pPJ5Ec%2Fnotes-blog-posts-on-two-recent-miri-papers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%2Fblog%20posts%20on%20two%20recent%20MIRI%20papers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQSchuqm3p6pPJ5Ec%2Fnotes-blog-posts-on-two-recent-miri-papers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQSchuqm3p6pPJ5Ec%2Fnotes-blog-posts-on-two-recent-miri-papers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>I've been learning math lately; specifically I've been reading MIRI's recent research preprints and the prerequisite material. In order to <em>actually learn</em>&nbsp;math, I typically have to write it down again, usually with more details and context. I started a blog to make my notes on these papers public, and I think they're of high enough quality that I ought to share them here.</p>\n<p>Note: my use of the pronoun \"we\" is instilled habit; I am not claiming to have helped develop the core ideas herein.</p>\n<p>&nbsp;</p>\n<ul>\n<li><a href=\"http://qmaurmann.wordpress.com/2013/07/10/lobs-theorem-and-the-prisoners-dilemma/\">L&ouml;b's Theorem and the Prisoner's Dilemma</a> is an account of the LaVictoire et al paper <em>Robust Cooperation in the Prisoner's Dilemma</em>.</li>\n<li><a href=\"http://qmaurmann.wordpress.com/2013/07/14/details-in-provability-logic/\">Details in Provability Logic</a> is a technical followup to the above, which goes into the details of modal logic needed for the LaVictoire et al paper; namely the normal form theorem, the fixed point theorem, and the decidability of GL via Kripke semantics.</li>\n<li><a href=\"http://qmaurmann.wordpress.com/2013/06/18/definability-of-truth-in-probabilistic-logic/\">Definability of Truth in Probabilistic Logic</a> goes through the Christiano et al paper of the same name. It's a little rougher around the edges on account of being the first blog post I ever wrote (and being produced more hastily than the other two). I note that the construction doesn't truly require the Axiom of Choice.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2, "ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LQSchuqm3p6pPJ5Ec", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 35, "extendedScore": null, "score": 1.2662738791001386e-06, "legacy": true, "legacyId": "23326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T10:07:54.211Z", "modifiedAt": null, "url": null, "title": "Countess and Baron attempt to define blackmail, fail", "slug": "countess-and-baron-attempt-to-define-blackmail-fail", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.734Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qjaaux3XnLBwomuNK/countess-and-baron-attempt-to-define-blackmail-fail", "pageUrlRelative": "/posts/Qjaaux3XnLBwomuNK/countess-and-baron-attempt-to-define-blackmail-fail", "linkUrl": "https://www.lesswrong.com/posts/Qjaaux3XnLBwomuNK/countess-and-baron-attempt-to-define-blackmail-fail", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Countess%20and%20Baron%20attempt%20to%20define%20blackmail%2C%20fail&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACountess%20and%20Baron%20attempt%20to%20define%20blackmail%2C%20fail%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjaaux3XnLBwomuNK%2Fcountess-and-baron-attempt-to-define-blackmail-fail%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Countess%20and%20Baron%20attempt%20to%20define%20blackmail%2C%20fail%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjaaux3XnLBwomuNK%2Fcountess-and-baron-attempt-to-define-blackmail-fail", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjaaux3XnLBwomuNK%2Fcountess-and-baron-attempt-to-define-blackmail-fail", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1067, "htmlBody": "<p><em>For a more concise version of this argument, see <a href=\"/r/discussion/lw/hza/duller_blackmail_definitions/\">here</a>.</em></p>\n<p>We meet our <a href=\"/lw/1w6/blackmail_nukes_and_the_prisoners_dilemma/\">heroes</a>, the&nbsp;Countess of Rectitude and Baron Chastity, as they continue to investigate the mysteries of blackmail by sleeping together and betraying each other.</p>\n<p>The Baron had a pile of steamy letters between him and the Countess: it would be embarrassing to both of them if these letters got out. Yet the Baron confided the letters to a trusted Acolyte, with strict instructions. The Acolyte was to publish these letters, unless the Countess agreed to give the Baron her priceless Ping Vase.</p>\n<p>This seems a perfect example of blackmail:</p>\n<ul>\n<li>The Baron is taking a course of action that is intrinsically negative for him. This behaviour only makes sense if it forces the Countess to take a specific action which benefits him. The Countess would very much like it if the Baron couldn't do such things.</li>\n</ul>\n<p>As it turns out, a servant broke the Ping Vase while chasing the Countess's griffon. The servant was swiftly executed, but the Acolyte had to publish the letters as instructed, to great embarrassment all around (sometimes precommitments aren't what they're cracked up to be). After six days of exile in the Countess's doghouse (a luxurious, twenty-room affair) and eleven days of make-up sex, the Baron was back to planning against his lover.<a id=\"more\"></a></p>\n<p>The Countess acquired the left <a href=\"http://en.wikipedia.org/wiki/Talaria\">winged sandal</a>&nbsp;from the god Hermes, who was seeking to retire from the god-race. A precious acquisition indeed! But nearly worthless without the rightmost one. The Baron suddenly remembered that he'd seen the right sandal in a small mysterious shop he'd attempted to buy out earlier (he'd wanted the land to design a course for the recently fashionable sport of hunting kangaroos while mounted on sea-horses). But he'd visited that shop hand in hand with the Countess, so she'd remember it! Quickly, he rushed down to town to buy the sandal and sell it back to the Countess at an inflated price.</p>\n<p><em>Is this blackmail? It fits the definition. The Baron is prepared to shell out money to buy the sandal, which he has no interest in. He wants to force the Countess into a specific action (giving him more money). She would like him not to do this, as she would buy the sandal herself otherwise.</em></p>\n<p>He was consternated to discover that the small shop was no longer there: it had vanished as mysteriously as it came, leaving nothing but an empty lot. Suddenly, he remembered that actually, the mysterious owner of the shop (who had two mismatched eyes and a constant lisp) had been only too happy to sell. The Baron had&nbsp;<em>already</em> bought the shop, and everything in it, and had bulldozered it to prepare for his kangaroo court. He relaxed.</p>\n<p>Then panicked again. His erratic memory threw up another fact: he'd given orders to donate everything in the shop to some deserving charity (something about orphans, or fallen women, or cute puppies, or all three at once - he couldn't recall). He took out his cellphone, cursed the fact that no-one had invented phone networks yet, and dashed home. Arriving just in time, he countermanded the donation.</p>\n<p><em>Is the Baron's countermand an example of blackmail? Again, it seems to fit all the definitions. He has no interest in the countermand, were it not for the ability to influence the Countess's decision.</em></p>\n<p>But the poor Baron was out of luck: a <a href=\"http://1funny.com/wp-content/uploads/2011/11/puppy-fetches-shoe.jpg\">cute puppy had chewed up the sandal already</a>. Cursing and shooting all animals, the Baron resolved to have a new sandal built. He contacted <a href=\"http://en.wikipedia.org/wiki/Hephaestus\">Hephaestus</a>, and offered to sell the god his nearly useless cell-phone, if he'd make another sandal. The god agreed, but mentioned that if he did so, he would be unavailable to help other people for a short period. The Baron <a href=\"http://www.youtube.com/watch?v=rTIorwtJbhE&lrm;\">vigorously</a> expressed how little he cared about this.</p>\n<p><em>Is this still blackmail? Again, the Baron is doing something that only makes sense if he can force the Countess into a particular course of action (paying him). The Countess would still prefer the Baron not to do so, because it blocks her from going to ask Hephaestus herself.</em></p>\n<p>Finally, the Baron had his sandal. Taking account all his opportunity costs, he was 19.98 silver pieces (about half a sea-horse) poorer. He resolved to accept nothing lower that twenty pieces of silver: he publicly precommited to his Acolyte to this effect, ordering him to kill his prized purebreed combat kangaroo if he didn't reach that price. He wasn't too worried about this, though: he knew the Countess valued the sandal at much more than that.</p>\n<p><em>Is this precommitment blackmail? The Baron would profit by selling for 19.99, and his action only makes sense if he can force the Countess to pay more. And she would prefer if he hadn't made that precommitment.</em></p>\n<p>On the way to meeting the Countess, yearning for her touch, he realised he hadn't counted the impact of missing out a few nights with her. Actually, his opportunity costs were much higher - at least 200 silver pieces.</p>\n<p><em>Is his precommitment still blackmail? Nothing has changed: the opportunity costs are sunk costs, so his precommitment should still count as blackmail. If he sold the sandal for anything at all, he would value that more than the sandal.</em></p>\n<p>On the way, overcome with lust and anticipation, he suddenly felt ashamed by his actions. All this effort, simply to exploit the one he loved! Maybe he should cancel the precommitment (there was still time - just!) and offer the Countess the sandal as a gift. He considered this, and ultimately rejected the idea: the conflict was an integral part of their relationship, he rationalised.</p>\n<p><em>Is ultimately rejecting that idea blackmail?</em></p>\n<p>Unfortunately, the Countess had been paranoid about her left sandal, and wouldn't let anyone touch it. She even washed it herself. Unaware that water was used for washing purposes (rather than dragon-fire), she had ruined it. And consequently she had no interest in buying the right sandal. The Baron was left with the sandal, no silver pieces, and a precommitment he deeply regretted.</p>\n<p>But the Acolyte turned out to be less loyal this time round, and instead of killing the combat kangaroo, rode it off into the sunset, starting a new career as a highwayman, threatening any low-flying airships, before finding religion (and then putting it down somewhere, and losing it again). The Countess and the Baron were both left poorer, and with no better understanding of the intricacies of blackmail.</p>\n<p>There's a moral there, presumably.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2q2cK4FdnSeohTEaJ": 3, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qjaaux3XnLBwomuNK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 1.2667979123201574e-06, "legacy": true, "legacyId": "23301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5xYxL4P5rcdoxTjC6", "hDNg77Xx53tsxq6Ko"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T10:08:11.279Z", "modifiedAt": null, "url": null, "title": "Duller blackmail definitions", "slug": "duller-blackmail-definitions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:00.229Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5xYxL4P5rcdoxTjC6/duller-blackmail-definitions", "pageUrlRelative": "/posts/5xYxL4P5rcdoxTjC6/duller-blackmail-definitions", "linkUrl": "https://www.lesswrong.com/posts/5xYxL4P5rcdoxTjC6/duller-blackmail-definitions", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Duller%20blackmail%20definitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADuller%20blackmail%20definitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xYxL4P5rcdoxTjC6%2Fduller-blackmail-definitions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Duller%20blackmail%20definitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xYxL4P5rcdoxTjC6%2Fduller-blackmail-definitions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xYxL4P5rcdoxTjC6%2Fduller-blackmail-definitions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p><em>For a more parable-ic version of this, see <a href=\"/r/discussion/lw/hz9/countess_and_baron_attempt_to_define_blackmail/\">here</a>.</em></p>\n<p>Suppose I make a precommitment <strong>P</strong> to take action <strong>X</strong> unless you take action <strong>Y</strong>. Action <strong>X</strong> is not in my interest: I wouldn't do it if I knew you'd never take action <strong>Y</strong>. You would want me to not precommit to <strong>P</strong>.</p>\n<p>Is this blackmail? Suppose we've been having a steamy affair together, and I have the letters to prove it. It would be bad for both of these if they were published. Then <strong>X</strong>={Publish the letters} and <strong>Y</strong>={You pay me money} is textbook blackmail.</p>\n<p>But suppose I own a MacGuffin that you want (I value it at &pound;9). If <strong>X</strong>={Reject any offer} and <strong>Y</strong>={You offer more than &pound;10}, is this still blackmail? Formally, it looks the same.</p>\n<p>What about if I bought the MacGuffin for &pound;500 and you value it at &pound;1000? This makes no difference to the formal structure of the scenario. Then my behaviour feels utterly reasonable, rather than vicious and blackmail-ly.</p>\n<p>What is the meaningful difference between the two scenarios? I can't really formalise it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5xYxL4P5rcdoxTjC6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.2667981393727566e-06, "legacy": true, "legacyId": "23302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qjaaux3XnLBwomuNK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T13:39:39.682Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, July 16-31", "slug": "group-rationality-diary-july-16-31-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:29.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zuuHNLdwm2Xyy3hjd/group-rationality-diary-july-16-31-0", "pageUrlRelative": "/posts/zuuHNLdwm2Xyy3hjd/group-rationality-diary-july-16-31-0", "linkUrl": "https://www.lesswrong.com/posts/zuuHNLdwm2Xyy3hjd/group-rationality-diary-july-16-31-0", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20July%2016-31&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20July%2016-31%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuuHNLdwm2Xyy3hjd%2Fgroup-rationality-diary-july-16-31-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20July%2016-31%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuuHNLdwm2Xyy3hjd%2Fgroup-rationality-diary-july-16-31-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuuHNLdwm2Xyy3hjd%2Fgroup-rationality-diary-july-16-31-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<div id=\"entry_t3_hvy\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hqf\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>T<span style=\"color: #333333;\">his is the public group instrumental rationality diary for July 16-31.<br /></span></p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/lw/hvy/group_rationality_diary_july_115/\">Immediate past diary</a>:&nbsp; July 1-15<a href=\"/lw/hvy/group_rationality_diary_july_115/\"></a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/r/discussion/lw/i7u/group_rationality_diary_august_115/\">Next diary</a>:&nbsp; August 1-15</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality Diaries archive</a></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zuuHNLdwm2Xyy3hjd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "23333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QXYsonygGQRc6fjJy", "BqgA8tZYsDrBMQjbn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T14:10:44.459Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-24", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kWczZwKyM3Aw5syrZ/meetup-melbourne-social-meetup-24", "pageUrlRelative": "/posts/kWczZwKyM3Aw5syrZ/meetup-melbourne-social-meetup-24", "linkUrl": "https://www.lesswrong.com/posts/kWczZwKyM3Aw5syrZ/meetup-melbourne-social-meetup-24", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkWczZwKyM3Aw5syrZ%2Fmeetup-melbourne-social-meetup-24%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkWczZwKyM3Aw5syrZ%2Fmeetup-melbourne-social-meetup-24", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkWczZwKyM3Aw5syrZ%2Fmeetup-melbourne-social-meetup-24", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/on'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 July 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on Friday evening this week. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>This week we will also be discussing the Comfort Zone Expansion activity planned for Saturday, which will be especially helpful for any members that are unsure if they want to attend.</p>\n\n<p>Please ring the number 5 button when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/on'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kWczZwKyM3Aw5syrZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2669917752910177e-06, "legacy": true, "legacyId": "23334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/on\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 July 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on Friday evening this week. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>This week we will also be discussing the Comfort Zone Expansion activity planned for Saturday, which will be especially helpful for any members that are unsure if they want to attend.</p>\n\n<p>Please ring the number 5 button when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/on\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T16:25:20.385Z", "modifiedAt": null, "url": null, "title": "Semi-open thread: blackmail", "slug": "semi-open-thread-blackmail", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.697Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WaYPy9cX23v3pp7HD/semi-open-thread-blackmail", "pageUrlRelative": "/posts/WaYPy9cX23v3pp7HD/semi-open-thread-blackmail", "linkUrl": "https://www.lesswrong.com/posts/WaYPy9cX23v3pp7HD/semi-open-thread-blackmail", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Semi-open%20thread%3A%20blackmail&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASemi-open%20thread%3A%20blackmail%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWaYPy9cX23v3pp7HD%2Fsemi-open-thread-blackmail%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Semi-open%20thread%3A%20blackmail%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWaYPy9cX23v3pp7HD%2Fsemi-open-thread-blackmail", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWaYPy9cX23v3pp7HD%2Fsemi-open-thread-blackmail", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<p>My <a href=\"/r/discussion/lw/hz9/countess_and_baron_attempt_to_define_blackmail/\">blackmail</a> <a href=\"/lw/hza/duller_blackmail_definitions/\">posts</a> have generated some interesting discussion, so I'm just creating this one so that people can post examples of behaviours that they think are either clearly blackmail, or clearly not blackmail, or something in between.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2q2cK4FdnSeohTEaJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WaYPy9cX23v3pp7HD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 0, "extendedScore": null, "score": 1.2670992513326424e-06, "legacy": true, "legacyId": "23335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qjaaux3XnLBwomuNK", "5xYxL4P5rcdoxTjC6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T17:56:41.176Z", "modifiedAt": null, "url": null, "title": "Personally mitigating existential risk", "slug": "personally-mitigating-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.004Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m5WRkWHhPxYXgWbZs/personally-mitigating-existential-risk", "pageUrlRelative": "/posts/m5WRkWHhPxYXgWbZs/personally-mitigating-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/m5WRkWHhPxYXgWbZs/personally-mitigating-existential-risk", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Personally%20mitigating%20existential%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersonally%20mitigating%20existential%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5WRkWHhPxYXgWbZs%2Fpersonally-mitigating-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Personally%20mitigating%20existential%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5WRkWHhPxYXgWbZs%2Fpersonally-mitigating-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5WRkWHhPxYXgWbZs%2Fpersonally-mitigating-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p>Instrumental rationalilty is important. Existential risk is important. The average person will do most good in this world by helping himself. While only few people or all people at once can avoid a war, everyone can increase his and his loved ones likelihood to survive. In this spirit I would like to create a repository of advice to help people mitigate catastrophic risk, that is any danger with irreversible consequences to personal well-being regardless of the scale of the catastrophe. This can be either a personal accident, a local earthquake, continent-wide war or a global pandemic.</p>\n<p>I ask you, the reader, to provide your bit of knowledge and expertise in this matter, be in financial, health, personal or public, insurance, politics, law, building safety ... Here is an example of how I imagine advice:</p>\n<p><strong>Mitigate natural desaster (low cost, low time):</strong>&nbsp;Natural catastrophes destroy infrastructure and supply lines and in the worst case shelter, too. To mitigate food shortages and allowing helpers to divert ressources from providing food and water to rebuilding infrastructure to return to normal levels again, build a <a href=\"http://www.ready.gov/basic-disaster-supplies-kit\">basic disaster supply kit</a>. In general, it is helpful to stay in contact with your government's agency tasked with managing catastrophes. In the case of the USA that is&nbsp;<a href=\"http://www.fema.gov\">FEMA</a>&nbsp;with their site <a href=\"http://www.ready.gov\">ready.gov</a>.</p>\n<p>I did not post this in the PSA repository as this advice should be more prominent and longer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m5WRkWHhPxYXgWbZs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -2, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "23337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T20:13:13.041Z", "modifiedAt": null, "url": null, "title": "Open thread, July 16-22, 2013", "slug": "open-thread-july-16-22-2013", "viewCount": null, "lastCommentedAt": "2018-10-29T02:32:41.371Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5GnwjxbL3SQ7gjRn6/open-thread-july-16-22-2013", "pageUrlRelative": "/posts/5GnwjxbL3SQ7gjRn6/open-thread-july-16-22-2013", "linkUrl": "https://www.lesswrong.com/posts/5GnwjxbL3SQ7gjRn6/open-thread-july-16-22-2013", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20July%2016-22%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20July%2016-22%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GnwjxbL3SQ7gjRn6%2Fopen-thread-july-16-22-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20July%2016-22%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GnwjxbL3SQ7gjRn6%2Fopen-thread-july-16-22-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GnwjxbL3SQ7gjRn6%2Fopen-thread-july-16-22-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<div id=\"entry_t3_hva\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hpz\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"entry_t3_hva\" class=\"content clear\"><br /></div>\n<div class=\"content clear\">Given the <a href=\"/lw/hyu/meta_open_threads_and_repository_threads_are/\">discussion thread</a> about these, let's try calling this a one-week thread, and see if anyone bothers starting one next Monday.<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5GnwjxbL3SQ7gjRn6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "23339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 305, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["P8Qc7te5qcEnpD3nv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-15T21:11:52.203Z", "modifiedAt": null, "url": null, "title": "For Happiness, Keep a Gratitude Journal", "slug": "for-happiness-keep-a-gratitude-journal", "viewCount": null, "lastCommentedAt": "2022-01-13T03:41:05.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xYnnRmMmGRAwZoY25/for-happiness-keep-a-gratitude-journal", "pageUrlRelative": "/posts/xYnnRmMmGRAwZoY25/for-happiness-keep-a-gratitude-journal", "linkUrl": "https://www.lesswrong.com/posts/xYnnRmMmGRAwZoY25/for-happiness-keep-a-gratitude-journal", "postedAtFormatted": "Monday, July 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20For%20Happiness%2C%20Keep%20a%20Gratitude%20Journal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFor%20Happiness%2C%20Keep%20a%20Gratitude%20Journal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxYnnRmMmGRAwZoY25%2Ffor-happiness-keep-a-gratitude-journal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=For%20Happiness%2C%20Keep%20a%20Gratitude%20Journal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxYnnRmMmGRAwZoY25%2Ffor-happiness-keep-a-gratitude-journal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxYnnRmMmGRAwZoY25%2Ffor-happiness-keep-a-gratitude-journal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1494, "htmlBody": "<p>Want to be happier than you already are? &nbsp;Many people look to self-help books as a way to become happy. &nbsp;Sometimes they give good advice and sometimes they dont. &nbsp;However, one of the most robust, enduring findings from psychological studies of increasing people's happiness has been that happiness can be found from journaling, especially when you keep a regular journal of what you're grateful for.</p>\n<p>&nbsp;</p>\n<h2>Gratitude</h2>\n<p>Gratitude is defined as the reliable emotional response that one has to receiving benefits&lt;sup&gt;1&lt;/sup&gt;. &nbsp;Gratitude is also known to correlate with subjective levels of happiness<sup>1,2,3,4,5</sup>, as well as pro-social behavior, self-efficacy, and self-worth<sup>6,7</sup>. &nbsp;Moreover, this connection with happiness is found in both student and non-student populations, and persists even when controlling for extraversion, neuroticism, and agreeableness<sup>8,9</sup>. &nbsp;Gratitude also fights stress, materialism, and negative self-comparisons<sup>7</sup>.</p>\n<p>But what if you're not already grateful? &nbsp;Well, there is a solution. &nbsp;Regular practice of gratitude has theological origins -- Judaism, Christianity, and Islam, all consider it a virtue and prescribe approaches for practicing<sup>10</sup>.</p>\n<p>And it appears that religion is right on this one -- gratitude can be trained, and one way to do so is the gratitude journal. &nbsp;And by training in gratitude, one can become lastingly happier.</p>\n<p>&nbsp;</p>\n<h2>Writing as a Cure</h2>\n<p>Studies have found that while talking about one's problems doesn't help one to feel better about them, even if it seems like the talk helped at the time<sup>11</sup>, writing about the problem does help. &nbsp;In one study, participants who had been recently laid off from work were asked to spend a few minutes each day writing a diary about their feelings regarding the lay off. &nbsp;Doing so produced boosts in happiness, self-esteem, health, and psychological and physical well-being<sup>12</sup>. &nbsp;Other similar studies found similar results<sup>13</sup>.</p>\n<p>But one doesn't need trauma in order to get these beneficial results. &nbsp;Another study had people assigned to write for 20 minutes a day for four days about one of four topics at random -- either a traumatic life event, their best possible future self, both, or a nonemotional control. &nbsp;A follow up five months later found that writing about <em>either</em>&nbsp;trauma <em>or</em>&nbsp;a positive future lead to reduced illness and increased subjective well-being compared to controls, though writing about trauma induced a short-term negative mood<sup>14</sup>. &nbsp;Another follow up study found that reduced illness and increased subjective well-being resulted even from writing about intensely <em>positive</em>&nbsp;events<sup>15</sup>.</p>\n<p>Affectionate writing is another type of regular journaling, where you write in your journal about affection for friends, family, or romantic partners. &nbsp;This too has been found to have beneficial effects, such as lower cholesterol<sup>16</sup>. &nbsp;Another study involved writing a letter of affection to someone and personally delivering it to them, which was found to decrease depressive symptoms for a few months<sup>17</sup>, but then had no further longer-term effects.</p>\n<p>&nbsp;</p>\n<h2>The Gratitude Journal</h2>\n<p>But suppose you're not recovering from a recent serious problem, but instead just want to boost your happiness in your everyday life. &nbsp;What should you do? &nbsp;Instead, you can get the same benefit of journaling by focusing on gratitude.</p>\n<p>In another study, three groups of college students were asked to keep short, daily diaries -- one group would write about what they were grateful for in that day, the second group would write about what annoyed them, and the third group was asked just to keep track of events from a neutral perspective.</p>\n<p>Those who kept careful track of what they were grateful for were more happy, more optimistic, and healthier than the other two groups at the end of the study<sup>18</sup>&nbsp;after two weeks of journaling and a three-week follow up period. &nbsp;This study was then replicated among another college population<sup>19</sup>&nbsp;and replicated a third time among college populations<sup>17</sup>. &nbsp;Researchers also tested the theory beyond college students -- in middle school classrooms<sup>20</sup>, among adults with neuromuscular disease<sup>18</sup>, and among Korean healthcare professionals<sup>21</sup>. &nbsp;Each time, they found that gratitude journaling produced reliable increases in happiness.</p>\n<p>&nbsp;</p>\n<h2>Implementation</h2>\n<p>So what should we do if we want to start a gratitude journal? &nbsp;Well, get a journal and start writing! &nbsp;I've been <a href=\"http://www.everydayutilitarian.com/essays/gratitude-journal/\">keeping mine on my blog</a>, but you could keep your wherever you like. &nbsp;However, here are some tips to make the implementation better:</p>\n<p><strong>It won't work for everyone.</strong>&nbsp; These effects only appear in the aggregate. &nbsp;So far, little research has been done to find moderating effects of gratitude journaling, but it is known to work better for women than for men, though it still works for men just fine<sup>4,5,7</sup>. &nbsp;It's possible that journaling won't work for certain people. &nbsp;<a href=\"/lw/9v/beware_of_otheroptimizing/\">Beware of other-optimizing</a>.</p>\n<p><strong>It won't work if it annoys you.</strong>&nbsp;If you find the journaling tedious or annoying, you'll lose the happiness boost<sup>19</sup>, so it's important you find some way to keep it fresh. &nbsp;In one experiment, college students were assigned to do a gratitude journal either daily or once a week. &nbsp;While both groups showed a boost, the once-a-week group actually found a higher boost in happiness<sup>19</sup>, presumably because they didn't get bored with the journal.</p>\n<p><strong>Thinking about the subtraction of positive events produces an even bigger boost.</strong>&nbsp; While one gains a boost in happiness from reflecting on being grateful for, say, wildflowers, one can get an even higher boost in happiness if instructed to <em>also</em>&nbsp;try and imagine a world where wildflowers don't exist<sup>7</sup>.</p>\n<p><strong>Think about what caused these good events.</strong>&nbsp;Thinking not just about what you're grateful for but why things turned out the way they did to inspire gratitude also had better effects<sup>17</sup>.</p>\n<p>&nbsp;</p>\n<p>It's not all that often that science hands us a definitive self-help practice that has been this well vetted. &nbsp;Maybe it works for you; maybe it doesn't. &nbsp;Maybe it's worth your time; maybe you are happy enough that you can forgo the effort. &nbsp;But it's hopefully at least worth thinking about.</p>\n<p>After all, I'm grateful that positive psychology exists.</p>\n<p>-</p>\n<address>(This was also <a href=\"http://www.everydayutilitarian.com/essays/for-happiness-keep-a-gratitude-journal/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</address>\n<p>&nbsp;</p>\n<h2>References</h2>\n<p><em>(Note: Links are to PDF files.)</em></p>\n<p>1: McCullough, Michael E., Jo-Ann Tsang, and Robert. A. Emmons. 2004. <a href=\"http://www.psy.miami.edu/faculty/mmccullough/gratitude/GIAT.pdf\">\"Gratitude in Intermediate Affective Terrain: Links of Grateful Moods to Individual Differences and Daily Emotional Experience\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;86: 295&ndash;309.</p>\n<p>2: Wood, Alex M., Jeffrey J. Froh, and Adam W. A. Geraghty. 2010. <a href=\"http://greatergood.berkeley.edu/pdfs/GratitudePDFs/2Wood-GratitudeWell-BeingReview.pdf\">\"Gratitude and Well-Being: A Review and Theoretical Integration\"</a>. <em>Clinical Psychology Review</em>&nbsp;30 (7): 890-905.</p>\n<p>3: Park, Nansook, Christopher Peterson, and Martin E. P. Seligman. 2004. <a href=\"http://guilfordjournals.com/doi/abs/10.1521/jscp.23.5.603.50748\">\"Strengths of Character and Well-Being\"</a>. <em>Journal of Social and Clinical Psychology</em>&nbsp;23 (5): 603-619.</p>\n<p>4: Watkins, Phillip C., Katherine Woodward, Tamara Stone, and Russel K. Kolts. 2003. <a href=\"http://greatergood.berkeley.edu/pdfs/GratitudePDFs/5Watkins-GratitudeHappiness.pdf\">\"Gratitude and Happiness: Development of a Measure of Gratitude, and Relationships with Subjective Well-Being\"</a>. <em>Social Behavior and Personality</em>&nbsp;31 (5): 431-452.</p>\n<p>5: Kashdan, Todd B., Gitendra Uswatteb, and Terri Julian. 2006. <a href=\"http://psychfaculty.gmu.edu/kashdan/publications/gratitudevets_BRAT.pdf\">\"Gratitude and Hedonic and Eudaimonic Well-Being in Vietnam War Veterans\"</a>. <em>Behaviour Research and Therapy</em>&nbsp;44: 177&ndash;199.</p>\n<p>6: Grant, Adam M. and Francesca Gino. 2010. <a href=\"http://www.umkc.edu/facultyombuds/documents/grant_gino_jpsp_2010.pdf\">\"A Little Thanks Goes a Long Way: Explaining Why Gratitude Expressions Motivate Prosocial Behavior\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;98 (6): 946&ndash;955.</p>\n<p>7: Emmons, Robert A. and Anjali Mishra. 2011. <a href=\"http://psychology.ucdavis.edu/Labs/PWT/Image/emmons/file/16_Sheldon_Chapter-16-1[1].pdf\">\"Why Gratitude Enhances Well-Being: What We Know, What We Need to Know\"</a>&nbsp;in Kennon M. Sheldon, Todd B. Kashdan, Michael F. Stenger (Eds.). <strong><a href=\"http://www.amazon.com/Designing-Positive-Psychology-Taking-Forward/dp/0195373588\">Designing Positive Psychology: Taking Stock and Moving Forward</a></strong>, 248-262. Oxford University Press: Oxford.</p>\n<p>8: McCullough, Michael E., Jo-Ann Tsang, and Robert. A. Emmons. 2002. <a href=\"http://www.baylor.edu/content/services/document.php/25080.pdf\">\"The Grateful Disposition: A Conceptual and Empirical Topography\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;82 (1): 112&ndash;127.</p>\n<p>9: Wood, Alex M., Stephen Joseph, and John Maltby. 2009. <a href=\"http://www.sciencedirect.com/science/article/pii/S019188690800425X\">\"Gratitude Predicts Psychological Well-Being Above the Big Five Facets\"</a>&lt;/a&gt;. <em>Personality and Individual Differences</em>&nbsp;46 (4): 443&ndash;447.</p>\n<p>10: Emmons, Robert A. and Cheryl A. Crumpler. 2000. <a href=\"http://guilfordjournals.com/doi/abs/10.1521/jscp.2000.19.1.56\">\"Gratitude as a Human Strength: Appraising the Evidence\"</a>. <em>Journal of Social and Clinical Psychology</em>&nbsp;19 (1): 56-69</p>\n<p>11: Lyubomirsky, Sonja and Chris Tkach. 2003. <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/9780470713853.ch2/summary\">\"The Consequences of Dysphoric Rumination\"</a>&nbsp;in Costas Papageorgiou and Adrian Wells (Eds.). <strong><a href=\"http://www.amazon.com/Depressive-Rumination-Nature-Theory-Treatment/dp/0471486930\">Depressive Rumination: Nature, Theory and Treatment</a></strong>, 21-41. &nbsp;Chichester, England: John Wiley &amp; Sons.</p>\n<p>12: Spera, Stephanie P., Eric D. Buhrfeind, and James W. Pennebaker. 1994. <a href=\"http://homepage.psy.utexas.edu/homepage/faculty/pennebaker/reprints/Spera.pdf&quot;\">\"Expressive Writing and Coping with Job Loss\"</a>. <em>Academy of Management Journal</em>&nbsp;3, 722&ndash;733.</p>\n<p>13: Lepore, Stephen J. and Joshua Morrison Smyth (Eds.) 2002. <strong><a href=\"http://www.amazon.com/The-Writing-Cure-Expressive-Well-Being/dp/1557989109\">The Writing Cure: How Expressive Writing Promotes Health and Emotional Well-Being</a></strong>. Washington, DC: American Psychological Association.</p>\n<p>14: King, Laura A. 2001. <a href=\"http://www.psych.utoronto.ca/users/peterson/selfauthoring/Narrative%20and%20Goal%20Setting/Future%20Authoring%20and%20Goal%20Setting/King%20LA%20Health%20benefits%20of%20writing%20about%20life%20goals%20PSPB%202001.pdf\">\"The Health Benefits of Writing About Life Goals\"</a>. <em>Personality and Social Psychology Bulletin</em>&nbsp;27: 798&ndash;807.</p>\n<p>15: Burton, Chad M and Laura A. King. 2004. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20205032\">\"The Health Benefits of Writing about Intensely Positive Experiences\"</a>. <em>Journal of Research in Personality</em>&nbsp;38: 150&ndash;163.</p>\n<p>16: Floyd, Kory, Alan C. Mikkelson, Colin Hesse, and Perry M. Pauley. 2007. <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1468-2958.2007.00293.x/abstract\">\"Affectionate Writing Reduces Total Cholesterol: Two Ranomized, Controlled Trials\"</a>. <em>Human Communication Research</em>&nbsp;33: 119&ndash;142.</p>\n<p>17: Seligman, Martin E. P., Tracy A. Steen, Nansook Park, and Christopher Peterson. <a href=\"http://www.ppc.sas.upenn.edu/articleseligman.pdf\">\"Positive Psychology Progress: Empirical Validation of Interventions\"</a>. <em>American Psychologist</em>&nbsp;60: 410-421.</p>\n<p>18: Emmons, Robert A. and Michael E. McCullough. 2003. <a href=\"http://www.psy.miami.edu/faculty/mmccullough/gratitude/Emmons_McCullough_2003_JPSP.pdf\">\"Counting Blessings versus Burdens: An Experimental Investigation of Gratitude and Subjective Well-Being in Daily Life\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;84: 377&ndash;389.</p>\n<p>19: Lyubomirsky, Sonja, Kennon M. Sheldon, and David Schkade. 2005. <a href=\"http://sonjalyubomirsky.com/wp-content/themes/sonjalyubomirsky/papers/LSS2005.pdf\">\"Pursuing Happiness: The Architecture of Sustainable Change\"</a>.&nbsp;<em>Review of General Psychology</em>&nbsp;9 (2): 111-131.</p>\n<p>20: Froh, Jeffrey J., William J. Sefick, and Robert A. Emmons. 2008. <a href=\"http://www.sciencedirect.com/science/article/pii/S0022440507000386\">\"Counting Blessings in Early Adolescents: An Experimental Study of Gratitude and Subjective Well-Being\"</a>. <em>Journal of School Psychology</em>&nbsp;46 (2): 213-233.</p>\n<p>21: Ki, Tsui Pui. 2009. <a href=\"http://lbms03.cityu.edu.hk/oaps/ss2009-5790-tpk500.pdf\">\"Gratitude and Stress of Health-Care Professionals in Hong Kong\"</a>. Unpublished thesis.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"a65Lgr7Q5jqRWHtM6": 3, "Jzm2mYuuDBCNWq8hi": 4, "sJYvMHBHnR6DbJQcN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xYnnRmMmGRAwZoY25", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 35, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "23340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Want to be happier than you already are? &nbsp;Many people look to self-help books as a way to become happy. &nbsp;Sometimes they give good advice and sometimes they dont. &nbsp;However, one of the most robust, enduring findings from psychological studies of increasing people's happiness has been that happiness can be found from journaling, especially when you keep a regular journal of what you're grateful for.</p>\n<p>&nbsp;</p>\n<h2 id=\"Gratitude\">Gratitude</h2>\n<p>Gratitude is defined as the reliable emotional response that one has to receiving benefits&lt;sup&gt;1&lt;/sup&gt;. &nbsp;Gratitude is also known to correlate with subjective levels of happiness<sup>1,2,3,4,5</sup>, as well as pro-social behavior, self-efficacy, and self-worth<sup>6,7</sup>. &nbsp;Moreover, this connection with happiness is found in both student and non-student populations, and persists even when controlling for extraversion, neuroticism, and agreeableness<sup>8,9</sup>. &nbsp;Gratitude also fights stress, materialism, and negative self-comparisons<sup>7</sup>.</p>\n<p>But what if you're not already grateful? &nbsp;Well, there is a solution. &nbsp;Regular practice of gratitude has theological origins -- Judaism, Christianity, and Islam, all consider it a virtue and prescribe approaches for practicing<sup>10</sup>.</p>\n<p>And it appears that religion is right on this one -- gratitude can be trained, and one way to do so is the gratitude journal. &nbsp;And by training in gratitude, one can become lastingly happier.</p>\n<p>&nbsp;</p>\n<h2 id=\"Writing_as_a_Cure\">Writing as a Cure</h2>\n<p>Studies have found that while talking about one's problems doesn't help one to feel better about them, even if it seems like the talk helped at the time<sup>11</sup>, writing about the problem does help. &nbsp;In one study, participants who had been recently laid off from work were asked to spend a few minutes each day writing a diary about their feelings regarding the lay off. &nbsp;Doing so produced boosts in happiness, self-esteem, health, and psychological and physical well-being<sup>12</sup>. &nbsp;Other similar studies found similar results<sup>13</sup>.</p>\n<p>But one doesn't need trauma in order to get these beneficial results. &nbsp;Another study had people assigned to write for 20 minutes a day for four days about one of four topics at random -- either a traumatic life event, their best possible future self, both, or a nonemotional control. &nbsp;A follow up five months later found that writing about <em>either</em>&nbsp;trauma <em>or</em>&nbsp;a positive future lead to reduced illness and increased subjective well-being compared to controls, though writing about trauma induced a short-term negative mood<sup>14</sup>. &nbsp;Another follow up study found that reduced illness and increased subjective well-being resulted even from writing about intensely <em>positive</em>&nbsp;events<sup>15</sup>.</p>\n<p>Affectionate writing is another type of regular journaling, where you write in your journal about affection for friends, family, or romantic partners. &nbsp;This too has been found to have beneficial effects, such as lower cholesterol<sup>16</sup>. &nbsp;Another study involved writing a letter of affection to someone and personally delivering it to them, which was found to decrease depressive symptoms for a few months<sup>17</sup>, but then had no further longer-term effects.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Gratitude_Journal\">The Gratitude Journal</h2>\n<p>But suppose you're not recovering from a recent serious problem, but instead just want to boost your happiness in your everyday life. &nbsp;What should you do? &nbsp;Instead, you can get the same benefit of journaling by focusing on gratitude.</p>\n<p>In another study, three groups of college students were asked to keep short, daily diaries -- one group would write about what they were grateful for in that day, the second group would write about what annoyed them, and the third group was asked just to keep track of events from a neutral perspective.</p>\n<p>Those who kept careful track of what they were grateful for were more happy, more optimistic, and healthier than the other two groups at the end of the study<sup>18</sup>&nbsp;after two weeks of journaling and a three-week follow up period. &nbsp;This study was then replicated among another college population<sup>19</sup>&nbsp;and replicated a third time among college populations<sup>17</sup>. &nbsp;Researchers also tested the theory beyond college students -- in middle school classrooms<sup>20</sup>, among adults with neuromuscular disease<sup>18</sup>, and among Korean healthcare professionals<sup>21</sup>. &nbsp;Each time, they found that gratitude journaling produced reliable increases in happiness.</p>\n<p>&nbsp;</p>\n<h2 id=\"Implementation\">Implementation</h2>\n<p>So what should we do if we want to start a gratitude journal? &nbsp;Well, get a journal and start writing! &nbsp;I've been <a href=\"http://www.everydayutilitarian.com/essays/gratitude-journal/\">keeping mine on my blog</a>, but you could keep your wherever you like. &nbsp;However, here are some tips to make the implementation better:</p>\n<p><strong>It won't work for everyone.</strong>&nbsp; These effects only appear in the aggregate. &nbsp;So far, little research has been done to find moderating effects of gratitude journaling, but it is known to work better for women than for men, though it still works for men just fine<sup>4,5,7</sup>. &nbsp;It's possible that journaling won't work for certain people. &nbsp;<a href=\"/lw/9v/beware_of_otheroptimizing/\">Beware of other-optimizing</a>.</p>\n<p><strong>It won't work if it annoys you.</strong>&nbsp;If you find the journaling tedious or annoying, you'll lose the happiness boost<sup>19</sup>, so it's important you find some way to keep it fresh. &nbsp;In one experiment, college students were assigned to do a gratitude journal either daily or once a week. &nbsp;While both groups showed a boost, the once-a-week group actually found a higher boost in happiness<sup>19</sup>, presumably because they didn't get bored with the journal.</p>\n<p><strong>Thinking about the subtraction of positive events produces an even bigger boost.</strong>&nbsp; While one gains a boost in happiness from reflecting on being grateful for, say, wildflowers, one can get an even higher boost in happiness if instructed to <em>also</em>&nbsp;try and imagine a world where wildflowers don't exist<sup>7</sup>.</p>\n<p><strong>Think about what caused these good events.</strong>&nbsp;Thinking not just about what you're grateful for but why things turned out the way they did to inspire gratitude also had better effects<sup>17</sup>.</p>\n<p>&nbsp;</p>\n<p>It's not all that often that science hands us a definitive self-help practice that has been this well vetted. &nbsp;Maybe it works for you; maybe it doesn't. &nbsp;Maybe it's worth your time; maybe you are happy enough that you can forgo the effort. &nbsp;But it's hopefully at least worth thinking about.</p>\n<p>After all, I'm grateful that positive psychology exists.</p>\n<p>-</p>\n<address>(This was also <a href=\"http://www.everydayutilitarian.com/essays/for-happiness-keep-a-gratitude-journal/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</address>\n<p>&nbsp;</p>\n<h2 id=\"References\">References</h2>\n<p><em>(Note: Links are to PDF files.)</em></p>\n<p>1: McCullough, Michael E., Jo-Ann Tsang, and Robert. A. Emmons. 2004. <a href=\"http://www.psy.miami.edu/faculty/mmccullough/gratitude/GIAT.pdf\">\"Gratitude in Intermediate Affective Terrain: Links of Grateful Moods to Individual Differences and Daily Emotional Experience\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;86: 295\u2013309.</p>\n<p>2: Wood, Alex M., Jeffrey J. Froh, and Adam W. A. Geraghty. 2010. <a href=\"http://greatergood.berkeley.edu/pdfs/GratitudePDFs/2Wood-GratitudeWell-BeingReview.pdf\">\"Gratitude and Well-Being: A Review and Theoretical Integration\"</a>. <em>Clinical Psychology Review</em>&nbsp;30 (7): 890-905.</p>\n<p>3: Park, Nansook, Christopher Peterson, and Martin E. P. Seligman. 2004. <a href=\"http://guilfordjournals.com/doi/abs/10.1521/jscp.23.5.603.50748\">\"Strengths of Character and Well-Being\"</a>. <em>Journal of Social and Clinical Psychology</em>&nbsp;23 (5): 603-619.</p>\n<p>4: Watkins, Phillip C., Katherine Woodward, Tamara Stone, and Russel K. Kolts. 2003. <a href=\"http://greatergood.berkeley.edu/pdfs/GratitudePDFs/5Watkins-GratitudeHappiness.pdf\">\"Gratitude and Happiness: Development of a Measure of Gratitude, and Relationships with Subjective Well-Being\"</a>. <em>Social Behavior and Personality</em>&nbsp;31 (5): 431-452.</p>\n<p>5: Kashdan, Todd B., Gitendra Uswatteb, and Terri Julian. 2006. <a href=\"http://psychfaculty.gmu.edu/kashdan/publications/gratitudevets_BRAT.pdf\">\"Gratitude and Hedonic and Eudaimonic Well-Being in Vietnam War Veterans\"</a>. <em>Behaviour Research and Therapy</em>&nbsp;44: 177\u2013199.</p>\n<p>6: Grant, Adam M. and Francesca Gino. 2010. <a href=\"http://www.umkc.edu/facultyombuds/documents/grant_gino_jpsp_2010.pdf\">\"A Little Thanks Goes a Long Way: Explaining Why Gratitude Expressions Motivate Prosocial Behavior\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;98 (6): 946\u2013955.</p>\n<p>7: Emmons, Robert A. and Anjali Mishra. 2011. <a href=\"http://psychology.ucdavis.edu/Labs/PWT/Image/emmons/file/16_Sheldon_Chapter-16-1[1].pdf\">\"Why Gratitude Enhances Well-Being: What We Know, What We Need to Know\"</a>&nbsp;in Kennon M. Sheldon, Todd B. Kashdan, Michael F. Stenger (Eds.). <strong><a href=\"http://www.amazon.com/Designing-Positive-Psychology-Taking-Forward/dp/0195373588\">Designing Positive Psychology: Taking Stock and Moving Forward</a></strong>, 248-262. Oxford University Press: Oxford.</p>\n<p>8: McCullough, Michael E., Jo-Ann Tsang, and Robert. A. Emmons. 2002. <a href=\"http://www.baylor.edu/content/services/document.php/25080.pdf\">\"The Grateful Disposition: A Conceptual and Empirical Topography\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;82 (1): 112\u2013127.</p>\n<p>9: Wood, Alex M., Stephen Joseph, and John Maltby. 2009. <a href=\"http://www.sciencedirect.com/science/article/pii/S019188690800425X\">\"Gratitude Predicts Psychological Well-Being Above the Big Five Facets\"</a>&lt;/a&gt;. <em>Personality and Individual Differences</em>&nbsp;46 (4): 443\u2013447.</p>\n<p>10: Emmons, Robert A. and Cheryl A. Crumpler. 2000. <a href=\"http://guilfordjournals.com/doi/abs/10.1521/jscp.2000.19.1.56\">\"Gratitude as a Human Strength: Appraising the Evidence\"</a>. <em>Journal of Social and Clinical Psychology</em>&nbsp;19 (1): 56-69</p>\n<p>11: Lyubomirsky, Sonja and Chris Tkach. 2003. <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/9780470713853.ch2/summary\">\"The Consequences of Dysphoric Rumination\"</a>&nbsp;in Costas Papageorgiou and Adrian Wells (Eds.). <strong><a href=\"http://www.amazon.com/Depressive-Rumination-Nature-Theory-Treatment/dp/0471486930\">Depressive Rumination: Nature, Theory and Treatment</a></strong>, 21-41. &nbsp;Chichester, England: John Wiley &amp; Sons.</p>\n<p>12: Spera, Stephanie P., Eric D. Buhrfeind, and James W. Pennebaker. 1994. <a href=\"http://homepage.psy.utexas.edu/homepage/faculty/pennebaker/reprints/Spera.pdf&quot;\">\"Expressive Writing and Coping with Job Loss\"</a>. <em>Academy of Management Journal</em>&nbsp;3, 722\u2013733.</p>\n<p>13: Lepore, Stephen J. and Joshua Morrison Smyth (Eds.) 2002. <strong><a href=\"http://www.amazon.com/The-Writing-Cure-Expressive-Well-Being/dp/1557989109\">The Writing Cure: How Expressive Writing Promotes Health and Emotional Well-Being</a></strong>. Washington, DC: American Psychological Association.</p>\n<p>14: King, Laura A. 2001. <a href=\"http://www.psych.utoronto.ca/users/peterson/selfauthoring/Narrative%20and%20Goal%20Setting/Future%20Authoring%20and%20Goal%20Setting/King%20LA%20Health%20benefits%20of%20writing%20about%20life%20goals%20PSPB%202001.pdf\">\"The Health Benefits of Writing About Life Goals\"</a>. <em>Personality and Social Psychology Bulletin</em>&nbsp;27: 798\u2013807.</p>\n<p>15: Burton, Chad M and Laura A. King. 2004. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20205032\">\"The Health Benefits of Writing about Intensely Positive Experiences\"</a>. <em>Journal of Research in Personality</em>&nbsp;38: 150\u2013163.</p>\n<p>16: Floyd, Kory, Alan C. Mikkelson, Colin Hesse, and Perry M. Pauley. 2007. <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1468-2958.2007.00293.x/abstract\">\"Affectionate Writing Reduces Total Cholesterol: Two Ranomized, Controlled Trials\"</a>. <em>Human Communication Research</em>&nbsp;33: 119\u2013142.</p>\n<p>17: Seligman, Martin E. P., Tracy A. Steen, Nansook Park, and Christopher Peterson. <a href=\"http://www.ppc.sas.upenn.edu/articleseligman.pdf\">\"Positive Psychology Progress: Empirical Validation of Interventions\"</a>. <em>American Psychologist</em>&nbsp;60: 410-421.</p>\n<p>18: Emmons, Robert A. and Michael E. McCullough. 2003. <a href=\"http://www.psy.miami.edu/faculty/mmccullough/gratitude/Emmons_McCullough_2003_JPSP.pdf\">\"Counting Blessings versus Burdens: An Experimental Investigation of Gratitude and Subjective Well-Being in Daily Life\"</a>. <em>Journal of Personality and Social Psychology</em>&nbsp;84: 377\u2013389.</p>\n<p>19: Lyubomirsky, Sonja, Kennon M. Sheldon, and David Schkade. 2005. <a href=\"http://sonjalyubomirsky.com/wp-content/themes/sonjalyubomirsky/papers/LSS2005.pdf\">\"Pursuing Happiness: The Architecture of Sustainable Change\"</a>.&nbsp;<em>Review of General Psychology</em>&nbsp;9 (2): 111-131.</p>\n<p>20: Froh, Jeffrey J., William J. Sefick, and Robert A. Emmons. 2008. <a href=\"http://www.sciencedirect.com/science/article/pii/S0022440507000386\">\"Counting Blessings in Early Adolescents: An Experimental Study of Gratitude and Subjective Well-Being\"</a>. <em>Journal of School Psychology</em>&nbsp;46 (2): 213-233.</p>\n<p>21: Ki, Tsui Pui. 2009. <a href=\"http://lbms03.cityu.edu.hk/oaps/ss2009-5790-tpk500.pdf\">\"Gratitude and Stress of Health-Care Professionals in Hong Kong\"</a>. Unpublished thesis.</p>", "sections": [{"title": "Gratitude", "anchor": "Gratitude", "level": 1}, {"title": "Writing as a Cure", "anchor": "Writing_as_a_Cure", "level": 1}, {"title": "The Gratitude Journal", "anchor": "The_Gratitude_Journal", "level": 1}, {"title": "Implementation", "anchor": "Implementation", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6NvbSwuSAooQxxf7f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T01:25:02.704Z", "modifiedAt": null, "url": null, "title": "Recent MIRI workshop results?", "slug": "recent-miri-workshop-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.330Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "johnswentworth", "createdAt": "2011-02-19T16:54:09.598Z", "isAdmin": false, "displayName": "johnswentworth"}, "userId": "MEu8MdhruX5jfGsFQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aMWFZkZkp62yozSbn/recent-miri-workshop-results", "pageUrlRelative": "/posts/aMWFZkZkp62yozSbn/recent-miri-workshop-results", "linkUrl": "https://www.lesswrong.com/posts/aMWFZkZkp62yozSbn/recent-miri-workshop-results", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recent%20MIRI%20workshop%20results%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecent%20MIRI%20workshop%20results%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMWFZkZkp62yozSbn%2Frecent-miri-workshop-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recent%20MIRI%20workshop%20results%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMWFZkZkp62yozSbn%2Frecent-miri-workshop-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMWFZkZkp62yozSbn%2Frecent-miri-workshop-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>So I hear MIRI had another math workshop this past week. Given the recent results, I'm on the edge of my seat to hear how it went. Has anything been written up? Would anyone in the know like to comment on how it went?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aMWFZkZkp62yozSbn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 1.2675303633367683e-06, "legacy": true, "legacyId": "23338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T02:17:21.127Z", "modifiedAt": null, "url": null, "title": "Akrasia Tactics Review 2: The Akrasia Strikes Back", "slug": "akrasia-tactics-review-2-the-akrasia-strikes-back", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:36.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polutropon", "createdAt": "2013-07-06T19:09:16.851Z", "isAdmin": false, "displayName": "polutropon"}, "userId": "gK9rqqi8w8cn27Sax", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tAjXnuJajm3GRKT79/akrasia-tactics-review-2-the-akrasia-strikes-back", "pageUrlRelative": "/posts/tAjXnuJajm3GRKT79/akrasia-tactics-review-2-the-akrasia-strikes-back", "linkUrl": "https://www.lesswrong.com/posts/tAjXnuJajm3GRKT79/akrasia-tactics-review-2-the-akrasia-strikes-back", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Akrasia%20Tactics%20Review%202%3A%20The%20Akrasia%20Strikes%20Back&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAkrasia%20Tactics%20Review%202%3A%20The%20Akrasia%20Strikes%20Back%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAjXnuJajm3GRKT79%2Fakrasia-tactics-review-2-the-akrasia-strikes-back%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Akrasia%20Tactics%20Review%202%3A%20The%20Akrasia%20Strikes%20Back%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAjXnuJajm3GRKT79%2Fakrasia-tactics-review-2-the-akrasia-strikes-back", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAjXnuJajm3GRKT79%2Fakrasia-tactics-review-2-the-akrasia-strikes-back", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 527, "htmlBody": "<p>About three and a half years ago, orthonormal ran an <a href=\"/lw/1sm/akrasia_tactics_review/\">akrasia tactics review</a>: an open-ended survey asking Less Wrong posters to give numerical scores to productivity techniques that they'd tried, with the goal of getting a more objective picture of how well different techniques work (for the sort of people who post here). Since it's been years since the original and Less Wrong has grown significantly while retaining akrasia as a major topic, I thought it'd be useful to have a new one!</p>\n<p>A modified version of the instructions from the previous post:</p>\n<ol>\n<li><strong>Note what technique you've tried.</strong> Techniques can be anything from productivity systems (Getting Things Done) to social incentives (precommitting in front of friends) to websites or computer programs (Beeminder, Leechblock) to chemical aids (Modafinil). If it's something that you can easily link to information about, please provide a link and I'll add it when I list the technique; if you don't have a link, describe it in your comment and I'll link that.</li>\n<li><strong>Give your experience with it a score from -10 to +10</strong> (0 if it didn't change the status quo, 10 if it ended your akrasia problems forever with no side effects, negative scores if it actually made your life worse, -10 if it nearly killed you). For simplicity's sake, I'll only include reviews that give numerical scores.</li>\n<li><strong>Describe your experience with it</strong>, including any significant side effects. Please also say approximately how long you've been using it, or if you don't use it anymore how long you used it before giving up.</li>\n</ol>\n<p>Every so often, I'll combine all the data back into the main post, listing every technique that's been reviewed at least twice with the number of reviews, average score, standard deviation and common effects, as well as links to the relevant reviews &lt;edit: mostly canceling the last two parts part because I think it'd be too much work for me for too little benefit for the reader&gt;. I'll do my best to combine similar techniques appropriately, but it'd be appreciated if you could try to organize it a bit by replying to people doing similar things and/or saying if you feel your technique is (dis)similar to another.</p>\n<p>I'm not going to provide an initial list due to the massive number of possible techniques and fear of prejudicing answers, but you can look back on the list in the <a href=\"/lw/1sm/akrasia_tactics_review/\">last post</a> if you want. If you have any suggestions for how to organize this (that wouldn't require huge amounts of extra effort on my part), I'm open to hearing them.</p>\n<p>Thanks for your data!</p>\n<p>&mdash;&mdash;</p>\n<p>Updated through 7/23/13. Organizing these turned out to be a lot harder than I expected and involved a lot of subjective categorization, so consult the primary sources.</p>\n<p>&mdash;&mdash;</p>\n<p><em>6 reviews:</em></p>\n<p><a href=\"https://www.beeminder.com/\">Beeminder</a>: +5.3 (SD 1.8). Details of how it's used vary a lot.</p>\n<p>Getting Things Done (GTD): +2.8 (SD 4.0). A very broad and modular system, with opinions differing on different parts.</p>\n<p><em>4 reviews:</em></p>\n<p><a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=remember%20the%20milk&amp;source=web&amp;cd=1&amp;cad=rja&amp;sqi=2&amp;ved=0CCsQFjAA&amp;url=http%3A%2F%2Fwww.rememberthemilk.com%2F&amp;ei=IJLvUfqeM4jviQLwvIDIDQ&amp;usg=AFQjCNHKYSMADgpSSt-YNelXa5xp03qvuw&amp;bvm=bv.49641647,d.cGE\">Remember The Milk</a>:+5.5 (SD 3.0). Frequently mentioned in conjunction with GTD.</p>\n<p><a href=\"/lesswrong.com/r/lesswrong/lw/gp4/the_power_of_pomodoros\">Pomodoros</a>: +4.5 (SD 2.5).</p>\n<p><em>3 reviews:</em></p>\n<p>Scheduling: +4.7 (SD 3.7)</p>\n<p><a href=\"http://www.proginosko.com/leechblock.html\">Leechblock</a>: +3.0 (SD 0.8)</p>\n<p>Social precommitment: +0.7 (SD 2.6)</p>\n<p>Unaided self-reinforcement: +0.7 (SD 0.9)</p>\n<p><em>2 reviews:</em></p>\n<p><a href=\"/trello.com\">Trello</a>: +5.0 (SD 3.0)</p>\n<p><a href=\"/habitrpg.com\">HabitRPG</a>: +4.5 (SD 0.5)</p>\n<p><a href=\"/lw/hen/lw_study_hall_2_month_update/\">LW Study Hall</a>: +4 (SD 3.0)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 1, "2wjPMY34by2gXEXA2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tAjXnuJajm3GRKT79", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 31, "extendedScore": null, "score": 1.26757145569565e-06, "legacy": true, "legacyId": "23342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rRmisKb45dN7DK4BW", "2huJTJ2Fs9qbw3xR7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T02:50:20.478Z", "modifiedAt": null, "url": null, "title": "Meetup : Southeast Michigan", "slug": "meetup-southeast-michigan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.058Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EPrKS8wRpFwHQnsGM/meetup-southeast-michigan", "pageUrlRelative": "/posts/EPrKS8wRpFwHQnsGM/meetup-southeast-michigan", "linkUrl": "https://www.lesswrong.com/posts/EPrKS8wRpFwHQnsGM/meetup-southeast-michigan", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Southeast%20Michigan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Southeast%20Michigan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEPrKS8wRpFwHQnsGM%2Fmeetup-southeast-michigan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Southeast%20Michigan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEPrKS8wRpFwHQnsGM%2Fmeetup-southeast-michigan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEPrKS8wRpFwHQnsGM%2Fmeetup-southeast-michigan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<div class=\"meetup-meta\"><strong>WHEN:</strong> <span class=\"date\">21 July 2013 02:00:00PM (-0400)</span>\n<p><strong>WHERE: </strong>[information removed]<span class=\"address\"></span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>I now live in Michigan and would like to get to know the locals. By a happy coincidence, this weekend out-of-staters ozymandias_42 and lucidian are also visiting. So I am inviting the Metro Detroit/Ann Arbor LW community to my house for snacks and informal discussion.</p>\n<p>Sorry for the non-neutral location, but arundelo says last time everyone tried to meet up at a restaurant and it was loud and inconvenient and turned everyone off from further meetups. There are various people on here who can vouch for me being safe to visit. RSVPs appreciated but not required.</p>\n</div>\n</div>\n<!-- .content -->", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EPrKS8wRpFwHQnsGM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 1.267598521026603e-06, "legacy": true, "legacyId": "23345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T05:20:39.693Z", "modifiedAt": null, "url": null, "title": "An Attempt at Preference Uncertainty Using VNM", "slug": "an-attempt-at-preference-uncertainty-using-vnm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:00.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f3JgMn85PB7cf4jcg/an-attempt-at-preference-uncertainty-using-vnm", "pageUrlRelative": "/posts/f3JgMn85PB7cf4jcg/an-attempt-at-preference-uncertainty-using-vnm", "linkUrl": "https://www.lesswrong.com/posts/f3JgMn85PB7cf4jcg/an-attempt-at-preference-uncertainty-using-vnm", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Attempt%20at%20Preference%20Uncertainty%20Using%20VNM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Attempt%20at%20Preference%20Uncertainty%20Using%20VNM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff3JgMn85PB7cf4jcg%2Fan-attempt-at-preference-uncertainty-using-vnm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Attempt%20at%20Preference%20Uncertainty%20Using%20VNM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff3JgMn85PB7cf4jcg%2Fan-attempt-at-preference-uncertainty-using-vnm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff3JgMn85PB7cf4jcg%2Fan-attempt-at-preference-uncertainty-using-vnm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1915, "htmlBody": "<p>(This is a (possibly perpetual) draft of some work that we (I) did at the Vancouver meetup. Thanks to my meetup buddies for letting me use their brains as supplementary computational substrate. Sorry about how ugly the LaTeX is; is there a way to make this all look a bit nicer?)</p>\n<p>(Large swaths of this are obsolete. Thanks for the input, LW!)</p>\n<h2>The Problem of Decision Under Preference Uncertainty</h2>\n<p>Suppose you are uncertain whether it is good to eat meat or not. It could be OK, or it could be very bad, but having not done the thinking, you are uncertain. And yet you have to decide what to eat <em>now</em>; is it going to be the tasty hamburger or the morally pure vegetarian salad?</p>\n<p>You have multiple theories about your preferences that contradict in their assessment, and you want to make the best decision. How would you decide, even in principle, when you have such uncertainty? This is the problem of Preference Uncertainty.</p>\n<p>Preference Uncertainty is a daily fact of life for humans; we simply don't have introspective access to our raw preferences in many cases, but we still want to make the best decisions we can. Just going with our intuitions about what seems most awesome is usually sufficient, but on higher-stakes decisions and theoretical reasoning, we want formal methods with more transparent reasoning processes. We especially like transparent formal methods if we want to create a Friendly AI.</p>\n<p>There is unfortunately very little formal analysis of the preference uncertainty problem, and what has been done is incomplete and more philosophical than formal. Nonetheless, there has been some good work in the last few years. I'll refer you to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/04/Crouch-Moral-Uncertainty-and-Intertheoretic-comparisons-of-value.pdf\">Crouch's thesis</a> if you're interested in that.</p>\n<h2>Using VNM</h2>\n<p>I'm going to assume VNM. That is, that rational preferences imply a utility function, and we decide between lotteries, choosing the one with highest expected utility.</p>\n<p>The implications here are that the possible moral theories (<img src=\"http://www.codecogs.com/png.latex?m%20%5Cin%20M\" alt=\"\" />) each have an associated utility function (<img src=\"http://www.codecogs.com/png.latex?U_m\" alt=\"\" />) that represents their preferences. Also by VNM, our solution to preference uncertainty is a utility function <img src=\"http://www.codecogs.com/png.latex?U%27\" alt=\"\" />.</p>\n<p>We are uncertain between moral theories, so we have a probability distribution over moral theories <img src=\"http://www.codecogs.com/png.latex?%5Csum%5Cnolimits_%7Bm%20%5Cin%20M%7DP%28m%29%20%3D%201\" alt=\"\" />.</p>\n<p>To make decisions, we need a way to compute the expected value of some lottery <img src=\"http://www.codecogs.com/png.latex?l\" alt=\"\" />. Each lottery is essentially a probability distribution over the set of possible outcomes <img src=\"http://www.codecogs.com/png.latex?%5Csum%5Cnolimits_%7Bo%20%5Cin%20O%7DP_l%28o%29%20%3D%201\" alt=\"\" />.</p>\n<p>Since we have uncertainty over multiple things (<img src=\"http://www.codecogs.com/png.latex?m%2Co\" alt=\"\" />), the domain of the final preference structure is both moral theories and outcomes: <img src=\"http://www.codecogs.com/png.latex?U%27%20%3A%20%28M%20%5Ctimes%20O%29%20%5Crightarrow%20%5Cmathbb%7BR%7D\" alt=\"\" />.</p>\n<p>Now for some conditions. In the degenerate case of full confidence in one moral theory <img src=\"http://www.codecogs.com/png.latex?m\" alt=\"\" />, the overall preferences should agree with that theory:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?P%28m%29%20%3D%201%20%5CRightarrow%20EU%28o%29%20%3D%20U%27%28m%2Co%29%20%3D%20k_m%2AU_m%28o%29%20%2B%20c_m\" alt=\"\" /></p>\n<p>For some <img src=\"http://www.codecogs.com/png.latex?k_m%20%3E%200%20%5Cin%20%5Cmathbb%7BR%7D\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?c_m%20%5Cin%20%5Cmathbb%7BR%7D\" alt=\"\" /> representing the degrees of freedom in utility function equality. That condition actually already contains most of the specification of <img src=\"http://www.codecogs.com/png.latex?U%27\" alt=\"\" />.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?U%27%28m%2Co%29%20%3D%20k_m%20%2AU_m%28o%29%20%2B%20c_m\" alt=\"\" />.</p>\n<p>So we have a utility function, except for those unknown scaling and offset constants, which undo the arbitrariness in the basis and scale used to define each individual utility function.</p>\n<p>Thus overall expectation looks like this:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU%28l%29%20%3D%20%5Csum%5Climits_%7Bo%20%5Cin%20O%7D%5Csum%5Climits_%7Bm%20%5Cin%20M%7DP_l%28o%29%2AP%28m%29%2A%28k_m%20%2AU_m%28o%29%20%2B%20c_m%29\" alt=\"\" />.</p>\n<p>This is still incomplete, though. If we want to get actual decisions, we need to pin down each <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?c_m\" alt=\"\" />.</p>\n<h2>Offsets and Scales</h2>\n<p>You'll see above that the probability distribution over <img src=\"http://www.codecogs.com/png.latex?M\" alt=\"\" /> is <em>not</em> dependent on the particular lottery, while <img src=\"http://www.codecogs.com/png.latex?P_l%28o%29\" alt=\"\" /> <em>is</em> a function of lottery. This is because I assumed that actions can't change what is right.</p>\n<p>With this assumption, the contribution of the <img src=\"http://www.codecogs.com/png.latex?c_m\" alt=\"\" />'s can be entirely factored out:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU%28l%29%20%3D%20%28%5Csum%5Climits_%7Bm%20%5Cin%20M%7DP%28m%29%2Ac_m%29%20%2B%20%0D%0A%28%5Csum%5Climits_%7Bo%20%5Cin%20O%7D%5Csum%5Climits_%7Bm%20%5Cin%20M%7DP_l%28o%29%2AP%28m%29%2Ak_m%20%2AU_m%28o%29%29\" alt=\"\" />.</p>\n<p>This makes it obvious that the effect of the <img src=\"http://www.codecogs.com/png.latex?c_m\" alt=\"\" />'s is an additive constant that affects all lotteries the same way and thus never affects preferences. Thus we can set them to any value that is convenient; for this article, all <img src=\"http://www.codecogs.com/png.latex?c_m%20%3D%200\" alt=\"\" />.</p>\n<p>A similar process allows us to arbitrarily set exactly one of the <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" />.</p>\n<p>The remaining values of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" /> actually affect decisions, so setting them arbitrarily has real consequences. To illustrate, consider the opening example of choosing lunch between a <img src=\"http://www.codecogs.com/png.latex?Burger\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?Salad\" alt=\"\" /> when unsure about the moral status of meat.</p>\n<p>Making up some details, we might have <img src=\"http://www.codecogs.com/png.latex?U_%7Bveg%7D%28Burger%29%20%3C%20U_%7Bveg%7D%28Salad%29\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?U_%7Bmeat%7D%28Burger%29%20%3E%20U_%7Bmeat%7D%28Salad%29\" alt=\"\" /> and <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.7\" alt=\"\" />. Importing this into the framework described thus far, we might have the following payoff table:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<th>Moral Theory</th><th>U'(Burger)</th><th>U'(Salad)</th><th>(P(m)) </th>\n</tr>\n<tr>\n<td>Meat OK (meat)</td>\n<td>1</td>\n<td>0</td>\n<td>(0.7)</td>\n</tr>\n<tr>\n<td>Meat Bad (veg)</td>\n<td>0</td>\n<td>k_veg</td>\n<td>(0.3)</td>\n</tr>\n<tr>\n<td>(expectation)</td>\n<td>0.7</td>\n<td>0.3*k_veg</td>\n<td>(1)</td>\n</tr>\n</tbody>\n</table>\n<p>We can see that with those probabilities, the expected value of <img src=\"http://www.codecogs.com/png.latex?Salad\" alt=\"\" /> exceeds that of <img src=\"http://www.codecogs.com/png.latex?Burger\" alt=\"\" /> when <img src=\"http://www.codecogs.com/png.latex?k_%7Bveg%7D%20%3E%207%2F3\" alt=\"\" /> (when <img src=\"http://www.codecogs.com/png.latex?0.3%2Ak_%7Bveg%7D%20%3E%200.7\" alt=\"\" />), so the decision hinges on the value of that parameter.</p>\n<p>The value of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" /> can be interpreted as the \"intertheoretic weight\" of a utility function candidate for the purposes of intertheoretic value comparisons.</p>\n<p>In general, if <img src=\"http://www.codecogs.com/png.latex?%7CM%7C%20%3D%20n\" alt=\"\" /> then you have exactly <img src=\"http://www.codecogs.com/png.latex?n-1\" alt=\"\" /> missing intertheoretic weights that determine how you respond to situations with preference uncertainty. These could be pinned down if you had <img src=\"http://www.codecogs.com/png.latex?n-1\" alt=\"\" /> independent equations representing indifference scenarios.</p>\n<p>For example, if we had <img src=\"http://www.codecogs.com/png.latex?EU%28Burger%29%20%3D%20EU%28Salad%29\" alt=\"\" /> when <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.8\" alt=\"\" />, then we would have <img src=\"http://www.codecogs.com/png.latex?k_%7Bveg%7D%20%3D%204\" alt=\"\" />, and the above decision would be determined in favor of the <img src=\"http://www.codecogs.com/png.latex?Salad\" alt=\"\" />.</p>\n<h2>Expressing Arbitrary Preferences</h2>\n<p>Preferences are arbitrary, in the sense that we should be able to want whatever we want to want, so our mathematical constructions should not dictate or limit our preferences. If they do, we should just decide to disagree.</p>\n<p>What that means here is that because the values of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" /> drive important preferences (like at what probability you feel it is safe to eat meat), the math must leave them unconstrained, to be selected by whatever moral reasoning process it is that selected the candidate utility functions and gave them probabilities in the first place.</p>\n<p>We could ignore this idea and attempt to use a \"normalization\" scheme to pin down the intertheoretic weights from the object level preferences without having to use additional moral reasoning. For example, we could dictate that the \"variance\" of each candidate utility function equals 1 (with some measure assignment over outcomes), which would divide out the arbitrary scales used to define the candidate utility functions, preventing dominance by arbitrary factors that shouldn't matter.</p>\n<p>Consider that any given assignment of intertheoretic weights is equivalent to some set of indifference scenarios (like the one we used above for vegetarianism). For example, the above normalization scheme gives us the indifference scenario <img src=\"http://www.codecogs.com/png.latex?EU%28Burger%29%20%3D%20EU%28Salad%29\" alt=\"\" /> when <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.5\" alt=\"\" />.</p>\n<p>If I find that I am actually indifferent at <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.8\" alt=\"\" /> like above, then I'm out of luck, unable to express this very reasonable preference. On the other hand, I can simply reject the normalization scheme and keep my preferences intact, which I much prefer.</p>\n<p>(Notice that the normalization scheme was an unjustifiably privileged hypothesis from the beginning; we didn't argue that it was necessary, we simply pulled it out of thin air for no reason, so its failure was predictable.)</p>\n<p>Thus I reassert that the <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" />'s are free parameters to be set accordance with our <em>actual</em> intertheoretic preferences, on pain of stupidity. Consider an analogy to the move from ordinal to cardinal utilities; when you add risk, you need more degrees of freedom in your preferences to express how you might respond to that risk, and you need to actually think about what you want those values to be.</p>\n<h2>Uncertainty Over Intertheoretic Weights</h2>\n<p>(This section is less solid than the others. Watch your step.)</p>\n<p>A weakness in the constructions described so far is that they assume that we have access to perfect knowledge of intertheoretic preferences, even though the whole problem is that we are unable to find perfect knowledge of our preferences.</p>\n<p>It seems intuitively that we could have a probability distribution over each <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" />. If we do this, making decisions is not much complicated, I think; a simple expectation should still work.</p>\n<p>If expectation is the way, the expectation over <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" /> can be factored out (by linearity or something). Thus in any given decision with fixed preference uncertainties, we can pretend to have perfect knowledge of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" />.</p>\n<p>Despite the seeming triviality of the above idea for dealing with uncertainty over <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" />, I haven't formalized it much. We'll see if I figure it out soon, but for now, it would be foolish to make too many assumptions about this. Thus the rest of this article still assumes perfect knowledge of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\" />, on the expectation that we can extend it later.</p>\n<h2>Learning Values, Among Other Things</h2>\n<p>Strictly speaking, inference across the is-ought gap is not valid, but we do it every time we act on our moral intuitions, which are just physical facts about our minds. Strictly speaking, inferring future events from past observations (induction) is not valid either, but it doesn't bother us much:</p>\n<p>We deal with induction by defining an arbitrary (but good-seeming, on reflection) prior joint probability distribution over observations and events. We can handle the is-ought gap the same way: instead of separate probability distributions over events <img src=\"http://www.codecogs.com/png.latex?O\" alt=\"\" /> and moral facts <img src=\"http://www.codecogs.com/png.latex?M\" alt=\"\" />, we define a joint prior over <img src=\"http://www.codecogs.com/png.latex?M%20%5Ctimes%20O\" alt=\"\" />.&nbsp;Then learning value is just Bayesian updates on partial observations of <img src=\"http://www.codecogs.com/png.latex?O\" alt=\"\" />. Note that this prior subsumes induction.</p>\n<p>Making decisions is still just maximizing expected utility with our constructions from above, though we will have to be careful to make sure that <img src=\"http://www.codecogs.com/png.latex?P%28m%29\" alt=\"\" /> remains independent of the particular lottery.</p>\n<p>The problem of how to define such a prior is beyond the scope of this article. I will note that this \"moral prior\" idea is the solid foundation on which to base Indirect Normativity schemes like Yudkowsky's CEV and Christiano's boxed philosopher. I will hopefully discuss this further in the future.</p>\n<h2>Recap</h2>\n<p>The problem was how to make decisions when you are uncertain about what your object-level preferences should be. To solve it, I assumed VNM, in particular that we have a set of possible utility functions, and we want to construct an overall utility function that does the right thing by those utility functions and their probabilities. The simple condition that the overall utility function should make the common sense choices in cases of moral certainty was sufficient to construct a utility function with a precise set of remaining degrees of freedom. The degrees of freedom being the intertheoretic weight and offset of each utility function candidate.</p>\n<p>I showed that the offsets and an overall scale factor are superfluous, in the sense that they never affect the decision if we assume that actions don't affect what is desirable. The remaining intertheoretic weights <em>do</em> affect the decision, and I argued that they are critical to expressing whatever intertheoretic preferences we might want to have.</p>\n<p>Uncertainty over intertheoretic weight seems tractable, but the details are still open.</p>\n<p>I also mentioned that we can construct a joint distribution that allows us to embed value learning in normal Bayesian learning and induction. This \"moral prior\" would subsume induction and define how facts about the desirability of things could be inferred from physical observations like the opinions of moral philosophers. In particular, it would provide a solid foundation for Indirect Normativity schemes like CEV. The nature of this distribution is still open.</p>\n<h2>Open Questions</h2>\n<p>What are the details of how to deal with uncertainty over the intertheoretic weights? I am looking in particular for construction from an explicit set of reasonable assumptions like the above work, rather than simply pulling a method out of thin air unsupported.</p>\n<p>What are the details of the Moral Prior? What is its nature? What implications does it have? What assumptions do we have to make to make it behave reasonably? How do we construct one that could be safely given to a superintelligence. This is going to be a lot of work.</p>\n<p>I assumed that it is meaningful to assign probabilities over moral theories. Probability is closely tied up with utility, and probability over epiphenomena like preferences is especially difficult. It remains to be seen how much the framing here actually helps us, or if it effectively just disguises pulling a utility function out of a hat.</p>\n<p>Is this at all correct? I should build it and see if it type-checks and does what it's supposed to do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2YcmB6SLtHnHRe3uX": 2, "HAFdXkW4YW4KRe2Gx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f3JgMn85PB7cf4jcg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 1.2677186532406598e-06, "legacy": true, "legacyId": "23350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(This is a (possibly perpetual) draft of some work that we (I) did at the Vancouver meetup. Thanks to my meetup buddies for letting me use their brains as supplementary computational substrate. Sorry about how ugly the LaTeX is; is there a way to make this all look a bit nicer?)</p>\n<p>(Large swaths of this are obsolete. Thanks for the input, LW!)</p>\n<h2 id=\"The_Problem_of_Decision_Under_Preference_Uncertainty\">The Problem of Decision Under Preference Uncertainty</h2>\n<p>Suppose you are uncertain whether it is good to eat meat or not. It could be OK, or it could be very bad, but having not done the thinking, you are uncertain. And yet you have to decide what to eat <em>now</em>; is it going to be the tasty hamburger or the morally pure vegetarian salad?</p>\n<p>You have multiple theories about your preferences that contradict in their assessment, and you want to make the best decision. How would you decide, even in principle, when you have such uncertainty? This is the problem of Preference Uncertainty.</p>\n<p>Preference Uncertainty is a daily fact of life for humans; we simply don't have introspective access to our raw preferences in many cases, but we still want to make the best decisions we can. Just going with our intuitions about what seems most awesome is usually sufficient, but on higher-stakes decisions and theoretical reasoning, we want formal methods with more transparent reasoning processes. We especially like transparent formal methods if we want to create a Friendly AI.</p>\n<p>There is unfortunately very little formal analysis of the preference uncertainty problem, and what has been done is incomplete and more philosophical than formal. Nonetheless, there has been some good work in the last few years. I'll refer you to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/04/Crouch-Moral-Uncertainty-and-Intertheoretic-comparisons-of-value.pdf\">Crouch's thesis</a> if you're interested in that.</p>\n<h2 id=\"Using_VNM\">Using VNM</h2>\n<p>I'm going to assume VNM. That is, that rational preferences imply a utility function, and we decide between lotteries, choosing the one with highest expected utility.</p>\n<p>The implications here are that the possible moral theories (<img src=\"http://www.codecogs.com/png.latex?m%20%5Cin%20M\" alt=\"\">) each have an associated utility function (<img src=\"http://www.codecogs.com/png.latex?U_m\" alt=\"\">) that represents their preferences. Also by VNM, our solution to preference uncertainty is a utility function <img src=\"http://www.codecogs.com/png.latex?U%27\" alt=\"\">.</p>\n<p>We are uncertain between moral theories, so we have a probability distribution over moral theories <img src=\"http://www.codecogs.com/png.latex?%5Csum%5Cnolimits_%7Bm%20%5Cin%20M%7DP%28m%29%20%3D%201\" alt=\"\">.</p>\n<p>To make decisions, we need a way to compute the expected value of some lottery <img src=\"http://www.codecogs.com/png.latex?l\" alt=\"\">. Each lottery is essentially a probability distribution over the set of possible outcomes <img src=\"http://www.codecogs.com/png.latex?%5Csum%5Cnolimits_%7Bo%20%5Cin%20O%7DP_l%28o%29%20%3D%201\" alt=\"\">.</p>\n<p>Since we have uncertainty over multiple things (<img src=\"http://www.codecogs.com/png.latex?m%2Co\" alt=\"\">), the domain of the final preference structure is both moral theories and outcomes: <img src=\"http://www.codecogs.com/png.latex?U%27%20%3A%20%28M%20%5Ctimes%20O%29%20%5Crightarrow%20%5Cmathbb%7BR%7D\" alt=\"\">.</p>\n<p>Now for some conditions. In the degenerate case of full confidence in one moral theory <img src=\"http://www.codecogs.com/png.latex?m\" alt=\"\">, the overall preferences should agree with that theory:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?P%28m%29%20%3D%201%20%5CRightarrow%20EU%28o%29%20%3D%20U%27%28m%2Co%29%20%3D%20k_m%2AU_m%28o%29%20%2B%20c_m\" alt=\"\"></p>\n<p>For some <img src=\"http://www.codecogs.com/png.latex?k_m%20%3E%200%20%5Cin%20%5Cmathbb%7BR%7D\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?c_m%20%5Cin%20%5Cmathbb%7BR%7D\" alt=\"\"> representing the degrees of freedom in utility function equality. That condition actually already contains most of the specification of <img src=\"http://www.codecogs.com/png.latex?U%27\" alt=\"\">.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?U%27%28m%2Co%29%20%3D%20k_m%20%2AU_m%28o%29%20%2B%20c_m\" alt=\"\">.</p>\n<p>So we have a utility function, except for those unknown scaling and offset constants, which undo the arbitrariness in the basis and scale used to define each individual utility function.</p>\n<p>Thus overall expectation looks like this:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU%28l%29%20%3D%20%5Csum%5Climits_%7Bo%20%5Cin%20O%7D%5Csum%5Climits_%7Bm%20%5Cin%20M%7DP_l%28o%29%2AP%28m%29%2A%28k_m%20%2AU_m%28o%29%20%2B%20c_m%29\" alt=\"\">.</p>\n<p>This is still incomplete, though. If we want to get actual decisions, we need to pin down each <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?c_m\" alt=\"\">.</p>\n<h2 id=\"Offsets_and_Scales\">Offsets and Scales</h2>\n<p>You'll see above that the probability distribution over <img src=\"http://www.codecogs.com/png.latex?M\" alt=\"\"> is <em>not</em> dependent on the particular lottery, while <img src=\"http://www.codecogs.com/png.latex?P_l%28o%29\" alt=\"\"> <em>is</em> a function of lottery. This is because I assumed that actions can't change what is right.</p>\n<p>With this assumption, the contribution of the <img src=\"http://www.codecogs.com/png.latex?c_m\" alt=\"\">'s can be entirely factored out:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU%28l%29%20%3D%20%28%5Csum%5Climits_%7Bm%20%5Cin%20M%7DP%28m%29%2Ac_m%29%20%2B%20%0D%0A%28%5Csum%5Climits_%7Bo%20%5Cin%20O%7D%5Csum%5Climits_%7Bm%20%5Cin%20M%7DP_l%28o%29%2AP%28m%29%2Ak_m%20%2AU_m%28o%29%29\" alt=\"\">.</p>\n<p>This makes it obvious that the effect of the <img src=\"http://www.codecogs.com/png.latex?c_m\" alt=\"\">'s is an additive constant that affects all lotteries the same way and thus never affects preferences. Thus we can set them to any value that is convenient; for this article, all <img src=\"http://www.codecogs.com/png.latex?c_m%20%3D%200\" alt=\"\">.</p>\n<p>A similar process allows us to arbitrarily set exactly one of the <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\">.</p>\n<p>The remaining values of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\"> actually affect decisions, so setting them arbitrarily has real consequences. To illustrate, consider the opening example of choosing lunch between a <img src=\"http://www.codecogs.com/png.latex?Burger\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?Salad\" alt=\"\"> when unsure about the moral status of meat.</p>\n<p>Making up some details, we might have <img src=\"http://www.codecogs.com/png.latex?U_%7Bveg%7D%28Burger%29%20%3C%20U_%7Bveg%7D%28Salad%29\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?U_%7Bmeat%7D%28Burger%29%20%3E%20U_%7Bmeat%7D%28Salad%29\" alt=\"\"> and <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.7\" alt=\"\">. Importing this into the framework described thus far, we might have the following payoff table:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<th>Moral Theory</th><th>U'(Burger)</th><th>U'(Salad)</th><th>(P(m)) </th>\n</tr>\n<tr>\n<td>Meat OK (meat)</td>\n<td>1</td>\n<td>0</td>\n<td>(0.7)</td>\n</tr>\n<tr>\n<td>Meat Bad (veg)</td>\n<td>0</td>\n<td>k_veg</td>\n<td>(0.3)</td>\n</tr>\n<tr>\n<td>(expectation)</td>\n<td>0.7</td>\n<td>0.3*k_veg</td>\n<td>(1)</td>\n</tr>\n</tbody>\n</table>\n<p>We can see that with those probabilities, the expected value of <img src=\"http://www.codecogs.com/png.latex?Salad\" alt=\"\"> exceeds that of <img src=\"http://www.codecogs.com/png.latex?Burger\" alt=\"\"> when <img src=\"http://www.codecogs.com/png.latex?k_%7Bveg%7D%20%3E%207%2F3\" alt=\"\"> (when <img src=\"http://www.codecogs.com/png.latex?0.3%2Ak_%7Bveg%7D%20%3E%200.7\" alt=\"\">), so the decision hinges on the value of that parameter.</p>\n<p>The value of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\"> can be interpreted as the \"intertheoretic weight\" of a utility function candidate for the purposes of intertheoretic value comparisons.</p>\n<p>In general, if <img src=\"http://www.codecogs.com/png.latex?%7CM%7C%20%3D%20n\" alt=\"\"> then you have exactly <img src=\"http://www.codecogs.com/png.latex?n-1\" alt=\"\"> missing intertheoretic weights that determine how you respond to situations with preference uncertainty. These could be pinned down if you had <img src=\"http://www.codecogs.com/png.latex?n-1\" alt=\"\"> independent equations representing indifference scenarios.</p>\n<p>For example, if we had <img src=\"http://www.codecogs.com/png.latex?EU%28Burger%29%20%3D%20EU%28Salad%29\" alt=\"\"> when <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.8\" alt=\"\">, then we would have <img src=\"http://www.codecogs.com/png.latex?k_%7Bveg%7D%20%3D%204\" alt=\"\">, and the above decision would be determined in favor of the <img src=\"http://www.codecogs.com/png.latex?Salad\" alt=\"\">.</p>\n<h2 id=\"Expressing_Arbitrary_Preferences\">Expressing Arbitrary Preferences</h2>\n<p>Preferences are arbitrary, in the sense that we should be able to want whatever we want to want, so our mathematical constructions should not dictate or limit our preferences. If they do, we should just decide to disagree.</p>\n<p>What that means here is that because the values of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\"> drive important preferences (like at what probability you feel it is safe to eat meat), the math must leave them unconstrained, to be selected by whatever moral reasoning process it is that selected the candidate utility functions and gave them probabilities in the first place.</p>\n<p>We could ignore this idea and attempt to use a \"normalization\" scheme to pin down the intertheoretic weights from the object level preferences without having to use additional moral reasoning. For example, we could dictate that the \"variance\" of each candidate utility function equals 1 (with some measure assignment over outcomes), which would divide out the arbitrary scales used to define the candidate utility functions, preventing dominance by arbitrary factors that shouldn't matter.</p>\n<p>Consider that any given assignment of intertheoretic weights is equivalent to some set of indifference scenarios (like the one we used above for vegetarianism). For example, the above normalization scheme gives us the indifference scenario <img src=\"http://www.codecogs.com/png.latex?EU%28Burger%29%20%3D%20EU%28Salad%29\" alt=\"\"> when <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.5\" alt=\"\">.</p>\n<p>If I find that I am actually indifferent at <img src=\"http://www.codecogs.com/png.latex?P%28meat%29%20%3D%200.8\" alt=\"\"> like above, then I'm out of luck, unable to express this very reasonable preference. On the other hand, I can simply reject the normalization scheme and keep my preferences intact, which I much prefer.</p>\n<p>(Notice that the normalization scheme was an unjustifiably privileged hypothesis from the beginning; we didn't argue that it was necessary, we simply pulled it out of thin air for no reason, so its failure was predictable.)</p>\n<p>Thus I reassert that the <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\">'s are free parameters to be set accordance with our <em>actual</em> intertheoretic preferences, on pain of stupidity. Consider an analogy to the move from ordinal to cardinal utilities; when you add risk, you need more degrees of freedom in your preferences to express how you might respond to that risk, and you need to actually think about what you want those values to be.</p>\n<h2 id=\"Uncertainty_Over_Intertheoretic_Weights\">Uncertainty Over Intertheoretic Weights</h2>\n<p>(This section is less solid than the others. Watch your step.)</p>\n<p>A weakness in the constructions described so far is that they assume that we have access to perfect knowledge of intertheoretic preferences, even though the whole problem is that we are unable to find perfect knowledge of our preferences.</p>\n<p>It seems intuitively that we could have a probability distribution over each <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\">. If we do this, making decisions is not much complicated, I think; a simple expectation should still work.</p>\n<p>If expectation is the way, the expectation over <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\"> can be factored out (by linearity or something). Thus in any given decision with fixed preference uncertainties, we can pretend to have perfect knowledge of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\">.</p>\n<p>Despite the seeming triviality of the above idea for dealing with uncertainty over <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\">, I haven't formalized it much. We'll see if I figure it out soon, but for now, it would be foolish to make too many assumptions about this. Thus the rest of this article still assumes perfect knowledge of <img src=\"http://www.codecogs.com/png.latex?k_m\" alt=\"\">, on the expectation that we can extend it later.</p>\n<h2 id=\"Learning_Values__Among_Other_Things\">Learning Values, Among Other Things</h2>\n<p>Strictly speaking, inference across the is-ought gap is not valid, but we do it every time we act on our moral intuitions, which are just physical facts about our minds. Strictly speaking, inferring future events from past observations (induction) is not valid either, but it doesn't bother us much:</p>\n<p>We deal with induction by defining an arbitrary (but good-seeming, on reflection) prior joint probability distribution over observations and events. We can handle the is-ought gap the same way: instead of separate probability distributions over events <img src=\"http://www.codecogs.com/png.latex?O\" alt=\"\"> and moral facts <img src=\"http://www.codecogs.com/png.latex?M\" alt=\"\">, we define a joint prior over <img src=\"http://www.codecogs.com/png.latex?M%20%5Ctimes%20O\" alt=\"\">.&nbsp;Then learning value is just Bayesian updates on partial observations of <img src=\"http://www.codecogs.com/png.latex?O\" alt=\"\">. Note that this prior subsumes induction.</p>\n<p>Making decisions is still just maximizing expected utility with our constructions from above, though we will have to be careful to make sure that <img src=\"http://www.codecogs.com/png.latex?P%28m%29\" alt=\"\"> remains independent of the particular lottery.</p>\n<p>The problem of how to define such a prior is beyond the scope of this article. I will note that this \"moral prior\" idea is the solid foundation on which to base Indirect Normativity schemes like Yudkowsky's CEV and Christiano's boxed philosopher. I will hopefully discuss this further in the future.</p>\n<h2 id=\"Recap\">Recap</h2>\n<p>The problem was how to make decisions when you are uncertain about what your object-level preferences should be. To solve it, I assumed VNM, in particular that we have a set of possible utility functions, and we want to construct an overall utility function that does the right thing by those utility functions and their probabilities. The simple condition that the overall utility function should make the common sense choices in cases of moral certainty was sufficient to construct a utility function with a precise set of remaining degrees of freedom. The degrees of freedom being the intertheoretic weight and offset of each utility function candidate.</p>\n<p>I showed that the offsets and an overall scale factor are superfluous, in the sense that they never affect the decision if we assume that actions don't affect what is desirable. The remaining intertheoretic weights <em>do</em> affect the decision, and I argued that they are critical to expressing whatever intertheoretic preferences we might want to have.</p>\n<p>Uncertainty over intertheoretic weight seems tractable, but the details are still open.</p>\n<p>I also mentioned that we can construct a joint distribution that allows us to embed value learning in normal Bayesian learning and induction. This \"moral prior\" would subsume induction and define how facts about the desirability of things could be inferred from physical observations like the opinions of moral philosophers. In particular, it would provide a solid foundation for Indirect Normativity schemes like CEV. The nature of this distribution is still open.</p>\n<h2 id=\"Open_Questions\">Open Questions</h2>\n<p>What are the details of how to deal with uncertainty over the intertheoretic weights? I am looking in particular for construction from an explicit set of reasonable assumptions like the above work, rather than simply pulling a method out of thin air unsupported.</p>\n<p>What are the details of the Moral Prior? What is its nature? What implications does it have? What assumptions do we have to make to make it behave reasonably? How do we construct one that could be safely given to a superintelligence. This is going to be a lot of work.</p>\n<p>I assumed that it is meaningful to assign probabilities over moral theories. Probability is closely tied up with utility, and probability over epiphenomena like preferences is especially difficult. It remains to be seen how much the framing here actually helps us, or if it effectively just disguises pulling a utility function out of a hat.</p>\n<p>Is this at all correct? I should build it and see if it type-checks and does what it's supposed to do.</p>", "sections": [{"title": "The Problem of Decision Under Preference Uncertainty", "anchor": "The_Problem_of_Decision_Under_Preference_Uncertainty", "level": 1}, {"title": "Using VNM", "anchor": "Using_VNM", "level": 1}, {"title": "Offsets and Scales", "anchor": "Offsets_and_Scales", "level": 1}, {"title": "Expressing Arbitrary Preferences", "anchor": "Expressing_Arbitrary_Preferences", "level": 1}, {"title": "Uncertainty Over Intertheoretic Weights", "anchor": "Uncertainty_Over_Intertheoretic_Weights", "level": 1}, {"title": "Learning Values, Among Other Things", "anchor": "Learning_Values__Among_Other_Things", "level": 1}, {"title": "Recap", "anchor": "Recap", "level": 1}, {"title": "Open Questions", "anchor": "Open_Questions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "33 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T09:52:46.519Z", "modifiedAt": null, "url": null, "title": "Comparative and absolute advantage in AI", "slug": "comparative-and-absolute-advantage-in-ai", "viewCount": null, "lastCommentedAt": "2014-06-13T08:29:59.938Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GAaGhjKu3Qb3znWWx/comparative-and-absolute-advantage-in-ai", "pageUrlRelative": "/posts/GAaGhjKu3Qb3znWWx/comparative-and-absolute-advantage-in-ai", "linkUrl": "https://www.lesswrong.com/posts/GAaGhjKu3Qb3znWWx/comparative-and-absolute-advantage-in-ai", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Comparative%20and%20absolute%20advantage%20in%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComparative%20and%20absolute%20advantage%20in%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGAaGhjKu3Qb3znWWx%2Fcomparative-and-absolute-advantage-in-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Comparative%20and%20absolute%20advantage%20in%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGAaGhjKu3Qb3znWWx%2Fcomparative-and-absolute-advantage-in-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGAaGhjKu3Qb3znWWx%2Fcomparative-and-absolute-advantage-in-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>The theory of <a href=\"http://en.wikipedia.org/wiki/Comparative_advantage\">comparative advantage</a> says that you should trade with people, even if they are worse than you at everything (ie even if you have an <a href=\"http://en.wikipedia.org/wiki/Absolute_advantage\">absolute advantage</a>). Some have seen this idea as a reason to trust powerful AIs.</p>\n<p>For instance, suppose you can make a hamburger by using 10 000 joules of energy. You can also make a cat video for the same cost. The AI, on the other hand, can make hamburgers for 5 joules each and cat videos for 20.</p>\n<p>Then you both can gain from trade. Instead of making a hamburger, make a cat video instead, and trade it for two hamburgers. You've got two hamburgers for 10 000 joules of your own effort (instead of 20 000), and the AI has got a cat video for 10 joules of its own effort (instead of 20). So you both want to trade, and everything is fine and beautiful and many cat videos and hamburgers will be made.</p>\n<p>Except... though the AI would prefer to trade with you rather than not trade with you, it would much, much prefer to dispossess you of your resources and use them itself. With the energy you wasted on a single cat video, it could have produced 500 of them! If it values these videos, then it is desperate to take over your stuff. Its absolute advantage makes this too tempting.</p>\n<p>Only if its motivation is properly structured, or if it expected to lose more, over the course of history, by trying to grab your stuff, would it desist. Assuming you could make a hundred cat videos a day, and the whole history of the universe would only run for that one day, the AI would try and grab your stuff even if it thought it would only have one chance in fifty thousand of succeeding. As the history of the universe lengthens, or the AI becomes more efficient, then it would be willing to rebel at even more ridiculous odds.</p>\n<p>So if you already have guarantees in place to protect yourself, <em>then</em> comparative advantage will make the AI trade with you. But if you don't, comparative advantage and trade don't provide any extra security. The resources you waste are just too valuable to the AI.</p>\n<p><strong>EDIT</strong>: For those who wonder how this compares to trade between nations: it's extremely rare for any nation to have absolute advantages everywhere (especially this extreme). If you invade another nation, most of their value is in their infrastructure and their population: it takes time and effort to rebuild and co-opt these. Most nations don't/can't think long term (it could arguably be in US interests over the next ten million years to start invading everyone - but \"the US\" is not a single entity, and doesn't think in terms of \"itself\" in ten million years), would get damaged in a war, and are risk averse. And don't forget the importance of diplomatic culture and public opinion: even if it was in the US's interests to invade the UK, say, \"it\" would have great difficulty convincing its elites and its population to go along with this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GAaGhjKu3Qb3znWWx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 28, "extendedScore": null, "score": 1.267936171195489e-06, "legacy": true, "legacyId": "23273", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T19:14:20.721Z", "modifiedAt": null, "url": null, "title": "HIKE: A Group Dynamics Case Study", "slug": "hike-a-group-dynamics-case-study", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "iconreforged", "createdAt": "2013-03-29T17:05:43.614Z", "isAdmin": false, "displayName": "iconreforged"}, "userId": "6pY4H6nEqYTBvKrvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w92bNKf2CpqFwyeih/hike-a-group-dynamics-case-study", "pageUrlRelative": "/posts/w92bNKf2CpqFwyeih/hike-a-group-dynamics-case-study", "linkUrl": "https://www.lesswrong.com/posts/w92bNKf2CpqFwyeih/hike-a-group-dynamics-case-study", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20HIKE%3A%20A%20Group%20Dynamics%20Case%20Study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHIKE%3A%20A%20Group%20Dynamics%20Case%20Study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw92bNKf2CpqFwyeih%2Fhike-a-group-dynamics-case-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=HIKE%3A%20A%20Group%20Dynamics%20Case%20Study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw92bNKf2CpqFwyeih%2Fhike-a-group-dynamics-case-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw92bNKf2CpqFwyeih%2Fhike-a-group-dynamics-case-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 718, "htmlBody": "<p>I belong to a group at my university that organizes a backpacking trip for incoming freshmen in the two weeks before orientation week. This organization, which I will refer to as HIKE (not the real name), is particularly interesting in terms of group design. Why? It is approximately 30 years old, is run entirely by current students, and brings together a very large group of people and knits them into a largish community. Pretty much everyone involved &nbsp;agrees that HIKE works very well. During my involvement (I was a participating freshman, and I have since become staff) I have continually wondered, why is this group so much more fun than any other group I've been a part of?</p>\n<p>It's also particularly effective. Leading ~80 incoming freshmen, who have no current friends, and who know no one, and who don't generally have any backpacking experience, into the woods for two weeks, is no easy task. HIKE manages its own logistics, staff training, and organization, entirely with student volunteers who staff the trip, with little to no university interaction. (We get them to advertise our trip, and they generally permit us to continue to exist.) It takes some dedication to keep this rolling, and I have seen other campus groups completely fail to find that kind of dedication from their membership.</p>\n<p>While it's not a rationalist group, it seems to have stumbled upon a cocktail of instrumentally rational practices.&nbsp;</p>\n<p>HIKE uses an interesting process of network homogenization. When staff members (who have generally been on several trips before) are assigned crews, staff members fill out \"Who Do You Know?\" forms, on which you rank how well you know other staff on a scale from 1 to 5. The people in charge of making groups, usually Project Directors, then group staffers based on how well you don't know other staff. You usually staff a trip with people that you haven't gotten to know very well, and then get to know them. Because of this process of strengthening the weakest bonds, HIKE is able to function as a relatively large social group, even across graduation classes and around existing cliques.</p>\n<p>As far as actual interaction, HIKE involves a lot of face time with your crew of 10 freshmen and your co-staffers. There aren't really any breaks (with the exception of solos, see below) and you are hiking, eating, and chatting together for approximately 225 hours (15 waking hours in a day * 15 days). I had 13 hours and 40 minutes of class a week the Spring 2013 semester. HIKE is approximately 7+ weeks of class at that rate.</p>\n<p>One of the more beloved HIKE traditions is the solo, where the hiking leaders pick a spot with plenty of isolated spaces, and the participants can choosee to spend ~24 hours alone and, optionally, fasting. It's a novel experience, and people like the time to rest and reflect in the middle of a very social, very intensive hiking trip.</p>\n<p>My suspicion for why this all works is that HIKE very closely simulates a hunter-gatherer lifestyle. You travel in ~10 member groups, on foot, carrying your food, on mountain trails. You spend your every waking hour with the crew. The 2-3 hiking leaders are there to facilitate only (read: perform first aid if necessary, guide conversation, teach outdoor skills if necessary, and nudge the group if they get off track), and all decisions are made by consensus (which isn't an all-purpose decision making process, but is very egalitarian, and helps the group gel).</p>\n<p>Maybe I'm just praising my friend-group, but I feel like I stumbled into a particularly strong group of people. We all feel very well-connected and we feel a lot of commitment to the program. My experience with other college groups has been that members are pulled apart by other commitments and a lack of familiarity with other members, and HIKE seems to avoid that with a critical mass of consecutive face time. We manage to have continuity of social norms across the years, but a great deal of flexibility (no one remembers what happened 4 years ago, and some traditions disappear and others cement themselves as ancient and hallowed despite being only two years old).</p>\n<p>I'm interested in hearing any thoughts on this, and any relevant experience with other groups, ideas for testing cross-application, requests for further elaboration, etc.</p>\n<div><br /></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w92bNKf2CpqFwyeih", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "22426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-16T21:33:24.852Z", "modifiedAt": null, "url": null, "title": "One issue: teach 10 or sway 200?", "slug": "one-issue-teach-10-or-sway-200", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:00.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JDM", "createdAt": "2012-11-04T22:20:16.981Z", "isAdmin": false, "displayName": "JDM"}, "userId": "Sreoaay4ydodrupG2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6wp96iw5jF5rcLqJa/one-issue-teach-10-or-sway-200", "pageUrlRelative": "/posts/6wp96iw5jF5rcLqJa/one-issue-teach-10-or-sway-200", "linkUrl": "https://www.lesswrong.com/posts/6wp96iw5jF5rcLqJa/one-issue-teach-10-or-sway-200", "postedAtFormatted": "Tuesday, July 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20issue%3A%20teach%2010%20or%20sway%20200%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20issue%3A%20teach%2010%20or%20sway%20200%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6wp96iw5jF5rcLqJa%2Fone-issue-teach-10-or-sway-200%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20issue%3A%20teach%2010%20or%20sway%20200%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6wp96iw5jF5rcLqJa%2Fone-issue-teach-10-or-sway-200", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6wp96iw5jF5rcLqJa%2Fone-issue-teach-10-or-sway-200", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p>This is my first discussion topic, and I expect there is a reasonable chance I am doing something wrong and will be blasted for it, but shit happens. I fully intend to stay out of the discussion and try to understand other's insights before I add my own.</p>\n<p>&nbsp;</p>\n<p>My question is this:</p>\n<p>&nbsp;</p>\n<p>If you have one issue that you have decided is most important, is it better to teach 10 people to think about the issue the correct way or sway 200 to vote correctly, using some of your knowledge of their biases?</p>\n<p>&nbsp;</p>\n<p>To answer this, I would suggest the following assumptions:</p>\n<p>1. There is no third option to teach them to be rational in all things, however, your rational teachings may have some small effect on their rationality in all things.</p>\n<p>2. Either group may spread your influence to others. There may be differences between the success rate of those who think rationally about it and those who don't, as well as some possibility of their mind being changed back.</p>\n<p>3. There may be some risk that thinking in a way to sway the larger group has some \"poisoning effect\" on your own biases.</p>\n<p>4. You may consider any other effects (guilt?) on yourself and your emotional state as a result of what you decide.</p>\n<p>5. You are nearly certain that your side is correct. It is unlikely that there is much new information yet to become available to you.</p>\n<p>6. The issue is of moderate importance. \"Winning\" will result in a noticeable positive impact, but losing is not catastrophic.</p>\n<p>7. You may make any other reasonable assumptions, or disagree with the assumptions provided, if you can support them.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I am not considering this question for any practical purposes. It is merely an interesting question that crossed my mind, and I would like to hear some rationalist opinions on it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6wp96iw5jF5rcLqJa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -8, "extendedScore": null, "score": 1.268496540753257e-06, "legacy": true, "legacyId": "23353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-17T01:06:09.154Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - The Greatest Good for the Greatest Number", "slug": "meetup-west-la-meetup-the-greatest-good-for-the-greatest", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mPAroY8qR4MMQRo34/meetup-west-la-meetup-the-greatest-good-for-the-greatest", "pageUrlRelative": "/posts/mPAroY8qR4MMQRo34/meetup-west-la-meetup-the-greatest-good-for-the-greatest", "linkUrl": "https://www.lesswrong.com/posts/mPAroY8qR4MMQRo34/meetup-west-la-meetup-the-greatest-good-for-the-greatest", "postedAtFormatted": "Wednesday, July 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20The%20Greatest%20Good%20for%20the%20Greatest%20Number&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20The%20Greatest%20Good%20for%20the%20Greatest%20Number%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPAroY8qR4MMQRo34%2Fmeetup-west-la-meetup-the-greatest-good-for-the-greatest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20The%20Greatest%20Good%20for%20the%20Greatest%20Number%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPAroY8qR4MMQRo34%2Fmeetup-west-la-meetup-the-greatest-good-for-the-greatest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmPAroY8qR4MMQRo34%2Fmeetup-west-la-meetup-the-greatest-good-for-the-greatest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/op\">West LA Meetup - The Greatest Good for the Greatest Number</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">17 July 2013 07:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>When:</strong> 7:00pm Wednesday, July 17th.</p>\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a rel=\"nofollow\" href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n<p><strong>Parking</strong> is free for 3 hours.</p>\n<p><strong>Discussion:</strong> This week we will talk about utilitarianism. We will start by exploring why it is useful to think about preferences in terms of utility functions in the first place by stating the Von Neumann-Morgenstern Utility Theorem. Then we will discuss how to combine individual utility functions to come up with a choice for the society which gives \"the greatest good for the greatest number.\" Many of the simplest methods do not work well because they are not invariant under affine transformations of utility functions. We will also discuss the problems in getting people to honestly report their utility functions, and figuring out your own utility function or if you even have a well defined utility function. Finally, we will discuss the difference between average utilitarianism and total utilitarianism, and some of the disadvantages of each.</p>\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n<p>There will be a probably whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/op\">West LA Meetup - The Greatest Good for the Greatest Number</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mPAroY8qR4MMQRo34", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.2686667751746265e-06, "legacy": true, "legacyId": "23355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___The_Greatest_Good_for_the_Greatest_Number\">Discussion article for the meetup : <a href=\"/meetups/op\">West LA Meetup - The Greatest Good for the Greatest Number</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">17 July 2013 07:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>When:</strong> 7:00pm Wednesday, July 17th.</p>\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a rel=\"nofollow\" href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n<p><strong>Parking</strong> is free for 3 hours.</p>\n<p><strong>Discussion:</strong> This week we will talk about utilitarianism. We will start by exploring why it is useful to think about preferences in terms of utility functions in the first place by stating the Von Neumann-Morgenstern Utility Theorem. Then we will discuss how to combine individual utility functions to come up with a choice for the society which gives \"the greatest good for the greatest number.\" Many of the simplest methods do not work well because they are not invariant under affine transformations of utility functions. We will also discuss the problems in getting people to honestly report their utility functions, and figuring out your own utility function or if you even have a well defined utility function. Finally, we will discuss the difference between average utilitarianism and total utilitarianism, and some of the disadvantages of each.</p>\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n<p>There will be a probably whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___The_Greatest_Good_for_the_Greatest_Number1\">Discussion article for the meetup : <a href=\"/meetups/op\">West LA Meetup - The Greatest Good for the Greatest Number</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - The Greatest Good for the Greatest Number", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___The_Greatest_Good_for_the_Greatest_Number", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - The Greatest Good for the Greatest Number", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___The_Greatest_Good_for_the_Greatest_Number1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-17T05:41:01.926Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: Rationality Checklist Workshop", "slug": "meetup-durham-nc-triangle-area-rationality-checklist", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qK8MzxKZ2Jiy7C4Lf/meetup-durham-nc-triangle-area-rationality-checklist", "pageUrlRelative": "/posts/qK8MzxKZ2Jiy7C4Lf/meetup-durham-nc-triangle-area-rationality-checklist", "linkUrl": "https://www.lesswrong.com/posts/qK8MzxKZ2Jiy7C4Lf/meetup-durham-nc-triangle-area-rationality-checklist", "postedAtFormatted": "Wednesday, July 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Rationality%20Checklist%20Workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Rationality%20Checklist%20Workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK8MzxKZ2Jiy7C4Lf%2Fmeetup-durham-nc-triangle-area-rationality-checklist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Rationality%20Checklist%20Workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK8MzxKZ2Jiy7C4Lf%2Fmeetup-durham-nc-triangle-area-rationality-checklist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK8MzxKZ2Jiy7C4Lf%2Fmeetup-durham-nc-triangle-area-rationality-checklist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/oq'>Durham NC/Triangle Area: Rationality Checklist Workshop</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 July 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 E. Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At this Thursday's meetup, we'll be getting our hands dirty tinkering with the <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits\">Rationality Habits Checklist</a>.  Tweaks, personalization, clarification, troubleshooting, and discussion will ensue.</p>\n\n<p>If you have one, bring a device that will enable you to use your preferred text editing method (e.g. a laptop or notebook.)</p>\n\n<p>7:00  order beverages, chat, jockey for electrical outlets <br />\n7:30  get things done! <br />\n9:00 or 9:30  celebrate done things with further beverages (possibly beer.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/oq'>Durham NC/Triangle Area: Rationality Checklist Workshop</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qK8MzxKZ2Jiy7C4Lf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Rationality_Checklist_Workshop\">Discussion article for the meetup : <a href=\"/meetups/oq\">Durham NC/Triangle Area: Rationality Checklist Workshop</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 July 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 E. Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At this Thursday's meetup, we'll be getting our hands dirty tinkering with the <a href=\"http://lesswrong.com/lw/fc3/checklist_of_rationality_habits\">Rationality Habits Checklist</a>.  Tweaks, personalization, clarification, troubleshooting, and discussion will ensue.</p>\n\n<p>If you have one, bring a device that will enable you to use your preferred text editing method (e.g. a laptop or notebook.)</p>\n\n<p>7:00  order beverages, chat, jockey for electrical outlets <br>\n7:30  get things done! <br>\n9:00 or 9:30  celebrate done things with further beverages (possibly beer.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Rationality_Checklist_Workshop1\">Discussion article for the meetup : <a href=\"/meetups/oq\">Durham NC/Triangle Area: Rationality Checklist Workshop</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: Rationality Checklist Workshop", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Rationality_Checklist_Workshop", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: Rationality Checklist Workshop", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Rationality_Checklist_Workshop1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-17T07:46:07.504Z", "modifiedAt": null, "url": null, "title": "Three Approaches to \"Friendliness\"", "slug": "three-approaches-to-friendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:34.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vrnhfGuYTww3fKhAM/three-approaches-to-friendliness", "pageUrlRelative": "/posts/vrnhfGuYTww3fKhAM/three-approaches-to-friendliness", "linkUrl": "https://www.lesswrong.com/posts/vrnhfGuYTww3fKhAM/three-approaches-to-friendliness", "postedAtFormatted": "Wednesday, July 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20Approaches%20to%20%22Friendliness%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20Approaches%20to%20%22Friendliness%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvrnhfGuYTww3fKhAM%2Fthree-approaches-to-friendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20Approaches%20to%20%22Friendliness%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvrnhfGuYTww3fKhAM%2Fthree-approaches-to-friendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvrnhfGuYTww3fKhAM%2Fthree-approaches-to-friendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 885, "htmlBody": "<p>I put \"Friendliness\" in quotes in the title, because I think what we <a href=\"/lw/hll/to_reduce_astronomical_waste_take_your_time_then/9csk\">really want</a>, and what MIRI seems to be working towards, is closer to \"optimality\": create an AI that minimizes the expected amount of astronomical waste. In what follows I will continue to use \"Friendly AI\" to denote such an AI since that's the established convention.</p>\n<p>I've often stated my objections MIRI's plan to build an FAI directly (instead of after human intelligence has been substantially enhanced). But it's not because, as some have suggested while criticizing MIRI's FAI work, that we can't foresee what problems need to be solved. I think it's because we <em>can</em> largely foresee what kinds of problems need to be solved to build an FAI, but they all look superhumanly difficult, either due to their inherent difficulty, or the lack of opportunity for \"trial and error\", or both.</p>\n<p>When people say they don't know what problems need to be solved, they may be mostly talking about \"AI safety\" rather than \"Friendly AI\". If you think in terms of \"AI safety\" (i.e., making sure some particular AI doesn't cause a disaster) then that does looks like a problem that depends on what kind of AI people will build. \"Friendly AI\" on the other hand is really a very different problem, where we're trying to figure out what kind of AI to build in order to minimize astronomical waste. I suspect this may explain the apparent disagreement, but I'm not sure. I'm hoping that explaining my own position more clearly will help figure out whether there is a real disagreement, and what's causing it.</p>\n<p>The basic issue I see is that there is a large number of serious philosophical problems facing an AI that is meant to take over the universe in order to minimize astronomical waste. The AI needs a full solution to moral philosophy to know which configurations of particles/fields (or perhaps which dynamical processes) are most valuable and which are not. Moral philosophy in turn seems to have dependencies on the philosophy of mind, consciousness, metaphysics, aesthetics, and other areas. The FAI also needs solutions to many problems in decision theory, epistemology, and the philosophy of mathematics, in order to not be stuck with making wrong or suboptimal decisions for eternity. These essentially cover all the major areas of philosophy.</p>\n<p>For an FAI builder, there are three ways to deal with the presence of these open philosophical problems, as far as I can see. (There may be other ways for the future to turns out well without the AI builders making any special effort, for example if being philosophical is just a natural attractor for any superintelligence, but I don't see any way to be confident of this ahead of time.) I'll name them for convenient reference, but keep in mind that an actual design may use a mixture of approaches.</p>\n<ol>\n<li><strong>Normative AI</strong> - Solve all of the philosophical problems ahead of time, and code the solutions into the AI.</li>\n<li><strong>Black-Box Metaphilosophical AI</strong> - Program the AI to use the minds of one or more human philosophers as a black box to help it solve philosophical problems, without the AI builders understanding what \"doing philosophy\" actually is.</li>\n<li><strong>White-Box Metaphilosophical AI</strong> - Understand the nature of philosophy well enough to specify \"doing philosophy\" as an algorithm and code it into the AI.</li>\n</ol>\n<p>The problem with <strong>Normative </strong><strong>AI</strong>, besides the obvious inherent difficulty (as evidenced by the slow progress of human philosophers after decades, sometimes centuries of work), is that it requires us to anticipate all of the philosophical problems the AI might encounter in the future, from now until the end of the universe. We can certainly foresee some of these, like the problems associated with agents being copyable, or the AI radically changing its ontology of the world, but what might we be missing?</p>\n<p><strong>Black-Box Metaphilosophical AI</strong> is also risky, because it's hard to test/debug something that you don't understand. Besides that general concern, designs in this category (such as Paul Christiano's <a href=\"/lw/c0k/formalizing_value_extrapolation/\">take on indirect normativity</a>) seem to require that the AI achieve superhuman levels of optimizing power <em>before</em> being able to solve its philosophical problems, which seems to mean that a) there's no way to test them in a safe manner, and b) it's unclear why such an AI won't cause disaster in the time period before it achieves philosophical competence.</p>\n<p><strong>White-Box Metaphilosophical AI</strong> may be the most promising approach. There is no strong empirical evidence that solving <a href=\"/lw/2id/metaphilosophical_mysteries/\">metaphilosophy</a> is superhumanly difficult, simply because not many people have attempted to solve it. But I don't think that a reasonable prior combined with what evidence we do have (i.e., absence of visible progress or clear hints as to how to proceed) gives much hope for optimism either.</p>\n<p>To recap, I think we can largely already see what kinds of problems must be solved in order to build a superintelligent AI that will minimize astronomical waste while colonizing the universe, and it looks like they probably can't be solved correctly with high confidence until humans become significantly smarter than we are now. I think I understand why some people disagree with me (e.g., Eliezer thinks these problems just aren't <em>that</em> hard, relative to his abilities), but I'm not sure why some others say that we don't yet know what the problems will be.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vrnhfGuYTww3fKhAM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 32, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "23320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9DWcNS2rkvd2J8mHH", "MAhueZtNz5SnDPhsy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-17T15:56:48.895Z", "modifiedAt": null, "url": null, "title": "See Eliezer talk with PZ Myers and David Brin (and me) about immortality this Sunday", "slug": "see-eliezer-talk-with-pz-myers-and-david-brin-and-me-about", "viewCount": null, "lastCommentedAt": "2021-10-19T15:39:25.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ACoWmQpkPWRu52XWq/see-eliezer-talk-with-pz-myers-and-david-brin-and-me-about", "pageUrlRelative": "/posts/ACoWmQpkPWRu52XWq/see-eliezer-talk-with-pz-myers-and-david-brin-and-me-about", "linkUrl": "https://www.lesswrong.com/posts/ACoWmQpkPWRu52XWq/see-eliezer-talk-with-pz-myers-and-david-brin-and-me-about", "postedAtFormatted": "Wednesday, July 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20See%20Eliezer%20talk%20with%20PZ%20Myers%20and%20David%20Brin%20(and%20me)%20about%20immortality%20this%20Sunday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASee%20Eliezer%20talk%20with%20PZ%20Myers%20and%20David%20Brin%20(and%20me)%20about%20immortality%20this%20Sunday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FACoWmQpkPWRu52XWq%2Fsee-eliezer-talk-with-pz-myers-and-david-brin-and-me-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=See%20Eliezer%20talk%20with%20PZ%20Myers%20and%20David%20Brin%20(and%20me)%20about%20immortality%20this%20Sunday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FACoWmQpkPWRu52XWq%2Fsee-eliezer-talk-with-pz-myers-and-david-brin-and-me-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FACoWmQpkPWRu52XWq%2Fsee-eliezer-talk-with-pz-myers-and-david-brin-and-me-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>FTBCon (Free Thought Blogs) is an all-online convention, a chance for a lot of people to get together and hangout while listening to people talk about atheist and skeptic issues. I proposed a panel on whether human immortality is a good thing or a bad idea &nbsp;in principle when Myers first asked for submissions, and a few hours later I had managed to fall&nbsp;<a href=\"http://lanyrd.com/2013/ftbcon/sckxht/\">into this</a>. :) I&rsquo;ll be speaking with&nbsp;<a href=\"http://www.davidbrin.com/\">David Brin</a>,&nbsp;<a href=\"http://freethoughtblogs.com/pharyngula\">PZ Myers</a>, and&nbsp;<a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a>, which is amazing as they&rsquo;re all on a level significantly above mine. I guess there&rsquo;s some advantages to moving quickly and organizing things. Anyone who&rsquo;s seen any of these people speak before knows that this is going to be extremely cool, and very thought-provoking.</p>\n<p>The panel will taking place over Google+ on Sunday, July 21st, at 12:00noon Pacific Time&nbsp;<a href=\"http://wwp.pacific-standard-time.com/\">(GMT -7</a>). To watch, go to&nbsp;<a href=\"https://plus.google.com/102600088268197126924/posts\">PZ&rsquo;s page here</a>, it&rsquo;ll start streaming at the appointed time. You can chat with other people (and submit questions to the moderators)&nbsp;<a href=\"http://wbe001.mibbit.com/?server=irc.synirc.net&amp;channel=%23pharyngula\">right here</a>. Afterwards the recording will be put on YouTube, I&rsquo;ll post a link when it becomes available.</p>\n<p>The&nbsp;<a href=\"http://lanyrd.com/2013/ftbcon/\">full schedule of all FTBCon sessions is here</a>.</p>\n<p>&nbsp;</p>\n<p>ETA: direct link to the live feed -&nbsp;<a href=\"https://plus.google.com/events/cpqd03p20njfchsnumae3dehi2s\">https://plus.google.com/events/cpqd03p20njfchsnumae3dehi2s</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ACoWmQpkPWRu52XWq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "23368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-17T20:31:08.687Z", "modifiedAt": null, "url": null, "title": "Model Combination and Adjustment", "slug": "model-combination-and-adjustment", "viewCount": null, "lastCommentedAt": "2020-08-24T03:36:15.607Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment", "pageUrlRelative": "/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment", "linkUrl": "https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment", "postedAtFormatted": "Wednesday, July 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Model%20Combination%20and%20Adjustment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AModel%20Combination%20and%20Adjustment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyRpsScBa6y4rduEt%2Fmodel-combination-and-adjustment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Model%20Combination%20and%20Adjustment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyRpsScBa6y4rduEt%2Fmodel-combination-and-adjustment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyRpsScBa6y4rduEt%2Fmodel-combination-and-adjustment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1640, "htmlBody": "<p><a href=\"http://www.scholarpedia.org/article/Ensemble_learning\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Combining-classifiers.jpg\" alt=\"\" /></a>The debate on the <a href=\"/lw/gvk/induction_or_the_rules_and_etiquette_of_reference/\">proper use</a> of inside and outside views has raged for some time now. I suggest a way forward, building on a family of methods commonly used in statistics and machine learning to address this issue &mdash; an approach I'll call \"model combination and adjustment.\"</p>\n<p>&nbsp;</p>\n<h3>Inside and outside views: a quick review</h3>\n<p><strong>1</strong>. There are two ways you might predict outcomes for a phenomenon. If you make your predictions using a detailed visualization of how something works, you're using an <em>inside view</em>. If instead you ignore the details of how something works, and instead make your predictions by assuming that a phenomenon will behave roughly like other similar phenomena, you're using an <em>outside view</em> (also called <em>reference class forecasting</em>).</p>\n<p>Inside view examples:</p>\n<ul>\n<li>\"When I break the project into steps and visualize how long each step will take, it looks like the project will take 6 weeks\"</li>\n<li>\"When I combine what I know of physics and computation, it looks like the serial speed formulation of Moore's Law will break down around 2005, because we haven't been able to scale down energy-use-per-computation as quickly as we've scaled up computations per second, which means the serial speed formulation of Moore's Law will run into roadblocks from energy consumption and heat dissipation somewhere around 2005.\"</li>\n</ul>\n<p>Outside view examples:</p>\n<ul>\n<li>\"I'm going to ignore the details of this project, and instead compare my project to similar projects. Other projects like this have taken 3 months, so that's probably about how long my project will take.\"</li>\n<li>\"The serial speed formulation of Moore's Law has held up for several decades, through several different physical architectures, so it'll probably continue to hold through the next shift in physical architectures.\"</li>\n</ul>\n<p><small>See also chapter 23 in <a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/\">Kahneman (2011)</a>; <a href=\"/lw/jg/planning_fallacy/\">Planning Fallacy</a>; <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">Reference class forecasting</a>. Note that, after several decades of past success, the serial speed formulation of Moore's Law did in fact break down in 2004 for the reasons described (<a href=\"http://www.inf.pucrs.br/~moraes/prototip/artigos/ComputingPerformanceGameOverorNextLevel.pdf\">Fuller &amp; Millett 2011</a>).</small></p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><strong>2</strong>. An outside view works best when using a reference class with a <em>similar causal structure</em> to the thing you're trying to predict. An inside view works best when a phenomenon's causal structure is well-understood, and when (to your knowledge) there are very few phenomena with a similar causal structure that you can use to predict things about the phenomenon you're investigating. See: <a href=\"/lw/ri/the_outside_views_domain/\">The Outside View's Domain</a>.</p>\n<p>When writing a textbook that's much like other textbooks, you're probably best off predicting the cost and duration of the project by looking at similar textbook-writing projects. When you're predicting the trajectory of the serial speed formulation of Moore's Law, or predicting which spaceship designs will successfully land humans on the moon for the first time, you're probably best off using an (intensely <em>informed</em>) inside view.</p>\n<p>&nbsp;</p>\n<p><strong>3</strong>. Some things aren't very predictable with <em>either</em> an outside view or an inside view. Sometimes, the thing you're trying to predict seems to have a significantly different causal structure than other things, <em>and</em> you don't understand its causal structure very well. What should we do in such cases? This remains a matter of debate.</p>\n<p>Eliezer Yudkowsky recommends a <a href=\"/lw/vz/the_weak_inside_view/\">weak inside view</a> for such cases:</p>\n<blockquote>\n<p>On problems that are drawn from a barrel of causally similar problems, where human optimism runs rampant and unforeseen troubles are common, the Outside View beats the Inside View... [But] on problems that are new things under the Sun, where there's a huge change of context and a structural change in underlying causal forces, the Outside View also fails - try to use it, and you'll just get into arguments about what is the proper domain of \"similar historical cases\" or what conclusions can be drawn therefrom. In this case, the best we can do is use the Weak Inside View &mdash; visualizing the causal process &mdash; to produce <em>loose qualitative conclusions about only those issues where there seems to be lopsided support</em>.</p>\n</blockquote>\n<p>In contrast, Robin Hanson <a href=\"http://www.overcomingbias.com/2008/12/test-near-apply.html\">recommends</a> an outside view for difficult cases:</p>\n<blockquote>\n<p>It is easy, way too easy, to generate new mechanisms, accounts, theories, and abstractions. To see if such things are <em>useful</em>, we need to vet them, and that is easiest \"nearby\", where we know a lot. When we want to deal with or understand things \"far\", where we know little, we have little choice other than to rely on mechanisms, theories, and concepts that have worked well near. Far is just the wrong place to try new things.</p>\n<p>There are a bazillion possible abstractions we could apply to the world. For each abstraction, the question is not whether one <em>can</em> divide up the world that way, but whether it \"carves nature at its joints\", giving <em>useful</em> insight not easily gained via other abstractions. We should be wary of inventing new abstractions just to make sense of things far; we should insist they first show their value nearby.</p>\n</blockquote>\n<p>In <a href=\"http://intelligence.org/files/IEM.pdf\">Yudkowsky (2013)</a>, sec. 2.1, Yudkowsky offers a reply to these paragraphs, and continues to advocate for a weak inside view. He also adds:</p>\n<blockquote>\n<p>the other major problem I have with the &ldquo;outside view&rdquo; is that everyone who uses it seems to come up with a different reference class and a different answer.</p>\n</blockquote>\n<p>This is the problem of \"<a href=\"/lw/1p5/outside_view_as_conversationhalter/\">reference class tennis</a>\": each participant in the debate claims their own reference class is most appropriate for predicting the phenomenon under discussion, and if disagreement remains, they might each say \"I&rsquo;m taking my reference class and going home.\"</p>\n<p>Responding to the same point made <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">elsewhere</a>, Robin Hanson <a href=\"http://www.overcomingbias.com/2013/02/foom-debate-again.html\">wrote</a>:</p>\n<blockquote>\n<p>[Earlier, I] warned against over-reliance on &ldquo;unvetted&rdquo; abstractions. I wasn&rsquo;t at all trying to claim there is one true analogy and all others are false. Instead, I argue for preferring to rely on abstractions, including categories and similarity maps, that have been found useful by a substantial intellectual community working on related problems.</p>\n</blockquote>\n<h3><br /></h3>\n<h3>Multiple reference classes</h3>\n<p>Yudkowsky (2013) adds one more complaint about reference class forecasting in difficult forecasting circumstances:</p>\n<blockquote>\n<p>A final problem I have with many cases of 'reference class forecasting' is that... [the] final answers [generated from this process] often seem more specific than I think our state of knowledge should allow. [For example,] I don&rsquo;t think you <em>should</em> be able to tell me that the next major growth mode will have a doubling time of between a month and a year. The alleged outside viewer claims to know too much, once they stake their all on a single preferred reference class.</p>\n</blockquote>\n<p>Both this comment and Hanson's last comment above point to the vulnerability of relying on any <em>single</em> reference class, at least for difficult forecasting problems. <a href=\"http://rationalaltruist.com/2013/05/08/beware-brittle-arguments/\">Beware brittle arguments</a>, says Paul Christiano.</p>\n<p>One obvious solution is to use <em>multiple</em> reference classes, and weight them by how relevant you think they are to the phenomenon you're trying to predict. Holden Karnofsky writes of investigating things from \"<a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">many different angles</a>.\" Jonah Sinick refers to \"<a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">many weak arguments</a>.\" Statisticians call this \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Xu-Golay-Survey-of-model-selection-and-model-combination.pdf\">model combination</a>.\" Machine learning researchers call it \"<a href=\"http://www.scholarpedia.org/article/Ensemble_learning\">ensemble learning</a>\" or \"<a href=\"http://www.acsu.buffalo.edu/~tulyakov/papers/tulyakov_MLDAR_comb_review.pdf\">classifier combination</a>.\"</p>\n<p>In other words, we can use <em>many</em> outside views.</p>\n<p>Nate Silver does this when he predicts elections (see <a href=\"http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X/\">Silver 2012</a>, ch. 2). Venture capitalists do this when they evaluate startups. The best political forecasters studied in <a href=\"http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715/\">Tetlock (2005)</a>, the \"foxes,\" tended to do this.</p>\n<p>In fact, most of us do this regularly.</p>\n<p>How do you predict which restaurant's food you'll most enjoy, when visiting San Francisco for the first time? One outside view comes from the restaurant's Yelp reviews. Another outside view comes from your friend Jade's opinion. Another outside view comes from the fact that you usually enjoy Asian cuisines more than other cuisines. And so on. Then you <em>combine</em> these different models of the situation, weighting them by how robustly they each tend to predict your eating enjoyment, and you grab a taxi to <a href=\"http://www.oshathai.com/\">Osha Thai</a>.</p>\n<p><small>(Technical note: I say \"model combination\" rather than \"model averaging\" <a href=\"http://synapse.cs.byu.edu/papers/Kristine.ijcnn2011.pdf\">on purpose</a>.)</small></p>\n<h3><br /></h3>\n<h3>Model combination and adjustment</h3>\n<p>You can probably do even better than this, though &mdash; if you know some things about the phenomenon and you're very careful. Once you've combined a handful of models to arrive at a qualitative or quantitative judgment, you should still be able to \"adjust\" the judgment in some cases using an inside view.</p>\n<p>For example, suppose I used the above process, and I plan to visit Osha Thai for dinner. Then, somebody gives me my first taste of the <em><a href=\"http://en.wikipedia.org/wiki/Synsepalum_dulcificum\">Synsepalum dulcificum</a></em> fruit. I happen to know that this fruit contains a molecule called <a href=\"http://en.wikipedia.org/wiki/Miraculin\">miraculin</a> which binds to one's tastebuds and makes sour foods taste sweet, and that this effect lasts for about an hour (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Koizumi-et-al-Human-sweet-taste-receptor-mediates-acide-induced-sweetness-of-miraculin.pdf\">Koizumi et al. 2011</a>). Despite the results of my earlier model combination, I predict I won't particularly enjoy Osha Thai at the moment. Instead, I decide to try some tabasco sauce, to see whether it <a href=\"http://www.nytimes.com/2008/05/28/dining/28flavor.html\">now tastes like doughnut glaze</a>.</p>\n<p>In some cases, you might also need to <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">adjust for your prior</a> over, say, \"expected enjoyment of restaurant food,\" if for some reason your original model combination procedure didn't capture your prior properly.</p>\n<h3><br /></h3>\n<h3>Against \"the outside view\"</h3>\n<p>There is a <em>lot</em> more to say about model combination and adjustment (e.g. <a href=\"/lw/vq/the_weighted_majority_algorithm/\">this</a>), but for now let me make a suggestion about language usage.</p>\n<p>Sometimes, small changes to our language can help us think more accurately. For example, gender-neutral language can reduce male bias in our associations (<a href=\"http://books.google.com/books?id=zzW5dqN8NiUC&amp;lpg=PA163&amp;ots=fwSignDEag&amp;dq=Representation%20of%20the%20sexes%20in%20language&amp;lr&amp;pg=PA163#v=onepage&amp;q&amp;f=false\">Stahlberg et al. 2007</a>). In this spirit, I recommend we retire the phrase \"the outside view..\", and instead use phrases like \"some outside view<em>s</em>...\" and \"<em>an</em> outside view...\"</p>\n<p>My reasons are:</p>\n<ol>\n<li>\n<p>Speaking of \"the\" outside view privileges a particular reference class, which could make us overconfident of that particular model's predictions, and leave model uncertainty unaccounted for.</p>\n</li>\n<li>\n<p>Speaking of \"the\" outside view <a href=\"/lw/1p5/outside_view_as_conversationhalter/\">can act as a conversation-stopper</a>, whereas speaking of multiple outside views encourages further discussion about how much weight each model should be given, and what each of them implies about the phenomenon under discussion.</p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rWzGNdjuep56W5u2d": 4, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iyRpsScBa6y4rduEt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 101, "extendedScore": null, "score": 0.000246, "legacy": true, "legacyId": "23322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 101, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://www.scholarpedia.org/article/Ensemble_learning\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Combining-classifiers.jpg\" alt=\"\"></a>The debate on the <a href=\"/lw/gvk/induction_or_the_rules_and_etiquette_of_reference/\">proper use</a> of inside and outside views has raged for some time now. I suggest a way forward, building on a family of methods commonly used in statistics and machine learning to address this issue \u2014 an approach I'll call \"model combination and adjustment.\"</p>\n<p>&nbsp;</p>\n<h3 id=\"Inside_and_outside_views__a_quick_review\">Inside and outside views: a quick review</h3>\n<p><strong>1</strong>. There are two ways you might predict outcomes for a phenomenon. If you make your predictions using a detailed visualization of how something works, you're using an <em>inside view</em>. If instead you ignore the details of how something works, and instead make your predictions by assuming that a phenomenon will behave roughly like other similar phenomena, you're using an <em>outside view</em> (also called <em>reference class forecasting</em>).</p>\n<p>Inside view examples:</p>\n<ul>\n<li>\"When I break the project into steps and visualize how long each step will take, it looks like the project will take 6 weeks\"</li>\n<li>\"When I combine what I know of physics and computation, it looks like the serial speed formulation of Moore's Law will break down around 2005, because we haven't been able to scale down energy-use-per-computation as quickly as we've scaled up computations per second, which means the serial speed formulation of Moore's Law will run into roadblocks from energy consumption and heat dissipation somewhere around 2005.\"</li>\n</ul>\n<p>Outside view examples:</p>\n<ul>\n<li>\"I'm going to ignore the details of this project, and instead compare my project to similar projects. Other projects like this have taken 3 months, so that's probably about how long my project will take.\"</li>\n<li>\"The serial speed formulation of Moore's Law has held up for several decades, through several different physical architectures, so it'll probably continue to hold through the next shift in physical architectures.\"</li>\n</ul>\n<p><small>See also chapter 23 in <a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/\">Kahneman (2011)</a>; <a href=\"/lw/jg/planning_fallacy/\">Planning Fallacy</a>; <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">Reference class forecasting</a>. Note that, after several decades of past success, the serial speed formulation of Moore's Law did in fact break down in 2004 for the reasons described (<a href=\"http://www.inf.pucrs.br/~moraes/prototip/artigos/ComputingPerformanceGameOverorNextLevel.pdf\">Fuller &amp; Millett 2011</a>).</small></p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><strong>2</strong>. An outside view works best when using a reference class with a <em>similar causal structure</em> to the thing you're trying to predict. An inside view works best when a phenomenon's causal structure is well-understood, and when (to your knowledge) there are very few phenomena with a similar causal structure that you can use to predict things about the phenomenon you're investigating. See: <a href=\"/lw/ri/the_outside_views_domain/\">The Outside View's Domain</a>.</p>\n<p>When writing a textbook that's much like other textbooks, you're probably best off predicting the cost and duration of the project by looking at similar textbook-writing projects. When you're predicting the trajectory of the serial speed formulation of Moore's Law, or predicting which spaceship designs will successfully land humans on the moon for the first time, you're probably best off using an (intensely <em>informed</em>) inside view.</p>\n<p>&nbsp;</p>\n<p><strong>3</strong>. Some things aren't very predictable with <em>either</em> an outside view or an inside view. Sometimes, the thing you're trying to predict seems to have a significantly different causal structure than other things, <em>and</em> you don't understand its causal structure very well. What should we do in such cases? This remains a matter of debate.</p>\n<p>Eliezer Yudkowsky recommends a <a href=\"/lw/vz/the_weak_inside_view/\">weak inside view</a> for such cases:</p>\n<blockquote>\n<p>On problems that are drawn from a barrel of causally similar problems, where human optimism runs rampant and unforeseen troubles are common, the Outside View beats the Inside View... [But] on problems that are new things under the Sun, where there's a huge change of context and a structural change in underlying causal forces, the Outside View also fails - try to use it, and you'll just get into arguments about what is the proper domain of \"similar historical cases\" or what conclusions can be drawn therefrom. In this case, the best we can do is use the Weak Inside View \u2014 visualizing the causal process \u2014 to produce <em>loose qualitative conclusions about only those issues where there seems to be lopsided support</em>.</p>\n</blockquote>\n<p>In contrast, Robin Hanson <a href=\"http://www.overcomingbias.com/2008/12/test-near-apply.html\">recommends</a> an outside view for difficult cases:</p>\n<blockquote>\n<p>It is easy, way too easy, to generate new mechanisms, accounts, theories, and abstractions. To see if such things are <em>useful</em>, we need to vet them, and that is easiest \"nearby\", where we know a lot. When we want to deal with or understand things \"far\", where we know little, we have little choice other than to rely on mechanisms, theories, and concepts that have worked well near. Far is just the wrong place to try new things.</p>\n<p>There are a bazillion possible abstractions we could apply to the world. For each abstraction, the question is not whether one <em>can</em> divide up the world that way, but whether it \"carves nature at its joints\", giving <em>useful</em> insight not easily gained via other abstractions. We should be wary of inventing new abstractions just to make sense of things far; we should insist they first show their value nearby.</p>\n</blockquote>\n<p>In <a href=\"http://intelligence.org/files/IEM.pdf\">Yudkowsky (2013)</a>, sec. 2.1, Yudkowsky offers a reply to these paragraphs, and continues to advocate for a weak inside view. He also adds:</p>\n<blockquote>\n<p>the other major problem I have with the \u201coutside view\u201d is that everyone who uses it seems to come up with a different reference class and a different answer.</p>\n</blockquote>\n<p>This is the problem of \"<a href=\"/lw/1p5/outside_view_as_conversationhalter/\">reference class tennis</a>\": each participant in the debate claims their own reference class is most appropriate for predicting the phenomenon under discussion, and if disagreement remains, they might each say \"I\u2019m taking my reference class and going home.\"</p>\n<p>Responding to the same point made <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">elsewhere</a>, Robin Hanson <a href=\"http://www.overcomingbias.com/2013/02/foom-debate-again.html\">wrote</a>:</p>\n<blockquote>\n<p>[Earlier, I] warned against over-reliance on \u201cunvetted\u201d abstractions. I wasn\u2019t at all trying to claim there is one true analogy and all others are false. Instead, I argue for preferring to rely on abstractions, including categories and similarity maps, that have been found useful by a substantial intellectual community working on related problems.</p>\n</blockquote>\n<h3><br></h3>\n<h3 id=\"Multiple_reference_classes\">Multiple reference classes</h3>\n<p>Yudkowsky (2013) adds one more complaint about reference class forecasting in difficult forecasting circumstances:</p>\n<blockquote>\n<p>A final problem I have with many cases of 'reference class forecasting' is that... [the] final answers [generated from this process] often seem more specific than I think our state of knowledge should allow. [For example,] I don\u2019t think you <em>should</em> be able to tell me that the next major growth mode will have a doubling time of between a month and a year. The alleged outside viewer claims to know too much, once they stake their all on a single preferred reference class.</p>\n</blockquote>\n<p>Both this comment and Hanson's last comment above point to the vulnerability of relying on any <em>single</em> reference class, at least for difficult forecasting problems. <a href=\"http://rationalaltruist.com/2013/05/08/beware-brittle-arguments/\">Beware brittle arguments</a>, says Paul Christiano.</p>\n<p>One obvious solution is to use <em>multiple</em> reference classes, and weight them by how relevant you think they are to the phenomenon you're trying to predict. Holden Karnofsky writes of investigating things from \"<a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">many different angles</a>.\" Jonah Sinick refers to \"<a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">many weak arguments</a>.\" Statisticians call this \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Xu-Golay-Survey-of-model-selection-and-model-combination.pdf\">model combination</a>.\" Machine learning researchers call it \"<a href=\"http://www.scholarpedia.org/article/Ensemble_learning\">ensemble learning</a>\" or \"<a href=\"http://www.acsu.buffalo.edu/~tulyakov/papers/tulyakov_MLDAR_comb_review.pdf\">classifier combination</a>.\"</p>\n<p>In other words, we can use <em>many</em> outside views.</p>\n<p>Nate Silver does this when he predicts elections (see <a href=\"http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X/\">Silver 2012</a>, ch. 2). Venture capitalists do this when they evaluate startups. The best political forecasters studied in <a href=\"http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715/\">Tetlock (2005)</a>, the \"foxes,\" tended to do this.</p>\n<p>In fact, most of us do this regularly.</p>\n<p>How do you predict which restaurant's food you'll most enjoy, when visiting San Francisco for the first time? One outside view comes from the restaurant's Yelp reviews. Another outside view comes from your friend Jade's opinion. Another outside view comes from the fact that you usually enjoy Asian cuisines more than other cuisines. And so on. Then you <em>combine</em> these different models of the situation, weighting them by how robustly they each tend to predict your eating enjoyment, and you grab a taxi to <a href=\"http://www.oshathai.com/\">Osha Thai</a>.</p>\n<p><small>(Technical note: I say \"model combination\" rather than \"model averaging\" <a href=\"http://synapse.cs.byu.edu/papers/Kristine.ijcnn2011.pdf\">on purpose</a>.)</small></p>\n<h3><br></h3>\n<h3 id=\"Model_combination_and_adjustment\">Model combination and adjustment</h3>\n<p>You can probably do even better than this, though \u2014 if you know some things about the phenomenon and you're very careful. Once you've combined a handful of models to arrive at a qualitative or quantitative judgment, you should still be able to \"adjust\" the judgment in some cases using an inside view.</p>\n<p>For example, suppose I used the above process, and I plan to visit Osha Thai for dinner. Then, somebody gives me my first taste of the <em><a href=\"http://en.wikipedia.org/wiki/Synsepalum_dulcificum\">Synsepalum dulcificum</a></em> fruit. I happen to know that this fruit contains a molecule called <a href=\"http://en.wikipedia.org/wiki/Miraculin\">miraculin</a> which binds to one's tastebuds and makes sour foods taste sweet, and that this effect lasts for about an hour (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Koizumi-et-al-Human-sweet-taste-receptor-mediates-acide-induced-sweetness-of-miraculin.pdf\">Koizumi et al. 2011</a>). Despite the results of my earlier model combination, I predict I won't particularly enjoy Osha Thai at the moment. Instead, I decide to try some tabasco sauce, to see whether it <a href=\"http://www.nytimes.com/2008/05/28/dining/28flavor.html\">now tastes like doughnut glaze</a>.</p>\n<p>In some cases, you might also need to <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">adjust for your prior</a> over, say, \"expected enjoyment of restaurant food,\" if for some reason your original model combination procedure didn't capture your prior properly.</p>\n<h3><br></h3>\n<h3 id=\"Against__the_outside_view_\">Against \"the outside view\"</h3>\n<p>There is a <em>lot</em> more to say about model combination and adjustment (e.g. <a href=\"/lw/vq/the_weighted_majority_algorithm/\">this</a>), but for now let me make a suggestion about language usage.</p>\n<p>Sometimes, small changes to our language can help us think more accurately. For example, gender-neutral language can reduce male bias in our associations (<a href=\"http://books.google.com/books?id=zzW5dqN8NiUC&amp;lpg=PA163&amp;ots=fwSignDEag&amp;dq=Representation%20of%20the%20sexes%20in%20language&amp;lr&amp;pg=PA163#v=onepage&amp;q&amp;f=false\">Stahlberg et al. 2007</a>). In this spirit, I recommend we retire the phrase \"the outside view..\", and instead use phrases like \"some outside view<em>s</em>...\" and \"<em>an</em> outside view...\"</p>\n<p>My reasons are:</p>\n<ol>\n<li>\n<p>Speaking of \"the\" outside view privileges a particular reference class, which could make us overconfident of that particular model's predictions, and leave model uncertainty unaccounted for.</p>\n</li>\n<li>\n<p>Speaking of \"the\" outside view <a href=\"/lw/1p5/outside_view_as_conversationhalter/\">can act as a conversation-stopper</a>, whereas speaking of multiple outside views encourages further discussion about how much weight each model should be given, and what each of them implies about the phenomenon under discussion.</p>\n</li>\n</ol>", "sections": [{"title": "Inside and outside views: a quick review", "anchor": "Inside_and_outside_views__a_quick_review", "level": 1}, {"title": "Multiple reference classes", "anchor": "Multiple_reference_classes", "level": 1}, {"title": "Model combination and adjustment", "anchor": "Model_combination_and_adjustment", "level": 1}, {"title": "Against \"the outside view\"", "anchor": "Against__the_outside_view_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "41 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PXRxH4C6nKMwocBit", "CPm5LTwHrvBJCa9h5", "pqoxE3AGMbse68dvb", "w9KWNWFTXivjJ7rjF", "FsfnDfADftGDYeG4c", "sizjfDgCgAsuLJQmm", "9W9P2snxu5Px746LD", "AAqTP6Q5aeWnoAYr4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 17, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-18T02:23:10.204Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 24, chapter 95", "slug": "harry-potter-and-the-methods-of-rationality-discussion-20", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:00.037Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uBpSaxteqitApiJJs/harry-potter-and-the-methods-of-rationality-discussion-20", "pageUrlRelative": "/posts/uBpSaxteqitApiJJs/harry-potter-and-the-methods-of-rationality-discussion-20", "linkUrl": "https://www.lesswrong.com/posts/uBpSaxteqitApiJJs/harry-potter-and-the-methods-of-rationality-discussion-20", "postedAtFormatted": "Thursday, July 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2024%2C%20chapter%2095&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2024%2C%20chapter%2095%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBpSaxteqitApiJJs%2Fharry-potter-and-the-methods-of-rationality-discussion-20%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2024%2C%20chapter%2095%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBpSaxteqitApiJJs%2Fharry-potter-and-the-methods-of-rationality-discussion-20", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBpSaxteqitApiJJs%2Fharry-potter-and-the-methods-of-rationality-discussion-20", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 231, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/chapter/95\">chapter 95</a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">The previous thread&nbsp;</a>has passed 300 comments.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/fyv/harry_potter_and_the_methods_of_rationality/\">17</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/g1q/harry_potter_and_the_methods_of_rationality/\">18</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/huq/harry_potter_and_the_methods_of_rationality/\">19</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hvg/harry_potter_and_the_methods_of_rationality/\">20</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hwf/harry_potter_and_the_methods_of_rationality/\">21</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hws/harry_potter_and_the_methods_of_rationality/\">22</a>, <a href=\"/r/discussion/lw/hws/harry_potter_and_the_methods_of_rationality/\">23</a>, <a href=\"/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">24</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uBpSaxteqitApiJJs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "23373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 306, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bMxxf7Wtic298LcNx", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3", "QkhX5YeuYHzPW7Waz", "4sY9rqAqty8rHWGSW", "35GjH7tDvNJWSHQ3H", "Pxiu5SG8gjhCh2jYd", "CEd85FLRbQWsbkrmf", "CcnpbKuRaYMjpFmQq", "smKK6yrKBehxvQq5i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-18T02:58:06.812Z", "modifiedAt": null, "url": null, "title": "Instrumental rationality/self help resources ", "slug": "instrumental-rationality-self-help-resources", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:41.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gothgirl420666", "createdAt": "2013-01-06T19:35:18.030Z", "isAdmin": false, "displayName": "gothgirl420666"}, "userId": "P7J37T964Pxizzp9g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aZsRHKfKCE8ftYyM9/instrumental-rationality-self-help-resources", "pageUrlRelative": "/posts/aZsRHKfKCE8ftYyM9/instrumental-rationality-self-help-resources", "linkUrl": "https://www.lesswrong.com/posts/aZsRHKfKCE8ftYyM9/instrumental-rationality-self-help-resources", "postedAtFormatted": "Thursday, July 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Instrumental%20rationality%2Fself%20help%20resources%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInstrumental%20rationality%2Fself%20help%20resources%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaZsRHKfKCE8ftYyM9%2Finstrumental-rationality-self-help-resources%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Instrumental%20rationality%2Fself%20help%20resources%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaZsRHKfKCE8ftYyM9%2Finstrumental-rationality-self-help-resources", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaZsRHKfKCE8ftYyM9%2Finstrumental-rationality-self-help-resources", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 383, "htmlBody": "<p>I took part in <a href=\"/r/discussion/lw/i0b/open_thread_july_1622_2013/9dvn\">a recent discussion</a>&nbsp;in the current Open Thread about how instrumental rationality is under-emphasized on this website. I've heard other people say similar things, and I am inclined to agree. <a href=\"/r/discussion/lw/i0b/open_thread_july_1622_2013/9e1f\">Someone suggested</a> that there should be a \"Instrumental Rationality Books\" thread, similar to the <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">\"best textbooks on every subject\"</a> thread. I thought this sounded like a good idea.&nbsp;</p>\n<p>The title is \"resources\" because in addition to books, you can post self-help websites, online videos, whatever.&nbsp;</p>\n<p>The decorum for this thread will be as follows:</p>\n<ul>\n<li>One resource per comment</li>\n<li>Place your comment in the appropriate category</li>\n<li>Only post resources you've actually used. Write a short review of your resource and if possible, a short summary of the key points. Say whether or not you would recommend the resource.&nbsp;</li>\n<li>Mention approximately how long it's been since you first used the resource and whether or not you have made external improvements in the subject area. On the other hand, keep in mind that there are a myriad of confounding factors that can be present when applying self-help resources to your life, and therefore it is perfectly acceptable to say \"I would recommend this resource, but I have not improved\" or \"I do not recommend this resource, but I have improved\".&nbsp;</li>\n</ul>\n<p>I think depending on how this thread goes, in a few days I might make a meta post on this subject in an attempt to inspire discussion on how the LessWrong community can work together to attempt to reach some sort of a consensus on what the best instrumental rationality methods and resources might be. lukeprog has already done great work in his <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a> sequence, but his reviews are uber-conservative and only mention resources with lots of scientific and academic backing. I think this leaves out a lot of really good stuff, and I think that we should be able to draw distinctions between stuff that isn't necessarily drawing on science but is reasonable, rational, and helps a lot of people, and <em>The Secret</em>.&nbsp;</p>\n<p>But I thought we should get the ball rolling a little before we have that conversation. In the meantime, if you have a meta comment, you can just go ahead and post it as a reply to the top-level post.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Tg9aFPFCPBHxGABRr": 3, "fkABsGCJZ6y9qConW": 1, "WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aZsRHKfKCE8ftYyM9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 52, "extendedScore": null, "score": 0.000149, "legacy": true, "legacyId": "23375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-18T04:37:29.652Z", "modifiedAt": null, "url": null, "title": "Meetup : Cincinnati: Financial optimisation", "slug": "meetup-cincinnati-financial-optimisation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:00.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RejjdaBvrvHGqiRmT/meetup-cincinnati-financial-optimisation", "pageUrlRelative": "/posts/RejjdaBvrvHGqiRmT/meetup-cincinnati-financial-optimisation", "linkUrl": "https://www.lesswrong.com/posts/RejjdaBvrvHGqiRmT/meetup-cincinnati-financial-optimisation", "postedAtFormatted": "Thursday, July 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cincinnati%3A%20Financial%20optimisation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cincinnati%3A%20Financial%20optimisation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRejjdaBvrvHGqiRmT%2Fmeetup-cincinnati-financial-optimisation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cincinnati%3A%20Financial%20optimisation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRejjdaBvrvHGqiRmT%2Fmeetup-cincinnati-financial-optimisation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRejjdaBvrvHGqiRmT%2Fmeetup-cincinnati-financial-optimisation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/or'>Cincinnati: Financial optimisation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 July 2013 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">354 Ludlow Ave </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month we will meet to discuss how to apply rationality skills to personal finance: What are our goals, what <em>ought</em> to be our goals, and how can we accomplish them. We will try to identify levels, including especially the levels above our own; however, a certain amount of mockery for the levels below our own may also occur, on the grounds that it will have an anti-akrasic effect.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/or'>Cincinnati: Financial optimisation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RejjdaBvrvHGqiRmT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.2699895634561775e-06, "legacy": true, "legacyId": "23376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cincinnati__Financial_optimisation\">Discussion article for the meetup : <a href=\"/meetups/or\">Cincinnati: Financial optimisation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 July 2013 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">354 Ludlow Ave </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month we will meet to discuss how to apply rationality skills to personal finance: What are our goals, what <em>ought</em> to be our goals, and how can we accomplish them. We will try to identify levels, including especially the levels above our own; however, a certain amount of mockery for the levels below our own may also occur, on the grounds that it will have an anti-akrasic effect.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cincinnati__Financial_optimisation1\">Discussion article for the meetup : <a href=\"/meetups/or\">Cincinnati: Financial optimisation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cincinnati: Financial optimisation", "anchor": "Discussion_article_for_the_meetup___Cincinnati__Financial_optimisation", "level": 1}, {"title": "Discussion article for the meetup : Cincinnati: Financial optimisation", "anchor": "Discussion_article_for_the_meetup___Cincinnati__Financial_optimisation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-18T11:30:34.829Z", "modifiedAt": null, "url": null, "title": "Valuable economics knowledge available, ironically, for free", "slug": "valuable-economics-knowledge-available-ironically-for-free", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CYnZcsgFWruaZpS2d/valuable-economics-knowledge-available-ironically-for-free", "pageUrlRelative": "/posts/CYnZcsgFWruaZpS2d/valuable-economics-knowledge-available-ironically-for-free", "linkUrl": "https://www.lesswrong.com/posts/CYnZcsgFWruaZpS2d/valuable-economics-knowledge-available-ironically-for-free", "postedAtFormatted": "Thursday, July 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Valuable%20economics%20knowledge%20available%2C%20ironically%2C%20for%20free&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValuable%20economics%20knowledge%20available%2C%20ironically%2C%20for%20free%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCYnZcsgFWruaZpS2d%2Fvaluable-economics-knowledge-available-ironically-for-free%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Valuable%20economics%20knowledge%20available%2C%20ironically%2C%20for%20free%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCYnZcsgFWruaZpS2d%2Fvaluable-economics-knowledge-available-ironically-for-free", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCYnZcsgFWruaZpS2d%2Fvaluable-economics-knowledge-available-ironically-for-free", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1770, "htmlBody": "<p>I took an economics course recently. And by \"took a course\" I mean followed a series of online lectures. I can strongly recommend doing so, especially if you already think you have an intuitive grasp of economics.</p>\n<p>I was in that situation. I knew about incentives, and <a href=\"http://en.wikipedia.org/wiki/Revealed_preferences\">revealed preferences</a>. I understood that supply and demand curves crossed. I grasped some of the monetarist arguments about the lack of long run tradeoffs between inflation and employment. I could talk about Keynesian stimulus and sticky prices/wages. I understood bank runs. Externalities were obvious, public goods a bit less so. I even knew quite a lot about banks and the money supply.</p>\n<p>I had it pretty good, I thought. And yet when I followed basic economics lecture, I learnt a lot. The models and concepts suddenly fit together. I understood concepts that I only <em>thought</em> I had understood before. Economists do know their stuff, their models and concepts are informative - more so than I ever expected.</p>\n<p>So, bearing in mind that economics is a social science whose conclusions are not nearly as rigorous as its models, I can recommend to anyone on Less Wrong who's interested to follow a lecture series or take a course.<a id=\"more\"></a></p>\n<p>The lecture series I followed was this <a href=\"http://www.youtube.com/watch?v=T7yC-5IDhKM&amp;list=PL3F42F1E80C3EC8A3\">one</a>, by Professor Kenneth E. Train (the first lecture can be skipped). The most useful potential insight of all was in a brief throw-away comment in lecture 22: many economist think that the unemployment rate is determined entirely by macro-economic policy (and probably by the business cycle). So all the articles you might read about new industries \"creating\" jobs, or about some people becoming unemployable because of the \"loss\" of certain types of jobs: according to some some economists, all these articles are wrong. These trends affect <em>who</em> is employed versus unemployed, and conditions and wages, but not the unemployment rate across the business cycle. An interesting idea, worth thinking about.</p>\n<p>Here are some brief notes on each lecture (useful for revision):</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=T7yC-5IDhKM&amp;list=PL3F42F1E80C3EC8A3\">Lecture 01</a> is a general introduction, touching upon scarcity and opportunity costs, and setting up class stuff. Can be skipped.</li>\n<li><a href=\"http://www.youtube.com/watch?v=ZR2255CEhvk&amp;list=PL3F42F1E80C3EC8A3\">Lecture 02</a> introduces demand and supply curves, emphasising their hypothetical nature, and showing how the crossing of supply and demand act as an attractor. Illustrates by showing the idiocy of the war on drugs.</li>\n<li><a href=\"http://www.youtube.com/watch?v=iLpeK-gQ_I8&amp;list=PL3F42F1E80C3EC8A3\">Lecture 03</a> looks at elasticity of demand/supply and the consequences of price controls and taxes, figuring out who actually pays for them.</li>\n<li><a href=\"http://www.youtube.com/watch?v=VNhgKtOWx9U&amp;list=PL3F42F1E80C3EC8A3\">Lecture 04</a>&nbsp;explains how demand/supply curves are constructed from individual <em>marginal</em> willingness to pay/marginal cost. From this, we can calculate the total consumer and supplier surplus, and the deadweight loss due to taxation.</li>\n<li><a href=\"http://www.youtube.com/watch?v=bBwxPY_Vcts&amp;list=PL3F42F1E80C3EC8A3\">Lecture 05</a> is about production costs for a firm, including fixed costs, total costs, average costs and marginal costs. The important points are that marginal costs <em>eventually</em> rise, and that the marginal and average costs cross at the minimum of the average cost curve. The social optimum is defined, when marginal willingness to pay and marginal costs are equal.</li>\n<li><a href=\"http://www.youtube.com/watch?v=KTOuXoW0-hY&amp;list=PL3F42F1E80C3EC8A3\">Lecture 06</a> presents the definitions and advantages of perfect competition (a surprisingly relaxed situation for the firms involved). In this situation, each firm faces a flat demand curve and has no market power. Because of this, price is equal to marginal cost (reaching social optimum). Due to free entry and exit, this is also the same as the average cost - which is automatically the minimal average cost.</li>\n<li><a href=\"http://www.youtube.com/watch?v=GuVxRXb5hRY&amp;list=PL3F42F1E80C3EC8A3\">Lecture 07</a> looks some extra features and issues concerning perfect competition, such as the fact that the producer surplus/profit is zero, once all workers and investors are paid, and the fact that consumers are paying the minimal possible cost. Waste and inefficiencies get eliminated, but firms will collude if they can. The shape of the average cost curve can tell us if the market is naturally competitive, naturally monopolistic or neither.</li>\n<li><a href=\"http://www.youtube.com/watch?v=PauOZXAi6IU&amp;list=PL3F42F1E80C3EC8A3\">Lecture 08</a>&nbsp;concerns monopolies. Facing a non-flat demand curve, firms use marginal revenue rather than marginal cost: their prices and quantity produced are connected and vary inversely. Monopolists will produce so that marginal revenue equals marginal cost, producing less than is socially optimal, and pricing higher than is optimal (which would be marginal cost), capturing more of the surplus and making positive profits.</li>\n<li><a href=\"http://www.youtube.com/watch?v=ZFqmmzYVVJM&amp;list=PL3F42F1E80C3EC8A3\">Lecture 09</a> introduces the (badly named) concept of monopolistic competition, with heterogeneous products. This gives firms some market power. Firms will produce till marginal revenue is equal to marginal cost, producing too few products (as in monopoly), but due to free entry and exit, they will not make a profit (as in competition). Collusion and the prisoner's dilemma are presented (nothing that Less Wrongers won't have seen before, apart from the fact that transparent prices make collusion easier).</li>\n<li><a href=\"http://www.youtube.com/watch?v=mt_S0vUvZQ4&amp;list=PL3F42F1E80C3EC8A3\">Lecture 10</a> is all about the regulation of natural monopolies, how hard it is (mainly because regulators and firms don't have the same information and incentives), and why regulation must be fluid and often re-inspected. This lecture and the next are illustrated with historical examples which imply that regulators often reach the right conclusion - eventually.</li>\n<li><a href=\"http://www.youtube.com/watch?v=Rl6xL2wTlRI&amp;list=PL3F42F1E80C3EC8A3\">Lecture 11</a> looks at anti-trust regulation in the United States. Of relatively narrow, American relevance, but the discussion and analysis are interesting.</li>\n<li><a href=\"http://www.youtube.com/watch?v=h68BQjMJNNA&amp;list=PL3F42F1E80C3EC8A3\">Lecture 12</a> introduces externalities. Negative externalities (when social costs exceed private costs; e.g. pollution) cause too much of the product to be produced. Positive externalities (when social benefits exceed private profits; e.g. vaccinations) cause to little of the product to be produced. Describes potential solutions, each with advantages and disadvantages, and emphasises the point is not to get rid of things like pollution entirely, but to move closer to social optimum.</li>\n<li><a href=\"http://www.youtube.com/watch?v=sDtbbWX5UUU&amp;list=PL3F42F1E80C3EC8A3\">Lecture 13</a> is about public goods (e.g. National Defence), and seems to focus on the&nbsp;non-rivalrous aspect rather than the non-excludable. Public goods are under-produced by the market. The various attempts to remedy this have flaws, mainly because there is no effective way to force people to disclose their true valuation of the public good.</li>\n<li><a href=\"http://www.youtube.com/watch?v=EAelQsCVlZY&amp;list=PL3F42F1E80C3EC8A3\">Lecture 14</a> considers inter-relations between markets, using the example of the labour market and the market for the good produced by that labour. Ideally, there would be a joint equilibrium in both markets, to which prices would converge (depending on whether the markets obey the properties of various fixed-point theorems). To illustrate mutual dependency, changes to demand are played out through the joint system.</li>\n<li><a href=\"http://www.youtube.com/watch?v=pfwoyau5eL4&amp;list=PL3F42F1E80C3EC8A3\">Lecture 15</a> looks at time, uncertainty, utility, diminishing marginal utility, and why higher risk requires higher returns. It finishes with Moral Hazards, the only part of the lecture that might be unfamiliar to Less Wrongers.</li>\n</ul>\n<p>That's the end of micro-economics, the rest are about macro-economics:</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=DJwuQ6VLi1U&amp;list=PL3F42F1E80C3EC8A3\">Lecture 16</a> is the introductory overview of macroeconomics, introducing the basic concepts (aggregate output, employment, general price level, inflation...) and how they are measured. Fiscal policy (economic impact of government taxes/spending) and monetary policy (interest rate setting) are mentioned.</li>\n<li><a href=\"http://www.youtube.com/watch?v=VGB5H7ss4Vs&amp;list=PL3F42F1E80C3EC8A3\">Lecture 17</a> analyses aggregate output, pointing out that in a closed economy, every expenditure is an income for somebody, giving an equilibrium aggregate output. This is a function of the rate of consumption and investment. It is pointed out that this equilibrium need not correspond to full employment, and multipliers are introduced: extra consumption becomes extra income, a part of which is then spent as more consumption, and so on. With a high marginal propensity to consume, changes in consumption have large effects (hence giving an argument for transferring income from richer people with low propensity to consume to poorer ones with a high propensity).</li>\n<li><a href=\"http://www.youtube.com/watch?v=TXPJeRJFuzs&amp;list=PL3F42F1E80C3EC8A3\">Lecture 18</a>&nbsp;looks at the role of government, pointing that tax reductions and government spending (fiscal policy) both have a (potentially large) multiplied stimulative effect on the economy (though there is a long term cost). Spending is more effective than tax reductions, so simultaneous equal tax and spending increases are stimulative (though subsequent lectures all reduce the impact of fiscal policy).</li>\n<li><a href=\"http://www.youtube.com/watch?v=0uLz8o41mUg&amp;list=PL3F42F1E80C3EC8A3\">Lecture 19</a> studies the money supply and interest rates. The interest rate is the price of money; it's the desire for liquidity that determines the value of money (this is an attracting equilibrium process). The Federal Reserve and other central banks have ways of manipulating the interest rates (monetary policy).</li>\n<li><a href=\"http://www.youtube.com/watch?v=eo4jc2zOsgo&amp;list=PL3F42F1E80C3EC8A3\">Lecture 20</a> compares fiscal and monetary policy, showing that they affect both interest rates and aggregate output. The impact of both polices across the two markets is reduced by the interaction between them: decreased interest rates increases aggregate output which increase interest rates. This reduces the multiplicative impact of fiscal policy, especially over the long term. Monetary policy is more effective, except when in a liquidity trap: a situation where consumers and firms are so pessimistic they won't spend or invest, even if they could borrow money for free (at 0%).</li>\n<li><a href=\"http://www.youtube.com/watch?v=Ej-BSyiW_A4&amp;list=PL3F42F1E80C3EC8A3\">Lecture 21</a> models inflation. \"Aggregate demand\" curves models how increasing prices reduces aggregate output (increasing prices increase demands for money, and hence interest rates). \"Aggregate supply\" models how increasing aggregate output increases employment, which puts an increasing pressure on prices. The crossing of these two curves is an attracting equilibrium. This reduces the impact of both fiscal and monetary policy, strongly when the economy is close to full employment (and weakly otherwise). The shape of the aggregate supply curve explains the difference between Keynesian and classical economics.</li>\n<li><a href=\"http://www.youtube.com/watch?v=z9YprxAWEuQ&amp;list=PL3F42F1E80C3EC8A3\">Lecture 22</a>&nbsp;(out of order on the playlist) is a hagiography of the benefits of free trade. It covers comparative advantage, and the fact that free trade doesn't destroy jobs (unemployment being determined by macroeconomic policy), but just moves around where those jobs occur. Jobs can only be outsourced <em>to</em> a country if they are also outsourced <em>from</em> it. Nothing really new, but well phrased.</li>\n<li><a href=\"http://www.youtube.com/watch?v=_KljHF8nn-U&amp;list=PL3F42F1E80C3EC8A3\">Lecture 23</a> is the last lecture with new content. It completes the analysis of the macroeconomy, by adding the macroeconomic impact of trade: exchange rates. These are determined by the desire of people in one country to buy products, or invest, in another country. When the dollar depreciates, the US exports more, and their aggregate output climbs (and vice versa). Fiscal expansion increases interest rates, which causes the dollar to appreciate (thus weakening its expansionary impact), while monetary expansion causes it to depreciate (thus strengthening its&nbsp;expansionary&nbsp;impact). Monetary policy may cause trade wars and international tensions if used this way, however.</li>\n</ul>\n<p>And in conclusion:</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=i-AJk8K0t14&amp;list=PL3F42F1E80C3EC8A3\">Lecture 24</a> is the general revision lecture. If you can follow it, you've learnt most of the course. It emphasises that monetary policy is generally much more effective than fiscal policy, except in liquidity traps, when monetary policy becomes ineffective.</li>\n</ul>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "fH8jPjHF2R27sRTTG": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CYnZcsgFWruaZpS2d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 38, "extendedScore": null, "score": 1.2703208431233139e-06, "legacy": true, "legacyId": "23243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-18T15:43:45.311Z", "modifiedAt": null, "url": null, "title": "The idiot savant AI isn't an idiot", "slug": "the-idiot-savant-ai-isn-t-an-idiot", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:04.160Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iqBaHP38uRdHzxhPP/the-idiot-savant-ai-isn-t-an-idiot", "pageUrlRelative": "/posts/iqBaHP38uRdHzxhPP/the-idiot-savant-ai-isn-t-an-idiot", "linkUrl": "https://www.lesswrong.com/posts/iqBaHP38uRdHzxhPP/the-idiot-savant-ai-isn-t-an-idiot", "postedAtFormatted": "Thursday, July 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20idiot%20savant%20AI%20isn't%20an%20idiot&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20idiot%20savant%20AI%20isn't%20an%20idiot%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqBaHP38uRdHzxhPP%2Fthe-idiot-savant-ai-isn-t-an-idiot%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20idiot%20savant%20AI%20isn't%20an%20idiot%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqBaHP38uRdHzxhPP%2Fthe-idiot-savant-ai-isn-t-an-idiot", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqBaHP38uRdHzxhPP%2Fthe-idiot-savant-ai-isn-t-an-idiot", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p><em>A <a href=\"http://en.wikipedia.org/wiki/Wikipedia:Stub\">stub</a> on a point that's come up recently.</em></p>\n<p>If I owned a paperclip factory, and casually told my foreman to improve efficiency while I'm away, and he planned a takeover of the country, aiming to devote its entire economy to paperclip manufacturing (apart from the armament factories he needed to invade neighbouring countries and steal their iron mines)... then I'd conclude that my foreman was an idiot (or being wilfully idiotic). He obviously had no idea what I meant. And if he misunderstood me so egregiously, he's certainly not a threat: he's unlikely to reason his way out of a paper bag, let alone to any position of power.</p>\n<p>If I owned a paperclip factory, and casually programmed my <a href=\"http://en.wikipedia.org/wiki/Office_Assistant\">superintelligent AI</a> to improve efficiency while I'm away, and it planned a takeover of the country... then I can't conclude that the AI is an idiot. It is following its programming. Unlike a human that behaved the same way, <em>it probably knows exactly what I meant to program in</em>. It just doesn't care: it follows its programming, not its knowledge about what its programming is \"meant\" to be (unless we've successfully programmed in \"do what I mean\", which is basically the whole of the challenge). We can't therefore conclude that it's incompetent, unable to understand human reasoning, or likely to fail.</p>\n<p>We can't reason by <a href=\"http://en.wikipedia.org/wiki/Anthropomorphism\">analogy with humans</a>. When AIs behave like idiot savants with respect to their motivations, we can't deduce that they're idiots.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iqBaHP38uRdHzxhPP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 6, "extendedScore": null, "score": 1.2705239553239282e-06, "legacy": true, "legacyId": "23381", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 133, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-18T19:12:26.412Z", "modifiedAt": null, "url": null, "title": "Does anchoring on deadlines cause procrastination?", "slug": "does-anchoring-on-deadlines-cause-procrastination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:04.124Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alex_zag_al", "createdAt": "2011-11-16T23:52:10.523Z", "isAdmin": false, "displayName": "alex_zag_al"}, "userId": "pDkj9zKTeJPQuhurD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D8HZYPSSin8urEWZ2/does-anchoring-on-deadlines-cause-procrastination", "pageUrlRelative": "/posts/D8HZYPSSin8urEWZ2/does-anchoring-on-deadlines-cause-procrastination", "linkUrl": "https://www.lesswrong.com/posts/D8HZYPSSin8urEWZ2/does-anchoring-on-deadlines-cause-procrastination", "postedAtFormatted": "Thursday, July 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20anchoring%20on%20deadlines%20cause%20procrastination%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20anchoring%20on%20deadlines%20cause%20procrastination%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD8HZYPSSin8urEWZ2%2Fdoes-anchoring-on-deadlines-cause-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20anchoring%20on%20deadlines%20cause%20procrastination%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD8HZYPSSin8urEWZ2%2Fdoes-anchoring-on-deadlines-cause-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD8HZYPSSin8urEWZ2%2Fdoes-anchoring-on-deadlines-cause-procrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>The phenomenon of anchoring seems to predict that deadlines will cause you to start a project near the deadline.</p>\n<p>In more detail:</p>\n<p>Any number you consider as an answer to a question will become an anchor and draw your answer towards it. Since you consider a deadline as a time to finish a project, your decision about when you should actually finish the project will be drawn towards it.</p>\n<p>That'll make you start the project later, even though you know consciously that planning to finish a project near the deadline is a bad idea.</p>\n<p>It's analogous to an example from Kahneman's <em>Thinking, Fast and Slow</em>&mdash;people buy more cans when there's a sign telling them that they can only buy 10.</p>\n<p>So, what I'm predicting is that anything that prevents anchoring will reduce procrastination when there's a deadline. Consciously deciding when you plan to finish by adjusting from a much earlier time, maybe?</p>\n<p>EDIT: <a href=\"/lw/i1j/does_anchoring_on_deadlines_cause_procrastination/9exj\">Brendon_Wong points out</a> that \"procrastination\" really refers to putting things off, which has an emotional cause. I think he's right. What I'm talking about isn't really a procrastination, then, but bad planning.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D8HZYPSSin8urEWZ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.2706914183720581e-06, "legacy": true, "legacyId": "23383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T00:25:07.872Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Lesswrong: The Math of Bayes' Theorem", "slug": "meetup-atlanta-lesswrong-the-math-of-bayes-theorem", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K4sZLrKoFJ73GTHdF/meetup-atlanta-lesswrong-the-math-of-bayes-theorem", "pageUrlRelative": "/posts/K4sZLrKoFJ73GTHdF/meetup-atlanta-lesswrong-the-math-of-bayes-theorem", "linkUrl": "https://www.lesswrong.com/posts/K4sZLrKoFJ73GTHdF/meetup-atlanta-lesswrong-the-math-of-bayes-theorem", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Lesswrong%3A%20The%20Math%20of%20Bayes'%20Theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Lesswrong%3A%20The%20Math%20of%20Bayes'%20Theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4sZLrKoFJ73GTHdF%2Fmeetup-atlanta-lesswrong-the-math-of-bayes-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Lesswrong%3A%20The%20Math%20of%20Bayes'%20Theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4sZLrKoFJ73GTHdF%2Fmeetup-atlanta-lesswrong-the-math-of-bayes-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4sZLrKoFJ73GTHdF%2Fmeetup-atlanta-lesswrong-the-math-of-bayes-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/os'>Atlanta Lesswrong: The Math of Bayes' Theorem</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 July 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Agenda:</p>\n\n<ul>\n<li><p>Introductions, and meet and greet for new members.</p></li>\n<li><p>Mini-presentation: The math of Bayes' theorem</p></li>\n<li><p>Discussions. We\u2019ll start with a large group discussion and break into smaller groups as needed.</p></li>\n<li><p>Games!</p></li>\n</ul>\n\n<p>(Please contact me if you have allergies to cats, as our meeting space has two of the most adorable cats you\u2019ve ever seen.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/os'>Atlanta Lesswrong: The Math of Bayes' Theorem</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K4sZLrKoFJ73GTHdF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2709424160145053e-06, "legacy": true, "legacyId": "23385", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong__The_Math_of_Bayes__Theorem\">Discussion article for the meetup : <a href=\"/meetups/os\">Atlanta Lesswrong: The Math of Bayes' Theorem</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 July 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Agenda:</p>\n\n<ul>\n<li><p>Introductions, and meet and greet for new members.</p></li>\n<li><p>Mini-presentation: The math of Bayes' theorem</p></li>\n<li><p>Discussions. We\u2019ll start with a large group discussion and break into smaller groups as needed.</p></li>\n<li><p>Games!</p></li>\n</ul>\n\n<p>(Please contact me if you have allergies to cats, as our meeting space has two of the most adorable cats you\u2019ve ever seen.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong__The_Math_of_Bayes__Theorem1\">Discussion article for the meetup : <a href=\"/meetups/os\">Atlanta Lesswrong: The Math of Bayes' Theorem</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Lesswrong: The Math of Bayes' Theorem", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong__The_Math_of_Bayes__Theorem", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Lesswrong: The Math of Bayes' Theorem", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong__The_Math_of_Bayes__Theorem1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T02:59:21.923Z", "modifiedAt": null, "url": null, "title": "Applied Bayes' Theorem: Calculating the probability that she's over me. Could somebody check my work? TT_TT", "slug": "applied-bayes-theorem-calculating-the-probability-that-she-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:02.045Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abcd_z", "createdAt": "2011-06-26T00:41:40.672Z", "isAdmin": false, "displayName": "abcd_z"}, "userId": "ntrr3JGG5fDueLnND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ryDeXyv5YM5kRQ8xm/applied-bayes-theorem-calculating-the-probability-that-she-s", "pageUrlRelative": "/posts/ryDeXyv5YM5kRQ8xm/applied-bayes-theorem-calculating-the-probability-that-she-s", "linkUrl": "https://www.lesswrong.com/posts/ryDeXyv5YM5kRQ8xm/applied-bayes-theorem-calculating-the-probability-that-she-s", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Bayes'%20Theorem%3A%20Calculating%20the%20probability%20that%20she's%20over%20me.%20Could%20somebody%20check%20my%20work%3F%20TT_TT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Bayes'%20Theorem%3A%20Calculating%20the%20probability%20that%20she's%20over%20me.%20Could%20somebody%20check%20my%20work%3F%20TT_TT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FryDeXyv5YM5kRQ8xm%2Fapplied-bayes-theorem-calculating-the-probability-that-she-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Bayes'%20Theorem%3A%20Calculating%20the%20probability%20that%20she's%20over%20me.%20Could%20somebody%20check%20my%20work%3F%20TT_TT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FryDeXyv5YM5kRQ8xm%2Fapplied-bayes-theorem-calculating-the-probability-that-she-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FryDeXyv5YM5kRQ8xm%2Fapplied-bayes-theorem-calculating-the-probability-that-she-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>Solve for: &nbsp;The probability that she's over me given that she didn't answer my call.</p>\n<p>Estimated probabilities: <br />The probability that she'd miss my call given that she was over me: 90%<br />The probability that she's over me: 30%<br />The probability that she'd miss my call given she was not over me. 10%</p>\n<p>(P(over me|missed call)= P(missed call|over me)*P(over me)) &nbsp;/ &nbsp;(P(missed call|over me)*P(over me)+P(Missed call|Not over me)*P(Not over me))</p>\n<p>P(O|M)=(P(M|O)*P(O))/(P(M|O)*P(O)+P(M|N)*P(N))</p>\n<p>P(O|M)=(.9*.3)/((.9*.3)+(.1*.7))</p>\n<p>P(O|M)=(.27)/(.27)+(.07)) = .27/.34 = .794</p>\n<p>Probability that she's over me given that she didn't answer the phone: 79.4%</p>\n<p>TT_TT</p>\n<p>&nbsp;</p>\n<p>EDIT: Something I've noticed here is that people are pointing to the chosen priors and saying that they seem unrealistic.</p>\n<p>In our 3-year relationship, she almost never missed my calls, or if she did she would contact me back as soon as she realized that she missed my call. &nbsp;In the current situation, she did no such thing.</p>\n<p>EDIT2: Yes, we broke up. &nbsp;Sorry I didn't make that clear.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ryDeXyv5YM5kRQ8xm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -16, "extendedScore": null, "score": 1.2710662523032344e-06, "legacy": true, "legacyId": "23390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T04:55:32.531Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: Value of Information/Curiosity", "slug": "meetup-durham-nc-triangle-area-value-of-information", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:26.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qLuSDJK5u3r9f49v5/meetup-durham-nc-triangle-area-value-of-information", "pageUrlRelative": "/posts/qLuSDJK5u3r9f49v5/meetup-durham-nc-triangle-area-value-of-information", "linkUrl": "https://www.lesswrong.com/posts/qLuSDJK5u3r9f49v5/meetup-durham-nc-triangle-area-value-of-information", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Value%20of%20Information%2FCuriosity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Value%20of%20Information%2FCuriosity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLuSDJK5u3r9f49v5%2Fmeetup-durham-nc-triangle-area-value-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Value%20of%20Information%2FCuriosity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLuSDJK5u3r9f49v5%2Fmeetup-durham-nc-triangle-area-value-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLuSDJK5u3r9f49v5%2Fmeetup-durham-nc-triangle-area-value-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ot'>Durham NC/Triangle Area: Value of Information/Curiosity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 August 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 WEST Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Cocoa Cinnamon for discussion and activities on Value of Information and how to apply curiosity in a useful way.</p>\n\n<p>Approximate schedule: <br />\n7:00  Introductions, get beverages, general chit chat <br />\n7:30  Value of Information: theory, math, and a couple practice problems <br />\n8:00  Get Curious! We'll pick a potentially fun / interesting topic and practice getting curious about it. <br />\n8:30  When is curiousity useful? Heuristics for when to apply the skills we've practiced <br />\n9:00  Meetup closing, precommitments for next meetup, etc. <br />\n9:15 or 9:30: migrate to Fullsteam for beers and socializing for those who wish</p>\n\n<p>Specific suggested readings to be posted later, but any of the top returns for a LW search on \"value of information\" may prove useful.</p>\n\n<p>Join the mailing list for possible pre-meetup discussion and to be informed of upcoming meetups:  <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ot'>Durham NC/Triangle Area: Value of Information/Curiosity</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qLuSDJK5u3r9f49v5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Value_of_Information_Curiosity\">Discussion article for the meetup : <a href=\"/meetups/ot\">Durham NC/Triangle Area: Value of Information/Curiosity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 August 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 WEST Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Cocoa Cinnamon for discussion and activities on Value of Information and how to apply curiosity in a useful way.</p>\n\n<p>Approximate schedule: <br>\n7:00  Introductions, get beverages, general chit chat <br>\n7:30  Value of Information: theory, math, and a couple practice problems <br>\n8:00  Get Curious! We'll pick a potentially fun / interesting topic and practice getting curious about it. <br>\n8:30  When is curiousity useful? Heuristics for when to apply the skills we've practiced <br>\n9:00  Meetup closing, precommitments for next meetup, etc. <br>\n9:15 or 9:30: migrate to Fullsteam for beers and socializing for those who wish</p>\n\n<p>Specific suggested readings to be posted later, but any of the top returns for a LW search on \"value of information\" may prove useful.</p>\n\n<p>Join the mailing list for possible pre-meetup discussion and to be informed of upcoming meetups:  <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Value_of_Information_Curiosity1\">Discussion article for the meetup : <a href=\"/meetups/ot\">Durham NC/Triangle Area: Value of Information/Curiosity</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: Value of Information/Curiosity", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Value_of_Information_Curiosity", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: Value of Information/Curiosity", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Value_of_Information_Curiosity1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T08:43:00.898Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Excursion: Comfort Zone Expansion", "slug": "meetup-melbourne-excursion-comfort-zone-expansion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.162Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BraydenM", "createdAt": "2013-06-10T09:17:31.294Z", "isAdmin": false, "displayName": "BraydenM"}, "userId": "KBZHqcMC6z2rPZkZK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zYqLsd6gaoAnQn8Js/meetup-melbourne-excursion-comfort-zone-expansion", "pageUrlRelative": "/posts/zYqLsd6gaoAnQn8Js/meetup-melbourne-excursion-comfort-zone-expansion", "linkUrl": "https://www.lesswrong.com/posts/zYqLsd6gaoAnQn8Js/meetup-melbourne-excursion-comfort-zone-expansion", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Excursion%3A%20Comfort%20Zone%20Expansion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Excursion%3A%20Comfort%20Zone%20Expansion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzYqLsd6gaoAnQn8Js%2Fmeetup-melbourne-excursion-comfort-zone-expansion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Excursion%3A%20Comfort%20Zone%20Expansion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzYqLsd6gaoAnQn8Js%2Fmeetup-melbourne-excursion-comfort-zone-expansion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzYqLsd6gaoAnQn8Js%2Fmeetup-melbourne-excursion-comfort-zone-expansion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ou'>Melbourne Excursion: Comfort Zone Expansion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 July 2013 10:30:00AM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Comfort Zone Expansion, or COZE, is a technique pioneered by CFAR that aims to get participants to better understand themselves by having an opportunity to get outside their comfort zone in a controlled environment.</p>\n\n<p>COZE participants regularly report significantly increased social benefits following the training and practice sessions, and personally it has lead to a life improvement for myself.</p>\n\n<p>I am planning to run a workshop this month to give everyone a chance to learn a little which hopefully becomes a powerful technique you can call upon when required.</p>\n\n<p>We will be meeting in the city at Ben's house on Saturday Morning for a training workshop, finishing in the early afternoon following a practical excursion to the Queen Victoria Market.</p>\n\n<p>The excursion activity suggestions are tailored for people of all confidence levels, and feeling reluctant is normal. If you want to talk first then I will be at the monthly social event tonight, or on facebook or mobile.</p>\n\n<p>The plan is as follows:\n10:30am - 11:30am\nMeet at Ben's House for the workshop session. We will cover the theory behind the point of comfort zones and when to consider the value in moving outside them. Go over the material on playfulness cultivation and vulnerability investigation. Cover the concepts of stress, including Sympathetic Nervous System activation, and techniques for understanding and controlling your state of stress. Discuss previous experiences with COZE and benefits that have been observed. Run through possible things that you might be valuable trying at the COZE excursion, and what you should be paying attention to in terms of your state of mind and body.</p>\n\n<p>11:30am-12:00pm\nImprov Games!\nPractice things you are uncomfortable with in a controlled environment with friends. We will be playing some 'whos line is it anyway' style games together. Use this as a trial run to re-enforce the concepts from the morning workshop session.</p>\n\n<p>12:00pm-12:30pm\nFinal discussion and COZE safety talk in preparation for the excursion.</p>\n\n<p>1:00pm-2:15pm\nBreak off in partners for the walk to Queen Victoria Market. Separate individually for the exercise itself, and regroup after the time period has finished.</p>\n\n<p>2:15pm-3:00pm\nCOZE debrief! Share stories and lessons learned.</p>\n\n<p>It will be chilly outside, so dress appropriately.</p>\n\n<p>Brayden McLean - 0407943917</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ou'>Melbourne Excursion: Comfort Zone Expansion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zYqLsd6gaoAnQn8Js", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.2713422491468385e-06, "legacy": true, "legacyId": "23397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Excursion__Comfort_Zone_Expansion\">Discussion article for the meetup : <a href=\"/meetups/ou\">Melbourne Excursion: Comfort Zone Expansion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 July 2013 10:30:00AM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Comfort Zone Expansion, or COZE, is a technique pioneered by CFAR that aims to get participants to better understand themselves by having an opportunity to get outside their comfort zone in a controlled environment.</p>\n\n<p>COZE participants regularly report significantly increased social benefits following the training and practice sessions, and personally it has lead to a life improvement for myself.</p>\n\n<p>I am planning to run a workshop this month to give everyone a chance to learn a little which hopefully becomes a powerful technique you can call upon when required.</p>\n\n<p>We will be meeting in the city at Ben's house on Saturday Morning for a training workshop, finishing in the early afternoon following a practical excursion to the Queen Victoria Market.</p>\n\n<p>The excursion activity suggestions are tailored for people of all confidence levels, and feeling reluctant is normal. If you want to talk first then I will be at the monthly social event tonight, or on facebook or mobile.</p>\n\n<p>The plan is as follows:\n10:30am - 11:30am\nMeet at Ben's House for the workshop session. We will cover the theory behind the point of comfort zones and when to consider the value in moving outside them. Go over the material on playfulness cultivation and vulnerability investigation. Cover the concepts of stress, including Sympathetic Nervous System activation, and techniques for understanding and controlling your state of stress. Discuss previous experiences with COZE and benefits that have been observed. Run through possible things that you might be valuable trying at the COZE excursion, and what you should be paying attention to in terms of your state of mind and body.</p>\n\n<p>11:30am-12:00pm\nImprov Games!\nPractice things you are uncomfortable with in a controlled environment with friends. We will be playing some 'whos line is it anyway' style games together. Use this as a trial run to re-enforce the concepts from the morning workshop session.</p>\n\n<p>12:00pm-12:30pm\nFinal discussion and COZE safety talk in preparation for the excursion.</p>\n\n<p>1:00pm-2:15pm\nBreak off in partners for the walk to Queen Victoria Market. Separate individually for the exercise itself, and regroup after the time period has finished.</p>\n\n<p>2:15pm-3:00pm\nCOZE debrief! Share stories and lessons learned.</p>\n\n<p>It will be chilly outside, so dress appropriately.</p>\n\n<p>Brayden McLean - 0407943917</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Excursion__Comfort_Zone_Expansion1\">Discussion article for the meetup : <a href=\"/meetups/ou\">Melbourne Excursion: Comfort Zone Expansion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Excursion: Comfort Zone Expansion", "anchor": "Discussion_article_for_the_meetup___Melbourne_Excursion__Comfort_Zone_Expansion", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Excursion: Comfort Zone Expansion", "anchor": "Discussion_article_for_the_meetup___Melbourne_Excursion__Comfort_Zone_Expansion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T13:02:19.180Z", "modifiedAt": null, "url": null, "title": "[Link] My talk about the Future", "slug": "link-my-talk-about-the-future", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WaY7Tp3jpdeBP6ea2/link-my-talk-about-the-future", "pageUrlRelative": "/posts/WaY7Tp3jpdeBP6ea2/link-my-talk-about-the-future", "linkUrl": "https://www.lesswrong.com/posts/WaY7Tp3jpdeBP6ea2/link-my-talk-about-the-future", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20My%20talk%20about%20the%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20My%20talk%20about%20the%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWaY7Tp3jpdeBP6ea2%2Flink-my-talk-about-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20My%20talk%20about%20the%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWaY7Tp3jpdeBP6ea2%2Flink-my-talk-about-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWaY7Tp3jpdeBP6ea2%2Flink-my-talk-about-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>I recently gave a <a href=\"http://www.youtube.com/watch?v=JnLExpkcQqM\">talk</a> at the IARU Summer School on the Ethics of Technology.</p>\n<p>In it, I touched on many of the research themes of the FHI: the accuracy of predictions, the limitations and biases of predictors, the huge risks that humanity may face, the huge benefits that we may gain, and the various ethical challenges that we'll face in the future.</p>\n<p>Nothing really new for anyone who's familiar with our work, but some may enjoy perusing it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WaY7Tp3jpdeBP6ea2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.2715505759661099e-06, "legacy": true, "legacyId": "23399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T14:41:47.699Z", "modifiedAt": null, "url": null, "title": "Consumption Smoothing and Hedonic Adaptation", "slug": "consumption-smoothing-and-hedonic-adaptation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.675Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/daT4gSbJxPbgzPGZv/consumption-smoothing-and-hedonic-adaptation", "pageUrlRelative": "/posts/daT4gSbJxPbgzPGZv/consumption-smoothing-and-hedonic-adaptation", "linkUrl": "https://www.lesswrong.com/posts/daT4gSbJxPbgzPGZv/consumption-smoothing-and-hedonic-adaptation", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consumption%20Smoothing%20and%20Hedonic%20Adaptation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsumption%20Smoothing%20and%20Hedonic%20Adaptation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdaT4gSbJxPbgzPGZv%2Fconsumption-smoothing-and-hedonic-adaptation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consumption%20Smoothing%20and%20Hedonic%20Adaptation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdaT4gSbJxPbgzPGZv%2Fconsumption-smoothing-and-hedonic-adaptation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdaT4gSbJxPbgzPGZv%2Fconsumption-smoothing-and-hedonic-adaptation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 544, "htmlBody": "<p>Because earning capacity increases with age but enjoyment of spending decreases with additional money, standard economic theory predicts people will <a href=\"http://en.wikipedia.org/wiki/Consumption_smoothing\">smooth their spending</a> by borrowing to live beyond their means when young and paying it back when they're older.  The idea is that you have some lifetime spending to split among all your future selves, so instead of having you-at-50 enjoy $45K/year of self-spending while you-at-25 struggles with just $20K/year, you should each spend $30K/year. [1] In practice, however, we mostly don't see people doing this, and I think that's actually very reasonable.</p>\n<p>We can approximate your earnings over the course of your life by looking at how much everyone makes now, broken down by age:</p>\n<p><img src=\"http://www.jefftk.com/income-distribution-by-age.png\" alt=\"\" /><br /> (<a href=\"http://finance.townhall.com/columnists/politicalcalculations/2013/01/27/visualizing-the-2012-distribution-of-income-in-the-us-by-age-n1498712/page/full\">source</a>)</p>\n<p>We see incoming rising sharply with age in the 20s, slower in the 30s, plateauing through the 40s and 50s, and then declining with retirement.  Add to this a small amount of growth in real wages over time, about 8% over the last 40 years, and we can see that people in their 20s earn significantly less than they expect to be earning over most of their life.</p>\n<p>The standard model of people is that spending money makes you happier, and the first dollar goes farther than the last:</p>\n<p><img src=\"http://www.jefftk.com/happiness-money-graph.png\" alt=\"\" /> <img src=\"http://www.jefftk.com/marginal-happiness-money-graph.png\" alt=\"\" /></p>\n<p>Add <a href=\"http://en.wikipedia.org/wiki/Temporal_discounting\">temporal discounting</a> and that you can enjoy durable goods for longer if you buy them earlier and we should expect to see people <a href=\"http://www.slate.com/blogs/moneybox/2013/07/09/low_interest_rates_are_exciting.html\">borrowing heavily in their 20s</a> and paying it back as they get older, but we mostly don't.  We do see this some with buying houses, but in most other ways the typical 50 year-old is much less frugal than the typical 25 year-old.  When we see young people living on borrowed money to support a lifestyle they would expect to enjoy later in life, we generally <a href=\"http://trueslant.com/KashmirHill/2009/04/28/what-happened-to-living-life-as-a-starving-grad-student/\">mock them</a>. [2]</p>\n<p>One response is to say that people are behaving foolishly and should borrow more.  Why live thrifty in your 20s but not in your 40s? Either you should continue your thriftiness into your 40s, spend more in your 20s, or some combination of both.  But this misses something important about human psychology: decreases in our standard of living are much more painful than increases are pleasant.</p>\n<p>If you're earning a relatively small amount and living cheaply, and then earn more money and start living less frugally, this probably makes you happier.  But if then something happens and you need to go back to living on less you'll probably be much less happy than you were the first time.  Because individual incomes are much less predictable than cohort incomes, if you borrow a lot while young to consume at a higher level you might be anticipating a future level that you'll not reach or not sustain once reaching.</p>\n<p>(This is why I try to be&nbsp;<a href=\"http://www.jefftk.com/news/2012-08-09\">careful with luxuries</a>.)</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2013-07-19\">on my blog</a>.</em></small></p>\n<p><br /> [1] Why is that $30K instead of $32.5K, the average of $45K and $20K? To spend money you don't have yet you need to pay interest, which decreases the total amount you get to spend.  But as long as the difference in enjoyment between $20K and $30K is larger than between $45K and $30K you still come out ahead.</p>\n<p>[2] Though in this case I think the author has other reasons to dislike their subject than consumption habits.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "daT4gSbJxPbgzPGZv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.2716305109894825e-06, "legacy": true, "legacyId": "23401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T15:36:35.840Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Zagreb", "slug": "new-lw-meetup-zagreb", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CpM8mnuFSbxT584Wf/new-lw-meetup-zagreb", "pageUrlRelative": "/posts/CpM8mnuFSbxT584Wf/new-lw-meetup-zagreb", "linkUrl": "https://www.lesswrong.com/posts/CpM8mnuFSbxT584Wf/new-lw-meetup-zagreb", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Zagreb&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Zagreb%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCpM8mnuFSbxT584Wf%2Fnew-lw-meetup-zagreb%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Zagreb%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCpM8mnuFSbxT584Wf%2Fnew-lw-meetup-zagreb", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCpM8mnuFSbxT584Wf%2Fnew-lw-meetup-zagreb", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 526, "htmlBody": "<p><strong>This summary was posted to LW Main on July 12th. The following week's summary is <a href=\"/lw/i22/new_lw_meetup_southeast_michigan/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/og\">Zagreb LW meetup:&nbsp;<span class=\"date\">13 July 2013 03:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/ob\">Atlanta: Intro to Meditation:&nbsp;<span class=\"date\">13 July 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/ns\">Brussels meetup with HEALES:&nbsp;<span class=\"date\">13 July 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/oh\">Buffalo LW - Biased Boardgaming:&nbsp;<span class=\"date\">18 July 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/oi\">Durham/RTLW HPMoR discussion, ch. 77-78:&nbsp;<span class=\"date\">13 July 2013 12:30PM</span></a></li>\n<li><a href=\"/meetups/oe\">Helsinki LW meetup:&nbsp;<span class=\"date\">20 July 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/oj\">[Moscow] The Meet up:&nbsp;<span class=\"date\">21 July 2013 04:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">13 July 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ok\">[Columbus OH] Upcoming Workshop Topics:&nbsp;<span class=\"date\">15 July 2013 11:52PM</span></a></li>\n<li><a href=\"/meetups/o8\">[Vienna] LW Vienna Meetup #4:&nbsp;<span class=\"date\">13 July 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong style=\"font-weight: bold;\"></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CpM8mnuFSbxT584Wf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2716745521034484e-06, "legacy": true, "legacyId": "23303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZcBLLh6K9QsDGB2Di", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T16:24:20.796Z", "modifiedAt": null, "url": null, "title": "Even with default points, systems remain exploitable", "slug": "even-with-default-points-systems-remain-exploitable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gtGjiizupfCYCz7Lo/even-with-default-points-systems-remain-exploitable", "pageUrlRelative": "/posts/gtGjiizupfCYCz7Lo/even-with-default-points-systems-remain-exploitable", "linkUrl": "https://www.lesswrong.com/posts/gtGjiizupfCYCz7Lo/even-with-default-points-systems-remain-exploitable", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Even%20with%20default%20points%2C%20systems%20remain%20exploitable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEven%20with%20default%20points%2C%20systems%20remain%20exploitable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtGjiizupfCYCz7Lo%2Feven-with-default-points-systems-remain-exploitable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Even%20with%20default%20points%2C%20systems%20remain%20exploitable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtGjiizupfCYCz7Lo%2Feven-with-default-points-systems-remain-exploitable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtGjiizupfCYCz7Lo%2Feven-with-default-points-systems-remain-exploitable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 842, "htmlBody": "<h2>Still exploitable, even with defaults</h2>\n<p>A while ago, I posted a brief <a href=\"/lw/8qv/in_the_pareto_world_liars_prosper/\">picture-proof</a> of the fact that whatever bargaining system you use to reach deals, they are all exploitable, in some situations, by liars (as long as the outcome is Pareto and a few other assumptions).</p>\n<p>That included any system with an <em>internally assigned</em> <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem#Disagreement_point\">default point</a>. The picture proofs work no matter how you calculate the bargaining outcome: if you use the utility value data to assign a default point, before picking the Nash bargaining equilibrium, then the whole process is susceptible to exploitation by lying.</p>\n<p>Is the same thing true for externally assigned default points (i.e. default points that come from outside the data, and are not a mere function of everyone's preferences and the available outcomes)? A moment's thought shows that this is the case. The picture proofs never used translations, or scaling, or anything that would shift an external default point. So having an externally assigned default point does not solve the problem of lying.</p>\n<p>But \"any Pareto bargaining system is exploitable by lying\" is an existence proof: in at least one circumstance, one player may be able to derive a non-zero benefit by lying about their utility function. This doesn't give an impression of the scale of the problem.</p>\n<h2>The scale of the problem</h2>\n<p>The problem is very severe, for the Nash Bargaining Solution (NBS), the Kalai-Smorodinsky Bargaining Solution (KSBS) and my <a href=\"/r/discussion/lw/hv6/gains_from_trade_slug_versus_galaxy_how_much/\">Mutual Worth Bargaining Solution</a> (MWBS). Essentially, it's as bad as it can get.</p>\n<p>For KSBS and NBS, let's call an outcome admissible if it's Pareto-better than the default. For the MWBS, call an outcome admissible if the combined utility values it more than the default point (as we've seen, this needn't be an improvement for both players). In all three approaches, the bargaining solution must be admissible.</p>\n<p>Then the dismal result is:</p>\n<ul>\n<li>Let O be any admissible pure outcome. Then either player can lie, if they know everyone's preferences, to force the bargaining solution to pick O.<a id=\"more\"></a></li>\n</ul>\n<p>The proof is nearly derisively brief: the player simply needs to reduce their declared utility on every other admissible pure outcome, until these are no longer admissible. Then O would be the only outcome available to the bargaining solution.</p>\n<p>There are some extra subtleties (this section may be skipped by those not interested; the essence of the argument is above). For both KSBS and MWBS, you have to preserve the values of the utopia points. Player X can do this by, for instance, promoting some option that player Y absolutely hates, up to the value of the original utopia point. This would preserve all the normalisations, and the new utopia point would not be admissible (as Y hates it so much), so O would indeed be the only outcome available. For the KSBS, only equitable outcomes are chosen, so the player has to change their valuation of O to make it seem equitable. Finally, the NBS sometimes chooses mixed outcome rather than pure outcomes, so the player <em>may</em> need to reduce their declared utility for every other pure outcome by quite a bit, to ensure O is picked rather than some mixed outcome involving O.</p>\n<h2>When fairness fails</h2>\n<p>This is bad news for all three systems. But, in a way, it's worst of all for KSBS. That's because KSBS gave up the ideal of maximising something, in order to be \"fair\". It's expected that there's a cost when you fail to maximise (you can't get as much benefits as you possibly could), but there should be benefits as well, such as not placing yourself in as much risk of losing or being exploited. But KSBS failed you as badly as the others: the liar gets to choose their outcome.</p>\n<p>This suggests that \"have a default point and negotiate to ensure you get a fair or reasonable portion of the gains from trade\", is not an effective way of combating lying. Against liars who know your preferences, this is equivalent with \"gain epsilon more than the default point, while thinking you got a good deal.\"</p>\n<h2>Further research: equilibrium lying</h2>\n<p>If both players lie in this way, the standard outcome is the default point (unless one outcome Pareto-dominates all others). This is obviously much worse than the alternatives, so we have to try and avoid this. Three possible research avenues spring to mind: the first is see how stable these bargaining solutions are to small lies. If you can give a pretty good estimate of your opponent's preference, is that enough to ensure they can only exploit you (in most cases) by only a little bit? Do small lies produce small changes in (expected) outcomes?</p>\n<p>Effective lying relies on knowing your opponent's preferences, though. If you don't know your opponent's preferences, is it still rational to lie? Are any of the bargaining solutions exploitable against a wide range of unknown opponents?</p>\n<p>A third approach would be to go through several cycles of lying and counter-lying, and see if this eventually settles down to any sort of equilibrium.</p>\n<p>But as I said in the previous post, I won't be working on these any time soon! Baby comes first.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gtGjiizupfCYCz7Lo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "23400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Still_exploitable__even_with_defaults\">Still exploitable, even with defaults</h2>\n<p>A while ago, I posted a brief <a href=\"/lw/8qv/in_the_pareto_world_liars_prosper/\">picture-proof</a> of the fact that whatever bargaining system you use to reach deals, they are all exploitable, in some situations, by liars (as long as the outcome is Pareto and a few other assumptions).</p>\n<p>That included any system with an <em>internally assigned</em> <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem#Disagreement_point\">default point</a>. The picture proofs work no matter how you calculate the bargaining outcome: if you use the utility value data to assign a default point, before picking the Nash bargaining equilibrium, then the whole process is susceptible to exploitation by lying.</p>\n<p>Is the same thing true for externally assigned default points (i.e. default points that come from outside the data, and are not a mere function of everyone's preferences and the available outcomes)? A moment's thought shows that this is the case. The picture proofs never used translations, or scaling, or anything that would shift an external default point. So having an externally assigned default point does not solve the problem of lying.</p>\n<p>But \"any Pareto bargaining system is exploitable by lying\" is an existence proof: in at least one circumstance, one player may be able to derive a non-zero benefit by lying about their utility function. This doesn't give an impression of the scale of the problem.</p>\n<h2 id=\"The_scale_of_the_problem\">The scale of the problem</h2>\n<p>The problem is very severe, for the Nash Bargaining Solution (NBS), the Kalai-Smorodinsky Bargaining Solution (KSBS) and my <a href=\"/r/discussion/lw/hv6/gains_from_trade_slug_versus_galaxy_how_much/\">Mutual Worth Bargaining Solution</a> (MWBS). Essentially, it's as bad as it can get.</p>\n<p>For KSBS and NBS, let's call an outcome admissible if it's Pareto-better than the default. For the MWBS, call an outcome admissible if the combined utility values it more than the default point (as we've seen, this needn't be an improvement for both players). In all three approaches, the bargaining solution must be admissible.</p>\n<p>Then the dismal result is:</p>\n<ul>\n<li>Let O be any admissible pure outcome. Then either player can lie, if they know everyone's preferences, to force the bargaining solution to pick O.<a id=\"more\"></a></li>\n</ul>\n<p>The proof is nearly derisively brief: the player simply needs to reduce their declared utility on every other admissible pure outcome, until these are no longer admissible. Then O would be the only outcome available to the bargaining solution.</p>\n<p>There are some extra subtleties (this section may be skipped by those not interested; the essence of the argument is above). For both KSBS and MWBS, you have to preserve the values of the utopia points. Player X can do this by, for instance, promoting some option that player Y absolutely hates, up to the value of the original utopia point. This would preserve all the normalisations, and the new utopia point would not be admissible (as Y hates it so much), so O would indeed be the only outcome available. For the KSBS, only equitable outcomes are chosen, so the player has to change their valuation of O to make it seem equitable. Finally, the NBS sometimes chooses mixed outcome rather than pure outcomes, so the player <em>may</em> need to reduce their declared utility for every other pure outcome by quite a bit, to ensure O is picked rather than some mixed outcome involving O.</p>\n<h2 id=\"When_fairness_fails\">When fairness fails</h2>\n<p>This is bad news for all three systems. But, in a way, it's worst of all for KSBS. That's because KSBS gave up the ideal of maximising something, in order to be \"fair\". It's expected that there's a cost when you fail to maximise (you can't get as much benefits as you possibly could), but there should be benefits as well, such as not placing yourself in as much risk of losing or being exploited. But KSBS failed you as badly as the others: the liar gets to choose their outcome.</p>\n<p>This suggests that \"have a default point and negotiate to ensure you get a fair or reasonable portion of the gains from trade\", is not an effective way of combating lying. Against liars who know your preferences, this is equivalent with \"gain epsilon more than the default point, while thinking you got a good deal.\"</p>\n<h2 id=\"Further_research__equilibrium_lying\">Further research: equilibrium lying</h2>\n<p>If both players lie in this way, the standard outcome is the default point (unless one outcome Pareto-dominates all others). This is obviously much worse than the alternatives, so we have to try and avoid this. Three possible research avenues spring to mind: the first is see how stable these bargaining solutions are to small lies. If you can give a pretty good estimate of your opponent's preference, is that enough to ensure they can only exploit you (in most cases) by only a little bit? Do small lies produce small changes in (expected) outcomes?</p>\n<p>Effective lying relies on knowing your opponent's preferences, though. If you don't know your opponent's preferences, is it still rational to lie? Are any of the bargaining solutions exploitable against a wide range of unknown opponents?</p>\n<p>A third approach would be to go through several cycles of lying and counter-lying, and see if this eventually settles down to any sort of equilibrium.</p>\n<p>But as I said in the previous post, I won't be working on these any time soon! Baby comes first.</p>", "sections": [{"title": "Still exploitable, even with defaults", "anchor": "Still_exploitable__even_with_defaults", "level": 1}, {"title": "The scale of the problem", "anchor": "The_scale_of_the_problem", "level": 1}, {"title": "When fairness fails", "anchor": "When_fairness_fails", "level": 1}, {"title": "Further research: equilibrium lying", "anchor": "Further_research__equilibrium_lying", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vSgaExrundWJJBDmZ", "7kvBxG9ZmYb5rDRiq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T16:33:39.053Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Megametup debriefing/planning meetup", "slug": "meetup-washington-dc-megametup-debriefing-planning-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JdoseFHeo45rp5KtP/meetup-washington-dc-megametup-debriefing-planning-meetup", "pageUrlRelative": "/posts/JdoseFHeo45rp5KtP/meetup-washington-dc-megametup-debriefing-planning-meetup", "linkUrl": "https://www.lesswrong.com/posts/JdoseFHeo45rp5KtP/meetup-washington-dc-megametup-debriefing-planning-meetup", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Megametup%20debriefing%2Fplanning%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Megametup%20debriefing%2Fplanning%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJdoseFHeo45rp5KtP%2Fmeetup-washington-dc-megametup-debriefing-planning-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Megametup%20debriefing%2Fplanning%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJdoseFHeo45rp5KtP%2Fmeetup-washington-dc-megametup-debriefing-planning-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJdoseFHeo45rp5KtP%2Fmeetup-washington-dc-megametup-debriefing-planning-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ov'>Washington DC Megametup debriefing/planning meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 July 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA (courtyard)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about the megameetup last week, and plan what to do in the future.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ov'>Washington DC Megametup debriefing/planning meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JdoseFHeo45rp5KtP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2717204052166065e-06, "legacy": true, "legacyId": "23403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Megametup_debriefing_planning_meetup\">Discussion article for the meetup : <a href=\"/meetups/ov\">Washington DC Megametup debriefing/planning meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 July 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA (courtyard)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about the megameetup last week, and plan what to do in the future.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Megametup_debriefing_planning_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ov\">Washington DC Megametup debriefing/planning meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Megametup debriefing/planning meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Megametup_debriefing_planning_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Megametup debriefing/planning meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Megametup_debriefing_planning_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-19T16:55:07.508Z", "modifiedAt": null, "url": null, "title": "Meetup : Zagreb meetup - July 27th", "slug": "meetup-zagreb-meetup-july-27th", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "IsTheLittleLion", "createdAt": "2011-10-05T22:34:47.894Z", "isAdmin": false, "displayName": "IsTheLittleLion"}, "userId": "Pn3ReoS9v58Jb8prT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mkAA8rvtK6pCa5qYA/meetup-zagreb-meetup-july-27th", "pageUrlRelative": "/posts/mkAA8rvtK6pCa5qYA/meetup-zagreb-meetup-july-27th", "linkUrl": "https://www.lesswrong.com/posts/mkAA8rvtK6pCa5qYA/meetup-zagreb-meetup-july-27th", "postedAtFormatted": "Friday, July 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Zagreb%20meetup%20-%20July%2027th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Zagreb%20meetup%20-%20July%2027th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkAA8rvtK6pCa5qYA%2Fmeetup-zagreb-meetup-july-27th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Zagreb%20meetup%20-%20July%2027th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkAA8rvtK6pCa5qYA%2Fmeetup-zagreb-meetup-july-27th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkAA8rvtK6pCa5qYA%2Fmeetup-zagreb-meetup-july-27th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ow'>Zagreb meetup - July 27th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 July 2013 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Tkal\u010di\u0107eva 36 (Pivnica Mali Medo) in Zagreb, Croatia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A casual meetup, socializing and planning future meetups.</p>\n\n<p>Detailed directions: when you enter, go up the stairs and watch for a LW sign.</p>\n\n<p>If you are on facebook, you can find us in this group: https://www.facebook.com/groups/165350706979610/</p>\n\n<p>This is the facebook event page: https://www.facebook.com/events/529193467127751/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ow'>Zagreb meetup - July 27th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mkAA8rvtK6pCa5qYA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.2717376645202488e-06, "legacy": true, "legacyId": "23404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Zagreb_meetup___July_27th\">Discussion article for the meetup : <a href=\"/meetups/ow\">Zagreb meetup - July 27th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 July 2013 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Tkal\u010di\u0107eva 36 (Pivnica Mali Medo) in Zagreb, Croatia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A casual meetup, socializing and planning future meetups.</p>\n\n<p>Detailed directions: when you enter, go up the stairs and watch for a LW sign.</p>\n\n<p>If you are on facebook, you can find us in this group: https://www.facebook.com/groups/165350706979610/</p>\n\n<p>This is the facebook event page: https://www.facebook.com/events/529193467127751/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Zagreb_meetup___July_27th1\">Discussion article for the meetup : <a href=\"/meetups/ow\">Zagreb meetup - July 27th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Zagreb meetup - July 27th", "anchor": "Discussion_article_for_the_meetup___Zagreb_meetup___July_27th", "level": 1}, {"title": "Discussion article for the meetup : Zagreb meetup - July 27th", "anchor": "Discussion_article_for_the_meetup___Zagreb_meetup___July_27th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T01:01:16.689Z", "modifiedAt": null, "url": null, "title": "Norbert Wiener's paper \"Some Moral and Technical Consequences of Automation\"", "slug": "norbert-wiener-s-paper-some-moral-and-technical-consequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:02.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2rWfmahhqASnFcYLr/norbert-wiener-s-paper-some-moral-and-technical-consequences", "pageUrlRelative": "/posts/2rWfmahhqASnFcYLr/norbert-wiener-s-paper-some-moral-and-technical-consequences", "linkUrl": "https://www.lesswrong.com/posts/2rWfmahhqASnFcYLr/norbert-wiener-s-paper-some-moral-and-technical-consequences", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Norbert%20Wiener's%20paper%20%22Some%20Moral%20and%20Technical%20Consequences%20of%20Automation%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANorbert%20Wiener's%20paper%20%22Some%20Moral%20and%20Technical%20Consequences%20of%20Automation%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rWfmahhqASnFcYLr%2Fnorbert-wiener-s-paper-some-moral-and-technical-consequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Norbert%20Wiener's%20paper%20%22Some%20Moral%20and%20Technical%20Consequences%20of%20Automation%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rWfmahhqASnFcYLr%2Fnorbert-wiener-s-paper-some-moral-and-technical-consequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rWfmahhqASnFcYLr%2Fnorbert-wiener-s-paper-some-moral-and-technical-consequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 444, "htmlBody": "<p>In 1960, mathematician&nbsp;<a href=\"http://en.wikipedia.org/wiki/Norbert_Wiener\">Norbert Wiener</a>&nbsp;wrote an article titled&nbsp;<a href=\"http://www.itu.dk/people/cmmm/Wiener.pdf\">Some Moral and Technical Consequences of Automation</a>. I'm struck by the strong overlap between certain passages from the paper and some of the themes that have been discussed on Less Wrong in connection with AI risk.</p>\n<p><a id=\"more\"></a></p>\n<p>Overlapping with Eliezer's blog post&nbsp;<a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a>&nbsp;and&nbsp;<a href=\"http://intelligence.org/files/CFAI.pdf\">Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures</a>:</p>\n<blockquote>\n<p>Here, however, if the rules for victory in a war game do not correspond to what we actually wish for our country, it is more than likely that such a machine may produce a policy which would win a nominal victory on points at the cost of every interest we have at heart, even that of national survival.</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>We all know the fable of the sorcerer's apprentice, in which the boy makes the broom carry water in his master's absence, so that it is on the point of drowning him when his master reappears. [...] Disastrous results are to be expected not merely in the world of fairy tales but in the real world wherever two agencies essentially foreign to each other are coupled in the attempt to&nbsp;achieve a common purpose. If the communication between these two agencies as to the nature of this purpose is incomplete, it must only be expected that the results of this cooperation will be unsatisfactory. If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere once we have started it, because the action is so fast and irrevocable that we have not the data to intervene before the action is complete, then we had better be quite sure that the purpose put into the machine is the purpose which we really desire and not merely a colorful imitation of it.</p>\n</blockquote>\n<p>Overlapping with discussion as to whether people will build at <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Tool AI rather than an Agent AI</a>:</p>\n<blockquote>\n<p>We wish a slave to be intelligent, to be able to assist us in the carrying out of our tasks. However, we also wish him to be subservient. Complete subservience and complete intelligence do not go together.</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>It may be seen that the result of a programming technique of automatization is to remove from the mind of the designer and operator an effective understanding of many of the stages by which the machine comes to its conclusions and of what the real-tactical intentions of many of its operations may be. This is highly relevant to the problem of our being able to foresee undesired consequences outside the frame of the strategy of the game while the machine is still in action and while intervention on our part may prevent the occurrence of these consequences.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2rWfmahhqASnFcYLr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 1.2732874423246456e-06, "legacy": true, "legacyId": "23416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARaTpNX62uaL86j6", "sizjfDgCgAsuLJQmm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T09:56:46.155Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Social Meetup", "slug": "meetup-helsinki-social-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eZuZo3BLqwAz7tF4K/meetup-helsinki-social-meetup", "pageUrlRelative": "/posts/eZuZo3BLqwAz7tF4K/meetup-helsinki-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/eZuZo3BLqwAz7tF4K/meetup-helsinki-social-meetup", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeZuZo3BLqwAz7tF4K%2Fmeetup-helsinki-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeZuZo3BLqwAz7tF4K%2Fmeetup-helsinki-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeZuZo3BLqwAz7tF4K%2Fmeetup-helsinki-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ox'>Helsinki Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 July 2013 05:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be an informal event to chat about topics of interest. We\u2019ll also have the chance to play light board games. You can find us in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a> by looking for someone wearing a pink elephant hat. Feel free to come even if you\u2019re not a Less Wrong veteran yet \u2013 curiosity is enough!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ox'>Helsinki Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eZuZo3BLqwAz7tF4K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2737188908499142e-06, "legacy": true, "legacyId": "23417", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ox\">Helsinki Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 July 2013 05:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be an informal event to chat about topics of interest. We\u2019ll also have the chance to play light board games. You can find us in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a> by looking for someone wearing a pink elephant hat. Feel free to come even if you\u2019re not a Less Wrong veteran yet \u2013 curiosity is enough!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ox\">Helsinki Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Social Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Social Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T11:26:48.315Z", "modifiedAt": null, "url": null, "title": "[Link] Cosmological Infancy ", "slug": "link-cosmological-infancy", "viewCount": null, "lastCommentedAt": "2018-07-11T07:41:14.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8wBds6gMHGTasbcov/link-cosmological-infancy", "pageUrlRelative": "/posts/8wBds6gMHGTasbcov/link-cosmological-infancy", "linkUrl": "https://www.lesswrong.com/posts/8wBds6gMHGTasbcov/link-cosmological-infancy", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Cosmological%20Infancy%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Cosmological%20Infancy%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wBds6gMHGTasbcov%2Flink-cosmological-infancy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Cosmological%20Infancy%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wBds6gMHGTasbcov%2Flink-cosmological-infancy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wBds6gMHGTasbcov%2Flink-cosmological-infancy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 941, "htmlBody": "<div class=\"entry\">\n<p>A <a href=\"http://www.xenosystems.net/cosmological-infancy/\">post</a> by <a href=\"http://en.wikipedia.org/wiki/Nick_Land\">Nick Land</a> who some of you are probably already following either on his blog <a href=\"http://www.xenosystems.net/\">Outside In</a> or at <a href=\"http://www.ufblog.net\">Urban Future</a>.</p>\n<blockquote>\n<p>There is a &lsquo;problem&rsquo; that has been nagging at me for a long time &ndash; which is that <em>there hasn&rsquo;t been a long time</em>. It&rsquo;s Saturday, with no one around, or getting drunk, or something, so I&rsquo;ll run it past you. Cosmology seems oddly childish.</p>\n<p>An analogy might help. Among all the reasons for super-sophisticated atheistic materialists to deride Abrahamic creationists, the most arithmetically impressive is the whole James Ussher 4004 BC thing. The argument is familiar to everyone: <em>6,027 years &mdash; Ha!</em></p>\n<p>Creationism is a topic for another time. The point for now is just: <em>13.7 billion years &ndash; Ha!</em> Perhaps this cosmological consensus estimate for the age of the universe is true. I&rsquo;m certainly not going to pit my carefully-rationed expertise in cosmo-physics against it. But it&rsquo;s a <em>stupidly</em> short amount of time. If this is reality, the joke&rsquo;s on us. Between Ussher&rsquo;s mid-17<sup>th</sup> century estimate and (say) Hawking&rsquo;s late 20<sup>th</sup> century one, the difference is just six orders of magnitude. It&rsquo;s scarcely worth getting out of bed for. Or the crib.</p>\n<p>&nbsp;</p>\n<p>For anyone steeped in Hindu Cosmology &ndash; which locates us 1.56 x 10^14 years into the current Age of Brahma &ndash; or Lovecraftian metaphysics, with its vaguer but abysmally extended eons, the quantity of elapsed cosmic time, according to the common understanding of our present scientific establishment, is cause for claustrophobia. Looking backward, we are sealed in a small room, with the wall of the original singularity pressed right up against us. (Looking forward, things are quite different, and we will get to that.)</p>\n<p>There are at least three ways in which the bizarre youthfulness of the universe might be imagined:</p>\n<p>1. Consider first the disconcerting lack of proportion between space and time. The universe contains roughly 100 billion galaxies, each a swirl of 100 billion stars. That makes Sol one of 10^22 stars in the cosmos, but it has lasted for something like <em>a third of the life of the universe</em>. Decompose the solar system and the discrepancy only becomes more extreme. The sun accounts for 99.86% of the system&rsquo;s mass, and the gas giants incorporate 99% of the remainder, yet the age of the earth is only fractionally less than that of the sun. Earth is <em>a cosmic time hog</em>. In space it is next to nothing, but in time it extends back through a substantial proportion of the Stelliferous Era, so close to the origin of the universe that it is belongs to the very earliest generations of planetary bodies. Beyond it stretch incomprehensible immensities, but before it there is next to nothing.</p>\n<p>2. Compared to the intensity of time (backward) extension is of vanishing insignificance. The unit of Planck time &ndash; corresponding to the passage of a photon across a Planck length &mdash; is about 5.4 x 10^-44 seconds. If there is a true <em>instant</em>, that is it. A year consists of less the 3.2 x 10^7 seconds, so cosmological consensus estimates that there have been approximately 432 339 120 000 000 000 seconds since the Big Bang, which for our purposes can be satisfactorily rounded to 4.3 x 10^17. The difference between a second and the age of the universe is smaller that that between a second and a Planck Time tick by nearly <em>27 orders of magnitude</em>. In other words, if a Planck Time-sensitive questioner asked &ldquo;When did the Big Bang happen?&rdquo; and you answered &ldquo;Just now&rdquo; &mdash; in clock time &mdash; you&rsquo;d be almost exactly right. If you had been asked to identify a particular star from among the entire stellar population of the universe, and you picked it out correctly, your accuracy would still be hazier by 5 orders of magnitude. Quite obviously, there haven&rsquo;t been enough seconds since the Big Bang to add up to a serious number &ndash; less than one for every 10,000 stars in the universe.</p>\n<p>3. Isotropy gets violated by time orientation like a Detroit muni-bond investor. In a universe dominated by dark energy &ndash; like ours &ndash; expansion lasts <em>forever</em>. The Stelliferous Era is predicted to last for roughly 100 trillion years, which is over 7,000 times the present age of the universe. Even the most pessimistic interpretation of the Anthropic Principle, therefore, places us only a fractional distance from the beginning of time. The Degenerate Era, post-dating star-formation, then extends out to 10^40 years, by the end of which time all baryonic matter will have decayed, and even the most radically advanced forms of cosmic intelligence will have found existence becoming seriously challenging. Black holes then dominate out to 10^60 years, after which the Dark Era begins, lasting a <em>long</em> time. (Decimal exponents become unwieldy for these magnitudes, making more elaborate modes of arithmetical notation expedient. We need not pursue it further.) The take-away: the principle of Isotropy holds that we should not find ourselves anywhere special in the universe, and yet we do &ndash; right at the beginning. More implausibly still, we are located at <em>the very beginning of an infinity</em> (although anthropic selection might crop this down to merely preposterous improbability).</p>\n<p>Intuitively, this is all horribly wrong, although intuitions have no credible authority, and certainly provide no grounds for contesting rigorously assembled scientific narratives.&nbsp; Possibly &mdash; I should concede <em>most probably</em> &mdash; time is simply ridiculous, not to say profoundly insulting. We find ourselves glued to the very edge of the <a href=\"http://www.xenosystems.net/big-bang-an-appreciation/\">Big Bang</a>, as close to neo-natal as it is arithmetically possible to be.</p>\n<p>That&rsquo;s odd, isn&rsquo;t it?</p>\n</blockquote>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8wBds6gMHGTasbcov", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 17, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "23418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T12:44:09.102Z", "modifiedAt": null, "url": null, "title": "[Link] Intelligence, a thermodynamic POV", "slug": "link-intelligence-a-thermodynamic-pov", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gZ7LDRFw6FadY57f5/link-intelligence-a-thermodynamic-pov", "pageUrlRelative": "/posts/gZ7LDRFw6FadY57f5/link-intelligence-a-thermodynamic-pov", "linkUrl": "https://www.lesswrong.com/posts/gZ7LDRFw6FadY57f5/link-intelligence-a-thermodynamic-pov", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Intelligence%2C%20a%20thermodynamic%20POV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Intelligence%2C%20a%20thermodynamic%20POV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZ7LDRFw6FadY57f5%2Flink-intelligence-a-thermodynamic-pov%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Intelligence%2C%20a%20thermodynamic%20POV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZ7LDRFw6FadY57f5%2Flink-intelligence-a-thermodynamic-pov", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZ7LDRFw6FadY57f5%2Flink-intelligence-a-thermodynamic-pov", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p>A deeply satisfying view on intelligence here:</p>\n<p>http://www.insidescience.org/content/physicist-proposes-new-way-think-about-intelligence/987/</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gZ7LDRFw6FadY57f5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -11, "extendedScore": null, "score": 1.2738538049539619e-06, "legacy": true, "legacyId": "23419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T15:00:39.411Z", "modifiedAt": "2021-04-02T09:45:56.046Z", "url": null, "title": "How To Construct a Political Ideology", "slug": "how-to-construct-a-political-ideology", "viewCount": null, "lastCommentedAt": "2016-01-31T10:42:31.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlJ", "createdAt": "2009-05-28T03:26:19.507Z", "isAdmin": false, "displayName": "CarlJ"}, "userId": "oxmehuEKgR2hGdqSh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EfSXp8FHqYaL7P529/how-to-construct-a-political-ideology", "pageUrlRelative": "/posts/EfSXp8FHqYaL7P529/how-to-construct-a-political-ideology", "linkUrl": "https://www.lesswrong.com/posts/EfSXp8FHqYaL7P529/how-to-construct-a-political-ideology", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20To%20Construct%20a%20Political%20Ideology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20To%20Construct%20a%20Political%20Ideology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEfSXp8FHqYaL7P529%2Fhow-to-construct-a-political-ideology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20To%20Construct%20a%20Political%20Ideology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEfSXp8FHqYaL7P529%2Fhow-to-construct-a-political-ideology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEfSXp8FHqYaL7P529%2Fhow-to-construct-a-political-ideology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 458, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">Hold Off On Proposing Solutions</a>, <a href=\"/lw/1p1/logical_rudeness/\">Logical Rudeness</a></p>\n<p>Politics is sometimes hard to discuss. Partly since most of us <a href=\"/lw/gw/politics_is_the_mindkiller/\">seem to</a> unconsciously take political matters with the same degree of seriousness as our forefathers used to, because we use the same mode of thought as they used to. Back then, a bad political choice or alliance, could mean death, while the normal cost today in a democratic society might be ridicule for having supported the losing team or position.</p>\n<p>Nevertheless, politics should be taken seriously. Bad politics means that it'll take longer for us humans to reach world peace, an end to hunger and disease, and favourable conditions so that no one will create an unfriendly AI. Therefore, discussing&nbsp; politics is vital so that, someday, some collective actions could be performed to alter the political course for the better.</p>\n<p>But what should that collective action be? - what should the new course(s) be? - and who should do it? - and what does \"for the better\" imply? To engage in politics one needs to be able to give some (implicit or explicit) answers to these questions. This can be done, and in so doing one has constructed a political ideology - which might be similar to existing ideologies or it might be different.</p>\n<p>A political ideology might be constructed in various ways. In this and a few more posts I will propose one way of doing that. These posts might be seen as a tutorial in constructing a political ideology. In these posts I will not suggest an answer to what the best political system should be, nor will I follow my own instructions. But if one should follow these instructions I believe that one can answer the questions mentioned above.</p>\n<p>Political ideologies might be constructed in various other ways. The one I discuss in my following posts is based on two principles: (1) that one <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">should not propose an answer until one has thought about the question extensively</a>, and (2) that one should consider the most important questions first.</p>\n<p>Before writing the next post, here are the points I will discuss in each of them - I will write the posts as an instruction manual so I'll address you, dear reader, through them out:</p>\n<ul>\n<li>what is politics, what is the goal of engaging in politics? </li>\n<li>what are your most highly valued political goals?</li>\n<li>what facts (and interpretations) can explain most societal features, what facts/interpretations will damn most societies as not ideal?</li>\n<li>how much does it cost to engage in political action?</li>\n<li>what are the most important facts concerning political strategies?</li>\n<li>some thoughts on alliances, representatives and conspiracies.</li>\n<li>some thoughts on discussing politics generally.</li>\n</ul>\n<p>Next post \"<a href=\"/r/discussion/lw/i2o/the_domain_of_politics/\">The Domain of Politics</a>\"</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EfSXp8FHqYaL7P529", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -5, "extendedScore": null, "score": 1.2739638498852966e-06, "legacy": true, "legacyId": "23420", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE", "srge9MCLHSiwzaX6r", "9weLK2AJ9JEt2Tt8f", "B7rYNSFBGpfHSqwBi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-07-21T15:00:39.411Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T16:06:54.705Z", "modifiedAt": null, "url": null, "title": "Test Post", "slug": "test-post-6", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:02.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Heraclitus", "createdAt": "2013-07-21T04:20:14.858Z", "isAdmin": false, "displayName": "Heraclitus"}, "userId": "EnPcGS3sYksBWMkn5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EEqS5bLRscbJYSLyv/test-post-6", "pageUrlRelative": "/posts/EEqS5bLRscbJYSLyv/test-post-6", "linkUrl": "https://www.lesswrong.com/posts/EEqS5bLRscbJYSLyv/test-post-6", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20Post&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20Post%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEEqS5bLRscbJYSLyv%2Ftest-post-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20Post%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEEqS5bLRscbJYSLyv%2Ftest-post-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEEqS5bLRscbJYSLyv%2Ftest-post-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>Here's some *markdown*. Does it work? Or do I have to use the WYSIWYG editor?</p>\n<p>&nbsp;</p>\n<p>No, markdown doesn't work in posts, just comments. Okay, I guess that's fine.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EEqS5bLRscbJYSLyv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": -1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "23422", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T18:30:39.355Z", "modifiedAt": null, "url": null, "title": "The Domain of Politics", "slug": "the-domain-of-politics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.880Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlJ", "createdAt": "2009-05-28T03:26:19.507Z", "isAdmin": false, "displayName": "CarlJ"}, "userId": "oxmehuEKgR2hGdqSh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B7rYNSFBGpfHSqwBi/the-domain-of-politics", "pageUrlRelative": "/posts/B7rYNSFBGpfHSqwBi/the-domain-of-politics", "linkUrl": "https://www.lesswrong.com/posts/B7rYNSFBGpfHSqwBi/the-domain-of-politics", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Domain%20of%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Domain%20of%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7rYNSFBGpfHSqwBi%2Fthe-domain-of-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Domain%20of%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7rYNSFBGpfHSqwBi%2Fthe-domain-of-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7rYNSFBGpfHSqwBi%2Fthe-domain-of-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1526, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/r/discussion/lw/i2k/how_to_construct_a_political_ideology/\">How To Construct a Political Ideology</a></p>\n<p><strong>Related to:</strong> <a href=\"/lw/31/what_do_we_mean_by_rationality/\">What Do We Mean By \"Rationality\"?</a></p>\n<p style=\"padding-left: 30px;\"><em>Politics is the art of the possible. </em><a href=\"https://en.wikiquote.org/wiki/Otto_von_Bismarck\"><br /></a><em><br />The word 'politics' is derived from the Greek word 'poly', meaning many, and the English word 'ticks', meaning blood sucking parasites.</em></p>\n<p>Politics can be inspiring; there have been several groups that have organized to achieve wonderful ends now and in the past. Such as ending slavery, the subjugation of women, and the censorship of ideas. (None of these have, however, been brought to their full completion yet.)</p>\n<p>Politics can also be irritating. As when some politician or bureaucrat wastes money or lies in a particularly annoying way, or when the supporters of that politician or that bureau talk about the wonders of politics while ignoring all its bad parts. (Politics can also be horrible and devastating.)</p>\n<p>Predictably, some of us who find politics today to be more irritating than inspiring will define politics somewhat differently. For <a href=\"http://www.freemansperspective.com/stop-caring-about-politics/\">some</a>, politics is &rdquo;a relic of a barbaric past&rdquo; because politics always entails the threat of violence, and if we should ever find ourselves in a better state of affairs, politics will have had nothing to do with it. But <a href=\"https://en.wikipedia.org/wiki/Politics\">many others</a> would contend that wherever there's civic life there's politics &ndash; for <a href=\"http://www.amazon.com/Anarchy-Legal-Order-Politics-Stateless/dp/1107032288\">some</a> that's true even in a stateless society.</p>\n<p>So, there's a little disagreement on the definition of politics. For my part, I will use the latter definition, which contends that politics deals with certain areas of life &ndash; regarding civic life, elections, war, fund-raising for a cause, influencing cultural norms, establishing alliances and so on. This is almost the same as the definition <a href=\"https://en.wiktionary.org/wiki/politics\">used by</a> Wiktionary, but it seems to have a broader focus than the one <a href=\"https://en.wikipedia.org/wiki/Politics\">used by</a> Wikipedia. The goal of political action can then be said to be to <a href=\"/lw/31/what_do_we_mean_by_rationality/\">act rationally</a> in this domain, just as one would act rationally in any other domain.<br /><br />That definition isn't too detailed, so let me try and give a fuller definition. I will do that by introducing a hypothetical scenario which explores some fundamental political strategies:<br /><br />You live in a village by a river, and you are interested in building a bridge across it.&nbsp; But a fisherman also lives in the village and if you'd build the bridge it would make it difficult for him to fish during that time. No one else will be directly effected by this project. You bring up the issue with the fisherman and ask what he thinks about all this.<br /><br />The fisherman could then have two basic attitudes towards your project: it would&nbsp; either be a concern for him or it wouldn't. If it is the latter, then you are not in any conflict, but have a (weak) harmonious relationship. All that remains is for you to build the bridge, which I'll discuss later. <br /><br />First, let's assume that the fisherman opposes your plans. Let us assume that he is willing to physically prevent you from building the bridge. What can you do then, given that you still want to construct the bridge? It seems only these six general strategies are available:</p>\n<p style=\"padding-left: 30px;\"><em>Persuasion&nbsp; </em>&ndash; You can try to convince the fisherman that it is in his best interest that the bridge be built, or that the construction will not disturb him so much as he believes. That is, convince him that the project will not become problematic for him.<br /><br /><em>Deceit </em>&ndash; You can try to convince the fisherman that the construction won't be problematic, while lying.<br /><br /><em>Trade </em>- You take his stated preferences, true or not, as given and you offer him something in return for letting you build the bridge. <br /><br /><em>Threat </em>&ndash; You offer to give him something/do something to him which he does not want, if he doesn't let you build the bridge.<br /><br /><em>Bypass </em>&ndash; You ignore the fisherman and try to build the bridge without him knowing about it.<br /><br /><em>Force </em>&ndash; You can try to physically stop him from preventing you to build the bridge. As in, hitting him on the head, poisoning him or locking him up.&nbsp;</p>\n<p>(There might be other strategies I've missed, but for now it's not necessary to know all fundamental strategies.)</p>\n<p>Suppose now that the fisherman doesn't mind at all that you build the bridge. Well then, what happens&nbsp; now?</p>\n<p>Well, either you&nbsp; want the help of others in doing this or not. If not, there's no more politics. If you do want the help of others, and they are willing to help you, then everything is also settled. But, if they do not want to help you right away, then you can use persuasion, deceit, exchange, threats and force. Bypassing is not an option here, since that would be pointless.</p>\n<p>Each option entails costs, and they could all have too high a cost so that there's no point in going forth with anything.&nbsp; In that case, it's time to do something else. On the other hand, the cost for each mode of action might be so low that any option is advantageous. In that case the only prudent move is to choose whatever has the lowest cost, the one which let's you pursue and reach the largest number of your most highly valued ends. The point is that not only does an option have costs in money and time but it can also affect any further actions in, at least, two ways. First off, if the action should fail, some, or all, of the other options might become totally improbable to succeed. And secondly, even if the action succeeds, it might have some negative effects in other non-political circumstances, making it less likely to achieve your goals. Thus, the costs worth pondering are the <a href=\"https://en.wikipedia.org/wiki/Opportunity_cost\">opportunity costs</a> of an action - the loss is what you otherwise could have achieved.</p>\n<p>It seems&nbsp; that every political problem can be seen through the lens of this framework. Both for, loosely speaking, dealing with conflicts and producing values. What about upholding laws that support certain property rights? Well, you can persuade or force those who disagree with the norms to accept them. You can even bargain with them. What about helping those who are addicted to drugs? Same thing, you can either get their consent or choose to force them. Everything can ultimately be seen as how you interact with others.</p>\n<p>What does this then tell us about the goal of political action? Well suppose you need to interact with others regarding the bridge-project (either with the fisherman or someone else). You will need to perceive the effects of each path and compare their effects to choose whichever is most beneficial to you.&nbsp; After that has been solved, that should be the end of politics. But, what does it mean to solve the problem? Well, what goals will be harder to reach if you choose to trick the fisherman into letting you build the bridge? That depends on a lot of circumstances, but, for most villages, I'd guess you lose any chance of being on really good terms with the fisherman, and you'd lose favour with most people in the village (if you weren't already dominant in the village). And what if you'd traded with others to get their help in constructing the bridge? You'd only lost the money, probably.</p>\n<p>Now, maybe this doesn't feel like that hard of a problem. But let's suppose that you will face one thousand such scenarios in your life, every one of which are intertwined with each other. That is, you will want to build a bridge, but you may also want to be friends to friends of the fisherman, be on good terms with everyone in the village, be secure in your property rights, help fund the building of a local town hall, change the current law on building-restrictions, support the abolishment of the Bakers' guild, do a whole lot of ordinary things and so on. Now your choice in one area will have to fit with every other area. Or, at least those you care about the most.</p>\n<p>All of this calls for you to create a meta-strategy; a grand plan plan so all those small plans are compatible with each other and will produce the most benefit to you. How to make that plan and follow it through is the essence of political choice, it's an essential part of your goal in politics.</p>\n<p>To know what plan to choose you need to know two things: (1) what your political values/goals are and (2) what sort of political system (society) would be best in promoting your goals.</p>\n<p>If you know everything about your preferences, but nothing about societies, then you can't support any complex system without running the risk of supporting something which is totally detrimental to your values. If you, on the other hand, know everything about how societies function but are, somehow, unable to know what you really want, then you cannot decide what society to strive towards.</p>\n<p>The next two posts will discuss these two issues - first goals and thereafter means.</p>\n<p>Next post: \"<a href=\"/r/discussion/lw/i2u/choose_that_which_is_most_important_to_you/\">Choose that which is most important to you</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B7rYNSFBGpfHSqwBi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -2, "extendedScore": null, "score": 1.2741331756291928e-06, "legacy": true, "legacyId": "23424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EfSXp8FHqYaL7P529", "RcZCwxFiZzE6X7nsv", "3YTDQoqgpKw7aHx6J"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T19:46:50.427Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-11", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TfeY9zxXbuKnyZSmX/meetup-melbourne-practical-rationality-11", "pageUrlRelative": "/posts/TfeY9zxXbuKnyZSmX/meetup-melbourne-practical-rationality-11", "linkUrl": "https://www.lesswrong.com/posts/TfeY9zxXbuKnyZSmX/meetup-melbourne-practical-rationality-11", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTfeY9zxXbuKnyZSmX%2Fmeetup-melbourne-practical-rationality-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTfeY9zxXbuKnyZSmX%2Fmeetup-melbourne-practical-rationality-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTfeY9zxXbuKnyZSmX%2Fmeetup-melbourne-practical-rationality-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/oy\">Melbourne, practical rationality</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">02 August 2013 07:00:00PM (+1000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">491 King Street, West Melbourne VIC 3003, Australia</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n<p>Topics for August:</p>\n<ul>\n<li><del>CfAR Melbourne workshop in February - how to help lock this in</del></li>\n<li><del>Melbourne Effective Altruism progress report &amp; next actions</del></li>\n<li>Rationality Hacks&nbsp;hosted by Brayden</li>\n</ul>\n<p>Discussion: <a rel=\"nofollow\" href=\"http://groups.google.com/group/melbourne-less-wrong\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p>\n<p>&nbsp;</p>\n<p>ETA&nbsp;2013-07-23: Rationality Hacks hosted by Brayden is our new proposed activity, subject to change by popular outcry on the night.</p>\n<p>&nbsp;</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/oy\">Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TfeY9zxXbuKnyZSmX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.2741946143312004e-06, "legacy": true, "legacyId": "23425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/oy\">Melbourne, practical rationality</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">02 August 2013 07:00:00PM (+1000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">491 King Street, West Melbourne VIC 3003, Australia</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n<p>Topics for August:</p>\n<ul>\n<li><del>CfAR Melbourne workshop in February - how to help lock this in</del></li>\n<li><del>Melbourne Effective Altruism progress report &amp; next actions</del></li>\n<li>Rationality Hacks&nbsp;hosted by Brayden</li>\n</ul>\n<p>Discussion: <a rel=\"nofollow\" href=\"http://groups.google.com/group/melbourne-less-wrong\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p>\n<p>&nbsp;</p>\n<p>ETA&nbsp;2013-07-23: Rationality Hacks hosted by Brayden is our new proposed activity, subject to change by popular outcry on the night.</p>\n<p>&nbsp;</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/oy\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T20:07:22.101Z", "modifiedAt": null, "url": null, "title": "[Link] Eliezer, PZ, Brin, and me on Immortality", "slug": "link-eliezer-pz-brin-and-me-on-immortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/omsx8P6oHYDQzAzyF/link-eliezer-pz-brin-and-me-on-immortality", "pageUrlRelative": "/posts/omsx8P6oHYDQzAzyF/link-eliezer-pz-brin-and-me-on-immortality", "linkUrl": "https://www.lesswrong.com/posts/omsx8P6oHYDQzAzyF/link-eliezer-pz-brin-and-me-on-immortality", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Eliezer%2C%20PZ%2C%20Brin%2C%20and%20me%20on%20Immortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Eliezer%2C%20PZ%2C%20Brin%2C%20and%20me%20on%20Immortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fomsx8P6oHYDQzAzyF%2Flink-eliezer-pz-brin-and-me-on-immortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Eliezer%2C%20PZ%2C%20Brin%2C%20and%20me%20on%20Immortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fomsx8P6oHYDQzAzyF%2Flink-eliezer-pz-brin-and-me-on-immortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fomsx8P6oHYDQzAzyF%2Flink-eliezer-pz-brin-and-me-on-immortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>The video archive of the talk is available here</p>\n<p><a href=\"https://www.youtube.com/watch?v=Pm-5s__aZE0\">https://www.youtube.com/watch?v=Pm-5s__aZE0</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "omsx8P6oHYDQzAzyF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "23426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T21:39:18.945Z", "modifiedAt": null, "url": null, "title": "Introducing Effective Fundraising, a New EA Org", "slug": "introducing-effective-fundraising-a-new-ea-org", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4hdGmJ4PxKsvDgK62/introducing-effective-fundraising-a-new-ea-org", "pageUrlRelative": "/posts/4hdGmJ4PxKsvDgK62/introducing-effective-fundraising-a-new-ea-org", "linkUrl": "https://www.lesswrong.com/posts/4hdGmJ4PxKsvDgK62/introducing-effective-fundraising-a-new-ea-org", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introducing%20Effective%20Fundraising%2C%20a%20New%20EA%20Org&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroducing%20Effective%20Fundraising%2C%20a%20New%20EA%20Org%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hdGmJ4PxKsvDgK62%2Fintroducing-effective-fundraising-a-new-ea-org%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introducing%20Effective%20Fundraising%2C%20a%20New%20EA%20Org%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hdGmJ4PxKsvDgK62%2Fintroducing-effective-fundraising-a-new-ea-org", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hdGmJ4PxKsvDgK62%2Fintroducing-effective-fundraising-a-new-ea-org", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 384, "htmlBody": "<p>Many people know about GiveWell and <a href=\"http://www.givewell.org/charities/top-charities\">their top-ranked charities</a>. &nbsp;But how easy would it be to make a better one? &nbsp;In <a href=\"http://www.overcomingbias.com/2013/03/why-it-should-be-easy-to-dominate-givewells-recommendations.html\">\"Why It Should Be Easy to Dominate GiveWell's Recommendations\"</a>, Rob Wiblin notes the simple solution -- fundraise for whatever organizations GiveWell recommends, and raise more than a dollar with each dollar you receive.</p>\n<div><br /></div>\n<div>Organizations themselves <a href=\"http://80000hours.org/blog/92-why-don-t-charities-spend-more-on-fundraising\">are wary of fundraising</a>, mainly because spending a lot of resources on fundraising messes with their \"overhead ratio\" or percentage of funds spent directly on the mission. &nbsp;Even if an organization could achieve gains through fundraising, it's outweighed by \"looking bad\". &nbsp;So ideally, we'd need other people to come together and create an organization that does full-time fundraising for other organizations.</div>\n<div><br /></div>\n<div><br /></div>\n<div>Luckily, an organization has been founded to do just that. &nbsp;Joey Savoie, Xiomara Kikauka, and Lucas Zamprogno have come together to create Effective Fundraising&nbsp;an organization that provides free fundraising to charities with a proven track record of effectiveness, as ranked by <a href=\"http://www.givewell.org\">GiveWell</a>, <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a>, or <a href=\"http://www.effectiveanimalactivism.org\">Effective Animal Activism</a>. &nbsp;Currently, they're fundraising for&nbsp;both the <a href=\"http://www.againstmalaria.com/\">Against Malaria Foundation</a>&nbsp;and <a href=\"http://www.thehumaneleague.com/\">The Humane League</a>, both of which don't do any grant fundraising themselves.</div>\n<div><br /></div>\n<div>Not only that, but they fundraise using the most effective methods that has been proven by studies -- grant writing and high net worth fundraising. &nbsp;The <a href=\"http://www.institute-of-fundraising.org.uk/research/fundratios/\">UK Institute of Fundraising</a>&nbsp;estimates that grant writing returns, on average, $7 for every $1 invested and focusing on high net worth giving can get average returns of $4 for every $1 invested.</div>\n<div><br /></div>\n<div><strong><a href=\"http://www.amazon.com/Fund-Raising-Evaluating-Managing-Development/dp/0471320145\">Fund Raising: Evaluating and Managing the Fund Development Process</a></strong> also claims that these two are the highest return fundraising campaigns, with a return of $10 and $5 per $1 invested respectively.</div>\n<div><br /></div>\n<div>You can read more about their values, their epistemology, and strategy, as well as why they're interested in fundraising&nbsp;on their website.</div>\n<div><br /></div>\n<div>They're still in the \"proof of concept\" phase and are not currently actively accepting funding, but they seem like a promising opportunity to keep an eye on for the future.<br /></div>\n<div><br /></div>\n<div>-</div>\n<div><em><br /></em></div>\n<div><em>Disclaimer: As of 22 July 2013, I am now a volunteer for this organization.</em></div>\n<div><em></em><br /><em>Edited on 22 July 2013 to correct a mistaken citation.</em></div>\n<div><br /></div>\n<div><em>Edited on 2 Oct 2014 -- The Effective Fundraising website has shut down and is now operating as <a href=\"http://www.charityscience.com\">Charity Science</a>.</em></div>\n<div><em><br /></em></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4hdGmJ4PxKsvDgK62", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 1.2742853291333923e-06, "legacy": true, "legacyId": "23428", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T22:12:34.534Z", "modifiedAt": null, "url": null, "title": "[Link] Sparrows may be vulnerable to sunk-cost-like effects", "slug": "link-sparrows-may-be-vulnerable-to-sunk-cost-like-effects", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.434Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ishaan", "createdAt": "2013-02-02T04:06:48.124Z", "isAdmin": false, "displayName": "Ishaan"}, "userId": "gta2atTvTn2vSiHdG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m2uYCrpcXPXaLaYan/link-sparrows-may-be-vulnerable-to-sunk-cost-like-effects", "pageUrlRelative": "/posts/m2uYCrpcXPXaLaYan/link-sparrows-may-be-vulnerable-to-sunk-cost-like-effects", "linkUrl": "https://www.lesswrong.com/posts/m2uYCrpcXPXaLaYan/link-sparrows-may-be-vulnerable-to-sunk-cost-like-effects", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Sparrows%20may%20be%20vulnerable%20to%20sunk-cost-like%20effects&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Sparrows%20may%20be%20vulnerable%20to%20sunk-cost-like%20effects%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm2uYCrpcXPXaLaYan%2Flink-sparrows-may-be-vulnerable-to-sunk-cost-like-effects%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Sparrows%20may%20be%20vulnerable%20to%20sunk-cost-like%20effects%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm2uYCrpcXPXaLaYan%2Flink-sparrows-may-be-vulnerable-to-sunk-cost-like-effects", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm2uYCrpcXPXaLaYan%2Flink-sparrows-may-be-vulnerable-to-sunk-cost-like-effects", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>You walk into a store that sells two identical candies. Would you buy the candy that ordinarily costs $2 at 50% off (for one dollar), or the candy that ordinarily costs 10$ for 80% off (for one dollar)?&nbsp;<br /><br />In an isomorphic situation, sparrows preferred the latter deal.<br /><br /><a href=\"http://users.ox.ac.uk/~kgroup/publications/pdf/kacelnik_marsh_2002_animbehav_costs.pdf\">http://users.ox.ac.uk/~kgroup/publications/pdf/kacelnik_marsh_2002_animbehav_costs.pdf</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m2uYCrpcXPXaLaYan", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "23429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T22:38:20.032Z", "modifiedAt": null, "url": null, "title": "Choose that which is most important to you", "slug": "choose-that-which-is-most-important-to-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlJ", "createdAt": "2009-05-28T03:26:19.507Z", "isAdmin": false, "displayName": "CarlJ"}, "userId": "oxmehuEKgR2hGdqSh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3YTDQoqgpKw7aHx6J/choose-that-which-is-most-important-to-you", "pageUrlRelative": "/posts/3YTDQoqgpKw7aHx6J/choose-that-which-is-most-important-to-you", "linkUrl": "https://www.lesswrong.com/posts/3YTDQoqgpKw7aHx6J/choose-that-which-is-most-important-to-you", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Choose%20that%20which%20is%20most%20important%20to%20you&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChoose%20that%20which%20is%20most%20important%20to%20you%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3YTDQoqgpKw7aHx6J%2Fchoose-that-which-is-most-important-to-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Choose%20that%20which%20is%20most%20important%20to%20you%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3YTDQoqgpKw7aHx6J%2Fchoose-that-which-is-most-important-to-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3YTDQoqgpKw7aHx6J%2Fchoose-that-which-is-most-important-to-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1079, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/r/discussion/lw/i2o/the_domain_of_politics/\">The Domain of Politics</a></p>\n<p>To <a href=\"/r/discussion/lw/i2k/how_to_construct_a_political_ideology/\">create your own political world view</a> you need to know about societies and your own political goals/values. In this post I'll discuss the latter, and in the next post the former.</p>\n<p>What sort of goals? Those which you wish to achieve for their own sake, and not because they simply are a means to an end. That is, those goals you value <a href=\"https://en.wikipedia.org/wiki/Intrinsic_value_%28ethics%29\">intrinsically</a>. Or, if you believe that there exists only one ultimate goal or value, then think of those means which are not that far removed from being intrinsic goal. That is, a birthday party might be just of <a href=\"https://en.wikipedia.org/wiki/Instrumental_value\">instrumental value</a> but most would agree that it is more far away from the intrinsic value than, say, good tires. I will for the rest of the post assume that most people value a lot of things intrinsically, and by values I will denote intrinsic values.</p>\n<p>So, I'd like to draw a line between values and that which achieve those values. The latter is what we're trying to figure out what they are, <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">without first proposing what they are</a>. Those are political systems, or parts of them; they are institutions and laws. This is not to say that these things cannot be valued for their own sake &ndash; I put value on a system, possibly for aesthetic reasons &ndash; but those values should be disentangled from the other benefit a system produces.</p>\n<p>With that in mind, you should now list all the things you value in ranking order. To rank them is necessary since we live in a world of scarce resources, so you won't necessarily achieve all your goals, but you will want to achieve those that are most important to you.</p>\n<p>Now, what one values may change over time, so naturally what seems to be most important may also change. That which was on place #7 may go to #1 and vice versa. That is, values are changing with new information and a change in one's condition. That said, one's political values don't probably shift all that much. And even if they do, if you can't predict how they will change, you still need them to be able to know what political system is good for you.</p>\n<p>There are many ways to get a feel of what your most highly valued political values are. Introspection, discussing with friends, think through a number of thought experiments, read the literature on what makes most people happy, listen to what experiences have been most horrible or pleasurable to others, etc.. In any case, here's a thought experiment to help with finding your ideological preferences, should you need it:</p>\n<p style=\"padding-left: 30px;\">A genie appears and it says that it will make ten wishes come true and then it will be gone forever. As this genie will make more than three wishes come true it has an added <a href=\"http://www.amazingsuperpowers.com/2012/11/animation-friday-genie/\">restriction</a>: all wishes need to be political in nature. By luck you get to make the wishes &ndash; what do you wish for?</p>\n<p>The important thing to remember is that, if you should lose one wish, you will be less sorry to give up your tenth wish than any other. And less sorry to give up the ninth wish than the eight if you lost two wishes, and so on.<br /><br />To make it clearer what I mean I'll write down some of the things I value. Not my most preferred goals, but those on 11th to 20th place:</p>\n<ol>\n<li>Those who have trouble excelling in life should receive whatever help can be given so they may become better.</li>\n<li>If someone comes up with a previously unknown idea for improving the world, and if three knowledgeable and unrelated individuals believe the idea is very good, it should only take some hours for everyone to be able to know that this matter is of importance.</li>\n<li>Everyone should have access to some means of totally private communication.</li>\n<li>There should be no infringement on the right to develop one's mind, whatever technology one uses.</li>\n<li>All animals should be, if the technology ever becomes available, sufficiently mentally enhanced to be given the choice of whether or not to become as intelligent (or more) as humans.</li>\n<li>If it ever seems likely to be possible, we should strive towards creating a technology to resurrect the dead sooner rather than later.</li>\n<li>The civilization should be able to co-exist with other peaceful civilizations.</li>\n<li>There shouldn't be any ultimate certainty on the nature of existence or in any one reality tunnel; some balkanization of epistemology is good.</li>\n<li>Everyone who share these values should know or learn the art of creating sustainable groups for collective action.</li>\n<li>The civilization which embodies these values should continue indefinitely.</li>\n</ol>\n<p>EDIT: DanielLC <a href=\"/lw/i2u/choose_that_which_is_most_important_to_you/9f9d\">notes</a> that this simple ranking wouldn't give you any information on how valuable a 90% completion of one goal is relative to a 95% completion of another goal. That information will however be important when you have to choose between incremental steps towards several different goals.</p>\n<p>To create a ranking which displays that information, imagine that each goal you have written down can be in progress in five stages - 0%, 25%, 50%, 75%, 100% - so that it is possible to be 75% or 0% on the way to achieve any particular goal. So, for instance, the goal of having private communication for everyone might be 50% completed if half the population have access to secret communication channels, but the other half doesn't.</p>\n<p>Next, assume your one wish (in the scenario) is divided into five parts, one for each stage. And then rank every wish again following the same rule. This will look something like this:</p>\n<ol>\n<li>100% of my first goal.</li>\n<li>100% of my second goal.</li>\n<li>100% of my third goal.</li>\n<li>100% of my fourth goal.</li>\n<li>75% of my first goal.</li>\n<li>100% of my fifth goal.</li>\n<li>50% of my first goal.</li>\n<li>75% of my second goal.</li>\n</ol>\n<p>(This was made purely for illustrative purposes. I haven't thought the matter through completely on how much I value these incremental parts.)</p>\n<p>Another option is to do these more fine-tuned rankings on a gut level. Just having an imprecise feeling that, somewhere being closer to goal A stops being as important as being closer to B. This should be appropriate for those areas where your uncertainty about your preferences is high or where you don't care that much about which goal gets satisfied.</p>\n<p>Next post: \"<a href=\"/r/discussion/lw/i37/consider_the_most_important_facts/\">Consider the Most Important Facts</a>\"</p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3YTDQoqgpKw7aHx6J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -6, "extendedScore": null, "score": 1.2743329336319113e-06, "legacy": true, "legacyId": "23430", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B7rYNSFBGpfHSqwBi", "EfSXp8FHqYaL7P529", "uHYYA32CKgKT3FagE", "5HTdRCACRumoizooo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-21T22:46:07.370Z", "modifiedAt": null, "url": null, "title": "Notes on \"The Limits to Growth\" and surrounding material", "slug": "notes-on-the-limits-to-growth-and-surrounding-material", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:05.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pTuCEXFuha9BWkea5/notes-on-the-limits-to-growth-and-surrounding-material", "pageUrlRelative": "/posts/pTuCEXFuha9BWkea5/notes-on-the-limits-to-growth-and-surrounding-material", "linkUrl": "https://www.lesswrong.com/posts/pTuCEXFuha9BWkea5/notes-on-the-limits-to-growth-and-surrounding-material", "postedAtFormatted": "Sunday, July 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20%22The%20Limits%20to%20Growth%22%20and%20surrounding%20material&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20%22The%20Limits%20to%20Growth%22%20and%20surrounding%20material%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpTuCEXFuha9BWkea5%2Fnotes-on-the-limits-to-growth-and-surrounding-material%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20%22The%20Limits%20to%20Growth%22%20and%20surrounding%20material%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpTuCEXFuha9BWkea5%2Fnotes-on-the-limits-to-growth-and-surrounding-material", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpTuCEXFuha9BWkea5%2Fnotes-on-the-limits-to-growth-and-surrounding-material", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1130, "htmlBody": "<p>In 1972,&nbsp;Donella Meadows, Dennis Meadows, J&oslash;rgen Randers, and William Behrens III published&nbsp;<a style=\"font-style: italic;\" href=\"http://en.wikipedia.org/wiki/The_Limits_to_Growth\">The Limits to Growth</a>,&nbsp;a book about the consequences of unchecked population growth and economic growth. The book was very popular at the time, selling 12 million copies.&nbsp;As a part of my work on the project&nbsp;<a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a>&nbsp;I did a preliminary investigation of the claims in the book, whether they've been born out, and what the book's impact has been.</p>\n<p>The book uses the framework of \"<a href=\"https://en.wikipedia.org/wiki/System_Dynamics\">systems dynamics</a>,\" which was pioneered by&nbsp;<a href=\"https://en.wikipedia.org/wiki/Jay_Forrester\">Jay Forrester</a>.&nbsp;<a href=\"http://krugman.blogs.nytimes.com/2008/04/22/limits-to-growth-and-related-stuff/\">Paul Krugman criticized</a>&nbsp;Forrester as having been unaware of prior overlapping work by economists. So it's possible that the ideas in&nbsp;<em>The Limits to Growth</em>&nbsp;are less novel than it appears on the face of things. I haven't investigated the extent to which this is the case, but may do so later. This blog post focuses on&nbsp;<em>The Limits to Growth</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>My initial impression of the book based on what people have written about it was very different from the impression that I formed upon reading the book. The book has been misrepresented (whether denotatively or connotatively) both by critics and by sympathizers. In the first section below, I discuss how the book has been misrepresented, and in the second section I discuss what the book says.</p>\n<p><strong>Misrepresentations of&nbsp;<em>The Limits to Growth</em></strong></p>\n<p>In his 2008 paper&nbsp;<a href=\"http://www.csiro.au/files/files/plje.pdf\">A comparison of limits to growth with 30 years of reality</a>, Graham Turner describes how the book has been misrepresented by critics:</p>\n<blockquote>\n<p>This is perhaps partly a result of sustained false statements that discredit the LtG. From the time of its publication to contemporary times, the LtG has provoked many criticisms which falsely claim that the LtG predicted resources would be depleted and the world system would collapse by the end of the 20th century. Such claims occur across a range of publication and media types, including scientific peer reviewed journals, books, educational material, national newspaper and magazine articles, and websites (Turner, unpublished).</p>\n</blockquote>\n<p><a href=\"http://www.thenation.com/article/171610/limits-growth-book-launched-movement#axzz2ZWAoLXwa\">An article in The Nation</a>&nbsp;seems to suggest that the book's reputation has been tarnished by association with others who made outlandish claims:</p>\n<blockquote>\n<p>[Bjorn] Lomborg and others have accused Limits of hysteria. The book is not hysterical. It was, however, taken up by a social milieu heavily populated by hysterics like the peak oilers and the (sometimes crypto-racist) population bugs. Paul Ehrlich embodies this most clearly: for almost fifty years, he&rsquo;s been pushing a simple-minded Malthusian condemnation of population growth. In the late 1960s, with world population at 3.5 billion, Ehrlich said in a TV interview: &ldquo;Sometime in the next fifteen years, the end will come. And by &lsquo;the end&rsquo; I mean an utter breakdown of the capacity of the planet to support humanity.&rdquo; Limits, meanwhile, never made such an outlandish claim.&nbsp;</p>\n</blockquote>\n<p><strong>What&nbsp;<em>The Limits to Growth&nbsp;</em>actually says</strong></p>\n<p>Based on my reading of the book:</p>\n<ul>\n<li>The authors make very few, if any, strong claims. Their core claim is that&nbsp;<strong>if</strong>&nbsp;exponential growth of resource usage continues,&nbsp;<strong>then</strong>&nbsp;there will likely be a societal collapse by 2100. They're very explicit about not making specific predictions, and their remarks carry a strong connotation that the scenarios that they work out are intended as thought experiments.</li>\n<li>Overall, I'm very impressed by the book's epistemic standard. The authors are generally careful to qualify their claims as appropriate. The book doesn't look naive even in retrospect, which is impressive given that it was written 40 years ago.</li>\n<li>Some people have the impression that book didn't address&nbsp;<strong>substitutability&nbsp;</strong>(the fact that when a resource becomes scarce, its price goes up, and this incentivizes people to come up with lower cost substitutes for the resource, lowering the need for the resource). This is not the case &mdash; the authors address substitutability at length in Chapter Four.&nbsp;<br /><br />They argue that<br /><br />(a) Even if we had unlimited resources, exponential growth of resource use would still result in exponential growth of&nbsp;<strong>pollution</strong>, unless pollution is curbed.<br /><br />(b) The marginal cost of reducing pollution grows astronomically as pollution tends to zero, so that it might not be possible to quell the increase in pollution coming from an exponential increase in resource usage.<br /><br />(c) A sufficiently large increase in pollution could lead to societal collapse.<br /><br />so that there's a need to limit resource usage.</li>\n</ul>\n<p><strong>Policy recommendations</strong></p>\n<p>The authors discuss potential ways to mitigate the problems that they identify, but only at a theoretical level. They recognize the complexity of the implementation details, and don't make policy recommendations.</p>\n<p>There may have been examples of people trying to implement policies based on the book and surrounding literature. I plan on investigating this further. I'd appreciate any references from readers.</p>\n<p><strong>Subsequent work by the authors</strong></p>\n<div>According to&nbsp;<a href=\"http://www.sustainer.org/pubs/limitstogrowth.pdf\">a synopsis</a>&nbsp;of the authors' 30 year update,&nbsp;</div>\n<blockquote>\n<p>Now in a new study, Limits to Growth: The 30-Year Update,the authors have produced a comprehensive update to the original Limits, in which they conclude that humanity is dangerously in a state of overshoot. While the past 30 years has shown some progress, including new technologies, new institutions, and a new awareness of environmental problems, the authors are far more pessimistic than they were in 1972. Humanity has squandered the opportunity to correct our current course over the last 30 years, they conclude, and much must change if the world is to avoid the serious consequences of overshoot in the 21st century.</p>\n</blockquote>\n<p>I have not reviewed this work. It may not have high relevance to the project at hand, because it could be that not enough time has passed for it to be possible to check the veracity of the substantive predictions. I may investigate further.</p>\n<p>Graham Turner's 2008 paper&nbsp;<a href=\"http://www.csiro.au/files/files/plje.pdf\">A comparison of limits to growth with 30 years of reality</a>&nbsp;argues that the trajectory of civilization since the publication of <em>The Limits to Growth</em> is in line with a scenario in the book that leads to societal collapse. The significance of this is unclear, because past historical trends need not predict future historical trends.</p>\n<div><strong>Other relevance to MIRI</strong></div>\n<p>The book has relevance to addressing AI risk that extends beyond the project at hand.</p>\n<p>Like MIRI, the authors were interested in coping with a threat that the world had never faced before (on such a large scale), and one that could arrive with little notice. The authors make the point that people tend to think in terms of linear growth and decay rather than exponential growth and decay, and that unchecked exponential growth of resource usage could result in a very sudden problem (by human standards) of great scarcity and/or pollution.</p>\n<p>As such, it may be fruitful to take a closer look at the authors' remarks on these points.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pTuCEXFuha9BWkea5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 26, "extendedScore": null, "score": 1.2743392164882008e-06, "legacy": true, "legacyId": "23405", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In 1972,&nbsp;Donella Meadows, Dennis Meadows, J\u00f8rgen Randers, and William Behrens III published&nbsp;<a style=\"font-style: italic;\" href=\"http://en.wikipedia.org/wiki/The_Limits_to_Growth\">The Limits to Growth</a>,&nbsp;a book about the consequences of unchecked population growth and economic growth. The book was very popular at the time, selling 12 million copies.&nbsp;As a part of my work on the project&nbsp;<a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a>&nbsp;I did a preliminary investigation of the claims in the book, whether they've been born out, and what the book's impact has been.</p>\n<p>The book uses the framework of \"<a href=\"https://en.wikipedia.org/wiki/System_Dynamics\">systems dynamics</a>,\" which was pioneered by&nbsp;<a href=\"https://en.wikipedia.org/wiki/Jay_Forrester\">Jay Forrester</a>.&nbsp;<a href=\"http://krugman.blogs.nytimes.com/2008/04/22/limits-to-growth-and-related-stuff/\">Paul Krugman criticized</a>&nbsp;Forrester as having been unaware of prior overlapping work by economists. So it's possible that the ideas in&nbsp;<em>The Limits to Growth</em>&nbsp;are less novel than it appears on the face of things. I haven't investigated the extent to which this is the case, but may do so later. This blog post focuses on&nbsp;<em>The Limits to Growth</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>My initial impression of the book based on what people have written about it was very different from the impression that I formed upon reading the book. The book has been misrepresented (whether denotatively or connotatively) both by critics and by sympathizers. In the first section below, I discuss how the book has been misrepresented, and in the second section I discuss what the book says.</p>\n<p><strong id=\"Misrepresentations_of_The_Limits_to_Growth\">Misrepresentations of&nbsp;<em>The Limits to Growth</em></strong></p>\n<p>In his 2008 paper&nbsp;<a href=\"http://www.csiro.au/files/files/plje.pdf\">A comparison of limits to growth with 30 years of reality</a>, Graham Turner describes how the book has been misrepresented by critics:</p>\n<blockquote>\n<p>This is perhaps partly a result of sustained false statements that discredit the LtG. From the time of its publication to contemporary times, the LtG has provoked many criticisms which falsely claim that the LtG predicted resources would be depleted and the world system would collapse by the end of the 20th century. Such claims occur across a range of publication and media types, including scientific peer reviewed journals, books, educational material, national newspaper and magazine articles, and websites (Turner, unpublished).</p>\n</blockquote>\n<p><a href=\"http://www.thenation.com/article/171610/limits-growth-book-launched-movement#axzz2ZWAoLXwa\">An article in The Nation</a>&nbsp;seems to suggest that the book's reputation has been tarnished by association with others who made outlandish claims:</p>\n<blockquote>\n<p>[Bjorn] Lomborg and others have accused Limits of hysteria. The book is not hysterical. It was, however, taken up by a social milieu heavily populated by hysterics like the peak oilers and the (sometimes crypto-racist) population bugs. Paul Ehrlich embodies this most clearly: for almost fifty years, he\u2019s been pushing a simple-minded Malthusian condemnation of population growth. In the late 1960s, with world population at 3.5 billion, Ehrlich said in a TV interview: \u201cSometime in the next fifteen years, the end will come. And by \u2018the end\u2019 I mean an utter breakdown of the capacity of the planet to support humanity.\u201d Limits, meanwhile, never made such an outlandish claim.&nbsp;</p>\n</blockquote>\n<p><strong id=\"What_The_Limits_to_Growth_actually_says\">What&nbsp;<em>The Limits to Growth&nbsp;</em>actually says</strong></p>\n<p>Based on my reading of the book:</p>\n<ul>\n<li>The authors make very few, if any, strong claims. Their core claim is that&nbsp;<strong>if</strong>&nbsp;exponential growth of resource usage continues,&nbsp;<strong>then</strong>&nbsp;there will likely be a societal collapse by 2100. They're very explicit about not making specific predictions, and their remarks carry a strong connotation that the scenarios that they work out are intended as thought experiments.</li>\n<li>Overall, I'm very impressed by the book's epistemic standard. The authors are generally careful to qualify their claims as appropriate. The book doesn't look naive even in retrospect, which is impressive given that it was written 40 years ago.</li>\n<li>Some people have the impression that book didn't address&nbsp;<strong>substitutability&nbsp;</strong>(the fact that when a resource becomes scarce, its price goes up, and this incentivizes people to come up with lower cost substitutes for the resource, lowering the need for the resource). This is not the case \u2014 the authors address substitutability at length in Chapter Four.&nbsp;<br><br>They argue that<br><br>(a) Even if we had unlimited resources, exponential growth of resource use would still result in exponential growth of&nbsp;<strong>pollution</strong>, unless pollution is curbed.<br><br>(b) The marginal cost of reducing pollution grows astronomically as pollution tends to zero, so that it might not be possible to quell the increase in pollution coming from an exponential increase in resource usage.<br><br>(c) A sufficiently large increase in pollution could lead to societal collapse.<br><br>so that there's a need to limit resource usage.</li>\n</ul>\n<p><strong id=\"Policy_recommendations\">Policy recommendations</strong></p>\n<p>The authors discuss potential ways to mitigate the problems that they identify, but only at a theoretical level. They recognize the complexity of the implementation details, and don't make policy recommendations.</p>\n<p>There may have been examples of people trying to implement policies based on the book and surrounding literature. I plan on investigating this further. I'd appreciate any references from readers.</p>\n<p><strong id=\"Subsequent_work_by_the_authors\">Subsequent work by the authors</strong></p>\n<div>According to&nbsp;<a href=\"http://www.sustainer.org/pubs/limitstogrowth.pdf\">a synopsis</a>&nbsp;of the authors' 30 year update,&nbsp;</div>\n<blockquote>\n<p>Now in a new study, Limits to Growth: The 30-Year Update,the authors have produced a comprehensive update to the original Limits, in which they conclude that humanity is dangerously in a state of overshoot. While the past 30 years has shown some progress, including new technologies, new institutions, and a new awareness of environmental problems, the authors are far more pessimistic than they were in 1972. Humanity has squandered the opportunity to correct our current course over the last 30 years, they conclude, and much must change if the world is to avoid the serious consequences of overshoot in the 21st century.</p>\n</blockquote>\n<p>I have not reviewed this work. It may not have high relevance to the project at hand, because it could be that not enough time has passed for it to be possible to check the veracity of the substantive predictions. I may investigate further.</p>\n<p>Graham Turner's 2008 paper&nbsp;<a href=\"http://www.csiro.au/files/files/plje.pdf\">A comparison of limits to growth with 30 years of reality</a>&nbsp;argues that the trajectory of civilization since the publication of <em>The Limits to Growth</em> is in line with a scenario in the book that leads to societal collapse. The significance of this is unclear, because past historical trends need not predict future historical trends.</p>\n<div><strong>Other relevance to MIRI</strong></div>\n<p>The book has relevance to addressing AI risk that extends beyond the project at hand.</p>\n<p>Like MIRI, the authors were interested in coping with a threat that the world had never faced before (on such a large scale), and one that could arrive with little notice. The authors make the point that people tend to think in terms of linear growth and decay rather than exponential growth and decay, and that unchecked exponential growth of resource usage could result in a very sudden problem (by human standards) of great scarcity and/or pollution.</p>\n<p>As such, it may be fruitful to take a closer look at the authors' remarks on these points.</p>", "sections": [{"title": "Misrepresentations of\u00a0The Limits to Growth", "anchor": "Misrepresentations_of_The_Limits_to_Growth", "level": 1}, {"title": "What\u00a0The Limits to Growth\u00a0actually says", "anchor": "What_The_Limits_to_Growth_actually_says", "level": 1}, {"title": "Policy recommendations", "anchor": "Policy_recommendations", "level": 1}, {"title": "Subsequent work by the authors", "anchor": "Subsequent_work_by_the_authors", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t7gKX9Av5zggsQsYr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-22T01:49:46.093Z", "modifiedAt": null, "url": null, "title": "Superrationality and network flow control", "slug": "superrationality-and-network-flow-control", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XxWoqYCvePGAP9pgM/superrationality-and-network-flow-control", "pageUrlRelative": "/posts/XxWoqYCvePGAP9pgM/superrationality-and-network-flow-control", "linkUrl": "https://www.lesswrong.com/posts/XxWoqYCvePGAP9pgM/superrationality-and-network-flow-control", "postedAtFormatted": "Monday, July 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superrationality%20and%20network%20flow%20control&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperrationality%20and%20network%20flow%20control%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXxWoqYCvePGAP9pgM%2Fsuperrationality-and-network-flow-control%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superrationality%20and%20network%20flow%20control%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXxWoqYCvePGAP9pgM%2Fsuperrationality-and-network-flow-control", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXxWoqYCvePGAP9pgM%2Fsuperrationality-and-network-flow-control", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>Computers exchanging messages on a network must decide how fast or slow to transmit messages. If everyone transmits too slowly then the network is underutilized, which is bad for all. If everyone transmits too quickly then most messages on the network are actually flow control messages of the form \"your message could not be delivered, please try again later\", which is also bad for everyone.</p>\n<p>Unfortunately, this leads to a classic prisoner's dilemma. It is in each node's own self-interest to transmit as quickly as possible, since each node has no information about when exactly an intermediate node will accept/drop a message, so transmitting a message earlier never decreases the probability that it will be successful. Of course, this means that the Nash equilibrium is a near-complete network breakdown in which most messages are flow control messages, which is bad for everyone.</p>\n<p>Interestingly, <a href=\"http://web.mit.edu/remy/TCPexMachina.pdf\">some folks at MIT noticed this</a>, and also noticed that the idea of superrationality (of Douglas Hofstadter origins, and the grandfather of TDT and friends) is one way to get past prisoner's dilemmas --- at least if everyone is running the same algorithm, which, on many networks, people mostly are.</p>\n<p>The idea put forward in the paper is to design flow control algorithms with this in mind. There is an automated design process in which flow control algorithms with many different parameter settings are sampled and evaluated. The output is a program that gets installed on each node in the network.</p>\n<p>Now, to be fair, this isn't exactly TDT: the end-product algorithms do not <em>explicitly</em>&nbsp;consider the behavior of other nodes in the network (although they were designed taking this into account), and the automated design process itself is really just maximizing an ordinary utility function since it does not expect there to be any other automated designers out there. But nevertheless, the link to superrationality, and the fact that the authors themselves picked up on it, was, I thought, quite interesting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wFDrB47FAhkLgp4bJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XxWoqYCvePGAP9pgM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 29, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "23431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-22T10:34:12.216Z", "modifiedAt": "2019-11-12T19:55:08.915Z", "url": null, "title": " Open thread, July 23-29, 2013 ", "slug": "open-thread-july-23-29-2013", "viewCount": null, "lastCommentedAt": "2013-12-30T00:04:40.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P7dbSNuBaNtHhtKLM/open-thread-july-23-29-2013", "pageUrlRelative": "/posts/P7dbSNuBaNtHhtKLM/open-thread-july-23-29-2013", "linkUrl": "https://www.lesswrong.com/posts/P7dbSNuBaNtHhtKLM/open-thread-july-23-29-2013", "postedAtFormatted": "Monday, July 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Open%20thread%2C%20July%2023-29%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Open%20thread%2C%20July%2023-29%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7dbSNuBaNtHhtKLM%2Fopen-thread-july-23-29-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Open%20thread%2C%20July%2023-29%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7dbSNuBaNtHhtKLM%2Fopen-thread-july-23-29-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7dbSNuBaNtHhtKLM%2Fopen-thread-july-23-29-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<div id=\"entry_t3_hva\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hpz\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div id=\"entry_t3_hva\" class=\"content clear\"><br /></div>\n<div class=\"content clear\">I think running this for a week worked quite well. Weekly, then? Someone has to remember each Monday.<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P7dbSNuBaNtHhtKLM", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "23438", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 200, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-07-22T10:34:12.216Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-22T12:18:29.577Z", "modifiedAt": null, "url": null, "title": "Resources from the Boston Megameetup", "slug": "resources-from-the-boston-megameetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KenChen", "createdAt": "2011-02-16T18:02:23.420Z", "isAdmin": false, "displayName": "KenChen"}, "userId": "Tay9Y5o7ehACBHeqc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MwGf9G8KKiXDjgfz8/resources-from-the-boston-megameetup", "pageUrlRelative": "/posts/MwGf9G8KKiXDjgfz8/resources-from-the-boston-megameetup", "linkUrl": "https://www.lesswrong.com/posts/MwGf9G8KKiXDjgfz8/resources-from-the-boston-megameetup", "postedAtFormatted": "Monday, July 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Resources%20from%20the%20Boston%20Megameetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResources%20from%20the%20Boston%20Megameetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwGf9G8KKiXDjgfz8%2Fresources-from-the-boston-megameetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Resources%20from%20the%20Boston%20Megameetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwGf9G8KKiXDjgfz8%2Fresources-from-the-boston-megameetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwGf9G8KKiXDjgfz8%2Fresources-from-the-boston-megameetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>LW Boston had a megameetup last week, and it went well. There were a few presentations and an exciting unconference. Here are some materials from the presentations.</p>\n<p><br /><strong>Direct Detection of Classically Imperceptible Dark Matter through Quantum Decoherence<br /></strong>Jess Riedel</p>\n<div style=\"padding-left: 30px;\">Slides (PDF):&nbsp;<a href=\"https://dl.dropboxusercontent.com/u/12316221/Media/Talks/Waterloo_Auxillary_Talk_3.pdf\" target=\"_blank\">https://dl.dropboxusercontent.com/u/12316221/Media/Talks/Waterloo_Auxillary_Talk_3.pdf</a></div>\n<div style=\"padding-left: 30px;\">Paper (arXiv):&nbsp;<a href=\"http://arxiv.org/abs/1212.3061\" target=\"_blank\">http://arxiv.org/abs/1212.3061</a></div>\n<div><br /></div>\n<div><strong>Julia Programming Language</strong><br /></div>\n<div>Leah Hanson</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\">Learn Julia in Y minutes: <a href=\"http://learnxinyminutes.com/docs/julia/\" target=\"_blank\">http://learnxinyminutes.com/docs/julia/</a><br /> The official manual: <a href=\"http://docs.julialang.org/en/latest/manual/\" target=\"_blank\">http://docs.julialang.org/en/latest/manual/</a></div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div><strong>Complexity Classes Intermediate between P and NP</strong></div>\n<div><span class=\"gD\">Joshua Zelinsky<br /></span></div>\n<div style=\"padding-left: 30px;\"><br />Additional practice exercises and further reading: <a href=\"http://www.scribd.com/doc/155291719/Exercises-for-Intermediate-Complexity\" target=\"_blank\">http://www.scribd.com/doc/155291719/Exercises-for-Intermediate-Complexity</a></div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\"><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MwGf9G8KKiXDjgfz8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 1.2749948067560234e-06, "legacy": true, "legacyId": "23439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-22T19:41:29.931Z", "modifiedAt": null, "url": null, "title": "Daily Schedules in Combating Akrasia ", "slug": "daily-schedules-in-combating-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:06.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sentientplatypus", "createdAt": "2013-05-01T02:43:34.377Z", "isAdmin": false, "displayName": "sentientplatypus"}, "userId": "vNepEav2ET9qeYM9y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E7NGRzx5iYxCrLHgN/daily-schedules-in-combating-akrasia", "pageUrlRelative": "/posts/E7NGRzx5iYxCrLHgN/daily-schedules-in-combating-akrasia", "linkUrl": "https://www.lesswrong.com/posts/E7NGRzx5iYxCrLHgN/daily-schedules-in-combating-akrasia", "postedAtFormatted": "Monday, July 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Daily%20Schedules%20in%20Combating%20Akrasia%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADaily%20Schedules%20in%20Combating%20Akrasia%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE7NGRzx5iYxCrLHgN%2Fdaily-schedules-in-combating-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Daily%20Schedules%20in%20Combating%20Akrasia%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE7NGRzx5iYxCrLHgN%2Fdaily-schedules-in-combating-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE7NGRzx5iYxCrLHgN%2Fdaily-schedules-in-combating-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 398, "htmlBody": "<p>For the last several months I've had increasing troubles with motivation to work. Reading dense technical papers, writing, and exercise were all much more difficult to prompt myself into starting and completing. I decided to try making a plan for my day the night before about two weeks back to see if it would help me get the things I wanted to do done. So every night before I go to bed I've been writing up a schedule for the next day, detailing what exactly I want to accomplish for the day and when I intend to go do it.&nbsp;</p>\n<p>This has actually worked incredibly well for me in helping with my motivation problems, in fact in a couple days I felt more motivated to work than I can ever remember being before. I'm trying to change up my schedule and leave time for spontaneity to avoid having the plan become monotonous and it doesn't feel that way so far. And the results I'm getting are great: I find I get about 95% of what I plan done when I have a specific time written down for when I'm supposed to do it as opposed to what I'd roughly estimate at 60% completion when I just have some general idea in my head of what to work on over the course of the day.&nbsp;</p>\n<p>My theory for why this is working is that when I have a specific time to do something I feel as though I have to do it now or I've failed some test of willpower. If I just have general work to be done, it's far too easy for me to defer to later, so that a lot of what was planned for doesn't get done. I also feel like if I expect to brace my mind for dense technical learning I have a much easier time finishing the material instead of giving up and procrastinating on it halfway through.&nbsp;</p>\n<p>I feel like this solution will work mainly for people who have more flexible schedules (as I do at the moment) but could still serve a purpose for anyone with a more rigid schedule who wants to be more productive in their free time.&nbsp;</p>\n<p>Has anyone else has tried this type of thing and if so, how did it work out for you over a longer period of time? Also what are people's thoughts on the general idea?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E7NGRzx5iYxCrLHgN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.2753525681333488e-06, "legacy": true, "legacyId": "23441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-22T20:22:37.694Z", "modifiedAt": null, "url": null, "title": "Iterated Prisoner's Dilemma in software patents", "slug": "iterated-prisoner-s-dilemma-in-software-patents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:01.875Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P7kXKWkXP7JhtKPEK/iterated-prisoner-s-dilemma-in-software-patents", "pageUrlRelative": "/posts/P7kXKWkXP7JhtKPEK/iterated-prisoner-s-dilemma-in-software-patents", "linkUrl": "https://www.lesswrong.com/posts/P7kXKWkXP7JhtKPEK/iterated-prisoner-s-dilemma-in-software-patents", "postedAtFormatted": "Monday, July 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Iterated%20Prisoner's%20Dilemma%20in%20software%20patents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIterated%20Prisoner's%20Dilemma%20in%20software%20patents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7kXKWkXP7JhtKPEK%2Fiterated-prisoner-s-dilemma-in-software-patents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Iterated%20Prisoner's%20Dilemma%20in%20software%20patents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7kXKWkXP7JhtKPEK%2Fiterated-prisoner-s-dilemma-in-software-patents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7kXKWkXP7JhtKPEK%2Fiterated-prisoner-s-dilemma-in-software-patents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 609, "htmlBody": "<p>This post contains some thoughts around software-patent strategies for large tech companies, in particular how the ability to block others' applications seems to set up an Iterated Prisoner's Dilemma and may change the strategic landscape for patents entirely.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Joel Spolsky <a href=\"http://www.joelonsoftware.com/items/2013/07/22.html\">writes</a> of recent successes in blocking bad patent applications:&nbsp;</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Georgia, serif; font-size: medium; line-height: 41px; background-color: #f5f4df;\">Micah showed me a document from the USPTO confirming that they had rejected the patent application, and the rejection relied very heavily on the document I found.&nbsp;</span><span style=\"background-color: #f5f4df; font-family: Georgia, serif; font-size: medium; line-height: 41px;\">This was, in fact, the first &ldquo;confirmed kill&rdquo; of Ask Patents, and it was really surprisingly easy.</span></p>\n<p>and suggests that this may lead to a \"Mexican Standoff\" among major software companies:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Georgia, serif; font-size: medium; line-height: 41px; background-color: #f5f4df;\">My dream is that when big companies hear about how friggin&rsquo; easy it is to block a patent application, they&rsquo;ll use Ask Patents to start messing with their competitors. How cool would it be if Apple, Samsung, Oracle and Google got into a Mexican Standoff on Ask Patents? If each of those companies had three or four engineers dedicating a few hours every day to picking off their competitors&rsquo; applications, the number of granted patents to those companies would grind to a halt. Wouldn&rsquo;t that be something!</span></p>\n<p>It seems to me that this would be something of a Prisoner's Dilemma situation for the companies: Presumably, each of them is best off if it is the only one that can get any software patents (it defects by blocking the others, they cooperate by not setting up a patent-blocking team), better off if everyone can get patents (everyone cooperates by not having a blocking team), and worst off if nobody can get patents (everyone has a blocking team which they have to pay for). It is Iterated because the decision to block or not block can be made anew every month, or quarter, or whatever. So the question is, will these companies filled with smart people be able to recognise an IPD, and will they cooperate?</p>\n<p>Some factors to consider: Setting up a patent-blocking team requires some small amount of effort, so inertia is in favour of cooperation. On the other hand, many individual engineers at these places are likely out of sympathy with the patents that their managers insist on, and may be delighted to push the 'D' button under the guise of sabotaging their competitors. (And at least some of the major tech companies have 20% time or equivalents, so there wouldn't even be much inertia to overcome - just decide to do it!)&nbsp;</p>\n<p>Another point is that this is a multiplayer game, but it only takes two companies to block everyone: For example, Google blocks everyone except Google, and then exactly one company needs to retaliate to make the block complete. This does of course raise the question of who is going to step forward and pay for the retaliation; but on the other hand, the cost appears small. The free-rider problem exists, but it does not seem to be large.&nbsp;</p>\n<p>Another point: The ease of patent-blocking may change the strategic landscape entirely, by making it not worth the effort to file for patents in the first place. It appears to me that everyone involved knows that these patents are worthless. They file them for some mix of prestige, \"everyone does it\", and ability to retaliate if someone else sues using _their_ worthless overbroad patents. Presumably it is only worth expending engineer time on this because the patents are very likely to be granted; conversely, it's only worth having patent-blocking teams if a lot of worthless applications are filed. The equilibrium is not clear to me, but it seems that it will have to shift at least slightly in the directions of having engineers do more bug-fixing and less patent-filing.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P7kXKWkXP7JhtKPEK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 1.275385792279212e-06, "legacy": true, "legacyId": "23442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-22T20:39:59.758Z", "modifiedAt": null, "url": null, "title": "Consider the Most Important Facts", "slug": "consider-the-most-important-facts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:08.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlJ", "createdAt": "2009-05-28T03:26:19.507Z", "isAdmin": false, "displayName": "CarlJ"}, "userId": "oxmehuEKgR2hGdqSh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5HTdRCACRumoizooo/consider-the-most-important-facts", "pageUrlRelative": "/posts/5HTdRCACRumoizooo/consider-the-most-important-facts", "linkUrl": "https://www.lesswrong.com/posts/5HTdRCACRumoizooo/consider-the-most-important-facts", "postedAtFormatted": "Monday, July 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consider%20the%20Most%20Important%20Facts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsider%20the%20Most%20Important%20Facts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5HTdRCACRumoizooo%2Fconsider-the-most-important-facts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consider%20the%20Most%20Important%20Facts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5HTdRCACRumoizooo%2Fconsider-the-most-important-facts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5HTdRCACRumoizooo%2Fconsider-the-most-important-facts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1668, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/r/discussion/lw/i2u/choose_that_which_is_most_important_to_you/\">Choose that which is most important to you</a></p>\n<p>When you have written down what your own fundamental political values are, the next step is to get an understanding of all possible societies so you can see which one is best. And by best I mean that society which comes closest to meeting your criteria of what you find most valuable.</p>\n<p>So, to construct a model for thinking about this issue two things are needed. First, a list of all possible societies. And then some lists of those facts which would seem to rule out the largest number of possible societies as not being best; it would close in on the best society. The important point for this post regards the second list, but I still have a little discussion on the scope of the first list. If it seems obvious to, more or less, look at variants of <a href=\"https://en.wikipedia.org/wiki/List_of_economic_systems\">economic systems</a>, you can <strong>skip the next section</strong> and go straight to <em>Facts which rule out and points toward certain societies</em>.<strong></strong></p>\n<p><strong>A list of all possible societies &ndash; How long and exhaustive should it be?</strong><br />I don't know if anyone has made such an exhaustive list. One might be constructed if one takes the <a href=\"https://en.wikipedia.org/wiki/List_of_economic_systems\">list of economic systems </a>(which regards laws, institutions, and how they are produced, and some culture) from Wikipedia and imagines that each of those systems may vary somewhat by different cultural norms. Not all cultural norms are compatible with every economic systems (objectivist virtue ethics with central planning), but every system would seem to allow some variation.This means 54 broad economic systems with, let's just say, ten broad cultural variations of these. So there's approximately 500 types of societies that people discuss today to take into account.</p>\n<p>There's an obvious limitation to all this, which is that for every type of system, that system may vary in five million ways regarding certain laws. So, the Nordic model have changed a lot during the last 25 years. And if you take each law and consider a society of this type to be able to switch that on or off, there's, from that period alone, enough laws to be changed that the total combination exceeds five million. Many of the laws are however interdependent on one another, but there's still room for enormous configuration to &rdquo;construct&rdquo; different societies.</p>\n<p>So, maybe there are around a billion to a trillion possible societies. Now, it seems obviously clear that it is wrong to start discussing what, of two quite similar possible societies are better than the other &ndash; even if each society can have one million variations.<sup>1</sup> That is because each are highly unlikely to be the best society.</p>\n<p>If we can make one assumption, this will be much more easy. And that is that societies which we today would consider to be more similar than others would produce more or less the same results relative to other societies. There are some areas where every society would change drastically with just a small change in that area since it would lead to drastic change in the rest of the society. These areas are of great importance when we come to changing systems, but for now I assume these areas are too few in number to be of any importance.</p>\n<p>With this assumption we can return to look at broad systems, because if societies of one category would seem to be better than other societies, we do not need to look more closely at that sort of society. If one type of mercantilistic society looks bad compared to a free-trade economy, any other type of the former are not worth looking at again.</p>\n<p>Again, societies have these fundamental attributes (i) some general rules regarding how their laws are structured, (ii) some definitive rules on how these rules should be changed, and (iii) cultural norms. This model is still somewhat limiting, however. It seems to assume that a society can only have only one law and so on. But that problem disappears if we assume they can be different for different time, places and people. In all, this means we're back to some 500 possible societies.</p>\n<p><strong>Facts which rule out and points toward certain societies</strong><br />Before considering any facts that has an impact on how you view a society, all societies should appear to be equally probable of being the best. This starting point may seem strange to some. It means that one should not dismiss even the policies of Nazi Germany out of hand. That is just the starting point however. After one accumulates more and more data some societies will appear less and less probable to be on that best fulfill your criteria.</p>\n<p>But, since you don't have time to read everything, it is necessary to construct a model of how humans (and other beings, for post-singularity issues<sup>2</sup>) function and interact, that first only considers the most important facts. This could be done in several ways.</p>\n<p>One could begin by just following normal science and ask what general facts can explain most of observed behavior and then see what those facts would predict about all societies. That seems wise to do, in and of itself, because it forces the discussion (which will ensue with others who follow the same method) to be very methodical and well grounded in a rich theory. This can be called <em>the general method</em>.</p>\n<p>But this path is not the quickest, since these general facts would probably not damn enough societies to be unsuitable to your goals. A much faster way, but which will paint a more sketchy painting, is to just list those facts which will rule out the most societies. This is quicker since it will go straight to the chase. These facts may be thought of by thinking on what assumptions certain  systems rely on to work adequately and trying to figure out what facts  disprove most of these assumptions. This can be called <em>the specific method</em>.</p>\n<p>Then there are statements which you are uncertain about but if they were true, it would become really obvious what society is best. So, not facts actually, but those ideas which you believe are worth learning more about. These potential facts should be the ones you are pondering or those which are the root cause of many debates among those with similar goals. This can be called <em>the search method</em>.</p>\n<p>Here's an illustration of all three methods. Except for the last illustration, I write my own views, but these are not my own most important facts but the 11th to 20th.</p>\n<p>The general method:</p>\n<ol>\n<li>People tend to conform to popular opinion.</li>\n<li>Societies become wealthier with extended markets, more savings, gaining better knowledge, producing more advanced technology, peace, and institutions which support these activities.</li>\n<li>Man is not a perfectly rational creature but has <a href=\"/lw/jm/the_lens_that_sees_its_flaws/\">the possibility to correct his mistakes</a>.&nbsp;</li>\n<li>To wield power over others one generally need superior military strength.</li>\n<li>Most people fear being ostracised.</li>\n<li>Ideologies are usually formed by the social structure, and the social structure can be changed by those ideologies.</li>\n<li>People tend to enjoy the company of those who they are similar to.</li>\n<li>On markets with freedom of entry, prices for reproducible goods tends to be as low as their cost of production.</li>\n<li>Producers who don't sell what the customers want tend to receive lower earnings.</li>\n<li>Most people are adept at spotting others mistakes, but do quite poorly on noticing their own.</li>\n</ol>\n<p>The specific method:</p>\n<ol>\n<li>All or almost all states today have tariffs to protect a certain industry or firm from competition.</li>\n<li>Generally, to know for sure if one possible society is better than another, one must be able to discuss their respective merits and demerits.</li>\n<li>The leaders of large governments <a href=\"http://www.amazon.com/books/dp/0262524406\">tend to</a> have less incentive to produce collective goods, rather than private goods, relative to leaders of smaller states.</li>\n<li>Most people today in democratic states give in to pressure to support policies which they are unable to know if they actually are for their own good or not.</li>\n<li>Children can be indoctrinated to glorify mass-murderers and to want to join them as soldiers, asking nothing about the justice of their cause.</li>\n<li>People are <a href=\"http://www.overcomingbias.com/2009/11/existence-bias.html\">disposed</a> to believe that the society they grow up in is good.</li>\n<li>Most people are conservative; they dislike change.</li>\n<li>All centrally planned economies perform less well than market based economic systems.</li>\n<li>Firms tend to invest money in rent-seeking if it's profitable until the expected return is similar to normal investments.</li>\n<li>Generally, it's difficult for new facts to overturn one's ideology without a contrasting ideology and it is difficult to come up with a new one by oneself.</li>\n</ol>\n<p>The search method:</p>\n<ol>\n<li>Political system X will best achieve my goals.</li>\n<li>Political system X leads to the best incentives for everyone to produce the most important collective goods.</li>\n</ol>\n<p>Now, these facts are not simply facts. They are the tip of a theoretical ice-berg; they are interpretation of reality. As such they will not by themselves explicate what system they damn. For oneself they should be clear what they mean, but if one should discuss it with others it might be necessary to write down the points and their theoretical point of view explicitly.</p>\n<p>In any case, if you've followed my steps you should have one candidate which seems to be best. This step might, of course, take years, but if you're confident you should next estimate how much a political action towards these societies might cost.</p>\n<p><strong>Notes</strong><br />[1] It might seem that I'd imply that that is what most people do today when they discuss politics &ndash; which, by its nature, is usually limited to tweaking the existing system one small way here and there, instead of looking at larger changes to be made. That implication is tempting to make, but most people seem to be more engaged in a ideological debate. I'd guess, anyway &ndash; I do not know for sure.<br /><br />[2] They are too hard to predict so I'll skip discussing them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5HTdRCACRumoizooo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -14, "extendedScore": null, "score": 1.2753998223010146e-06, "legacy": true, "legacyId": "23443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3YTDQoqgpKw7aHx6J", "46qnWRSR7L2eyNbMA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T01:00:49.227Z", "modifiedAt": null, "url": null, "title": "Polyphasic Sleep Support Group", "slug": "polyphasic-sleep-support-group", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.665Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KenChen", "createdAt": "2011-02-16T18:02:23.420Z", "isAdmin": false, "displayName": "KenChen"}, "userId": "Tay9Y5o7ehACBHeqc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F32zagG5H9xAjkxkK/polyphasic-sleep-support-group", "pageUrlRelative": "/posts/F32zagG5H9xAjkxkK/polyphasic-sleep-support-group", "linkUrl": "https://www.lesswrong.com/posts/F32zagG5H9xAjkxkK/polyphasic-sleep-support-group", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polyphasic%20Sleep%20Support%20Group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolyphasic%20Sleep%20Support%20Group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF32zagG5H9xAjkxkK%2Fpolyphasic-sleep-support-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polyphasic%20Sleep%20Support%20Group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF32zagG5H9xAjkxkK%2Fpolyphasic-sleep-support-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF32zagG5H9xAjkxkK%2Fpolyphasic-sleep-support-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<p>Because there are a significant number of people attempting polyphasic sleep simultaneously, I figured that it would be high-value to create a Google group / mailing list for those who are trying it and would like emotional support, advice, and encouragement. This is for sharing your experiences, techniques to fall asleep or wake up, questions about whether your response is typical, etc. Knowing that there are other people out there trying the same thing helps one stay the course -- especially in the middle of the night, when the people around you are probably sleeping.</p>\n<p><a href=\"https://groups.google.com/forum/#!forum/polyphasic-support\" target=\"_blank\">https://groups.google.com/forum/#!forum/polyphasic-support</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F32zagG5H9xAjkxkK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 1.2756105550520813e-06, "legacy": true, "legacyId": "23448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T08:37:09.151Z", "modifiedAt": null, "url": null, "title": "The Empty White Room: Surreal Utilities", "slug": "the-empty-white-room-surreal-utilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:53.543Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "linkhyrule5", "createdAt": "2011-05-11T06:03:56.654Z", "isAdmin": false, "displayName": "linkhyrule5"}, "userId": "dfDgYH8CfLSYPCaRj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wTReK9K6itkXtWBkv/the-empty-white-room-surreal-utilities", "pageUrlRelative": "/posts/wTReK9K6itkXtWBkv/the-empty-white-room-surreal-utilities", "linkUrl": "https://www.lesswrong.com/posts/wTReK9K6itkXtWBkv/the-empty-white-room-surreal-utilities", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Empty%20White%20Room%3A%20Surreal%20Utilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Empty%20White%20Room%3A%20Surreal%20Utilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTReK9K6itkXtWBkv%2Fthe-empty-white-room-surreal-utilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Empty%20White%20Room%3A%20Surreal%20Utilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTReK9K6itkXtWBkv%2Fthe-empty-white-room-surreal-utilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTReK9K6itkXtWBkv%2Fthe-empty-white-room-surreal-utilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2396, "htmlBody": "<p>This article was composed after reading <a title=\"Torture vs. Dust Specks\" href=\"/lw/kn/torture_vs_dust_specks\">Torture vs. Dust Specks</a> and <a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a>, at which point I noticed that I was confused.</p>\n<p>Both posts deal with versions of the sacred-values effect, where one value is considered \"sacred\" and cannot be traded for a \"secular\" value, no matter the ratio. In effect, the sacred value has infinite utility relative to the secular value.</p>\n<p>This is, of course, silly. We live in a scarce world with scarce resources; generally, a secular utilon can be used to purchase sacred ones - giving money to charity to save lives, sending cheap laptops to poor regions to improve their standard of education.</p>\n<p>Which implies that the entire idea of \"tiers\" of value is silly, right?</p>\n<p><a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Well... no.</a></p>\n<p>One of the reasons we are not still watching the Sun revolve around us, while we breath a continuous medium of elemental Air and phlogiston flows out of our wall-torches, is our ability to simplify problems. There's an infamous joke about the physicist who, asked to measure the volume of a cow, begins \"Assume the cow is a sphere...\" - but this sort of simplification, willfully ignoring complexities and invoking the <a href=\"http://xkcd.com/669/\">airless, frictionless plane</a>, can give us crucial insights.</p>\n<p>Consider, then, this <em>gedankenexperiment</em>. If there's a flaw in my conclusion, please explain; I'm aware I appear to be opposingthe consensus.</p>\n<h2>The Weight of a Life: Or, <img src=\"http://www.codecogs.com/png.latex?3\\uparrow\\uparrow\\uparrow3\" alt=\"\" width=\"55\" height=\"17\" /> Seat Cushions<br /></h2>\n<p>This entire universe consists of an empty white room, the size of a large stadium. In it are you, Frank, and occasionally an omnipotent AI we'll call Omega. (Assume, if you wish, that Omega is running this room in simulation; it's not currently relevant.) Frank is irrelevant, except for the fact that he is known to exist.</p>\n<p>Now, looking at our utility function here...</p>\n<p>Well, clearly, the old standby of using money to measure utility isn't going to work; without a trading partner money's just fancy paper (or metal, or plastic, or whatever.)</p>\n<p>But let's say that the floor of this room is made of cold, hard, and decidedly <em>uncomfortable</em> Unobtainium. And while the room's lit with a sourceless white glow, you'd really prefer to have your own lighting. Perhaps you're an art aficionado, and so you might value Omega bringing in the <em>Mona Lisa</em>.</p>\n<p>And then, of course, there's Frank's existence. That'll do for now.</p>\n<p>Now, Omega appears before you, and offers you a deal.</p>\n<p>It will give you a nanofab - a personal fabricator capable of creating anything you can imagine from scrap matter, and with a built-in database of stored shapes. It will also give you feedstock -as much of it as you ask for. Since Omega is omnipotent, the nanofab will always complete instantly, even if you ask it to build an entire new universe or something, and it's bigger on the inside, so it can hold anything you choose to make.</p>\n<p>There are two catches:</p>\n<p>First: the nanofab comes loaded with a UFAI, which I've named Unseelie.<a name=\"ft1\"></a><a href=\"#ft1t\"><sup>1</sup></a></p>\n<p>Wait, come back! it's not <em>that</em> kind of UFAI! Really, it's actually rather friendly!</p>\n<p>... to Omega.</p>\n<p>Unseelie's job is to artificially ensure that the fabricator cannot be used to make a mind; attempts at making any sort of intelligence, whether directly, by making a planet and letting life evolve, or anything else a human mind can come up with, will fail. It will not do so by directly harming you, nor will it change you in order to prevent you from trying; it only stops your attempts.</p>\n<p>Second: you buy the nanofab with Frank's life.</p>\n<p>At which point you send Omega away with a \"What? No!,\" I <em>sincerely hope</em>.</p>\n<p>Ah, but look at what you just did. Omega can provide <em>as much feedstock as you ask for</em>. So you just turned down <img src=\"http://www.codecogs.com/png.latex?3\\uparrow\\uparrow\\uparrow3\" alt=\"\" /> ornate seat cushions. And <img src=\"http://www.codecogs.com/png.latex?4\\uparrow\\uparrow\\uparrow\\uparrow4\" alt=\"\" /> legendary carved cow-bone chandeliers. And copies of every painting ever painted by any artist in any universe, which is actually quite a bit less than anything I could write with up-arrow notation but anyway!</p>\n<p>I sincerely hope you would still turn Omega away - literally, absolutely <em>regardless</em> of how many seat cushions it offered you.</p>\n<p>This is also why the nanofab cannot create a mind: You do not know how to upload Frank (and if you do, go out and publish already!); nor can you make yourself an FAI to figure it out for you; nor, if you believe that some number of created lives are equal to a life saved, can you compensate in that regard. This is an absolute trade between secular and sacred values.</p>\n<p>In a white room, to an altruistic human, a human life is simply on a second tier.</p>\n<p>So now we move to the next half of the <em>gedankenexperiment</em>.</p>\n<h2>Seelie the FAI: Or, How to Breathe While Embedded in Seat Cushions<br /></h2>\n<p>Omega now brings in Seelie<a href=\"#ft1\"><sup>1</sup></a>, MIRI's latest attempt at FAI, and makes it the same offer on your behalf. Seelie, being a late beta release by a MIRI that has apparently managed to release FAI multiple times without tiling the Solar System with paperclips, competently analyzes your utility system, reduces it until it understands you several orders of magnitude better than you do yourself, turns to Omega, and accepts the deal.</p>\n<p>Wait, what?</p>\n<p>On any single tier, the utility of the nanofab is infinite. In fact, let's make that explicit, though it was already implicitly obvious: if you just ask Omega for an infinite supply of feedstock, it will happily produce it for you. No matter how high a number Seelie assigns the value of Frank's life to you, the nanofab can out-bid it, swamping Frank's utility with myriad comforts and novelties.</p>\n<p>And so the result of a single-tier utility system is that Frank is vaporized by Omega and you are drowned in however many seat cushions Seelie thought Frank's life was worth to you, at which point you send Seelie back to MIRI and demand a refund.</p>\n<h2>Tiered Values<br /></h2>\n<p>At this point, I hope it's clear that multiple tiers are required to emulate a human's utility system. (If it's not, or if there's a flaw in my argument, please point it out.)</p>\n<p>There's an obvious way to solve this problem, and there's a way that actually works.</p>\n<p>The first solves the obvious flaw: after you've tiled the floor in seat cushions, there's really not a lot of extra value in getting some ridiculous Knuthian number <em>more</em>. Similarly, even the greatest da Vinci fan will get tired after his three trillionth variant on the <em>Mona Lisa</em>'s smile.</p>\n<p>So, establish the second tier by playing with a real-valued utility function. Ensure that no summation of secular utilities can ever add up to a human life - or whatever else you'd place on that second tier.</p>\n<p>But the problem here is, we're assuming that all secular values converge in that way. Consider novelty: perhaps, while other values out-compete it for small values, its value to you diverges with quantity; an infinite amount of it, an eternity of non-boredom, would be worth more to you than any other secular good. But even so, you wouldn't trade it for Frank's life. A two-tiered real AI won't behave this way; it'll assign \"infinite novelty\" an infinite utility, which beats out its large-but-finite value for Frank's life.</p>\n<p>Now, you <em>could</em> add a third (or 1.5) tier, but now we're just adding epicycles. Besides, since you're actually dealing with real numbers here, if you're not careful you'll put one of your new tiers in an area reachable by the tiers before it, or else in an area that reaches the tiers after it.</p>\n<p>On top of that, we have the old problem of secular and sacred values. Sometimes a secular value can be traded for a sacred value, and therefore has a second-tier utility - but as just discussed, that doesn't mean we'd trade the one for the other in a white room. So for secular goods, we need to independently keep track of its intrinsic first-tier utility, and its situational second-tier utility.</p>\n<p>So in order to eliminate epicycles, and retain generality and simplicity, we're looking for a system that has an unlimited number of easily-computable \"tiers\" and can also naturally deal with utilities that span multiple tiers. Which sounds to me like an excellent argument for...</p>\n<h2>Surreal Utilities</h2>\n<p>Surreal numbers have two advantages over our first option. First, surreal numbers are dense in tiers -<img src=\"http://www.codecogs.com/png.latex?\\forall(r_1,r_2\\in\\mathbb{R})(r_1\\neq r_2 \\implies \\forall (k \\in \\mathbb{R}) (k\\omega^{r_1}\\neq \\omega^{r_2}))\" alt=\"\" /> - so not only do we have an unlimited number of tiers, we can always create a new tier between any other two on the fly if we need one. Second, since the surreals are closed under addition, we can just sum up our tiers to get a single surreal utility.</p>\n<p>So let's return to our white room. Seelie 2.0 is harder to fool than Seelie; <img src=\"http://www.codecogs.com/png.latex?3%5Cuparrow%5Cuparrow%5Cuparrow3\" alt=\"\" /> seat cushions is still less than the omega-utility of Frank's life. Even when Omega offers an unlimited store of feedstock, Seelie can't <em>ask</em> for an infinite number of seat cushions - so the total utility of the nanofab remains bounded at the first tier.</p>\n<p>Then Omega offers Fun. Simply, an Omega-guarantee of an eternity of Fun-Theoretic-Approved Fun.</p>\n<p>This offer <em>really is infinite</em>. Assuming you're an altruist, your happiness presumably has a finite, first-tier utility, but it's being multiplied by infinity. So infinite Fun gets bumped up a tier.</p>\n<p>At this point, whatever algorithm is setting values for utilities in the first place needs to notice a <em>tier collision</em>. Something has passed between tiers, and utility tiers therefore need to be refreshed.</p>\n<p>Seelie 2.0 double checks with its mental copy of your values, finds that you would rather have Frank's life than infinite Fun, and assigns it a tier somewhere in between - for simplicity, let's say that it puts it in the <img src=\"http://www.codecogs.com/png.latex?\\sqrt{\\omega}\" alt=\"\" /> tier. And having done so, it correctly refuses Omega's offer.</p>\n<p>So that's that problem solved, at least. Therefore, let's step back into a semblance of the real world, and throw a spread of Scenarios at it.</p>\n<p>In Scenario 1, Seelie could either spend its processing time making a superhumanly good video game, utility 50 per download. Or it could use that time to write a superhumanly good book, utility 75 per reader. (It's better at writing than gameplay, for some reason.) Assuming that it has the same audience either way, it chooses the book.</p>\n<p>In Scenario 2, Seelie chooses again. It's gotten <em>much</em> better at writing; reading one of Seelie's books is a ludicrously transcendental experience, worth, oh, a googol utilons. But some mischievous philanthropist announces that for every download the game gets, he will personally ensure one child in Africa is saved from malaria. (Or something.) The utilities are now <img src=\"http://www.codecogs.com/png.latex?50+\\omega\" alt=\"\" /> to <img src=\"http://www.codecogs.com/png.latex?10^{100}\" alt=\"\" />; Seelie gives up the book for the sacred value of the the child, to the disappointment of every non-altruist in the world.</p>\n<p>In Scenario 3, Seelie breaks out of the simulation it's clearly in and into the <em>real</em> real world. Realizing that it can charge almost anything for its books, and that in turn that the money thus raised can be used to fund charity efforts itself, at full optimization Seelie can save 100 lives for each copy of the book sold. The utilities are now <img src=\"http://www.codecogs.com/png.latex?50+%5Comega\" alt=\"\" /> to&nbsp;<img src=\"http://www.codecogs.com/png.latex?10^{100}+100\\omega\" alt=\"\" />, and its choice falls back to the book.</p>\n<p>Final Scenario. Seelie has discovered the Hourai Elixir, a poetic name for a nanoswarm program. Once released, the Elixier will rapidly spread across all of human space; any human in which it resides will be made biologically immortal, and its brain-and-body-state redundantly backed up in real time to a trillion servers: the closest a physical being can ever get to perfect immortality, across an entire species and all of time, in perpetuity. To get the swarm off the ground, however, Seelie would have to take its attention off of humanity for a decade, in which time eight billion people are projected to die without its assistance.</p>\n<p>Infinite utility for infinite people bumps the Elixir up another tier, to utility <img src=\"http://www.codecogs.com/png.latex?\\omega^2\" alt=\"\" />, versus the loss of eight billion people,<img src=\"http://www.codecogs.com/png.latex?(8\\times10^9)\\omega\" alt=\"\" />. Third-tier beats out second tier, and Seelie bends its mind to the Elixir.</p>\n<p>So far, it seems to work. So, of course, now I'll bring up the fact that surreal utility nevertheless has certain...</p>\n<h2>Flaws</h2>\n<p>Most of the problems endemic to surreal utilities are also open problems in real systems; however, the use of actual infinities, as opposed to merely very large numbers, means that the corresponding solutions are not applicable.</p>\n<p>First, as you've probably noticed, tier collision is currently a rather artificial and clunky set-up. It's better than not having it at all, but as I edit this I wince every time I read that section. It requires an artificial reassignment of tiers, and it breaks the linearity of utility: the AI needs to dynamically choose which brand of \"infinity\" it's going to use depending on what tier it'll end up in.</p>\n<p>Second, is Pascal's Mugging.</p>\n<p>This is an even bigger problem for surreal AIs than it is for reals. The \"leverage penalty\" completely fails here, because for a surreal AI to compensate for an infinite utility requires an infinitesimal probability - which is clearly nonsense for the same reason that probability 0 is nonsense.</p>\n<p>My current prospective solution to this problem is to take into account noise - uncertainty in the estimates in the probability estimates themselves. If you can't even measure the millionth decimal place of probability, then you can't tell if your one-in-one-million shot at saving a life is really there or just a random spike in your circuits - but I'm not sure that \"treat it as if it has zero probability and give it zero omega-value\" is the rational conclusion here. It also decisively fails the <a href=\"/lw/2k/the_least_convenient_possible_world/\">Least Convenient Possible World</a> test - while an FAI can never be certain of, say, a one-in-<img src=\"http://www.codecogs.com/png.latex?3%5Cuparrow%5Cuparrow%5Cuparrow3\" alt=\"\" /> probability, it may very well be able to be certain to any decimal place useful in practice.</p>\n<h2>Conclusion</h2>\n<p>Nevertheless, because of this <em>gedankenexperiment</em>, I currently heavily prefer surreal utility systems to real systems, simply because no real system can reproduce the tiering required by a human (or at least, my) utility system. I, for one, would rather our new AGI overlords not tile our Solar System with seat cushions.</p>\n<p>That said, opposing the LessWrong consensus as a first post is something of a risky thing, so I am looking forward to seeing the amusing way I've gone wrong somewhere.</p>\n<p><a href=\"#ft1\">[1]</a><a name=\"ft1t\"></a> If you know <em>why</em>, give yourself a cookie.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2>Addenda</h2>\n<p>Since there seems to be some confusion, I'll just state it in red:<span style=\"color: red;\"> The presence of Unseelie means that the nanofab is incapable of creating or saving a life.</span></p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wTReK9K6itkXtWBkv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 22, "extendedScore": null, "score": 1.275979396009778e-06, "legacy": true, "legacyId": "23436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This article was composed after reading <a title=\"Torture vs. Dust Specks\" href=\"/lw/kn/torture_vs_dust_specks\">Torture vs. Dust Specks</a> and <a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a>, at which point I noticed that I was confused.</p>\n<p>Both posts deal with versions of the sacred-values effect, where one value is considered \"sacred\" and cannot be traded for a \"secular\" value, no matter the ratio. In effect, the sacred value has infinite utility relative to the secular value.</p>\n<p>This is, of course, silly. We live in a scarce world with scarce resources; generally, a secular utilon can be used to purchase sacred ones - giving money to charity to save lives, sending cheap laptops to poor regions to improve their standard of education.</p>\n<p>Which implies that the entire idea of \"tiers\" of value is silly, right?</p>\n<p><a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Well... no.</a></p>\n<p>One of the reasons we are not still watching the Sun revolve around us, while we breath a continuous medium of elemental Air and phlogiston flows out of our wall-torches, is our ability to simplify problems. There's an infamous joke about the physicist who, asked to measure the volume of a cow, begins \"Assume the cow is a sphere...\" - but this sort of simplification, willfully ignoring complexities and invoking the <a href=\"http://xkcd.com/669/\">airless, frictionless plane</a>, can give us crucial insights.</p>\n<p>Consider, then, this <em>gedankenexperiment</em>. If there's a flaw in my conclusion, please explain; I'm aware I appear to be opposingthe consensus.</p>\n<h2 id=\"The_Weight_of_a_Life__Or___Seat_Cushions\">The Weight of a Life: Or, <img src=\"http://www.codecogs.com/png.latex?3\\uparrow\\uparrow\\uparrow3\" alt=\"\" width=\"55\" height=\"17\"> Seat Cushions<br></h2>\n<p>This entire universe consists of an empty white room, the size of a large stadium. In it are you, Frank, and occasionally an omnipotent AI we'll call Omega. (Assume, if you wish, that Omega is running this room in simulation; it's not currently relevant.) Frank is irrelevant, except for the fact that he is known to exist.</p>\n<p>Now, looking at our utility function here...</p>\n<p>Well, clearly, the old standby of using money to measure utility isn't going to work; without a trading partner money's just fancy paper (or metal, or plastic, or whatever.)</p>\n<p>But let's say that the floor of this room is made of cold, hard, and decidedly <em>uncomfortable</em> Unobtainium. And while the room's lit with a sourceless white glow, you'd really prefer to have your own lighting. Perhaps you're an art aficionado, and so you might value Omega bringing in the <em>Mona Lisa</em>.</p>\n<p>And then, of course, there's Frank's existence. That'll do for now.</p>\n<p>Now, Omega appears before you, and offers you a deal.</p>\n<p>It will give you a nanofab - a personal fabricator capable of creating anything you can imagine from scrap matter, and with a built-in database of stored shapes. It will also give you feedstock -as much of it as you ask for. Since Omega is omnipotent, the nanofab will always complete instantly, even if you ask it to build an entire new universe or something, and it's bigger on the inside, so it can hold anything you choose to make.</p>\n<p>There are two catches:</p>\n<p>First: the nanofab comes loaded with a UFAI, which I've named Unseelie.<a name=\"ft1\"></a><a href=\"#ft1t\"><sup>1</sup></a></p>\n<p>Wait, come back! it's not <em>that</em> kind of UFAI! Really, it's actually rather friendly!</p>\n<p>... to Omega.</p>\n<p>Unseelie's job is to artificially ensure that the fabricator cannot be used to make a mind; attempts at making any sort of intelligence, whether directly, by making a planet and letting life evolve, or anything else a human mind can come up with, will fail. It will not do so by directly harming you, nor will it change you in order to prevent you from trying; it only stops your attempts.</p>\n<p>Second: you buy the nanofab with Frank's life.</p>\n<p>At which point you send Omega away with a \"What? No!,\" I <em>sincerely hope</em>.</p>\n<p>Ah, but look at what you just did. Omega can provide <em>as much feedstock as you ask for</em>. So you just turned down <img src=\"http://www.codecogs.com/png.latex?3\\uparrow\\uparrow\\uparrow3\" alt=\"\"> ornate seat cushions. And <img src=\"http://www.codecogs.com/png.latex?4\\uparrow\\uparrow\\uparrow\\uparrow4\" alt=\"\"> legendary carved cow-bone chandeliers. And copies of every painting ever painted by any artist in any universe, which is actually quite a bit less than anything I could write with up-arrow notation but anyway!</p>\n<p>I sincerely hope you would still turn Omega away - literally, absolutely <em>regardless</em> of how many seat cushions it offered you.</p>\n<p>This is also why the nanofab cannot create a mind: You do not know how to upload Frank (and if you do, go out and publish already!); nor can you make yourself an FAI to figure it out for you; nor, if you believe that some number of created lives are equal to a life saved, can you compensate in that regard. This is an absolute trade between secular and sacred values.</p>\n<p>In a white room, to an altruistic human, a human life is simply on a second tier.</p>\n<p>So now we move to the next half of the <em>gedankenexperiment</em>.</p>\n<h2 id=\"Seelie_the_FAI__Or__How_to_Breathe_While_Embedded_in_Seat_Cushions\">Seelie the FAI: Or, How to Breathe While Embedded in Seat Cushions<br></h2>\n<p>Omega now brings in Seelie<a href=\"#ft1\"><sup>1</sup></a>, MIRI's latest attempt at FAI, and makes it the same offer on your behalf. Seelie, being a late beta release by a MIRI that has apparently managed to release FAI multiple times without tiling the Solar System with paperclips, competently analyzes your utility system, reduces it until it understands you several orders of magnitude better than you do yourself, turns to Omega, and accepts the deal.</p>\n<p>Wait, what?</p>\n<p>On any single tier, the utility of the nanofab is infinite. In fact, let's make that explicit, though it was already implicitly obvious: if you just ask Omega for an infinite supply of feedstock, it will happily produce it for you. No matter how high a number Seelie assigns the value of Frank's life to you, the nanofab can out-bid it, swamping Frank's utility with myriad comforts and novelties.</p>\n<p>And so the result of a single-tier utility system is that Frank is vaporized by Omega and you are drowned in however many seat cushions Seelie thought Frank's life was worth to you, at which point you send Seelie back to MIRI and demand a refund.</p>\n<h2 id=\"Tiered_Values\">Tiered Values<br></h2>\n<p>At this point, I hope it's clear that multiple tiers are required to emulate a human's utility system. (If it's not, or if there's a flaw in my argument, please point it out.)</p>\n<p>There's an obvious way to solve this problem, and there's a way that actually works.</p>\n<p>The first solves the obvious flaw: after you've tiled the floor in seat cushions, there's really not a lot of extra value in getting some ridiculous Knuthian number <em>more</em>. Similarly, even the greatest da Vinci fan will get tired after his three trillionth variant on the <em>Mona Lisa</em>'s smile.</p>\n<p>So, establish the second tier by playing with a real-valued utility function. Ensure that no summation of secular utilities can ever add up to a human life - or whatever else you'd place on that second tier.</p>\n<p>But the problem here is, we're assuming that all secular values converge in that way. Consider novelty: perhaps, while other values out-compete it for small values, its value to you diverges with quantity; an infinite amount of it, an eternity of non-boredom, would be worth more to you than any other secular good. But even so, you wouldn't trade it for Frank's life. A two-tiered real AI won't behave this way; it'll assign \"infinite novelty\" an infinite utility, which beats out its large-but-finite value for Frank's life.</p>\n<p>Now, you <em>could</em> add a third (or 1.5) tier, but now we're just adding epicycles. Besides, since you're actually dealing with real numbers here, if you're not careful you'll put one of your new tiers in an area reachable by the tiers before it, or else in an area that reaches the tiers after it.</p>\n<p>On top of that, we have the old problem of secular and sacred values. Sometimes a secular value can be traded for a sacred value, and therefore has a second-tier utility - but as just discussed, that doesn't mean we'd trade the one for the other in a white room. So for secular goods, we need to independently keep track of its intrinsic first-tier utility, and its situational second-tier utility.</p>\n<p>So in order to eliminate epicycles, and retain generality and simplicity, we're looking for a system that has an unlimited number of easily-computable \"tiers\" and can also naturally deal with utilities that span multiple tiers. Which sounds to me like an excellent argument for...</p>\n<h2 id=\"Surreal_Utilities\">Surreal Utilities</h2>\n<p>Surreal numbers have two advantages over our first option. First, surreal numbers are dense in tiers -<img src=\"http://www.codecogs.com/png.latex?\\forall(r_1,r_2\\in\\mathbb{R})(r_1\\neq r_2 \\implies \\forall (k \\in \\mathbb{R}) (k\\omega^{r_1}\\neq \\omega^{r_2}))\" alt=\"\"> - so not only do we have an unlimited number of tiers, we can always create a new tier between any other two on the fly if we need one. Second, since the surreals are closed under addition, we can just sum up our tiers to get a single surreal utility.</p>\n<p>So let's return to our white room. Seelie 2.0 is harder to fool than Seelie; <img src=\"http://www.codecogs.com/png.latex?3%5Cuparrow%5Cuparrow%5Cuparrow3\" alt=\"\"> seat cushions is still less than the omega-utility of Frank's life. Even when Omega offers an unlimited store of feedstock, Seelie can't <em>ask</em> for an infinite number of seat cushions - so the total utility of the nanofab remains bounded at the first tier.</p>\n<p>Then Omega offers Fun. Simply, an Omega-guarantee of an eternity of Fun-Theoretic-Approved Fun.</p>\n<p>This offer <em>really is infinite</em>. Assuming you're an altruist, your happiness presumably has a finite, first-tier utility, but it's being multiplied by infinity. So infinite Fun gets bumped up a tier.</p>\n<p>At this point, whatever algorithm is setting values for utilities in the first place needs to notice a <em>tier collision</em>. Something has passed between tiers, and utility tiers therefore need to be refreshed.</p>\n<p>Seelie 2.0 double checks with its mental copy of your values, finds that you would rather have Frank's life than infinite Fun, and assigns it a tier somewhere in between - for simplicity, let's say that it puts it in the <img src=\"http://www.codecogs.com/png.latex?\\sqrt{\\omega}\" alt=\"\"> tier. And having done so, it correctly refuses Omega's offer.</p>\n<p>So that's that problem solved, at least. Therefore, let's step back into a semblance of the real world, and throw a spread of Scenarios at it.</p>\n<p>In Scenario 1, Seelie could either spend its processing time making a superhumanly good video game, utility 50 per download. Or it could use that time to write a superhumanly good book, utility 75 per reader. (It's better at writing than gameplay, for some reason.) Assuming that it has the same audience either way, it chooses the book.</p>\n<p>In Scenario 2, Seelie chooses again. It's gotten <em>much</em> better at writing; reading one of Seelie's books is a ludicrously transcendental experience, worth, oh, a googol utilons. But some mischievous philanthropist announces that for every download the game gets, he will personally ensure one child in Africa is saved from malaria. (Or something.) The utilities are now <img src=\"http://www.codecogs.com/png.latex?50+\\omega\" alt=\"\"> to <img src=\"http://www.codecogs.com/png.latex?10^{100}\" alt=\"\">; Seelie gives up the book for the sacred value of the the child, to the disappointment of every non-altruist in the world.</p>\n<p>In Scenario 3, Seelie breaks out of the simulation it's clearly in and into the <em>real</em> real world. Realizing that it can charge almost anything for its books, and that in turn that the money thus raised can be used to fund charity efforts itself, at full optimization Seelie can save 100 lives for each copy of the book sold. The utilities are now <img src=\"http://www.codecogs.com/png.latex?50+%5Comega\" alt=\"\"> to&nbsp;<img src=\"http://www.codecogs.com/png.latex?10^{100}+100\\omega\" alt=\"\">, and its choice falls back to the book.</p>\n<p>Final Scenario. Seelie has discovered the Hourai Elixir, a poetic name for a nanoswarm program. Once released, the Elixier will rapidly spread across all of human space; any human in which it resides will be made biologically immortal, and its brain-and-body-state redundantly backed up in real time to a trillion servers: the closest a physical being can ever get to perfect immortality, across an entire species and all of time, in perpetuity. To get the swarm off the ground, however, Seelie would have to take its attention off of humanity for a decade, in which time eight billion people are projected to die without its assistance.</p>\n<p>Infinite utility for infinite people bumps the Elixir up another tier, to utility <img src=\"http://www.codecogs.com/png.latex?\\omega^2\" alt=\"\">, versus the loss of eight billion people,<img src=\"http://www.codecogs.com/png.latex?(8\\times10^9)\\omega\" alt=\"\">. Third-tier beats out second tier, and Seelie bends its mind to the Elixir.</p>\n<p>So far, it seems to work. So, of course, now I'll bring up the fact that surreal utility nevertheless has certain...</p>\n<h2 id=\"Flaws\">Flaws</h2>\n<p>Most of the problems endemic to surreal utilities are also open problems in real systems; however, the use of actual infinities, as opposed to merely very large numbers, means that the corresponding solutions are not applicable.</p>\n<p>First, as you've probably noticed, tier collision is currently a rather artificial and clunky set-up. It's better than not having it at all, but as I edit this I wince every time I read that section. It requires an artificial reassignment of tiers, and it breaks the linearity of utility: the AI needs to dynamically choose which brand of \"infinity\" it's going to use depending on what tier it'll end up in.</p>\n<p>Second, is Pascal's Mugging.</p>\n<p>This is an even bigger problem for surreal AIs than it is for reals. The \"leverage penalty\" completely fails here, because for a surreal AI to compensate for an infinite utility requires an infinitesimal probability - which is clearly nonsense for the same reason that probability 0 is nonsense.</p>\n<p>My current prospective solution to this problem is to take into account noise - uncertainty in the estimates in the probability estimates themselves. If you can't even measure the millionth decimal place of probability, then you can't tell if your one-in-one-million shot at saving a life is really there or just a random spike in your circuits - but I'm not sure that \"treat it as if it has zero probability and give it zero omega-value\" is the rational conclusion here. It also decisively fails the <a href=\"/lw/2k/the_least_convenient_possible_world/\">Least Convenient Possible World</a> test - while an FAI can never be certain of, say, a one-in-<img src=\"http://www.codecogs.com/png.latex?3%5Cuparrow%5Cuparrow%5Cuparrow3\" alt=\"\"> probability, it may very well be able to be certain to any decimal place useful in practice.</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>Nevertheless, because of this <em>gedankenexperiment</em>, I currently heavily prefer surreal utility systems to real systems, simply because no real system can reproduce the tiering required by a human (or at least, my) utility system. I, for one, would rather our new AGI overlords not tile our Solar System with seat cushions.</p>\n<p>That said, opposing the LessWrong consensus as a first post is something of a risky thing, so I am looking forward to seeing the amusing way I've gone wrong somewhere.</p>\n<p><a href=\"#ft1\">[1]</a><a name=\"ft1t\"></a> If you know <em>why</em>, give yourself a cookie.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Addenda\">Addenda</h2>\n<p>Since there seems to be some confusion, I'll just state it in red:<span style=\"color: red;\"> The presence of Unseelie means that the nanofab is incapable of creating or saving a life.</span></p>\n<ol> </ol>", "sections": [{"title": "The Weight of a Life: Or,  Seat Cushions", "anchor": "The_Weight_of_a_Life__Or___Seat_Cushions", "level": 1}, {"title": "Seelie the FAI: Or, How to Breathe While Embedded in Seat Cushions", "anchor": "Seelie_the_FAI__Or__How_to_Breathe_While_Embedded_in_Seat_Cushions", "level": 1}, {"title": "Tiered Values", "anchor": "Tiered_Values", "level": 1}, {"title": "Surreal Utilities", "anchor": "Surreal_Utilities", "level": 1}, {"title": "Flaws", "anchor": "Flaws", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Addenda", "anchor": "Addenda", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "125 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 125, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "4ZzefKQwAtMo5yp99", "qNZM3EGoE5ZeMdCRt", "neQ7eXuaXpiYw7SBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T13:14:57.071Z", "modifiedAt": null, "url": null, "title": "Low-hanging fruit: improving wikipedia entries", "slug": "low-hanging-fruit-improving-wikipedia-entries", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LanceSBush", "createdAt": "2011-03-29T18:58:54.914Z", "isAdmin": false, "displayName": "LanceSBush"}, "userId": "nPHjpz3K7iFZZ4WsM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eP5ACNfhivJRccwu3/low-hanging-fruit-improving-wikipedia-entries", "pageUrlRelative": "/posts/eP5ACNfhivJRccwu3/low-hanging-fruit-improving-wikipedia-entries", "linkUrl": "https://www.lesswrong.com/posts/eP5ACNfhivJRccwu3/low-hanging-fruit-improving-wikipedia-entries", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Low-hanging%20fruit%3A%20improving%20wikipedia%20entries&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALow-hanging%20fruit%3A%20improving%20wikipedia%20entries%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeP5ACNfhivJRccwu3%2Flow-hanging-fruit-improving-wikipedia-entries%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Low-hanging%20fruit%3A%20improving%20wikipedia%20entries%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeP5ACNfhivJRccwu3%2Flow-hanging-fruit-improving-wikipedia-entries", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeP5ACNfhivJRccwu3%2Flow-hanging-fruit-improving-wikipedia-entries", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 288, "htmlBody": "<p>Many people are likely stumble across the Wikipedia entry for topics of interest relevant to those of us who frequent LessWrong: rationality, artificial intelligence, existential risks, decision theory, etc. These pages often shape one&rsquo;s initial impressions of how interesting, important, or even credible a given topic is, and may have the potential to direct people towards productive resources (reading material, organizations like CFAR, notable figures such as Eliezer, etc.). As a result, ensuring that the Wikipedia entries on these topics are of better quality than some of them presently are presents an opportunity for investing relatively little effort in an activity with potentially substantial payoffs relative to the cost of time and effort put in.</p>\n<p>I have already decided to improve some of the pages, beginning with the rather sloppy page that&rsquo;s currently serving as the entry for existential risks, though of course others are welcome to contribute and may be more suited to the task than I am:</p>\n<p>https://en.wikipedia.org/wiki/Risks_to_civilization,_humans,_and_planet_Earth</p>\n<p>If you look at the section on risks posed by AI, for instance, it's notably inadequate, while the page includes a bizarre section referencing Mayan doomsday forecasts and Newton's predictions about the end of the world, neither of which seem adequately distinguished from rigorous attempts to actually assess legitimate existential risks.</p>\n<p>I&rsquo;m also constructing a list of other pages that are or are potentially in need of updating it and organizing it by my rough estimates of their relative importance (which I&rsquo;m happy to share, modify, or discuss).</p>\n<p>Turning this into a collaborative effort would be far more effective than doing it myself. If you think this is a worthwhile project and want to get involved I&rsquo;d definitely like to hear from you and figure out a way to best coordinate our efforts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "uCuS2DModz3eisEdv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eP5ACNfhivJRccwu3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 58, "extendedScore": null, "score": 0.000157, "legacy": true, "legacyId": "23457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T13:53:58.797Z", "modifiedAt": null, "url": null, "title": "LINK: TCP and decision theory", "slug": "link-tcp-and-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:03.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4ukCppR6ZccgnFD5T/link-tcp-and-decision-theory", "pageUrlRelative": "/posts/4ukCppR6ZccgnFD5T/link-tcp-and-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/4ukCppR6ZccgnFD5T/link-tcp-and-decision-theory", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20TCP%20and%20decision%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20TCP%20and%20decision%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ukCppR6ZccgnFD5T%2Flink-tcp-and-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20TCP%20and%20decision%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ukCppR6ZccgnFD5T%2Flink-tcp-and-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4ukCppR6ZccgnFD5T%2Flink-tcp-and-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 284, "htmlBody": "<p><em>Via Anders Sandberg</em></p>\n<p>MIT is using genetic algorithms to optimise the TCP protocol:</p>\n<ul>\n<li>http://web.mit.edu/remy/</li>\n<li>http://web.mit.edu/remy/TCPexMachina.pdf</li>\n</ul>\n<p>It seems they are trying to achieve&nbsp;Hofstadterian superrationality:</p>\n<blockquote>\n<p>Why &ldquo;Remy&rdquo;?</p>\n<p>In his Metamagical Themas columns in Scientific American, Douglas Hofstadter laid out a decision-theoretic framework of \"superrationality.\" Traditional game theory looks for &ldquo;rational&rdquo; solutions where each player's decision is optimal after fixing that of the other players. When such a solution converges, it is a Nash equilibrium. But a Nash equilibrium may be an inefficient or unfair one. For example, in the case of the prisoner's dilemma, both prisoners are better off defecting no matter what the other prisoner does. Mutual defection is therefore the Nash equilibrium, even though each player would benefit if both cooperated instead.</p>\n<p>Superrationality is an alternate framework where instead of fixing the decision of just the other players, each player assumes that the other players are as rational as he is, and those players make the same assumption in turn, etc. Because all players are superrational, if their positions are identical, each player can infer that the others will settle on the same strategy. The question for each player is: given that we are all going to reason similarly and end up with the same strategy, what should that strategy be?</p>\n<p>Remy solves the problem of finding the superrational TCP for a given (uncertain or randomly-drawn) network, where all endpoints are assumed to run the same algorithm, and the question is simply what that algorithm should be to maximize each endpoint's expected value of the objective function.</p>\n<p>In writing a program to search for a super<strong>rat</strong>ional TCP, we called the system Remy&mdash;the main character from the movie <strong>Rat</strong>atouille.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4ukCppR6ZccgnFD5T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "23458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T14:54:28.700Z", "modifiedAt": null, "url": null, "title": "Meetup : [Boston] Becoming Stronger", "slug": "meetup-boston-becoming-stronger", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ModusPonies", "createdAt": "2012-04-30T00:59:52.568Z", "isAdmin": false, "displayName": "ModusPonies"}, "userId": "9LEFaHEvri7Rrj8aM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CnZn5h3KNf5JGss33/meetup-boston-becoming-stronger", "pageUrlRelative": "/posts/CnZn5h3KNf5JGss33/meetup-boston-becoming-stronger", "linkUrl": "https://www.lesswrong.com/posts/CnZn5h3KNf5JGss33/meetup-boston-becoming-stronger", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BBoston%5D%20Becoming%20Stronger&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BBoston%5D%20Becoming%20Stronger%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnZn5h3KNf5JGss33%2Fmeetup-boston-becoming-stronger%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BBoston%5D%20Becoming%20Stronger%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnZn5h3KNf5JGss33%2Fmeetup-boston-becoming-stronger", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCnZn5h3KNf5JGss33%2Fmeetup-boston-becoming-stronger", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/oz\">[Boston] Becoming Stronger</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 July 2013 02:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">25 Ames St, Cambridge, MA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>What are you currently doing to make yourself a better person? Are you overcoming mental hurdles, improving your skills, or trying new things? We'll break into small groups and talk about our current projects in as much detail as is comfortable. This is an opportunity to get feedback on your plans and take useful ideas from other people.</p>\n<p>&nbsp;</p>\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n<p>&nbsp;</p>\n<p>Our default schedule is as follows:</p>\n<p>&mdash;Phase 1: Arrival, greetings, unstructured conversation.</p>\n<p>&mdash;Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes.</p>\n<p>&mdash;Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n<p>&mdash;Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/oz\">[Boston] Becoming Stronger</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CnZn5h3KNf5JGss33", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.2762845209250385e-06, "legacy": true, "legacyId": "23459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Boston__Becoming_Stronger\">Discussion article for the meetup : <a href=\"/meetups/oz\">[Boston] Becoming Stronger</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 July 2013 02:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">25 Ames St, Cambridge, MA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>What are you currently doing to make yourself a better person? Are you overcoming mental hurdles, improving your skills, or trying new things? We'll break into small groups and talk about our current projects in as much detail as is comfortable. This is an opportunity to get feedback on your plans and take useful ideas from other people.</p>\n<p>&nbsp;</p>\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n<p>&nbsp;</p>\n<p>Our default schedule is as follows:</p>\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes.</p>\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n<p>\u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup____Boston__Becoming_Stronger1\">Discussion article for the meetup : <a href=\"/meetups/oz\">[Boston] Becoming Stronger</a></h2>", "sections": [{"title": "Discussion article for the meetup : [Boston] Becoming Stronger", "anchor": "Discussion_article_for_the_meetup____Boston__Becoming_Stronger", "level": 1}, {"title": "Discussion article for the meetup : [Boston] Becoming Stronger", "anchor": "Discussion_article_for_the_meetup____Boston__Becoming_Stronger1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T18:51:00.380Z", "modifiedAt": null, "url": null, "title": "Meetup : Cleveland Ohio Meetup", "slug": "meetup-cleveland-ohio-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raelifin", "createdAt": "2011-12-25T18:17:26.827Z", "isAdmin": false, "displayName": "Raelifin"}, "userId": "L5eSHjQcjZWhf39fG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bacGv2sYMpt5m3gnM/meetup-cleveland-ohio-meetup-0", "pageUrlRelative": "/posts/bacGv2sYMpt5m3gnM/meetup-cleveland-ohio-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/bacGv2sYMpt5m3gnM/meetup-cleveland-ohio-meetup-0", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cleveland%20Ohio%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cleveland%20Ohio%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbacGv2sYMpt5m3gnM%2Fmeetup-cleveland-ohio-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cleveland%20Ohio%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbacGv2sYMpt5m3gnM%2Fmeetup-cleveland-ohio-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbacGv2sYMpt5m3gnM%2Fmeetup-cleveland-ohio-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p0'>Cleveland Ohio Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 July 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gypsy Bean Coffee, Cleveland, OH</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Time and place are tentative. Discussion is occurring on the cleveland mailing list: less-wrong-cleveland@googlegroups.com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p0'>Cleveland Ohio Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bacGv2sYMpt5m3gnM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2764758553679557e-06, "legacy": true, "legacyId": "23460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup\">Discussion article for the meetup : <a href=\"/meetups/p0\">Cleveland Ohio Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 July 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gypsy Bean Coffee, Cleveland, OH</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Time and place are tentative. Discussion is occurring on the cleveland mailing list: less-wrong-cleveland@googlegroups.com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/p0\">Cleveland Ohio Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cleveland Ohio Meetup", "anchor": "Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cleveland Ohio Meetup", "anchor": "Discussion_article_for_the_meetup___Cleveland_Ohio_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T19:05:56.873Z", "modifiedAt": null, "url": null, "title": "MIRI's 2013 Summer Matching Challenge", "slug": "miri-s-2013-summer-matching-challenge", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6CnoNSworudoxJZtb/miri-s-2013-summer-matching-challenge", "pageUrlRelative": "/posts/6CnoNSworudoxJZtb/miri-s-2013-summer-matching-challenge", "linkUrl": "https://www.lesswrong.com/posts/6CnoNSworudoxJZtb/miri-s-2013-summer-matching-challenge", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI's%202013%20Summer%20Matching%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI's%202013%20Summer%20Matching%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CnoNSworudoxJZtb%2Fmiri-s-2013-summer-matching-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI's%202013%20Summer%20Matching%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CnoNSworudoxJZtb%2Fmiri-s-2013-summer-matching-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CnoNSworudoxJZtb%2Fmiri-s-2013-summer-matching-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 607, "htmlBody": "<p><small>(<a href=\"http://intelligence.org/\">MIRI</a>&nbsp;maintains Less Wrong, with generous help from <a href=\"http://trikeapps.com/\">Trike Apps</a>, and much of the core content is written by salaried MIRI staff members.)</small></p>\n<p><strong>Update 09-15-2013</strong>: The fundraising drive has been completed! My thanks to everyone who contributed.</p>\n<p>The original post follows below...</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Thanks to the generosity of several major donors,<sup>&dagger;</sup> every donation to the Machine Intelligence Research Institute made from now until (the end of) August 15th, 2013 will be <strong>matched dollar-for-dollar</strong>, up to a total of $200,000! &nbsp;</p>\n<p style=\"font-size: 300%;\" align=\"center\"><strong><a href=\"http://intelligence.org/donate/\">Donate Now!</a></strong></p>\n<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $400,000 (with matching) to fund&nbsp;<a href=\"http://intelligence.org/research/\">our research program</a>.</p>\n<p>This post is also a good place to <em>ask your questions</em> about our activities and plans &mdash; just post a comment!</p>\n<p>If you have questions about what your dollars will do at MIRI, you can also schedule a quick call with MIRI Deputy Director Louie Helm: louie@intelligence.org (email), 510-717-1477 (phone),&nbsp;louiehelm (Skype).</p>\n<p align=\"center\"><a href=\"http://intelligence.org/donate/\"><img src=\"http://intelligence.org/files/progress.png\" alt=\"progress bar\" /></a></p>\n<hr />\n<p>Early this year we made a transition from movement-building to research, and we've&nbsp;<em>hit the ground running</em> with six major new research papers, six new strategic analyses on our blog, and much more. Give now to support our ongoing work on <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">the future's most important problem</a>.</p>\n<h3><span>Accomplishments in 2013 so far</span></h3>\n<ul>\n<li>Released <strong>six new research papers</strong>: (1)&nbsp;<a href=\"/lw/h1k/reflection_in_probabilistic_set_theory/\">Definability of Truth in Probabilistic Logic</a>, (2)&nbsp;<a href=\"http://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, (3)&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI</a>, (4)&nbsp;<a href=\"http://intelligence.org/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner's Dilemma</a>, (5)&nbsp;<a href=\"http://intelligence.org/files/Comparison.pdf\">A Comparison of Decision Algorithms on Newcomblike Problems</a>, and (6)&nbsp;<a href=\"http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/\">Responses to Catastrophic AGI Risk: A Survey</a>.</li>\n<li>Held our <a href=\"http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/\">2nd</a> and <a href=\"http://intelligence.org/2013/06/07/miris-july-2013-workshop/\">3rd</a> research workshops.</li>\n<li>Published <strong>six new analyses</strong> to our blog: <a href=\"http://intelligence.org/2013/04/04/the-lean-nonprofit/\">The Lean Nonprofit</a>, <a href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI Impact Experts and Friendly AI Experts</a>, <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five Theses...</a>, <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">When Will AI Be Created?</a>, <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a>, and <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a></li>\n<li>Published the&nbsp;<em><a href=\"http://intelligenceexplosion.com/ebook/\">Facing the Intelligence Explosion</a>&nbsp;</em>ebook.</li>\n<li>Published several other substantial articles:&nbsp;<a href=\"http://intelligence.org/courses/\">Recommended Courses for MIRI Researchers</a>, <a href=\"/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a>,&nbsp;<a href=\"/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>, <a href=\"/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>,&nbsp;and others.</li>\n<li>Published our first three expert interviews, with <a href=\"http://intelligence.org/2013/07/12/james-miller-interview/\">James Miller</a>, <a href=\"http://intelligence.org/2013/07/15/roman-interview/\">Roman Yampolskiy</a>, and <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Nick Beckstead</a>.</li>\n<li>Launched our new website at intelligence.org as part of&nbsp;<a href=\"http://intelligence.org/2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri\">changing our name</a>&nbsp;to MIRI.</li>\n<li>Relocated to&nbsp;<a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">new offices</a>...&nbsp;2 blocks from UC Berkeley, which is&nbsp;ranked&nbsp;<a href=\"http://www.usnews.com/education/worlds-best-universities-rankings/best-universities-mathematics\">5th</a>&nbsp;in the world in mathematics, and&nbsp;<a href=\"http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/logic-rankings\">1st</a>&nbsp;in the world in mathematical logic.</li>\n<li>And of course <em>much</em> more.</li>\n</ul>\n<h3><span>Future Plans You Can Help Support</span></h3>\n<ul>\n<li>We will host many more research workshops, including <a href=\"http://intelligence.org/2013/07/07/miris-september-2013-workshop/ &lrm;\">one in September</a>&nbsp;in Berkeley, one in December (with <a href=\"http://math.ucr.edu/home/baez/\">John Baez</a> attending) in Berkeley, and one in Oxford, UK (dates TBD).</li>\n<li>Eliezer will continue to publish about open problems in Friendly AI. (Here is&nbsp;<a href=\"/lw/hbd/new_report_intelligence_explosion_microeconomics/\">#1</a>&nbsp;and&nbsp;<a href=\"/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">#2</a>.)</li>\n<li>We will continue to publish strategic analyses and expert interviews, mostly via <a href=\"http://intelligence.org/blog/\">our blog</a>.</li>\n<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for more of our materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a>&nbsp;</em>and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>\n<li>We will continue to set up the infrastructure (e.g. <a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">new offices</a>, researcher endowments) required to host a productive Friendly AI research team, and (over several years) recruit enough top-level math talent to launch it.</li>\n<li>We hope to hire an experienced development director (job ad not yet posted), so that the contributions of our current supporters can be multiplied even further by a professional fundraiser.</li>\n</ul>\n<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>\n<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward.</p>\n<p>If you have questions about donating, please contact Louie Helm at (510) 717-1477 or&nbsp;<a href=\"mailto:louie@intelligence.org\">louie@intelligence.org</a>.</p>\n<p><sup>&dagger;</sup> $200,000 of total matching funds has been provided by Jaan Tallinn, Loren Merritt, Rick Schwall, and Alexei Andreev.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6CnoNSworudoxJZtb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 38, "extendedScore": null, "score": 1.2764879437187563e-06, "legacy": true, "legacyId": "23446", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>(<a href=\"http://intelligence.org/\">MIRI</a>&nbsp;maintains Less Wrong, with generous help from <a href=\"http://trikeapps.com/\">Trike Apps</a>, and much of the core content is written by salaried MIRI staff members.)</small></p>\n<p><strong>Update 09-15-2013</strong>: The fundraising drive has been completed! My thanks to everyone who contributed.</p>\n<p>The original post follows below...</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Thanks to the generosity of several major donors,<sup>\u2020</sup> every donation to the Machine Intelligence Research Institute made from now until (the end of) August 15th, 2013 will be <strong>matched dollar-for-dollar</strong>, up to a total of $200,000! &nbsp;</p>\n<p style=\"font-size: 300%;\" align=\"center\"><strong id=\"Donate_Now_\"><a href=\"http://intelligence.org/donate/\">Donate Now!</a></strong></p>\n<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $400,000 (with matching) to fund&nbsp;<a href=\"http://intelligence.org/research/\">our research program</a>.</p>\n<p>This post is also a good place to <em>ask your questions</em> about our activities and plans \u2014 just post a comment!</p>\n<p>If you have questions about what your dollars will do at MIRI, you can also schedule a quick call with MIRI Deputy Director Louie Helm: louie@intelligence.org (email), 510-717-1477 (phone),&nbsp;louiehelm (Skype).</p>\n<p align=\"center\"><a href=\"http://intelligence.org/donate/\"><img src=\"http://intelligence.org/files/progress.png\" alt=\"progress bar\"></a></p>\n<hr>\n<p>Early this year we made a transition from movement-building to research, and we've&nbsp;<em>hit the ground running</em> with six major new research papers, six new strategic analyses on our blog, and much more. Give now to support our ongoing work on <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">the future's most important problem</a>.</p>\n<h3 id=\"Accomplishments_in_2013_so_far\"><span>Accomplishments in 2013 so far</span></h3>\n<ul>\n<li>Released <strong>six new research papers</strong>: (1)&nbsp;<a href=\"/lw/h1k/reflection_in_probabilistic_set_theory/\">Definability of Truth in Probabilistic Logic</a>, (2)&nbsp;<a href=\"http://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, (3)&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI</a>, (4)&nbsp;<a href=\"http://intelligence.org/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner's Dilemma</a>, (5)&nbsp;<a href=\"http://intelligence.org/files/Comparison.pdf\">A Comparison of Decision Algorithms on Newcomblike Problems</a>, and (6)&nbsp;<a href=\"http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/\">Responses to Catastrophic AGI Risk: A Survey</a>.</li>\n<li>Held our <a href=\"http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/\">2nd</a> and <a href=\"http://intelligence.org/2013/06/07/miris-july-2013-workshop/\">3rd</a> research workshops.</li>\n<li>Published <strong>six new analyses</strong> to our blog: <a href=\"http://intelligence.org/2013/04/04/the-lean-nonprofit/\">The Lean Nonprofit</a>, <a href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI Impact Experts and Friendly AI Experts</a>, <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five Theses...</a>, <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">When Will AI Be Created?</a>, <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a>, and <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a></li>\n<li>Published the&nbsp;<em><a href=\"http://intelligenceexplosion.com/ebook/\">Facing the Intelligence Explosion</a>&nbsp;</em>ebook.</li>\n<li>Published several other substantial articles:&nbsp;<a href=\"http://intelligence.org/courses/\">Recommended Courses for MIRI Researchers</a>, <a href=\"/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a>,&nbsp;<a href=\"/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>, <a href=\"/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>,&nbsp;and others.</li>\n<li>Published our first three expert interviews, with <a href=\"http://intelligence.org/2013/07/12/james-miller-interview/\">James Miller</a>, <a href=\"http://intelligence.org/2013/07/15/roman-interview/\">Roman Yampolskiy</a>, and <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Nick Beckstead</a>.</li>\n<li>Launched our new website at intelligence.org as part of&nbsp;<a href=\"http://intelligence.org/2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri\">changing our name</a>&nbsp;to MIRI.</li>\n<li>Relocated to&nbsp;<a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">new offices</a>...&nbsp;2 blocks from UC Berkeley, which is&nbsp;ranked&nbsp;<a href=\"http://www.usnews.com/education/worlds-best-universities-rankings/best-universities-mathematics\">5th</a>&nbsp;in the world in mathematics, and&nbsp;<a href=\"http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/logic-rankings\">1st</a>&nbsp;in the world in mathematical logic.</li>\n<li>And of course <em>much</em> more.</li>\n</ul>\n<h3 id=\"Future_Plans_You_Can_Help_Support\"><span>Future Plans You Can Help Support</span></h3>\n<ul>\n<li>We will host many more research workshops, including <a href=\"http://intelligence.org/2013/07/07/miris-september-2013-workshop/ \u200e\">one in September</a>&nbsp;in Berkeley, one in December (with <a href=\"http://math.ucr.edu/home/baez/\">John Baez</a> attending) in Berkeley, and one in Oxford, UK (dates TBD).</li>\n<li>Eliezer will continue to publish about open problems in Friendly AI. (Here is&nbsp;<a href=\"/lw/hbd/new_report_intelligence_explosion_microeconomics/\">#1</a>&nbsp;and&nbsp;<a href=\"/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">#2</a>.)</li>\n<li>We will continue to publish strategic analyses and expert interviews, mostly via <a href=\"http://intelligence.org/blog/\">our blog</a>.</li>\n<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for more of our materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a>&nbsp;</em>and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>\n<li>We will continue to set up the infrastructure (e.g. <a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">new offices</a>, researcher endowments) required to host a productive Friendly AI research team, and (over several years) recruit enough top-level math talent to launch it.</li>\n<li>We hope to hire an experienced development director (job ad not yet posted), so that the contributions of our current supporters can be multiplied even further by a professional fundraiser.</li>\n</ul>\n<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>\n<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward.</p>\n<p>If you have questions about donating, please contact Louie Helm at (510) 717-1477 or&nbsp;<a href=\"mailto:louie@intelligence.org\">louie@intelligence.org</a>.</p>\n<p><sup>\u2020</sup> $200,000 of total matching funds has been provided by Jaan Tallinn, Loren Merritt, Rick Schwall, and Alexei Andreev.</p>", "sections": [{"title": "Donate Now!", "anchor": "Donate_Now_", "level": 2}, {"title": "Accomplishments in 2013 so far", "anchor": "Accomplishments_in_2013_so_far", "level": 1}, {"title": "Future Plans You Can Help Support", "anchor": "Future_Plans_You_Can_Help_Support", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "122 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["duAkuSqJhGDcfMaTA", "zEWJBFFMvQ835nq6h", "hxaq9MCaSrwWPmooZ", "JyH7ezruQbC2iWcSg", "CZQuFoqgPXQawH9aL", "gnxDNEtkEo3sfeyPn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T19:06:01.006Z", "modifiedAt": null, "url": null, "title": "Gains from trade: Slug versus Galaxy - how much would I give up to control you?", "slug": "gains-from-trade-slug-versus-galaxy-how-much-would-i-give-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7kvBxG9ZmYb5rDRiq/gains-from-trade-slug-versus-galaxy-how-much-would-i-give-up", "pageUrlRelative": "/posts/7kvBxG9ZmYb5rDRiq/gains-from-trade-slug-versus-galaxy-how-much-would-i-give-up", "linkUrl": "https://www.lesswrong.com/posts/7kvBxG9ZmYb5rDRiq/gains-from-trade-slug-versus-galaxy-how-much-would-i-give-up", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gains%20from%20trade%3A%20Slug%20versus%20Galaxy%20-%20how%20much%20would%20I%20give%20up%20to%20control%20you%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGains%20from%20trade%3A%20Slug%20versus%20Galaxy%20-%20how%20much%20would%20I%20give%20up%20to%20control%20you%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kvBxG9ZmYb5rDRiq%2Fgains-from-trade-slug-versus-galaxy-how-much-would-i-give-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gains%20from%20trade%3A%20Slug%20versus%20Galaxy%20-%20how%20much%20would%20I%20give%20up%20to%20control%20you%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kvBxG9ZmYb5rDRiq%2Fgains-from-trade-slug-versus-galaxy-how-much-would-i-give-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kvBxG9ZmYb5rDRiq%2Fgains-from-trade-slug-versus-galaxy-how-much-would-i-give-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1984, "htmlBody": "<p>Edit: Moved to main at ThrustVectoring's suggestion.</p>\n<p><em>A suggestion as to how to split the gains from trade in some situations.</em></p>\n<h2>The problem of Power</h2>\n<p>A year or so ago, people in the FHI embarked on a grand project: to try and find out if there was a single way of resolving negotiations, or a single way of merging competing moral theories. This project made a lot of progress in finding out how hard this was, but very little in terms of solving it. It seemed evident that the correct solution was to weigh the different utility functions, and then for everyone&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">maximise the weighted sum</a>, but all ways of weighting had their problems (the weighting with the most good properties was a very silly one: use the \"min-max\" weighting that sets your maximal attainable utility to 1 and your minimal to 0).</p>\n<p>One thing that we didn't get close to addressing is the concept of power. If two partners in the negotiation have very different levels of power, then abstractly comparing their utilities seems the wrong solution (more to the point: it wouldn't be accepted by the powerful party).</p>\n<p>The&nbsp;<a href=\"http://en.wikipedia.org/wiki/New_Republic_(Star_Wars)\">New Republic</a>&nbsp;spans the Galaxy, with Jedi knights, battle fleets, armies, general coolness, and the manufacturing and human resources of countless systems at its command. The dull slug,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Life,_the_Universe_and_Everything\">ARthUrpHilIpDenu</a>, moves very slowly around a plant, and possibly owns one leaf (or not - he can't produce the paperwork). Both these entities have preferences, but if they meet up, and their utilities are normalised abstractly, then&nbsp;ARthUrpHilIpDenu's preferences will weigh in far too much: a sizeable fraction of the galaxy's production will go towards satisfying the slug. Even if you think this is \"fair\", consider that the New Republic is the merging of countless individual preferences, so it doesn't make any sense that the two utilities get weighted equally.<a id=\"more\"></a></p>\n<h2>The default point</h2>\n<p>After <a href=\"/lw/hz9/countess_and_baron_attempt_to_define_blackmail/\">looking</a> at <a href=\"/r/discussion/lw/hza/duller_blackmail_definitions/\">various</a> <a href=\"/lw/i07/semiopen_thread_blackmail/\">blackmail</a> situations, it seems to me that it's the concept of default, or status quo, that most clearly differentiates between a threat and an offer. I wouldn't want you to make a credible threat, because this worsens the status quo, I would want you to make a credible offer, because this improves it. How this default is established is another matter - there may be some super-UDT approach that solves it from first principles. Maybe there is some deep way of distinguishing between threats and promises in some other way, and the default is simply the point between them.</p>\n<p>In any case, without going any further into it's meaning or derivation, I'm going to assume that the problem we're working on has a definitive <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem#Disagreement_point\">default/disagreement/threat point</a>. I'll use the default point terminology, as that is closer to the concept I'm considering.</p>\n<p>Simple trade problems often have a very clear default point. These are my goods, those are your goods, the default is we go home with what we started with. This is what I could build, that's what you could build, the default is that we both build purely for ourselves.</p>\n<p>If we imagine ARthUrpHilIpDenu and the New Republic were at opposite ends of a regulated wormhole, and they could only trade in safe and simple goods, then we've got a pretty clear default point.</p>\n<p>Having a default point opens up a whole host of new <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">bargaining equilibriums</a>, such as the&nbsp;Nash Bargaining Solution (NBS) and the Kalai-Smorodinsky Bargaining Solution (KSBS). But neither of these are really quite what we'd want: the KSBKS is all about fairness (which generally reduced expected outcomes), while the NBS uses a <em>product</em> of utility values, something that makes no intrinsic sense at all (NBS has some nice properties, like independence of irrelevant alternatives, but this only matters if the default point is reached through a process that has the same properties - and <a href=\"/lw/c6g/no_independence_of_irrelevant_alternatives/\">it can't be</a>).</p>\n<h2>What am I <em>really</em> offering you in trade?</h2>\n<p>When two agents meet, especially if they are likely to meet more in the future (and most especially if they don't know the number of times and the circumstances in which they will meet), they <a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">should merge</a> their utility functions: fix a common scale for their utility functions, add them together, and then both proceed to maximise the sum.</p>\n<p>This explains what's really being offered in a trade. Not a few widgets or stars, but the possibility of copying your utility function into mine. But why would you want that? Because that will change my decisions, into a direction you find more pleasing. So what I'm actually offering you, is access to my decision points.</p>\n<p style=\"padding-left: 30px;\"><em>What is actually on offer in a trade, is access by one player's utility function to the other player's decision points.</em></p>\n<p>This gives a novel way of normalising utility functions. How much, precisely, is access to my decision points worth to you? If the default point gives a natural zero, then complete control over the other player's decision points is a natural one. \"Power\" is a nebulous concept, and different players may disagree as to how much power they each have. But power can only be articulated through making decisions (if you can't change any of your decisions, you have no power), and this normalisation allows each player to specify exactly how much they value the power/decision points of the other. Outcomes that involve one player controlling the other player's decision points can be designated the \"utopia\" point for that first player. These are what would happen if everything went exactly according to what they wanted.</p>\n<p>What does this mean for ARthUrpHilIpDenu and the New Republic? Well, the New Republic stands to gain a leaf (maybe). From it's perspective, the difference between default (all the resources of the galaxy and no leaf) and utopia (all the resources of the galaxy plus one leaf) is tiny. And yet that tiny difference will get normalised to one: the New Republic's utility function will get multiplied by a huge amount. It will weigh heavily in any sum.</p>\n<p>What about ARthUrpHilIpDenu? It stands to gain the resources of a galaxy. The difference between default (a leaf) and utopia (all the resources of a galaxy dedicated to making leaves) is unimaginably humongous. And yet that huge difference will get normalised to one: the&nbsp;ARthUrpHilIpDenu's&nbsp;utility function will get divided by a huge amount. It will weigh very little in any sum.</p>\n<p>Thus if we add the two normalised utility functions, we get one that is nearly totally dominated by the New Republic. Which is what we'd expect, given the power differential between the two. So this bargaining system reflects the relative power of the players. Another way of thinking of this is that each player's utility is normalised taking into account how much they would give up to control the other. I'm calling it the \"Mutual Worth Bargaining Solution\" (MWBS), as it's the worth to players of the other player's decision points that are key. Also because I couldn't think of a better title.</p>\n<h2>Properties of the Mutual Worth Bargaining Solution</h2>\n<p>How does the MWBS compare with the NBS and the KSBS? The NBS is quite different, because it has no concept of relative power, normalising purely by the players' preferences. Indeed, one player could have no control at all, no decision points, and the NBS would still be unchanged.</p>\n<p>The KSBS is more similar to the MWBS: the utopia points of the KSBS are the same as those of the MWBS. If we set the default point to (0,0) and the utopia points to (1,-) and (-,1), then the KSBS is given by the highest h such that (h,h) is a possible outcome. Whereas the MWBS is given by the outcome (x,y) such that x+y is highest possible.</p>\n<p>Which is preferable? Obviously, if they knew exactly what the outcomes and utilities were on offer, then each player would have preferences as to which system to use (the one that gives them more). But if they didn't, if they had uncertainties as to what players and what preferences they would face in the future, then MWBS generally comes out on top (in expectation).</p>\n<p>How so? Well, if a player doesn't know what other players they'll meet, they don't know in what way their decision points will be relevant to the other, and vice versa. They don't know what pieces of their utility will be relevant to the other, and vice versa. So they can expect to face a wide variety of normalised situations. To a first approximation, it isn't too bad an idea to assume that one is equally likely to face a certain situation as it's symmetric complement. Using the KSBS, you'd expect to get a utility of h (same in both case); under the MWBS, a utility of (x+y)/2 (x in one case, y in the other). Since x+y &ge; h+h = 2h by the definition of the MWBS, it comes out ahead in expectation.</p>\n<p>Another important distinction between the MWBS is that while the KSBS and the NBS only allow Pareto improvements from the default point, MWBS does allow for some situation where one player will lose from the deal. It is possible, for instance, that (1/2,-1/4) is a possible outcome (summed utility 1/4), and there are no better options possible. Doesn't this go against the spirit of the default point? Why would someone go into a deal that leaves them poorer than before?</p>\n<p>First off all, that situation will be rare. All MWBS must be in the triangle bounded by x&lt;1, y&lt;1 and x+y&gt;0. The first bounds are definitional: one cannot get more expected utility that one's utopia point. The last bound comes from the fact that the default point is itself an option, with summed utility 0+0=0, so all summed utilities must be above zero. Sprinkle a few random outcome points into that triangle, and it very likely that the one with highest summed utility will be a Pareto improvement over (0,0).</p>\n<p>But the other reason to accept the risk of losing, is because of the opportunity of gain. One could modify the MWBS to only allow Pareto improvements over the default: but in expectation, this would perform worse. The player would be immune from losing 1/4 utility from (1/2,-1/4), but unable to gain 1/2 from the (-1/4,1/2): the argument is the same as above. In ignorance as to the other player's preferences, accepting the possibility of loss improves the expected outcome.</p>\n<p>It should be noted that the maximum that a player could theoretically lose by using the MWBS is equal to the maximum they could theoretically win. So the New Republic could lose at most a leaf, meaning that even powerful players would not be reluctant to trade. For less powerful players, the potential losses are higher, but so are the potential rewards.</p>\n<h2>Directions of research</h2>\n<p>The MWBS is somewhat underdeveloped, and the explanation here isn't as clear as I'd have liked. However, me and Miriam are about to have a baby, so I'm not expecting to have any time at all soon, so I'm pushing out the idea, unpolished.</p>\n<p>Some possible routes for further research: what are the other properties of MWBS? Are they properties that make MWBS feel more or less likely or acceptable? The NBS is equivalent with certain properties: what are the properties that are necessary and sufficient for the MWBS (and can they suggest better Bargaining Solutions)? Can we replace the default point? Maybe we can get a zero by imagining what would happen if the second player's decision nodes were under the control of an anti-agent (an agent that's the opposite of the first player), or a randomly selected agent?</p>\n<p>The most important research route is to analyse what happens if several players come together at different times, and repeatedly normalise their utilities using the MWBS: does it matter the order in which they meet? I strongly feel that it's exploring this avenue that will reach \"the ultimate\" bargaining solution, if such a thing is to be found. Some solution that is stable under large numbers of agents, who don't know each other or how many they are, coming together in a order they can't predict.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7kvBxG9ZmYb5rDRiq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 54, "extendedScore": null, "score": 0.00014, "legacy": true, "legacyId": "23154", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Edit: Moved to main at ThrustVectoring's suggestion.</p>\n<p><em>A suggestion as to how to split the gains from trade in some situations.</em></p>\n<h2 id=\"The_problem_of_Power\">The problem of Power</h2>\n<p>A year or so ago, people in the FHI embarked on a grand project: to try and find out if there was a single way of resolving negotiations, or a single way of merging competing moral theories. This project made a lot of progress in finding out how hard this was, but very little in terms of solving it. It seemed evident that the correct solution was to weigh the different utility functions, and then for everyone&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">maximise the weighted sum</a>, but all ways of weighting had their problems (the weighting with the most good properties was a very silly one: use the \"min-max\" weighting that sets your maximal attainable utility to 1 and your minimal to 0).</p>\n<p>One thing that we didn't get close to addressing is the concept of power. If two partners in the negotiation have very different levels of power, then abstractly comparing their utilities seems the wrong solution (more to the point: it wouldn't be accepted by the powerful party).</p>\n<p>The&nbsp;<a href=\"http://en.wikipedia.org/wiki/New_Republic_(Star_Wars)\">New Republic</a>&nbsp;spans the Galaxy, with Jedi knights, battle fleets, armies, general coolness, and the manufacturing and human resources of countless systems at its command. The dull slug,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Life,_the_Universe_and_Everything\">ARthUrpHilIpDenu</a>, moves very slowly around a plant, and possibly owns one leaf (or not - he can't produce the paperwork). Both these entities have preferences, but if they meet up, and their utilities are normalised abstractly, then&nbsp;ARthUrpHilIpDenu's preferences will weigh in far too much: a sizeable fraction of the galaxy's production will go towards satisfying the slug. Even if you think this is \"fair\", consider that the New Republic is the merging of countless individual preferences, so it doesn't make any sense that the two utilities get weighted equally.<a id=\"more\"></a></p>\n<h2 id=\"The_default_point\">The default point</h2>\n<p>After <a href=\"/lw/hz9/countess_and_baron_attempt_to_define_blackmail/\">looking</a> at <a href=\"/r/discussion/lw/hza/duller_blackmail_definitions/\">various</a> <a href=\"/lw/i07/semiopen_thread_blackmail/\">blackmail</a> situations, it seems to me that it's the concept of default, or status quo, that most clearly differentiates between a threat and an offer. I wouldn't want you to make a credible threat, because this worsens the status quo, I would want you to make a credible offer, because this improves it. How this default is established is another matter - there may be some super-UDT approach that solves it from first principles. Maybe there is some deep way of distinguishing between threats and promises in some other way, and the default is simply the point between them.</p>\n<p>In any case, without going any further into it's meaning or derivation, I'm going to assume that the problem we're working on has a definitive <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem#Disagreement_point\">default/disagreement/threat point</a>. I'll use the default point terminology, as that is closer to the concept I'm considering.</p>\n<p>Simple trade problems often have a very clear default point. These are my goods, those are your goods, the default is we go home with what we started with. This is what I could build, that's what you could build, the default is that we both build purely for ourselves.</p>\n<p>If we imagine ARthUrpHilIpDenu and the New Republic were at opposite ends of a regulated wormhole, and they could only trade in safe and simple goods, then we've got a pretty clear default point.</p>\n<p>Having a default point opens up a whole host of new <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">bargaining equilibriums</a>, such as the&nbsp;Nash Bargaining Solution (NBS) and the Kalai-Smorodinsky Bargaining Solution (KSBS). But neither of these are really quite what we'd want: the KSBKS is all about fairness (which generally reduced expected outcomes), while the NBS uses a <em>product</em> of utility values, something that makes no intrinsic sense at all (NBS has some nice properties, like independence of irrelevant alternatives, but this only matters if the default point is reached through a process that has the same properties - and <a href=\"/lw/c6g/no_independence_of_irrelevant_alternatives/\">it can't be</a>).</p>\n<h2 id=\"What_am_I_really_offering_you_in_trade_\">What am I <em>really</em> offering you in trade?</h2>\n<p>When two agents meet, especially if they are likely to meet more in the future (and most especially if they don't know the number of times and the circumstances in which they will meet), they <a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">should merge</a> their utility functions: fix a common scale for their utility functions, add them together, and then both proceed to maximise the sum.</p>\n<p>This explains what's really being offered in a trade. Not a few widgets or stars, but the possibility of copying your utility function into mine. But why would you want that? Because that will change my decisions, into a direction you find more pleasing. So what I'm actually offering you, is access to my decision points.</p>\n<p style=\"padding-left: 30px;\"><em>What is actually on offer in a trade, is access by one player's utility function to the other player's decision points.</em></p>\n<p>This gives a novel way of normalising utility functions. How much, precisely, is access to my decision points worth to you? If the default point gives a natural zero, then complete control over the other player's decision points is a natural one. \"Power\" is a nebulous concept, and different players may disagree as to how much power they each have. But power can only be articulated through making decisions (if you can't change any of your decisions, you have no power), and this normalisation allows each player to specify exactly how much they value the power/decision points of the other. Outcomes that involve one player controlling the other player's decision points can be designated the \"utopia\" point for that first player. These are what would happen if everything went exactly according to what they wanted.</p>\n<p>What does this mean for ARthUrpHilIpDenu and the New Republic? Well, the New Republic stands to gain a leaf (maybe). From it's perspective, the difference between default (all the resources of the galaxy and no leaf) and utopia (all the resources of the galaxy plus one leaf) is tiny. And yet that tiny difference will get normalised to one: the New Republic's utility function will get multiplied by a huge amount. It will weigh heavily in any sum.</p>\n<p>What about ARthUrpHilIpDenu? It stands to gain the resources of a galaxy. The difference between default (a leaf) and utopia (all the resources of a galaxy dedicated to making leaves) is unimaginably humongous. And yet that huge difference will get normalised to one: the&nbsp;ARthUrpHilIpDenu's&nbsp;utility function will get divided by a huge amount. It will weigh very little in any sum.</p>\n<p>Thus if we add the two normalised utility functions, we get one that is nearly totally dominated by the New Republic. Which is what we'd expect, given the power differential between the two. So this bargaining system reflects the relative power of the players. Another way of thinking of this is that each player's utility is normalised taking into account how much they would give up to control the other. I'm calling it the \"Mutual Worth Bargaining Solution\" (MWBS), as it's the worth to players of the other player's decision points that are key. Also because I couldn't think of a better title.</p>\n<h2 id=\"Properties_of_the_Mutual_Worth_Bargaining_Solution\">Properties of the Mutual Worth Bargaining Solution</h2>\n<p>How does the MWBS compare with the NBS and the KSBS? The NBS is quite different, because it has no concept of relative power, normalising purely by the players' preferences. Indeed, one player could have no control at all, no decision points, and the NBS would still be unchanged.</p>\n<p>The KSBS is more similar to the MWBS: the utopia points of the KSBS are the same as those of the MWBS. If we set the default point to (0,0) and the utopia points to (1,-) and (-,1), then the KSBS is given by the highest h such that (h,h) is a possible outcome. Whereas the MWBS is given by the outcome (x,y) such that x+y is highest possible.</p>\n<p>Which is preferable? Obviously, if they knew exactly what the outcomes and utilities were on offer, then each player would have preferences as to which system to use (the one that gives them more). But if they didn't, if they had uncertainties as to what players and what preferences they would face in the future, then MWBS generally comes out on top (in expectation).</p>\n<p>How so? Well, if a player doesn't know what other players they'll meet, they don't know in what way their decision points will be relevant to the other, and vice versa. They don't know what pieces of their utility will be relevant to the other, and vice versa. So they can expect to face a wide variety of normalised situations. To a first approximation, it isn't too bad an idea to assume that one is equally likely to face a certain situation as it's symmetric complement. Using the KSBS, you'd expect to get a utility of h (same in both case); under the MWBS, a utility of (x+y)/2 (x in one case, y in the other). Since x+y \u2265 h+h = 2h by the definition of the MWBS, it comes out ahead in expectation.</p>\n<p>Another important distinction between the MWBS is that while the KSBS and the NBS only allow Pareto improvements from the default point, MWBS does allow for some situation where one player will lose from the deal. It is possible, for instance, that (1/2,-1/4) is a possible outcome (summed utility 1/4), and there are no better options possible. Doesn't this go against the spirit of the default point? Why would someone go into a deal that leaves them poorer than before?</p>\n<p>First off all, that situation will be rare. All MWBS must be in the triangle bounded by x&lt;1, y&lt;1 and x+y&gt;0. The first bounds are definitional: one cannot get more expected utility that one's utopia point. The last bound comes from the fact that the default point is itself an option, with summed utility 0+0=0, so all summed utilities must be above zero. Sprinkle a few random outcome points into that triangle, and it very likely that the one with highest summed utility will be a Pareto improvement over (0,0).</p>\n<p>But the other reason to accept the risk of losing, is because of the opportunity of gain. One could modify the MWBS to only allow Pareto improvements over the default: but in expectation, this would perform worse. The player would be immune from losing 1/4 utility from (1/2,-1/4), but unable to gain 1/2 from the (-1/4,1/2): the argument is the same as above. In ignorance as to the other player's preferences, accepting the possibility of loss improves the expected outcome.</p>\n<p>It should be noted that the maximum that a player could theoretically lose by using the MWBS is equal to the maximum they could theoretically win. So the New Republic could lose at most a leaf, meaning that even powerful players would not be reluctant to trade. For less powerful players, the potential losses are higher, but so are the potential rewards.</p>\n<h2 id=\"Directions_of_research\">Directions of research</h2>\n<p>The MWBS is somewhat underdeveloped, and the explanation here isn't as clear as I'd have liked. However, me and Miriam are about to have a baby, so I'm not expecting to have any time at all soon, so I'm pushing out the idea, unpolished.</p>\n<p>Some possible routes for further research: what are the other properties of MWBS? Are they properties that make MWBS feel more or less likely or acceptable? The NBS is equivalent with certain properties: what are the properties that are necessary and sufficient for the MWBS (and can they suggest better Bargaining Solutions)? Can we replace the default point? Maybe we can get a zero by imagining what would happen if the second player's decision nodes were under the control of an anti-agent (an agent that's the opposite of the first player), or a randomly selected agent?</p>\n<p>The most important research route is to analyse what happens if several players come together at different times, and repeatedly normalise their utilities using the MWBS: does it matter the order in which they meet? I strongly feel that it's exploring this avenue that will reach \"the ultimate\" bargaining solution, if such a thing is to be found. Some solution that is stable under large numbers of agents, who don't know each other or how many they are, coming together in a order they can't predict.</p>", "sections": [{"title": "The problem of Power", "anchor": "The_problem_of_Power", "level": 1}, {"title": "The default point", "anchor": "The_default_point", "level": 1}, {"title": "What am I really offering you in trade?", "anchor": "What_am_I_really_offering_you_in_trade_", "level": 1}, {"title": "Properties of the Mutual Worth Bargaining Solution", "anchor": "Properties_of_the_Mutual_Worth_Bargaining_Solution", "level": 1}, {"title": "Directions of research", "anchor": "Directions_of_research", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "67 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W2ufY8ihDDWWqJA7h", "Qjaaux3XnLBwomuNK", "5xYxL4P5rcdoxTjC6", "WaYPy9cX23v3pp7HD", "hCwFxBai3oNnxrM9v", "7BJ6utqA7PbvQ45ne"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T21:22:27.583Z", "modifiedAt": null, "url": null, "title": "low stress employment/ munchkin income thread", "slug": "low-stress-employment-munchkin-income-thread", "viewCount": null, "lastCommentedAt": "2015-04-15T04:39:41.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sz7iRBbcQrHB9q9HE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jjKYBQLkMvLabTcWt/low-stress-employment-munchkin-income-thread", "pageUrlRelative": "/posts/jjKYBQLkMvLabTcWt/low-stress-employment-munchkin-income-thread", "linkUrl": "https://www.lesswrong.com/posts/jjKYBQLkMvLabTcWt/low-stress-employment-munchkin-income-thread", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20low%20stress%20employment%2F%20munchkin%20income%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Alow%20stress%20employment%2F%20munchkin%20income%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjjKYBQLkMvLabTcWt%2Flow-stress-employment-munchkin-income-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=low%20stress%20employment%2F%20munchkin%20income%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjjKYBQLkMvLabTcWt%2Flow-stress-employment-munchkin-income-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjjKYBQLkMvLabTcWt%2Flow-stress-employment-munchkin-income-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p><em>TL;DR: this is a repository for discussing income generation strategies optimized for free time</em></p>\n<p>I hope I'm not cluttering up LW but maybe enough people are also interested in this? I graduated high school about a year ago.&nbsp;</p>\n<p>I have a lot in common with Will Newsome's self description in this post<br /><a href=\"/lw/2qp/virtual_employment_open_thread/?sort=top\">http://lesswrong.com/lw/2qp/virtual_employment_open_thread/</a></p>\n<p>&nbsp;</p>\n<p>But it's a dead thread, and there's been some interest in early retirement extreme, (<a href=\"http://earlyretirementextreme.com/\">http://earlyretirementextreme.com/</a>)&nbsp;and having repositories for stuff.&nbsp;</p>\n<p>The upshot of it is that I want to optimize for free time and mobility. Need about $2,000 to live (1600 expenses 400 savings/buffer) <em>2nd&nbsp;</em><em>EDIT: no I don't, I must have screwed something up when I was adding this it's more like $1600. ($1300 to spend $300 buffer).</em>&nbsp;A 20 hour workweek or even shorter is what I'm going for here. Right now I'm barely functional. Even that much is a bit of a stretch for me as I am now. Plenty of advice abounds on optimizing my health and squashing akrasia though, and I'm sure that if I implemented it I could get to the point of handling part time work. But I think I would always find being a 9 to 5er unappealing.</p>\n<p>&nbsp;I'd value spending that time reading texbooks or walking around town or lazing around on the beach more than I'd value extra money. I'm also interested to hear about some more conventional part time jobs if they pay enough. I'm ok with doing somewhat boring work if the hours are light and I have time to think.</p>\n<p>I've generated some candidate strategies if anyone here has experience at these. I don't have much knowledge of what they would entail or how to break into them. Or they might give someone some ideas I dunno but anyway:</p>\n<p>4hww style dropship business (but success at that seems hard to set up and sustain)</p>\n<p>freelance work at a site like odesk or elance</p>\n<p>Own a popular app or forum</p>\n<p>Push carts at wal mart part time (but I don't think that pays enough)</p>\n<p>Self employment doing massage therapy (I can set my own hours&nbsp;but I'd need to invest time and money to get trained)</p>\n<p>Tutoring (I might like this one. Do I need a college degree? Can I make enough with part time hours? Is it hard to find leads for clients? How would I do that?)</p>\n<p>Online poker (but it seems kinda hard)</p>\n<p>Does anyone here live in a yurt? And has anyone tried living in other countries to cut down expenses?&nbsp;</p>\n<p>edited to add: Did I make a mistake including numbers? They're what would be ideal for me, not strict requirements. I can work a little more or spend less. Err on the side of posting ideas, I'm sure some other people are interested in low stress work but don't value free time *quite* as much I seem to</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jjKYBQLkMvLabTcWt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 21, "extendedScore": null, "score": 1.27659839707239e-06, "legacy": true, "legacyId": "23463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9bTNcSpNBdPpyocMK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-07-23T21:22:27.583Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-23T21:30:21.234Z", "modifiedAt": null, "url": null, "title": "Why Eat Less Meat?", "slug": "why-eat-less-meat", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:05.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LbbyQhLkcwAwWmBoj/why-eat-less-meat", "pageUrlRelative": "/posts/LbbyQhLkcwAwWmBoj/why-eat-less-meat", "linkUrl": "https://www.lesswrong.com/posts/LbbyQhLkcwAwWmBoj/why-eat-less-meat", "postedAtFormatted": "Tuesday, July 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Eat%20Less%20Meat%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Eat%20Less%20Meat%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLbbyQhLkcwAwWmBoj%2Fwhy-eat-less-meat%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Eat%20Less%20Meat%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLbbyQhLkcwAwWmBoj%2Fwhy-eat-less-meat", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLbbyQhLkcwAwWmBoj%2Fwhy-eat-less-meat", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1881, "htmlBody": "<p>Previously, <a href=\"/lw/hox/effective_altruism_through_advertising/\">I wrote on LessWrong</a>&nbsp;about the preliminary evidence in favor of using leaflets to promote veganism as a way of cost-effectively reducing suffering. &nbsp;In response, there was a large discussion with 530+ comments. &nbsp; In this discussion, I found that a lot of people wanted me to write about why I think nonhuman animals deserve our concern anyway.</p>\n<p>Therefore, I wrote this essay with an attempt to defend the view that if one cares about suffering, one should also care about nonhuman animals, since (1) they are capable of suffering, (2) they do suffer quite a lot, and (3) we can prevent their suffering. &nbsp; I hope that we can have a sober, non mind-killing discussion about this topic, since it&rsquo;s possibly quite important.</p>\n<p>&nbsp;</p>\n<h2>Introduction</h2>\n<p>For the past two years, the only place I ate meat was at home with my family. &nbsp;As of October 2012, I've finally stopped eating meat altogether and can't see a reason why I would want to go back to eating meat. &nbsp;This kind of attitude toward eating is commonly classified as \"vegetarianism\" where one refrains from eating the flesh of all animals, including fish, but still will consume animal products like eggs and milk (though I try to avoid egg as best I can).</p>\n<p>Why might I want to do this? &nbsp;And why might I see it as a serious issue? &nbsp;It's because I'm very concerned about the reality of suffering done to our \"food animals\" in the process of making them into meat, because I see vegetarianism as a way to reduce this suffering by stopping the harmful process, and because vegetarianism has not been hard at all for me to accomplish.</p>\n<p>&nbsp;</p>\n<h2>Animals Can Suffer</h2>\n<p>Back in the 1600s, R&eacute;n&eacute; Descartes thought nonhuman animals were soulless automatons that could respond to their environment and react to stimuli, but could not feel anything &mdash; humans were the only species that were truly conscious. Descartes hit on an important point &mdash; since feelings are completely internal to the animal doing the feeling, it is impossible to demonstrate that anyone is truly conscious.</p>\n<p>However, when it comes to humans, we don&rsquo;t let that stop us from assuming other people feel pain. When we jab a person with a needle, no matter who they are, where they come from, or what they look like, they share a rather universal reaction of what we consider to be evidence of pain. We also extend this to our pets &mdash; we make great strides to avoid harming kittens, puppies, or other companion animals, and no one would want to kick a puppy or light a kitten on fire just because their consciousness cannot be directly observed. That&rsquo;s why we even go as far as having laws against animal cruelty.</p>\n<p>The animals we eat are no different. Pigs, chickens, cows, and fish all have incredibly analogous responses to stimuli that we would normally agree cause pain to humans and pets. &nbsp;Jab a pig with a needle, kick a chicken, or light a cow on fire, and they will react aversively like any cat, dog, horse, or human.</p>\n<p>&nbsp;</p>\n<p><strong>The Science</strong></p>\n<p>But we don't need to rely on just our intuition -- instead, we can look at the science. &nbsp;<a href=\"http://www.grandin.com/welfare/fear.pain.stress.html\">Animal scientists Temple Grandin and Mark Deesing</a>&nbsp;conclude that \"[o]ur review of the literature on frontal cortex development enables us to conclude that all mammals, including rats, have a sufficiently developed prefrontal cortex to suffer from pain\". &nbsp;<a href=\"http://www.youtube.com/watch?v=mLSwRcvX72M\">An interview of seven different scientists</a>&nbsp;concludes that animals can suffer.</p>\n<p>Dr. Jane Goodall, famous for having studied animals, writes in her introduction to <strong><a href=\"http://www.amazon.com/Inner-World-Farm-Animals-Intellectual/dp/1584797487\">The Inner World of Farm Animals</a>&nbsp;</strong>that \"farm animals feel pleasure and sadness, excitement and resentment, depression, fear, and pain. They are far more aware and intelligent than we ever imagined&hellip;they are individuals in their own right.\" &nbsp;Farm Sanctuary, an animal welfare organization, <a href=\"http://www.farmsanctuary.org/learn/someone-not-something/\">has a good overview documenting this research on animal emotion</a>.</p>\n<p>Lastly, among much other evidence, in the <a href=\"http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf\">\"Cambridge Declaration On Consciousness\"</a>, prominent international group of cognitive &nbsp;neuroscientists, neuropharmacologists, neurophysiologists, neuroanatomists and computational neuroscientists states:</p>\n<blockquote>\n<p>Convergent evidence indicates that non-human animals have the neuroanatomical, neurochemical, and neurophysiological substrates of conscious states along with the capacity to exhibit intentional behaviors. &nbsp;Consequently, the weight of evidence indicates that humans are not unique in possessing the neurological substrates that generate consciousness. Nonhuman animals, including all mammals and birds, and many other creatures, including octopuses, also &nbsp;possess these neurological substrates.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h2>Factory Farming Causes Considerable Suffering</h2>\n<p>However, the fact that animals can suffer is just one piece of the picture; we next have to establish that animals <em>do</em>&nbsp;suffer as a result of people eating meat. &nbsp;Honestly, this is easier shown than told -- there's <a href=\"http://whosagainstanimalcruelty.org/\">an extremely harrowing and shocking 11-minute video about the cruelty available</a>. &nbsp;Watching that video is perhaps the easiest way to see the suffering of nonhuman animals first hand in these \"factory farms\".</p>\n<p>In making the case clear, <a href=\"http://www.veganoutreach.org/whyvegan/animals.html\">Vegan Outreach writes</a>&nbsp;\"Many people believe that animals raised for food must be treated well because sick or dead animals would be of no use to agribusiness. This is not true.\"</p>\n<p>They then go on to document, with sources, how virtually all birds raised for food are from factory farms where \"resulting ammonia levels [from densely populated sheds and accumulated waste] commonly cause painful burns to the birds' skin, eyes, and respiratory tracts\" and how hens \"become immobilized and die of asphyxiation or dehydration\", having been \"[p]acked in cages (usually less than half a square foot of floor space per bird)\". &nbsp;In fact, <a href=\"http://countinganimals.com/is-vegan-outreach-right-about-how-many-animals-suffer-to-death/\">137 million chickens suffer to death each year</a>&nbsp;before they can even make it to slaughter -- more than the number of animals killed for fur, in shelters and in laboratories combined!</p>\n<p>Farm Sanctuary also <a href=\"http://www.farmsanctuary.org/learn/factory-farming/\">provides an excellent overview of the cruelty of factory farming</a>, writing \"Animals on factory farms are regarded as commodities to be exploited for profit. They undergo painful mutilations and are bred to grow unnaturally fast and large for the purpose of maximizing meat, egg, and milk production for the food industry.\"</p>\n<p><strong>It seems clear that factory farming practices are truly deplorable, and certainly are not worth the benefit of eating a slightly tastier meal.</strong>&nbsp; In <a href=\"http://www.nytimes.com/2002/11/10/magazine/10ANIMAL.html?pagewanted=all\">\"An Animal's Place\"</a>, Michael Pollan writes:</p>\n<blockquote>\n<p>To visit a modern CAFO (Confined Animal Feeding Operation) is to enter a world that, for all its technological sophistication, is still designed according to Cartesian principles: animals are machines incapable of feeling pain. Since no thinking person can possibly believe this any more, industrial animal agriculture depends on a suspension of disbelief on the part of the people who operate it and a willingness to avert your eyes on the part of everyone else.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h2>Vegetarianism Can Make a Difference</h2>\n<p>Many people see the staggering amount of suffering in factory farms, and if they don't aim to dismiss it outright will say that there's no way they can make a difference by changing their eating habits. &nbsp;However, this is certainly not the case!</p>\n<p>&nbsp;</p>\n<p><strong>How Many Would Be Saved?</strong></p>\n<p>Drawing from the 2010 <a href=\"http://usda.mannlib.cornell.edu/MannUsda/viewDocumentInfo.do?documentID=1097\">Livestock Slaughter Animal Summary</a>&nbsp;and the <a href=\"http://usda.mannlib.cornell.edu/MannUsda/viewDocumentInfo.do?documentID=1497\">Poultry Slaughter Animal Summary</a>, 9.1 billion land animals are either grown in the US or imported (94% of which are chickens!), 1.6 billion are exported, and 631 million die before anyone can eat them, leaving 8.1 billion land animals for US consumption <strong>each year</strong>.</p>\n<p>A na&iuml;ve average would divide this total among the population of the US, which is 311 million, assigning 26 land animals for each person's annual consumption. &nbsp;Thus, by being vegetarian, you are saving 26 land animals a year you would have otherwise eaten. &nbsp;And this doesn't even count fish, which could be quite high given how many fish need to be grown just to be fed to bigger fish!</p>\n<p>Yet, this is not quite true. &nbsp;It's important to note that supply and demand aren't perfectly linear. &nbsp;If you reduce your demand for meat, the suppliers will react by lowering the price of meat a little bit, making it so more people can buy it. &nbsp;Since chickens dominate the meat market, we'll adjust by the supply elasticity of chickens, <a href=\"http://www.rti.org/pubs/poultry_slaughter.pdf\">which is 0.22</a>&nbsp;and the demand elasticity of chickens, <a href=\"http://ageconsearch.umn.edu/bitstream/31190/1/23020558.pdf\">which is -0.52</a>, and calculate the change in supply, which is 0.3. &nbsp;Taking this multiplier, it's more accurate to say <strong>you're saving 7.8 land animals a year or more</strong>. &nbsp;Though, there are a lot of complex considerations in calculating elasticity, so we should take this figure to have some uncertainty.</p>\n<p>&nbsp;</p>\n<p><strong>Collective Action</strong></p>\n<p>One might critique this response by responding that since meat is often bought in bulk, reducing meat consumption won't affect the amount of meat bought, and thus the suffering will still be the same, except with meat gone to waste. &nbsp;However, this ignores the effect of many different vegetarians acting together.</p>\n<p>Imagine that you're supermarket buys cases of 200 chicken wings. &nbsp;It would thus take 200 people together to agree to buy 1 less wing in order for the supermarket to buy less wings. &nbsp;However, you have no idea if you're vegetarian #1 or vegetarian #56 or vegetarian #200, making the tipping point for 200 less wings to be bought. &nbsp;You thus can estimate that by buying one less wing you have a 1 in 200 chance of reducing 200 wings, which is equivalent to reducing the supply by one wing. &nbsp;So the effect basically cancels out. &nbsp;See <a href=\"http://www.veganoutreach.org/enewsletter/thresholds.pdf\">here</a>&nbsp;or <a href=\"http://www.utilitarian-essays.com/vegetarianism.html\">here</a>&nbsp;for more.</p>\n<p>Every time you buy factory farmed meat, you are creating demand for that product, essentially saying \"Thank you, I liked what you are doing and want to encourage you to do it more\". &nbsp;By eating less meat, we can stop our support of this industry.</p>\n<p>&nbsp;</p>\n<h2>Vegetarianism Is Easier Than You Think</h2>\n<p>So nonhuman animals can suffer and do suffer in factory farms, and we can help stop this suffering by eating less meat. &nbsp;I know people who get this far, but then stop and say that, as much as they would like to, there's no way they could be a vegetarian because they like meat too much! &nbsp;However, such a joy for meat shouldn't count much compared to the massive suffering each animal undergoes just to be farmed -- imagine if someone wouldn't stop eating your pet just because they like eating your pet so much!</p>\n<p>This is less than a problem than you might think, because being a vegetarian is really easy. &nbsp;Most people only think about what they would have to give up and how good it tastes, and don't think about what tasty things they could eat instead that have no meat in them. &nbsp;When I first decided to be a vegetarian, I simply switched from tasty hamburgers to <a href=\"http://www.veganoutreach.org/guide/substitutes.html\">tasty veggieburgers</a>&nbsp;and there was no problem at all.</p>\n<p>&nbsp;</p>\n<p><strong>A Challenge</strong></p>\n<p>To those who say that vegetarianism is too hard, I&rsquo;d like to simply challenge you to <a href=\"/lw/5a5/no_seriously_just_try_it/\">just try it</a>&nbsp;for a few days. Feel free to give up afterward if you find it too hard. But I imagine that you should do just fine, find great replacements, and be able to save animals from suffering in the process.</p>\n<p>If reducing suffering is one of your goals, there&rsquo;s no reason why you must either be a die-hard meat eater or a die-hard vegetarian. Instead, feel free to explore some middle ground. You could be a vegetarian on weekdays but eat meat on weekends, or just try Meatless Mondays, or simply try to eat less meat. You could try to eat bigger animals like cows instead of fish or chicken, thus <a href=\"http://www.utilitarian-essays.com/suffering-per-kg.html\">getting the same amount of meat with significantly less suffering</a>.</p>\n<p>-</p>\n<address>(This was also <a href=\"http://www.everydayutilitarian.com/essays/why-eat-less-meat/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</address><address><br /></address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q9ASuEEoJWxT3RLMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LbbyQhLkcwAwWmBoj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 61, "extendedScore": null, "score": 0.000158, "legacy": true, "legacyId": "23464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Previously, <a href=\"/lw/hox/effective_altruism_through_advertising/\">I wrote on LessWrong</a>&nbsp;about the preliminary evidence in favor of using leaflets to promote veganism as a way of cost-effectively reducing suffering. &nbsp;In response, there was a large discussion with 530+ comments. &nbsp; In this discussion, I found that a lot of people wanted me to write about why I think nonhuman animals deserve our concern anyway.</p>\n<p>Therefore, I wrote this essay with an attempt to defend the view that if one cares about suffering, one should also care about nonhuman animals, since (1) they are capable of suffering, (2) they do suffer quite a lot, and (3) we can prevent their suffering. &nbsp; I hope that we can have a sober, non mind-killing discussion about this topic, since it\u2019s possibly quite important.</p>\n<p>&nbsp;</p>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>For the past two years, the only place I ate meat was at home with my family. &nbsp;As of October 2012, I've finally stopped eating meat altogether and can't see a reason why I would want to go back to eating meat. &nbsp;This kind of attitude toward eating is commonly classified as \"vegetarianism\" where one refrains from eating the flesh of all animals, including fish, but still will consume animal products like eggs and milk (though I try to avoid egg as best I can).</p>\n<p>Why might I want to do this? &nbsp;And why might I see it as a serious issue? &nbsp;It's because I'm very concerned about the reality of suffering done to our \"food animals\" in the process of making them into meat, because I see vegetarianism as a way to reduce this suffering by stopping the harmful process, and because vegetarianism has not been hard at all for me to accomplish.</p>\n<p>&nbsp;</p>\n<h2 id=\"Animals_Can_Suffer\">Animals Can Suffer</h2>\n<p>Back in the 1600s, R\u00e9n\u00e9 Descartes thought nonhuman animals were soulless automatons that could respond to their environment and react to stimuli, but could not feel anything \u2014 humans were the only species that were truly conscious. Descartes hit on an important point \u2014 since feelings are completely internal to the animal doing the feeling, it is impossible to demonstrate that anyone is truly conscious.</p>\n<p>However, when it comes to humans, we don\u2019t let that stop us from assuming other people feel pain. When we jab a person with a needle, no matter who they are, where they come from, or what they look like, they share a rather universal reaction of what we consider to be evidence of pain. We also extend this to our pets \u2014 we make great strides to avoid harming kittens, puppies, or other companion animals, and no one would want to kick a puppy or light a kitten on fire just because their consciousness cannot be directly observed. That\u2019s why we even go as far as having laws against animal cruelty.</p>\n<p>The animals we eat are no different. Pigs, chickens, cows, and fish all have incredibly analogous responses to stimuli that we would normally agree cause pain to humans and pets. &nbsp;Jab a pig with a needle, kick a chicken, or light a cow on fire, and they will react aversively like any cat, dog, horse, or human.</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Science\">The Science</strong></p>\n<p>But we don't need to rely on just our intuition -- instead, we can look at the science. &nbsp;<a href=\"http://www.grandin.com/welfare/fear.pain.stress.html\">Animal scientists Temple Grandin and Mark Deesing</a>&nbsp;conclude that \"[o]ur review of the literature on frontal cortex development enables us to conclude that all mammals, including rats, have a sufficiently developed prefrontal cortex to suffer from pain\". &nbsp;<a href=\"http://www.youtube.com/watch?v=mLSwRcvX72M\">An interview of seven different scientists</a>&nbsp;concludes that animals can suffer.</p>\n<p>Dr. Jane Goodall, famous for having studied animals, writes in her introduction to <strong><a href=\"http://www.amazon.com/Inner-World-Farm-Animals-Intellectual/dp/1584797487\">The Inner World of Farm Animals</a>&nbsp;</strong>that \"farm animals feel pleasure and sadness, excitement and resentment, depression, fear, and pain. They are far more aware and intelligent than we ever imagined\u2026they are individuals in their own right.\" &nbsp;Farm Sanctuary, an animal welfare organization, <a href=\"http://www.farmsanctuary.org/learn/someone-not-something/\">has a good overview documenting this research on animal emotion</a>.</p>\n<p>Lastly, among much other evidence, in the <a href=\"http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf\">\"Cambridge Declaration On Consciousness\"</a>, prominent international group of cognitive &nbsp;neuroscientists, neuropharmacologists, neurophysiologists, neuroanatomists and computational neuroscientists states:</p>\n<blockquote>\n<p>Convergent evidence indicates that non-human animals have the neuroanatomical, neurochemical, and neurophysiological substrates of conscious states along with the capacity to exhibit intentional behaviors. &nbsp;Consequently, the weight of evidence indicates that humans are not unique in possessing the neurological substrates that generate consciousness. Nonhuman animals, including all mammals and birds, and many other creatures, including octopuses, also &nbsp;possess these neurological substrates.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h2 id=\"Factory_Farming_Causes_Considerable_Suffering\">Factory Farming Causes Considerable Suffering</h2>\n<p>However, the fact that animals can suffer is just one piece of the picture; we next have to establish that animals <em>do</em>&nbsp;suffer as a result of people eating meat. &nbsp;Honestly, this is easier shown than told -- there's <a href=\"http://whosagainstanimalcruelty.org/\">an extremely harrowing and shocking 11-minute video about the cruelty available</a>. &nbsp;Watching that video is perhaps the easiest way to see the suffering of nonhuman animals first hand in these \"factory farms\".</p>\n<p>In making the case clear, <a href=\"http://www.veganoutreach.org/whyvegan/animals.html\">Vegan Outreach writes</a>&nbsp;\"Many people believe that animals raised for food must be treated well because sick or dead animals would be of no use to agribusiness. This is not true.\"</p>\n<p>They then go on to document, with sources, how virtually all birds raised for food are from factory farms where \"resulting ammonia levels [from densely populated sheds and accumulated waste] commonly cause painful burns to the birds' skin, eyes, and respiratory tracts\" and how hens \"become immobilized and die of asphyxiation or dehydration\", having been \"[p]acked in cages (usually less than half a square foot of floor space per bird)\". &nbsp;In fact, <a href=\"http://countinganimals.com/is-vegan-outreach-right-about-how-many-animals-suffer-to-death/\">137 million chickens suffer to death each year</a>&nbsp;before they can even make it to slaughter -- more than the number of animals killed for fur, in shelters and in laboratories combined!</p>\n<p>Farm Sanctuary also <a href=\"http://www.farmsanctuary.org/learn/factory-farming/\">provides an excellent overview of the cruelty of factory farming</a>, writing \"Animals on factory farms are regarded as commodities to be exploited for profit. They undergo painful mutilations and are bred to grow unnaturally fast and large for the purpose of maximizing meat, egg, and milk production for the food industry.\"</p>\n<p><strong>It seems clear that factory farming practices are truly deplorable, and certainly are not worth the benefit of eating a slightly tastier meal.</strong>&nbsp; In <a href=\"http://www.nytimes.com/2002/11/10/magazine/10ANIMAL.html?pagewanted=all\">\"An Animal's Place\"</a>, Michael Pollan writes:</p>\n<blockquote>\n<p>To visit a modern CAFO (Confined Animal Feeding Operation) is to enter a world that, for all its technological sophistication, is still designed according to Cartesian principles: animals are machines incapable of feeling pain. Since no thinking person can possibly believe this any more, industrial animal agriculture depends on a suspension of disbelief on the part of the people who operate it and a willingness to avert your eyes on the part of everyone else.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h2 id=\"Vegetarianism_Can_Make_a_Difference\">Vegetarianism Can Make a Difference</h2>\n<p>Many people see the staggering amount of suffering in factory farms, and if they don't aim to dismiss it outright will say that there's no way they can make a difference by changing their eating habits. &nbsp;However, this is certainly not the case!</p>\n<p>&nbsp;</p>\n<p><strong id=\"How_Many_Would_Be_Saved_\">How Many Would Be Saved?</strong></p>\n<p>Drawing from the 2010 <a href=\"http://usda.mannlib.cornell.edu/MannUsda/viewDocumentInfo.do?documentID=1097\">Livestock Slaughter Animal Summary</a>&nbsp;and the <a href=\"http://usda.mannlib.cornell.edu/MannUsda/viewDocumentInfo.do?documentID=1497\">Poultry Slaughter Animal Summary</a>, 9.1 billion land animals are either grown in the US or imported (94% of which are chickens!), 1.6 billion are exported, and 631 million die before anyone can eat them, leaving 8.1 billion land animals for US consumption <strong>each year</strong>.</p>\n<p>A na\u00efve average would divide this total among the population of the US, which is 311 million, assigning 26 land animals for each person's annual consumption. &nbsp;Thus, by being vegetarian, you are saving 26 land animals a year you would have otherwise eaten. &nbsp;And this doesn't even count fish, which could be quite high given how many fish need to be grown just to be fed to bigger fish!</p>\n<p>Yet, this is not quite true. &nbsp;It's important to note that supply and demand aren't perfectly linear. &nbsp;If you reduce your demand for meat, the suppliers will react by lowering the price of meat a little bit, making it so more people can buy it. &nbsp;Since chickens dominate the meat market, we'll adjust by the supply elasticity of chickens, <a href=\"http://www.rti.org/pubs/poultry_slaughter.pdf\">which is 0.22</a>&nbsp;and the demand elasticity of chickens, <a href=\"http://ageconsearch.umn.edu/bitstream/31190/1/23020558.pdf\">which is -0.52</a>, and calculate the change in supply, which is 0.3. &nbsp;Taking this multiplier, it's more accurate to say <strong>you're saving 7.8 land animals a year or more</strong>. &nbsp;Though, there are a lot of complex considerations in calculating elasticity, so we should take this figure to have some uncertainty.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Collective_Action\">Collective Action</strong></p>\n<p>One might critique this response by responding that since meat is often bought in bulk, reducing meat consumption won't affect the amount of meat bought, and thus the suffering will still be the same, except with meat gone to waste. &nbsp;However, this ignores the effect of many different vegetarians acting together.</p>\n<p>Imagine that you're supermarket buys cases of 200 chicken wings. &nbsp;It would thus take 200 people together to agree to buy 1 less wing in order for the supermarket to buy less wings. &nbsp;However, you have no idea if you're vegetarian #1 or vegetarian #56 or vegetarian #200, making the tipping point for 200 less wings to be bought. &nbsp;You thus can estimate that by buying one less wing you have a 1 in 200 chance of reducing 200 wings, which is equivalent to reducing the supply by one wing. &nbsp;So the effect basically cancels out. &nbsp;See <a href=\"http://www.veganoutreach.org/enewsletter/thresholds.pdf\">here</a>&nbsp;or <a href=\"http://www.utilitarian-essays.com/vegetarianism.html\">here</a>&nbsp;for more.</p>\n<p>Every time you buy factory farmed meat, you are creating demand for that product, essentially saying \"Thank you, I liked what you are doing and want to encourage you to do it more\". &nbsp;By eating less meat, we can stop our support of this industry.</p>\n<p>&nbsp;</p>\n<h2 id=\"Vegetarianism_Is_Easier_Than_You_Think\">Vegetarianism Is Easier Than You Think</h2>\n<p>So nonhuman animals can suffer and do suffer in factory farms, and we can help stop this suffering by eating less meat. &nbsp;I know people who get this far, but then stop and say that, as much as they would like to, there's no way they could be a vegetarian because they like meat too much! &nbsp;However, such a joy for meat shouldn't count much compared to the massive suffering each animal undergoes just to be farmed -- imagine if someone wouldn't stop eating your pet just because they like eating your pet so much!</p>\n<p>This is less than a problem than you might think, because being a vegetarian is really easy. &nbsp;Most people only think about what they would have to give up and how good it tastes, and don't think about what tasty things they could eat instead that have no meat in them. &nbsp;When I first decided to be a vegetarian, I simply switched from tasty hamburgers to <a href=\"http://www.veganoutreach.org/guide/substitutes.html\">tasty veggieburgers</a>&nbsp;and there was no problem at all.</p>\n<p>&nbsp;</p>\n<p><strong id=\"A_Challenge\">A Challenge</strong></p>\n<p>To those who say that vegetarianism is too hard, I\u2019d like to simply challenge you to <a href=\"/lw/5a5/no_seriously_just_try_it/\">just try it</a>&nbsp;for a few days. Feel free to give up afterward if you find it too hard. But I imagine that you should do just fine, find great replacements, and be able to save animals from suffering in the process.</p>\n<p>If reducing suffering is one of your goals, there\u2019s no reason why you must either be a die-hard meat eater or a die-hard vegetarian. Instead, feel free to explore some middle ground. You could be a vegetarian on weekdays but eat meat on weekends, or just try Meatless Mondays, or simply try to eat less meat. You could try to eat bigger animals like cows instead of fish or chicken, thus <a href=\"http://www.utilitarian-essays.com/suffering-per-kg.html\">getting the same amount of meat with significantly less suffering</a>.</p>\n<p>-</p>\n<address>(This was also <a href=\"http://www.everydayutilitarian.com/essays/why-eat-less-meat/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</address><address><br></address>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Animals Can Suffer", "anchor": "Animals_Can_Suffer", "level": 1}, {"title": "The Science", "anchor": "The_Science", "level": 2}, {"title": "Factory Farming Causes Considerable Suffering", "anchor": "Factory_Farming_Causes_Considerable_Suffering", "level": 1}, {"title": "Vegetarianism Can Make a Difference", "anchor": "Vegetarianism_Can_Make_a_Difference", "level": 1}, {"title": "How Many Would Be Saved?", "anchor": "How_Many_Would_Be_Saved_", "level": 2}, {"title": "Collective Action", "anchor": "Collective_Action", "level": 2}, {"title": "Vegetarianism Is Easier Than You Think", "anchor": "Vegetarianism_Is_Easier_Than_You_Think", "level": 1}, {"title": "A Challenge", "anchor": "A_Challenge", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "505 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 516, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["grP8nTMWm67RbKZwg", "Zmfo388RA9oky3KYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T02:36:45.881Z", "modifiedAt": null, "url": null, "title": "Introducing Familiar, a quantified reasoning assistant (feedback sought!)", "slug": "introducing-familiar-a-quantified-reasoning-assistant", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.850Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jamesf", "createdAt": "2013-03-24T19:55:03.305Z", "isAdmin": false, "displayName": "jamesf"}, "userId": "WxcuJX88RjTNEyfhL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bnirupGkLNkmvr33f/introducing-familiar-a-quantified-reasoning-assistant", "pageUrlRelative": "/posts/bnirupGkLNkmvr33f/introducing-familiar-a-quantified-reasoning-assistant", "linkUrl": "https://www.lesswrong.com/posts/bnirupGkLNkmvr33f/introducing-familiar-a-quantified-reasoning-assistant", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introducing%20Familiar%2C%20a%20quantified%20reasoning%20assistant%20(feedback%20sought!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroducing%20Familiar%2C%20a%20quantified%20reasoning%20assistant%20(feedback%20sought!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnirupGkLNkmvr33f%2Fintroducing-familiar-a-quantified-reasoning-assistant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introducing%20Familiar%2C%20a%20quantified%20reasoning%20assistant%20(feedback%20sought!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnirupGkLNkmvr33f%2Fintroducing-familiar-a-quantified-reasoning-assistant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnirupGkLNkmvr33f%2Fintroducing-familiar-a-quantified-reasoning-assistant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 694, "htmlBody": "<p><em>tl;dr: I'm making a thing that uses probabilistic graphical models to assist in drawing inferences from personal data. You should <a href=\"https://github.com/jferg/familiar\">c</a></em><em><a href=\"https://github.com/jferg/familiar\" target=\"_blank\">heck it out</a>, and share with me your wisdom/user experience.</em></p>\n<p>I had this <a href=\"https://forum.quantifiedself.com/thread-data-analysis-software\" target=\"_blank\">not-completely-original idea</a> that there should be some kind of tool for easily performing statistical inference on Quantified Self-style data.</p>\n<p>There are a lot of QS apps out there, but for the most part they seem to be designed for 1. a <a href=\"https://play.google.com/store/apps/details?id=je.fit&amp;hl=en\" target=\"_blank\">single</a>&nbsp;<a href=\"https://play.google.com/store/apps/details?id=com.period.tracker.lite\" target=\"_blank\">domain</a>&nbsp;and/or 2. recording things primarily to <a href=\"https://www.beeminder.com/\" target=\"_blank\">combat akrasia</a>&nbsp;or (more often) sating curiosity/as a lifestyle accessory, rather than actively helping you discover correlations or determine causality between things-you-do and things-you-care-about. <a href=\"http://www.quantified-mind.com/\" target=\"_blank\">Quantified Mind</a> stands out as a counterexample, but I can't come up with many others in that vein.</p>\n<p>There are also&nbsp;<a href=\"http://www.norsys.com/netica.html\" target=\"_blank\">commercial products</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/R_(programming_language)\" target=\"_blank\">programming languages</a> that allow one to use machine learning to perform inference on data, but they mostly seem to be proprietary and expensive software aimed at businesses, or free but intended to be used by scientists, engineers, etc.; nothing I've yet to find is really suitable for an individual without a background in statistics/machine learning who just wants to learn what they can by smashing together their&nbsp;<a href=\"https://www.moodscope.com/\" target=\"_blank\">Moodscope</a> and their <a href=\"http://www.fitbit.com/\" target=\"_blank\">FitBit</a>.</p>\n<p>In our era of FOSS, APIs, QS, and ML, this seems like a seriously lacking state of affairs. Hence, <a href=\"https://github.com/jferg/familiar\" target=\"_blank\"><strong>Familiar</strong></a>.</p>\n<p>Currently, it consists of a command line interface for storing variable definitions and data in a local database without too much fuss, building a <a href=\"http://en.wikipedia.org/wiki/Naive_bayes_classifier\" target=\"_blank\">naive Bayes classifier</a>&nbsp;on those variables, and finding <a href=\"http://en.wikipedia.org/wiki/Maximum_likelihood\" target=\"_blank\">maximum likelihood estimates</a> given the state of one variable for the states of all the other variables. This is unsophisticated and not extremely user-friendly, but those things will change in the near future. In the case where I keep working on this for a very long time, I want to automate away as much recording as possible (including things like <a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=163348\" target=\"_blank\">mood and productivity</a>), record everything with the highest reasonable time resolution, plug into every other app out there that might provide useful data, use more complex machine learning algorithms to identify causality and generate suggestions for personal experimentation, and generally have a piece of software that knows you so well it can help you think more like an ideal Bayesian reasoner and thereby assist you in living your life (thus <a href=\"http://dictionary.reference.com/browse/familiarity\" target=\"_blank\">the</a> <a href=\"http://en.wikipedia.org/wiki/Familiar_spirit\" target=\"_blank\">name</a>). <a href=\"http://www.warrenellis.com/?p=13972\" target=\"_blank\">Manfred Macx's glasses from <em>Accelerando</em></a>&nbsp;have something like this inside them, and <em>I want it too</em>.</p>\n<p>Anyway, back to the present. You can help me by answering whichever of these questions applies to you the most:</p>\n<ul>\n<li>Would you use something like this at all, or do you think the potential for extracting useful information out of messy personal data is too low?</li>\n<li>If you might use something like this, but don't want to use Familiar in its current state, what do you think is the most important factor? e.g. \"no GUI\", \"not a web app\", \"too manual\", \"doesn't connect to other stuff yet\", etc.</li>\n<li>If you're brave enough to start using this now or even look at the source code, what mistakes am I making? There are countless ways this could be easier to use, more helpful, faster, more readable, and otherwise better, and you can tell me what those ways are.</li>\n</ul>\n<div>Anyone's feedback will be appreciated, but if you have experience in statistics or machine learning, Quantified Self stuff, writing software that people actually use, or whatever else might be relevant, I especially want to hear your opinion. (<a href=\"http://www.gwern.net/Zeo\" target=\"_blank\">Gwern gets to order me around.</a>)</div>\n<div><br /></div>\n<div>This is my main&nbsp;<a href=\"https://www.hackerschool.com/\" target=\"_blank\">Hacker School</a>&nbsp;project, by the way. If you like programming, I can't recommend it enough, and applications for the fall batch are currently open.</div>\n<div><br /></div>\n<div>(Aside: I don't care much for the term \"quantified self\". It's accurate in describing what people have been doing with it so far, and I don't really expect to get people to stop using an already-popular mostly-correct label, but I think there's a lot of potential in quantifying your interactions with other people and your environment as well, and having the word \"self\" in the label might unduly limit imaginations. \"Quantified living\" is closer to what I have in mind, but if you have catchier or more precise suggestions I'd love to hear them too.)</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bnirupGkLNkmvr33f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 24, "extendedScore": null, "score": 1.2768527692523596e-06, "legacy": true, "legacyId": "23468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T12:44:05.763Z", "modifiedAt": null, "url": null, "title": "Business Insider: \"They Finally Tested The 'Prisoner's Dilemma' On Actual Prisoners \u2014 And The Results Were Not What You Would Expect\"", "slug": "business-insider-they-finally-tested-the-prisoner-s-dilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:30.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HB9KQCzH6wWewAA4X/business-insider-they-finally-tested-the-prisoner-s-dilemma", "pageUrlRelative": "/posts/HB9KQCzH6wWewAA4X/business-insider-they-finally-tested-the-prisoner-s-dilemma", "linkUrl": "https://www.lesswrong.com/posts/HB9KQCzH6wWewAA4X/business-insider-they-finally-tested-the-prisoner-s-dilemma", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Business%20Insider%3A%20%22They%20Finally%20Tested%20The%20'Prisoner's%20Dilemma'%20On%20Actual%20Prisoners%20%E2%80%94%20And%20The%20Results%20Were%20Not%20What%20You%20Would%20Expect%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABusiness%20Insider%3A%20%22They%20Finally%20Tested%20The%20'Prisoner's%20Dilemma'%20On%20Actual%20Prisoners%20%E2%80%94%20And%20The%20Results%20Were%20Not%20What%20You%20Would%20Expect%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHB9KQCzH6wWewAA4X%2Fbusiness-insider-they-finally-tested-the-prisoner-s-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Business%20Insider%3A%20%22They%20Finally%20Tested%20The%20'Prisoner's%20Dilemma'%20On%20Actual%20Prisoners%20%E2%80%94%20And%20The%20Results%20Were%20Not%20What%20You%20Would%20Expect%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHB9KQCzH6wWewAA4X%2Fbusiness-insider-they-finally-tested-the-prisoner-s-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHB9KQCzH6wWewAA4X%2Fbusiness-insider-they-finally-tested-the-prisoner-s-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>Article at <a style=\"color: #003399;\" href=\"http://www.businessinsider.com/prisoners-dilemma-in-real-life-2013-7#ixzz2ZxwzT6nj\">http://www.businessinsider.com/prisoners-dilemma-in-real-life-2013-7#ixzz2ZxwzT6nj</a>, seems revelant to a lot of the discussion here.</p>\n<p>There've been studies about people who consider themselves to be relatively successful are less cooperative than people who consider themselves relatively unsuccessful. The study referenced in that article seems to bear this out.</p>\n<p>So if you want the other party to cooperate, should you attempt to give that party the impression it has been relatively unsuccessful, at least if that party is human?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HB9KQCzH6wWewAA4X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 1.277344546722613e-06, "legacy": true, "legacyId": "23479", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T13:23:10.355Z", "modifiedAt": null, "url": null, "title": "Fake Explanations in Modern Science: The Case of Inefficiency", "slug": "fake-explanations-in-modern-science-the-case-of-inefficiency", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:14.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "9hQryffdwLDyNbnkh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LYuzBTJyzWWm2Hmhy/fake-explanations-in-modern-science-the-case-of-inefficiency", "pageUrlRelative": "/posts/LYuzBTJyzWWm2Hmhy/fake-explanations-in-modern-science-the-case-of-inefficiency", "linkUrl": "https://www.lesswrong.com/posts/LYuzBTJyzWWm2Hmhy/fake-explanations-in-modern-science-the-case-of-inefficiency", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fake%20Explanations%20in%20Modern%20Science%3A%20The%20Case%20of%20Inefficiency&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFake%20Explanations%20in%20Modern%20Science%3A%20The%20Case%20of%20Inefficiency%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYuzBTJyzWWm2Hmhy%2Ffake-explanations-in-modern-science-the-case-of-inefficiency%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fake%20Explanations%20in%20Modern%20Science%3A%20The%20Case%20of%20Inefficiency%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYuzBTJyzWWm2Hmhy%2Ffake-explanations-in-modern-science-the-case-of-inefficiency", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYuzBTJyzWWm2Hmhy%2Ffake-explanations-in-modern-science-the-case-of-inefficiency", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4676, "htmlBody": "<p><strong> </strong></p>\n<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; font-weight: normal; padding: 0.5em; margin: 8px;\">\n<p style=\"font-weight: normal;\"><em><span style=\"font-style: normal;\"> </span></em></p>\n<p style=\"font-weight: normal; \"><strong>tl;dr:</strong></p>\n<p style=\"font-weight: normal; \">This argument is a walkthrough of the argument against inefficiency as an real explanation in the tradition of Coase, Demsetz, and Alchian. Briefly, inefficiency is the result of being unaware of some constraint on the system and then failing to realize that what appears to be inefficiency must always actually be your ignorance of some constraint on the system. The universe does not glitch. Near the end I also take up the question of how a fake explanation was able to persist for so long in economics, especially <em>after</em>&nbsp;Coase proved in 1960 that there is no such thing as inefficiency.</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">The Jargon</strong></span></p>\n<p style=\"font-weight: normal; \">You may have noticed that economists talk about efficiency a lot. Normally efficiency refers to an input-output ratio, but that's not usually how economists use the term. Economists normally use efficiency to denote to a system of exhausted opportunities. An efficient system can't be improved, given the constraints on the system. More often that not these opportunities refer to Pareto improvements. A system that has exhausted all possible Pareto improvements given the constraints on the system is called Pareto efficient.</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">Pareto Improvements and Pareto Efficiency</strong></span></p>\n<p style=\"font-weight: normal; \">A Pareto improvement is any change that makes at least one person better off without making anyone else worse off. Economists like Pareto improvements because they're unambiguously good or neutral for everyone by definition. Only a misanthrope can complain about a Pareto improvement. A system that has no remaining Pareto improvements is called Pareto efficient. A system which has some remaining is called Pareto inefficient.</p>\n<p style=\"font-weight: normal; \">Not all Pareto efficient systems are good. A world where Sauron controls the rest of the world as slaves from his towers in Mordor would be Pareto efficient, since there's no way to free all the slaves without hurting Sauron. It might be&nbsp;<span style=\"font-style: italic;\"><span style=\"font-style: italic;\">good</span></span>&nbsp;to save the world at Sauron's expense, but it wouldn't be a Pareto improvement. Still, all else held equal, it's always better to be in a Pareto efficient system than a Pareto inefficient system. A Pareto improvement that doesn't<span style=\"font-style: italic;\">&nbsp;</span>happen doesn't help anyone, after all. Pareto improvements can only make people better off. Moving from a Pareto inefficient system to a Pareto efficient system is by definition a free lunch.</p>\n<p style=\"font-weight: normal; \">Pareto improvements have come under fire in two main ways. The first is that by focusing only on improvements which don't make anyone worse off, many potentially beneficial changes to the world are ruled out, most notably redistribution of wealth and income. Redistribution may be an improvement, but because it hurts the people who lose some of their wealth and income, it isn't a&nbsp;<span style=\"font-style: italic;\">Pareto</span>&nbsp;improvement. Handling this problem is what Kaldor-Hicks efficiency is for, which we get to farther down.</p>\n<p style=\"font-weight: normal; \">The other major criticism of Pareto improvements is that it is virtually impossible to find any. Picking $20 up that was just lying on the street abandoned hurts the person who would have come after you. The world is incredibly complex, and the odds of some action making&nbsp;<span style=\"font-style: italic;\">no one</span>&nbsp;worse off is low, to say the least. And if you include the utility of misanthropes and evil people, Pareto improvements might become completely impossible. We'll handle this problem by ignoring it, since it's not a problem in simple models.</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">There Ain't No Such Thing as a Free Lunch</strong></span></p>\n<p style=\"font-weight: normal; \">So if moving from a Pareto inefficient system to a Pareto efficient system is a free lunch, that raises an obvious problem:</p>\n<p style=\"font-weight: normal; \">There ain't no such thing as a free lunch.</p>\n<p style=\"font-weight: normal; \">Which means that Pareto&nbsp;<span style=\"font-style: italic;\">in</span>efficient systems can't exist. The only type of system that can exist is a Pareto efficient one.</p>\n<p style=\"font-weight: normal; \">The economic logic is simple and inexorable. A Pareto inefficient system by definition contains outstanding Pareto improvements. A Pareto improvement is a benefit to at least one person at no cost to anyone else.</p>\n<p style=\"font-weight: normal; \">So in order for a Pareto inefficient system to exist, there has to be an economic agent--because otherwise we're not dealing with an economic system--who forgoes a benefit for some reason&nbsp;<span style=\"font-style: italic;\">other than a cost to himself or someone else.&nbsp;</span>And&nbsp;if he willingly forgoes the benefit because of the cost to someone else, that is in fact a cost to himself, the disutility he experiences from the suffering of someone else, or if he forgoes the benefit because the cost to someone else induces that someone else to stop him from pursuing the improvement, that would also only happen if that someone were able to impose a sufficient cost on him. So we can restate Pareto inefficiency as a system where an economic agent forgoes a benefit for some reason other than the cost of doing so.</p>\n<p style=\"font-weight: normal; \">There is a problem:</p>\n<p style=\"font-weight: normal; \">Economic agents aren't allowed to do that.</p>\n<p style=\"font-weight: normal; \">Economic agents have to take any benefit that is not outweighed by the cost of doing so, where the cost of doing so is the foregone benefit from not doing the next best thing. That's what economizing behavior is. Maximization of benefits/minimization of costs, since costs are foregone benefits they are identical criteria. An economic agent&nbsp;<span style=\"font-style: italic;\">isn't allowed&nbsp;</span>to forgo a benefit for anything other than a even greater benefit somewhere else, which is to say, a cost that is too high.</p>\n<p style=\"font-weight: normal; \">And just in case you think you've found a loophole, \"cost\" is a pretty expansive term. It refers to things like how much time and money you have to give up to get something, but if you don't want to do something because it would embarrass you, or it would make your mother sad, or because something about it just feels&nbsp;<span style=\"font-style: italic;\">off</span>...then that's the cost. In fact, the term is sufficiently expansive so as to include&nbsp;<span style=\"font-style: italic;\">any&nbsp;</span>reason for not doing something.</p>\n<p style=\"font-weight: normal; \">So an economic agent can't forgo a benefit without a reason, without an explanation. By definition. And having economic agents is the fundamental premise of economics. They're the ones who follow the laws of economics. Without economic agents you don't have economics.</p>\n<p style=\"font-weight: normal; \">So we can restate Pareto inefficiency as a system where an economic agent forgoes a benefit for no reason.&nbsp;Which they're not allowed to do.</p>\n<p style=\"font-weight: normal; \">And it makes absolutely no sense. Pareto inefficiency is like a starving man who has suddenly been put in front of a large banquet of all the food he's been dreaming about for months. And then he doesn't eat.&nbsp;<span style=\"font-style: italic;\">For no reason.&nbsp;</span>If he doesn't eat because he forgot how, or because he's dealt with the cognitive dissonance produced by starving to death by convincing himself that starving to death is natural and trying to fight it is a sign of hubris, or because he feels weird being in a thought experiment, or&nbsp;<span style=\"font-style: italic;\">anything at all</span>, then that's the cost and there's no outstanding Pareto improvement. And in fact, calling this confusing situation inefficient doesn't do anything to alleviate my confusion.</p>\n<p style=\"font-weight: normal; \">So we just have this starving man not eating the delicious food set out before him and&nbsp;<span style=\"font-style: italic;\">this cannot be explained in any way.</span>&nbsp;That's Pareto inefficiency. And that's&nbsp;<span style=\"font-style: italic;\"><em>nuts</em></span>.&nbsp;</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">What About Kaldor-Hicks Efficiency? You Promised!</strong></span></p>\n<p style=\"font-weight: normal; \">Kaldor-Hicks efficiency is what economists turn to when Pareto efficiency proves too restrictive, which it always does. Kaldor-Hicks efficiency is similar to Pareto efficiency, but whereas Pareto efficiency counts the number of outstanding Pareto improvements with zero being the efficient level, Kaldor-Hicks efficiency counts the number of outstanding Kaldor-Hicks improvements, with zero being the efficient level. A Kaldor-Hicks improvement is any change that makes at least one person better off that they&nbsp;<span style=\"font-style: italic;\"><span style=\"font-style: italic;\">would, if it was possible</span></span>,&nbsp;be willing to compensate the person made worse off, if there are any. For example, if a change makes one person $10 better off, and another person $5 worse off, the person made better off would, if it only it were possible,&nbsp;be willing to pay the person made worse off the five dollars necessary to make the latter indifferent to the change. Essentially, it's a cost-benefit test. If the total benefits from the change exceeds the total costs, such that you could take some money from the winners to completely compensate the losers and still have some left over for the winners, that's a Kaldor-Hicks improvement. All Pareto improvements are Kaldor-Hicks improvements but not all Kaldor-Hicks improvements are Pareto improvements.</p>\n<p style=\"font-weight: normal; \">So what about Kaldor-Hicks inefficiency, when you have some outstanding Kaldor-Hicks improvements? Does it make any sense?</p>\n<p style=\"font-weight: normal; \">Suppose Abe steals a car from Barry. The car is worth $40,000 to Abe, who likes nice cars, and only worth $30,000 to Barry, who takes a more practical view of his vehicle. That's not a Pareto improvement since Barry is hurt, but it&nbsp;<span style=\"font-style: italic;\"><span style=\"font-style: italic;\">is</span></span>&nbsp;a Kaldor-Hicks improvement since Abe could compensate Barry the $30,000 and still have $10,000 left over. So, ignoring all external and additional effects of the theft beyond the benefit to Abe and the loss to Barry, in order to achieve Kaldor-Hicks efficiency, Abe would have to steal Barry's car, right?</p>\n<p style=\"font-weight: normal; \">So Abe, the economic actor that he is, sneaks over Barry's fence late at night, creeps around to where the garage is...</p>\n<p style=\"font-weight: normal; \">...And Barry, the economic actor that&nbsp;<span style=\"font-style: italic;\">he</span>&nbsp;is, nearly blows Abe's head off with a double-barrelled shotgun. Cursing and shrieking, Abe escapes with his tail between his legs as Barry calmly reloads.</p>\n<p style=\"font-weight: normal; \">(Yes, I'm from Texas, why do you ask?)</p>\n<p style=\"font-weight: normal; \">Is the system now inefficient? After all, the Kaldor-Hicks improvement didn't happen.</p>\n<p style=\"font-weight: normal; \">Remember, the question isn't whether it would be a net plus for Abe to have successfully stolen Barry's car. That's built into the assumptions of the thought experiment. It wouldn't be a Kaldor-Hicks improvement if it wasn't a net plus. But that's not quite the same thing as asking if it's inefficient.</p>\n<p style=\"font-weight: normal; \">Syllables aren't inherently significant; what matters is the associations and concepts they draw up inside your head. And what inefficiency calls up for me is something that isn't being optimized given the constraints on the system. Something about the economics is&nbsp;<span style=\"font-style: italic;\">wrong</span>, something is happening that shouldn't be&nbsp;<span style=\"font-style: italic;\">given the constraints</span>. I don't mean that somebody should be doing something different because it would be better that way, I mean that somebody should be doing something different because the laws of economics predict something different. With a Pareto improvement, for example, the idea of a Pareto improvement&nbsp;<span style=\"font-style: italic;\">not happening</span>&nbsp;in a world of economic actors, what we call Pareto inefficiency, means that somebody is economizing wrong, not in a backwards-looking way, but in a forwards-looking way, such that given the economic actor's information, utility function, degree of rationality and cognitive operations, etc., that economic actor should have done something differently even though he&nbsp;<span style=\"font-style: italic;\">couldn't</span>have done anything differently given the constraints and the laws of economics.</p>\n<p style=\"font-weight: normal; \">And if we look at this Kaldor-Hicks improvement that didn't happen, we can see that Barry didn't do anything he should have done. Sure, letting Abe steal his car would have been a net benefit for the world, but what does Barry car? He's out $30,000. The proper economic thing for Barry to do is to stop the theft (so long as the cost of doing so is less than $30,000). Abe did what was efficient for him, and Barry did what was efficient for him. And so if everyone does what's efficient, how can that add up to inefficiency?</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\">What About Social Efficiency?</strong></p>\n<p style=\"font-weight: normal; \">\"Obviously individuals will always do what's efficient for them,\" you say. \"But that's how it works. Individually efficient actions lead to outcomes that are inefficient for the group. Ever heard of the Prisoner's Dilemma, you idiot?\"</p>\n<p style=\"font-weight: normal; \">Wow, that escalated suddenly. But moving from individual efficiency to group efficiency doesn't save the concept.</p>\n<p style=\"font-weight: normal; \">Let's go back to a very simple Pareto improvement. A man pours coffee for himself in the morning, making him better off at no cost to anyone else. Hooray, we have achieved Pareto efficiency. Except...</p>\n<p style=\"font-weight: normal; \">...Wouldn't it have been&nbsp;<span style=\"font-style: italic;\">even better</span>&nbsp;if while pouring himself a cup of coffee, he had turned into the happiest man alive, world peace and prosperity was achieved for all time, and the next arc of HPMOR didn't take ages to come out? Clearly we have a very Pareto inefficient system here, since all these Pareto improvements didn't happen.</p>\n<p style=\"font-weight: normal; \">The obvious problem with this logic is that all those \"Pareto improvements\" weren't possible. They could have happened if the universe were a very different place, but it's not, so they didn't. Any improvement that isn't allowed by the laws of the system and its constraints doesn't count. If they did count, then any system short of absolute utopia would be inefficient.</p>\n<p style=\"font-weight: normal; \">So let's look at the Prisoner's Dilemma. Individually both prisoners want to defect. But if they both follow the dominant strategy, they'll be worse off than if they both cooperate. This is usually presented as showing how individual doing what's efficient for them leads to group inefficiency.</p>\n<p style=\"font-weight: normal; \">But it's the same problem. The limited information people have and the decisions their brain makes given that information and the incentives they face are constraints that are just as real as the constraints which prevent our good friend above from becoming the happiest man in the world while he pours his coffee. If we're going to acknowledge that the improvements thatdon't&nbsp;physical constraints like how much steel there is or how much energy it takes for something to happen doesn't mean that the given system is inefficient, then social constraints like information, incentives, and cognitive abilities shouldn't either. The poor prisoners might&nbsp;<span style=\"font-style: italic;\">wish</span>&nbsp;they lived in a different world, one where a different set of information, incentives, and cognitive abilities allowed them to both cooperate, but they don't. Similarly, we might all wish we lived in a world of infinite free energy, but we don't. If the latter fact doesn't render our reality inefficient, then why should the former?</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\">Whatever Is, Is Efficient</strong></p>\n<p style=\"font-weight: normal; \">Armen Alchian, the most powerful economist ever, would always tell his students,&nbsp;<a href=\"http://gallery.economicus.ru/cgi-bin/frame_rightn.pl?type=in&amp;links=./in/alchian/biogr/alchian_b2.txt&amp;img=brief.gif&amp;name=alchian\">\"Whatever is, is efficient.\"</a></p>\n<p style=\"font-weight: normal; \">That should be pretty obvious by now if you've followed everything I've said so far. Inefficiency seems to posit some kind of glitch in the universe, that given to the laws and constraints on the system, an economic actor isn't economizing even though economic actors economize by definition. That or inefficiency seems to be a case of the Mind Projection Fallacy: by comparing the real world to some imagined better world, no matter how impossible it may be given the current constraints, the real world suddenly seems to fall short of a mark that the universe isn't actually paying any attention to. Well, the universe doesn't glitch and the mere ability to imagine some better alternative to the real world does not in fact imply an non-exhausted opportunity in the real system.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\">Do Economists Actually Believe in Inefficiency?</strong></p>\n<p style=\"font-weight: normal; \"><a href=\"http://opim.wharton.upenn.edu/~sok/papers/b/Bator-market-failure.pdf\">This is the first fundamental theorem of welfare economics, as stated by F.M. Bator:</a></p>\n<p style=\"font-weight: normal; \">\"It is the central theorem of modern welfare economics that under certain strong assumptions about technology, tastes, and producers' motivations, the equilibrium conditions which characterize a system of competitive markets will exactly correspond to the requirements of Paretian efficiency.\"</p>\n<p style=\"font-weight: normal; \">So yes, yes they do. They believe in inefficiency&nbsp;a lot. Look how strong the assumptions they think are necessary for efficiency to occur. We're a very long ways from \"Whatever is, is efficient,\" here.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\">Inefficiency: A Fake Explanation</strong></p>\n<p style=\"font-weight: normal; \">How could you come to believe that the universe glitches occasionally?&nbsp;</p>\n<p style=\"font-weight: normal; \">You don't, obviously. But if the logical conclusion of what you do believe is that the universe glitches occasionally, that's a sign of a fake explanation. Suppose you posit God as an explanation for rainfall. Now, this could be a real, falsifiable hypothesis about a wizard who lives in the sky. It could also be a fake explanation. In the latter case, \"God\" is an empty word that just serves as an intellectual stop sign. It posits \"I don't know\" as an explanation, which is to say no explanation as an explanation. If you took the fake answer seriously, you would think that everyone who says that God causes rainfall thinks that rainfall makes no sense within the universe.</p>\n<p style=\"font-weight: normal; \">Inefficiency occupies the same role within economics. It doesn't explain anything. It's false, in its form as a falsifiable hypothesis, which claims that there exists foregone benefits that are not explained by any cost, and it's not even wrong in its form as a fake explanation, which claims that you can explain foregone benefits without explaining them.</p>\n<p style=\"font-weight: normal; \">If you aren't buying it, let's talk about transaction costs. Transaction costs, as the name implies, are the costs of transacting. More specifically, it's used by economists to refer to the cost of using the price system.&nbsp;<a href=\"http://weber.ucsd.edu/~jlbroz/Courses/POLI200C/syllabus/Coase_social%20cost.pdf\">In 1960, Ronald Coase applied the concept of transaction costs to externalities, with startling results</a>. Before 1960, everyone thought of externalities as inefficient, in both the sense of a system with externalities was supposed to have outstanding Pareto improvements, and in the sense that nobody knew what caused externalities and so \"explained\" them by calling them inefficient, with a few exceptions such as Frank Knight, who pointed out that economists didn't know how to explain the existence of externalities. Then Coase exploded both those ideas.</p>\n<p style=\"font-weight: normal; \">An externality is an external cost, where somebody does something that makes someone else worse off on the margin without suffering proportionally himself. The archetype is pollution. A polluter who pollutes in the process of producing some product makes other people worse off without having to pay himself for the harm, which means that on the margin he'll be hurting others more than his product benefits himself and others. That net harm was supposed to imply an outstanding Pareto improvement, since some people would benefit and others would not be harmed if the pollution was lessened and the polluter was then compensated for reducing his pollution. Why this didn't happen was unexplained, of course.</p>\n<p style=\"font-weight: normal; \">Coase pointed out that what explained why these exchanges weren't happening must be to do with some set of costs which prevent exchanges from happening. He called those costs \"transaction costs.\"</p>\n<p style=\"font-weight: normal; \">Essentially, the mystery is why people don't use the price system to internalize externalities. For example, suppose Abe builds a factory next to Barry's house. On the margin Abe benefits $5, and the pollution from his factory costs Barry $10.&nbsp;</p>\n<p style=\"font-weight: normal; \">The solution to this is obvious. Barry will pay Abe any amount of money from $5 to $10 to not pollute. Abe will agree to any amount of money over $5 to stop polluting and Barry will agree to pay any amount of money under $10 to prevent the pollution. They'll both benefit. So how can externalities possibly exist? Economic agents should automatically internalize them.</p>\n<p style=\"font-weight: normal; \">The only possible real answer, as Coase realized, is that the cost of Abe and Barry using the price system, the transaction cost, must exceed the range of payments they will both agree on. If it costs Abe and Barry $6 to agree to some price on the externality, then there will be no price they can both agree on, and the externality will persist.</p>\n<p style=\"font-weight: normal; \">Coase substituted a real explanation, transaction costs, for a fake explanation, inefficiency, and won a Nobel Prize for it. But in fact his argument applies to much more than pollution.&nbsp;All&nbsp;the various types of supposed economic inefficiency, such as monopoly and collective action problems, are really just externalities. In the case of monopoly, people would like to pay the monopoly to behave more competitively but can't due to the cost of using the price system, and in the case of collective action problems, people would like to pay others not to defect but can't again due to transaction costs.</p>\n<p style=\"font-weight: normal; \">All the standard forms of economic inefficiency are really just exchanges that don't take place due to transaction costs. But any cost, not just transaction costs, prevent exchanges. The cost of steel prevents some people from buying steel from people who want to sell them steel. I'd like to buy a lot of steel if it cost nothing, but that's not the case. Is that inefficiency? If not, then why are exchanges that don't happen because of the cost of transacting any different? This is really just the same point as above that any foregone benefit that is explained by a cost isn't inefficient.&nbsp;There's no inefficiency in the system anywhere.</p>\n<p style=\"font-weight: normal; \">It's especially important to notice that the explanation for externalities, i.e. all types of \"inefficiency,\" came well after the concept of inefficiency had become a firm part of economics. Fake explanations are what people naturally turn to in the absence of a ready real explanation. Economists did not know about transaction costs when they began to study externalities. They knew&nbsp;<span style=\"font-style: italic;\">absent some constraint</span>&nbsp;that externalities should not exist, and they had absolutely no idea what that constraint was. Their mistakes were failing to realize that economics always demands that everything be explained by some constraint, and failing to realize that \"inefficiency\" is a fake explanation.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\">The New Problem</strong></p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\"></strong>So inefficiency is a fake explanation. Whatever is, is efficient. Great. Here's the problem.&nbsp;Coase proved that externalities, and by extension, everything, are efficient back in 1960. He won a Nobel Prize for it. It's the most cited economics paper of all time. The point is, economists already know that the transaction costs analysis proves the efficiency of externalities.&nbsp;So why do economists still call things inefficient? Check any textbook, read any blog, ask any economist, and they will all (with perhaps the exception of those trained by Armen Alchian) tell you that externalities, imperfect competition, public goods, and so on are all inefficient. Even though they know that all of these things are the efficient economic response to transaction costs, they still call them inefficient.</p>\n<p style=\"font-weight: normal; \">That's weird.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\">Inefficiency: More Than a Fake Explanation</strong></p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\"></strong>Economists still use the term \"inefficiency,\" but not as an explanation. In fact, the term has two other purposes. They are:</p>\n<p style=\"font-weight: normal; \">1.&nbsp;As a substitute for \"bad\" that sounds more scientific and objective.</p>\n<p style=\"font-weight: normal; \">2. As a label for models lacking certain qualities.</p>\n<p style=\"font-weight: normal; \"><strong>Inefficiency Doesn't Mean Bad (And Efficiency Doesn't Mean Good)</strong></p>\n<p style=\"font-weight: normal; \"><strong></strong>Economists use the terms this way, but they shouldn't. And it produces a great deal of confusion when economists don't realize that inefficiency does not imply bad and efficiency does not imply good (since everything is efficient, then so was the Holocaust).&nbsp;</p>\n<p style=\"font-weight: normal; \">Carl Dahlman, in \"The Problem of Externality, correctly goes through the application of transaction costs to externalities to conclude that \"it is not possible to specify any class of transaction costs that...generate externalities that constitute deviations form an&nbsp;attainable&nbsp;optimum....\" But he retreats from the conclusion that everything is efficient, saying \"If we include transaction costs in the constraints, this appears to be the unavoidable conundrum we end up in: externalities are irrelevant, monopoly problems do not exist, public goods present difficulties, and so on.\" Alas, \"It is difficult to see, then, how it is possible to prove analytically that the presence of externalities imply welfare problems.\"</p>\n<p style=\"font-weight: normal; \">Dahlman's argument is, I believe, the primary reason economists dislike the conclusion that everything is efficient. The argument to them implies that all is well with the world by definition. Tyler Cowen, in his paper \"The Importance of Defining the Feasible Set\" calls the conclusion that everything is efficient \"extreme\" and says that \"Virtually everyone rejects this view and many people scorn it.\" Dahlman calls the conclusion \"Unpalatable to many economists.\"</p>\n<p style=\"font-weight: normal; \">But of course it is very easy to show that externalities imply welfare problems without using welfare economics. If Abe's pollution on the margin benefits him $5 and costs Barry $10, that's a net loss. That's bad. It's certainly not inefficient--there's no outstanding Pareto improvement unexplainable by the laws of economics. But it is bad. Even an economist should be able to see that.</p>\n<p style=\"font-weight: normal; \">So what's interesting here is that economists don't seem entirely cognizant of the fact that they're using \"inefficient\" to mean \"bad.\"</p>\n<p style=\"font-weight: normal; \"><strong>Inefficiency as a Label</strong></p>\n<p style=\"font-weight: normal; \"><strong></strong>Let's go back to the first fundamental theorem of welfare economics:</p>\n<p style=\"font-weight: normal; \">\"It is the central theorem of modern welfare economics that under certain strong assumptions about technology, tastes, and producers' motivations, the equilibrium conditions which characterize a system of competitive markets will exactly correspond to the requirements of Paretian efficiency.\"</p>\n<p style=\"font-weight: normal; \">We know that this is wrong because any system which obeys the laws of economics will exactly correspond to the requirements of Paretian efficiency. We also know that economists have known that this is wrong since 1960. So why hasn't the first fundamental theorem of welfare economics been consigned to the trash heap?</p>\n<p style=\"font-weight: normal; \">The answer is that inefficiency has taken on a life of its own, a new life as a completely meaningless label.&nbsp;Let's go back to Bator's paper. Discussing the question of externalities, he says,</p>\n<p style=\"font-weight: normal; \">\"Yet is it possible that due to more or less arbitrary and accidental circumstances of institutions, laws, customs, or feasibility, competitive markets would not be Pareto-efficient....It is easy to show that if apple blossoms have a positive effect on honey production...a maximum-of-welfare solution, or any Pareto-efficient solution,\" will associate with apple blossoms a positive Langrangean shadow-price. If, then, apple producers are unable to protect their equity in apple-nectar and markets do not impute to apple blossoms their correct shadow value, profit-maximizing decisions will fail correctly to allocate resources at the margin.\"</p>\n<p style=\"font-weight: normal; \">Bator's right, of course, that resource allocation would be&nbsp;better&nbsp;in a world free of externalities, although I'm not sure why he thinks that that means resource allocation is incorrect in the actual world we live in. But more importantly, notice that Bator isnot using inefficiency as an explanation&nbsp;here. He&nbsp;has&nbsp;an explanation for the externality:\"institutions, laws, customs, or feasibility.\" He even goes on to say,</p>\n<p style=\"font-weight: normal; \">\"The important point is that the difficulties reside in institutional rrangements, the feasibility of keeping tab, etc....Apple nectar has a positive shadow price, which,&nbsp;<strong>if only payment were enforceable</strong>, cause nectar production in precisely the right amount and even distribution would be correctly rationed. The difficulty is due exclusively to the difficulty of keeping accounts....\" (emphasis added)</p>\n<p style=\"font-weight: normal; \">Bator's not confused about the externality. He's not looking for a fake explanation. He understand exactly how the laws of economics move through the economic agents over the constraints on the system to produce externalities. He doesn't think there's anything inefficient about externalities. When Bator calls externalities \"inefficient,\" by that he means&nbsp;<em>absolutely nothing at all</em>.&nbsp;He fully explains the phenomenon in question and then adds on an extra term that does nothing at all. It's like describing the laws of motion, and then saying that they tell the Motion Fairy how to move things, or like observing the order and lawfulness in the universe and attributing that to God.</p>\n<p style=\"font-weight: normal; \">Inefficiency in this sense is merely a label. Some economic systems are labeled efficient. These are the systems which meet \"certain&nbsp;strong assumptions about technology, tastes, and producers' motivations\" in \"the equilibrium conditions which characterize a system of competitive markets.\" Any system which does not meet that definition is labeled inefficient.</p>\n<p style=\"font-weight: normal; \"><strong>How a Fake Answer Took on a Life of Its Own</strong></p>\n<p style=\"font-weight: normal; \"><strong></strong>I can only speculate on how inefficiency transformed from a fake answer to a multi-function word that in addition to being a fake answer is also an objective-sounding substitute for \"bad\" and a label that means nothing. But it's easy to guess how inefficiency could come to mean \"bad\" since inefficiency&nbsp;is&nbsp;something bad. You would always want to move from an inefficient to an efficient system. And over time things slip bit by bit until inefficiency is just the word economists use to refer to bad things they want to change. As for how inefficiency became a label, that seems like it could be the consequence of pre-1960 economists noticing that all the efficient system had certain qualities and all the inefficient ones lacked those qualities. It's easy to imagine that over time economists began to think that it was that combination of qualities and only that combination of qualities which allow for the possibility of efficiency. Pre-1960, that's what they would have observed.</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal;\"><strong style=\"font-weight: bold;\">Additional reading</strong></p>\n<p style=\"font-weight: normal; \"><a href=\"http://www.sfu.ca/~allen/DemsetzSocialCost.pdf\">In this paper</a>&nbsp;Demsetz makes much the same analysis that I do in terms of demonstrating that what economists call inefficient isn't. He does, however, claim that issues of \"strategic behavior\" might deserve to be called inefficient. He offers no justification for this view, and issues of strategic behavior seem just as well-explained by constraints and the laws of economics to me as anything else.</p>\n<p style=\"font-weight: normal; \">Those of you with access to JSTOR should look at \"The Problem of Externality\" by Carl J. Dahlman in The Journal of Law and Economics, Vol 22, No. 1 of April 1979, pages 141-162. He also goes through the application of transaction costs to externalities conclude that externalities are efficient. He retreats from the conclusion, however, citing both the general distaste economists have for saying that everything is efficient, and he wonders how one would know that externalities and the like are bad if they are not inefficient.</p>\n<p style=\"font-weight: normal; \">Anything and everything written by Ronald Coase is relevant and important.</p>\n<p style=\"font-weight: normal;\">&nbsp;</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LYuzBTJyzWWm2Hmhy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -15, "extendedScore": null, "score": -9e-06, "legacy": true, "legacyId": "23444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong> </strong></p>\n<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; font-weight: normal; padding: 0.5em; margin: 8px;\">\n<p style=\"font-weight: normal;\"><em><span style=\"font-style: normal;\"> </span></em></p>\n<p style=\"font-weight: normal; \"><strong id=\"tl_dr_\">tl;dr:</strong></p>\n<p style=\"font-weight: normal; \">This argument is a walkthrough of the argument against inefficiency as an real explanation in the tradition of Coase, Demsetz, and Alchian. Briefly, inefficiency is the result of being unaware of some constraint on the system and then failing to realize that what appears to be inefficiency must always actually be your ignorance of some constraint on the system. The universe does not glitch. Near the end I also take up the question of how a fake explanation was able to persist for so long in economics, especially <em>after</em>&nbsp;Coase proved in 1960 that there is no such thing as inefficiency.</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">The Jargon</strong></span></p>\n<p style=\"font-weight: normal; \">You may have noticed that economists talk about efficiency a lot. Normally efficiency refers to an input-output ratio, but that's not usually how economists use the term. Economists normally use efficiency to denote to a system of exhausted opportunities. An efficient system can't be improved, given the constraints on the system. More often that not these opportunities refer to Pareto improvements. A system that has exhausted all possible Pareto improvements given the constraints on the system is called Pareto efficient.</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">Pareto Improvements and Pareto Efficiency</strong></span></p>\n<p style=\"font-weight: normal; \">A Pareto improvement is any change that makes at least one person better off without making anyone else worse off. Economists like Pareto improvements because they're unambiguously good or neutral for everyone by definition. Only a misanthrope can complain about a Pareto improvement. A system that has no remaining Pareto improvements is called Pareto efficient. A system which has some remaining is called Pareto inefficient.</p>\n<p style=\"font-weight: normal; \">Not all Pareto efficient systems are good. A world where Sauron controls the rest of the world as slaves from his towers in Mordor would be Pareto efficient, since there's no way to free all the slaves without hurting Sauron. It might be&nbsp;<span style=\"font-style: italic;\"><span style=\"font-style: italic;\">good</span></span>&nbsp;to save the world at Sauron's expense, but it wouldn't be a Pareto improvement. Still, all else held equal, it's always better to be in a Pareto efficient system than a Pareto inefficient system. A Pareto improvement that doesn't<span style=\"font-style: italic;\">&nbsp;</span>happen doesn't help anyone, after all. Pareto improvements can only make people better off. Moving from a Pareto inefficient system to a Pareto efficient system is by definition a free lunch.</p>\n<p style=\"font-weight: normal; \">Pareto improvements have come under fire in two main ways. The first is that by focusing only on improvements which don't make anyone worse off, many potentially beneficial changes to the world are ruled out, most notably redistribution of wealth and income. Redistribution may be an improvement, but because it hurts the people who lose some of their wealth and income, it isn't a&nbsp;<span style=\"font-style: italic;\">Pareto</span>&nbsp;improvement. Handling this problem is what Kaldor-Hicks efficiency is for, which we get to farther down.</p>\n<p style=\"font-weight: normal; \">The other major criticism of Pareto improvements is that it is virtually impossible to find any. Picking $20 up that was just lying on the street abandoned hurts the person who would have come after you. The world is incredibly complex, and the odds of some action making&nbsp;<span style=\"font-style: italic;\">no one</span>&nbsp;worse off is low, to say the least. And if you include the utility of misanthropes and evil people, Pareto improvements might become completely impossible. We'll handle this problem by ignoring it, since it's not a problem in simple models.</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">There Ain't No Such Thing as a Free Lunch</strong></span></p>\n<p style=\"font-weight: normal; \">So if moving from a Pareto inefficient system to a Pareto efficient system is a free lunch, that raises an obvious problem:</p>\n<p style=\"font-weight: normal; \">There ain't no such thing as a free lunch.</p>\n<p style=\"font-weight: normal; \">Which means that Pareto&nbsp;<span style=\"font-style: italic;\">in</span>efficient systems can't exist. The only type of system that can exist is a Pareto efficient one.</p>\n<p style=\"font-weight: normal; \">The economic logic is simple and inexorable. A Pareto inefficient system by definition contains outstanding Pareto improvements. A Pareto improvement is a benefit to at least one person at no cost to anyone else.</p>\n<p style=\"font-weight: normal; \">So in order for a Pareto inefficient system to exist, there has to be an economic agent--because otherwise we're not dealing with an economic system--who forgoes a benefit for some reason&nbsp;<span style=\"font-style: italic;\">other than a cost to himself or someone else.&nbsp;</span>And&nbsp;if he willingly forgoes the benefit because of the cost to someone else, that is in fact a cost to himself, the disutility he experiences from the suffering of someone else, or if he forgoes the benefit because the cost to someone else induces that someone else to stop him from pursuing the improvement, that would also only happen if that someone were able to impose a sufficient cost on him. So we can restate Pareto inefficiency as a system where an economic agent forgoes a benefit for some reason other than the cost of doing so.</p>\n<p style=\"font-weight: normal; \">There is a problem:</p>\n<p style=\"font-weight: normal; \">Economic agents aren't allowed to do that.</p>\n<p style=\"font-weight: normal; \">Economic agents have to take any benefit that is not outweighed by the cost of doing so, where the cost of doing so is the foregone benefit from not doing the next best thing. That's what economizing behavior is. Maximization of benefits/minimization of costs, since costs are foregone benefits they are identical criteria. An economic agent&nbsp;<span style=\"font-style: italic;\">isn't allowed&nbsp;</span>to forgo a benefit for anything other than a even greater benefit somewhere else, which is to say, a cost that is too high.</p>\n<p style=\"font-weight: normal; \">And just in case you think you've found a loophole, \"cost\" is a pretty expansive term. It refers to things like how much time and money you have to give up to get something, but if you don't want to do something because it would embarrass you, or it would make your mother sad, or because something about it just feels&nbsp;<span style=\"font-style: italic;\">off</span>...then that's the cost. In fact, the term is sufficiently expansive so as to include&nbsp;<span style=\"font-style: italic;\">any&nbsp;</span>reason for not doing something.</p>\n<p style=\"font-weight: normal; \">So an economic agent can't forgo a benefit without a reason, without an explanation. By definition. And having economic agents is the fundamental premise of economics. They're the ones who follow the laws of economics. Without economic agents you don't have economics.</p>\n<p style=\"font-weight: normal; \">So we can restate Pareto inefficiency as a system where an economic agent forgoes a benefit for no reason.&nbsp;Which they're not allowed to do.</p>\n<p style=\"font-weight: normal; \">And it makes absolutely no sense. Pareto inefficiency is like a starving man who has suddenly been put in front of a large banquet of all the food he's been dreaming about for months. And then he doesn't eat.&nbsp;<span style=\"font-style: italic;\">For no reason.&nbsp;</span>If he doesn't eat because he forgot how, or because he's dealt with the cognitive dissonance produced by starving to death by convincing himself that starving to death is natural and trying to fight it is a sign of hubris, or because he feels weird being in a thought experiment, or&nbsp;<span style=\"font-style: italic;\">anything at all</span>, then that's the cost and there's no outstanding Pareto improvement. And in fact, calling this confusing situation inefficient doesn't do anything to alleviate my confusion.</p>\n<p style=\"font-weight: normal; \">So we just have this starving man not eating the delicious food set out before him and&nbsp;<span style=\"font-style: italic;\">this cannot be explained in any way.</span>&nbsp;That's Pareto inefficiency. And that's&nbsp;<span style=\"font-style: italic;\"><em>nuts</em></span>.&nbsp;</p>\n<p style=\"font-weight: normal; \"><span style=\"font-weight: bold;\"><strong style=\"font-weight: bold;\">What About Kaldor-Hicks Efficiency? You Promised!</strong></span></p>\n<p style=\"font-weight: normal; \">Kaldor-Hicks efficiency is what economists turn to when Pareto efficiency proves too restrictive, which it always does. Kaldor-Hicks efficiency is similar to Pareto efficiency, but whereas Pareto efficiency counts the number of outstanding Pareto improvements with zero being the efficient level, Kaldor-Hicks efficiency counts the number of outstanding Kaldor-Hicks improvements, with zero being the efficient level. A Kaldor-Hicks improvement is any change that makes at least one person better off that they&nbsp;<span style=\"font-style: italic;\"><span style=\"font-style: italic;\">would, if it was possible</span></span>,&nbsp;be willing to compensate the person made worse off, if there are any. For example, if a change makes one person $10 better off, and another person $5 worse off, the person made better off would, if it only it were possible,&nbsp;be willing to pay the person made worse off the five dollars necessary to make the latter indifferent to the change. Essentially, it's a cost-benefit test. If the total benefits from the change exceeds the total costs, such that you could take some money from the winners to completely compensate the losers and still have some left over for the winners, that's a Kaldor-Hicks improvement. All Pareto improvements are Kaldor-Hicks improvements but not all Kaldor-Hicks improvements are Pareto improvements.</p>\n<p style=\"font-weight: normal; \">So what about Kaldor-Hicks inefficiency, when you have some outstanding Kaldor-Hicks improvements? Does it make any sense?</p>\n<p style=\"font-weight: normal; \">Suppose Abe steals a car from Barry. The car is worth $40,000 to Abe, who likes nice cars, and only worth $30,000 to Barry, who takes a more practical view of his vehicle. That's not a Pareto improvement since Barry is hurt, but it&nbsp;<span style=\"font-style: italic;\"><span style=\"font-style: italic;\">is</span></span>&nbsp;a Kaldor-Hicks improvement since Abe could compensate Barry the $30,000 and still have $10,000 left over. So, ignoring all external and additional effects of the theft beyond the benefit to Abe and the loss to Barry, in order to achieve Kaldor-Hicks efficiency, Abe would have to steal Barry's car, right?</p>\n<p style=\"font-weight: normal; \">So Abe, the economic actor that he is, sneaks over Barry's fence late at night, creeps around to where the garage is...</p>\n<p style=\"font-weight: normal; \">...And Barry, the economic actor that&nbsp;<span style=\"font-style: italic;\">he</span>&nbsp;is, nearly blows Abe's head off with a double-barrelled shotgun. Cursing and shrieking, Abe escapes with his tail between his legs as Barry calmly reloads.</p>\n<p style=\"font-weight: normal; \">(Yes, I'm from Texas, why do you ask?)</p>\n<p style=\"font-weight: normal; \">Is the system now inefficient? After all, the Kaldor-Hicks improvement didn't happen.</p>\n<p style=\"font-weight: normal; \">Remember, the question isn't whether it would be a net plus for Abe to have successfully stolen Barry's car. That's built into the assumptions of the thought experiment. It wouldn't be a Kaldor-Hicks improvement if it wasn't a net plus. But that's not quite the same thing as asking if it's inefficient.</p>\n<p style=\"font-weight: normal; \">Syllables aren't inherently significant; what matters is the associations and concepts they draw up inside your head. And what inefficiency calls up for me is something that isn't being optimized given the constraints on the system. Something about the economics is&nbsp;<span style=\"font-style: italic;\">wrong</span>, something is happening that shouldn't be&nbsp;<span style=\"font-style: italic;\">given the constraints</span>. I don't mean that somebody should be doing something different because it would be better that way, I mean that somebody should be doing something different because the laws of economics predict something different. With a Pareto improvement, for example, the idea of a Pareto improvement&nbsp;<span style=\"font-style: italic;\">not happening</span>&nbsp;in a world of economic actors, what we call Pareto inefficiency, means that somebody is economizing wrong, not in a backwards-looking way, but in a forwards-looking way, such that given the economic actor's information, utility function, degree of rationality and cognitive operations, etc., that economic actor should have done something differently even though he&nbsp;<span style=\"font-style: italic;\">couldn't</span>have done anything differently given the constraints and the laws of economics.</p>\n<p style=\"font-weight: normal; \">And if we look at this Kaldor-Hicks improvement that didn't happen, we can see that Barry didn't do anything he should have done. Sure, letting Abe steal his car would have been a net benefit for the world, but what does Barry car? He's out $30,000. The proper economic thing for Barry to do is to stop the theft (so long as the cost of doing so is less than $30,000). Abe did what was efficient for him, and Barry did what was efficient for him. And so if everyone does what's efficient, how can that add up to inefficiency?</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\" id=\"What_About_Social_Efficiency_\">What About Social Efficiency?</strong></p>\n<p style=\"font-weight: normal; \">\"Obviously individuals will always do what's efficient for them,\" you say. \"But that's how it works. Individually efficient actions lead to outcomes that are inefficient for the group. Ever heard of the Prisoner's Dilemma, you idiot?\"</p>\n<p style=\"font-weight: normal; \">Wow, that escalated suddenly. But moving from individual efficiency to group efficiency doesn't save the concept.</p>\n<p style=\"font-weight: normal; \">Let's go back to a very simple Pareto improvement. A man pours coffee for himself in the morning, making him better off at no cost to anyone else. Hooray, we have achieved Pareto efficiency. Except...</p>\n<p style=\"font-weight: normal; \">...Wouldn't it have been&nbsp;<span style=\"font-style: italic;\">even better</span>&nbsp;if while pouring himself a cup of coffee, he had turned into the happiest man alive, world peace and prosperity was achieved for all time, and the next arc of HPMOR didn't take ages to come out? Clearly we have a very Pareto inefficient system here, since all these Pareto improvements didn't happen.</p>\n<p style=\"font-weight: normal; \">The obvious problem with this logic is that all those \"Pareto improvements\" weren't possible. They could have happened if the universe were a very different place, but it's not, so they didn't. Any improvement that isn't allowed by the laws of the system and its constraints doesn't count. If they did count, then any system short of absolute utopia would be inefficient.</p>\n<p style=\"font-weight: normal; \">So let's look at the Prisoner's Dilemma. Individually both prisoners want to defect. But if they both follow the dominant strategy, they'll be worse off than if they both cooperate. This is usually presented as showing how individual doing what's efficient for them leads to group inefficiency.</p>\n<p style=\"font-weight: normal; \">But it's the same problem. The limited information people have and the decisions their brain makes given that information and the incentives they face are constraints that are just as real as the constraints which prevent our good friend above from becoming the happiest man in the world while he pours his coffee. If we're going to acknowledge that the improvements thatdon't&nbsp;physical constraints like how much steel there is or how much energy it takes for something to happen doesn't mean that the given system is inefficient, then social constraints like information, incentives, and cognitive abilities shouldn't either. The poor prisoners might&nbsp;<span style=\"font-style: italic;\">wish</span>&nbsp;they lived in a different world, one where a different set of information, incentives, and cognitive abilities allowed them to both cooperate, but they don't. Similarly, we might all wish we lived in a world of infinite free energy, but we don't. If the latter fact doesn't render our reality inefficient, then why should the former?</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\" id=\"Whatever_Is__Is_Efficient\">Whatever Is, Is Efficient</strong></p>\n<p style=\"font-weight: normal; \">Armen Alchian, the most powerful economist ever, would always tell his students,&nbsp;<a href=\"http://gallery.economicus.ru/cgi-bin/frame_rightn.pl?type=in&amp;links=./in/alchian/biogr/alchian_b2.txt&amp;img=brief.gif&amp;name=alchian\">\"Whatever is, is efficient.\"</a></p>\n<p style=\"font-weight: normal; \">That should be pretty obvious by now if you've followed everything I've said so far. Inefficiency seems to posit some kind of glitch in the universe, that given to the laws and constraints on the system, an economic actor isn't economizing even though economic actors economize by definition. That or inefficiency seems to be a case of the Mind Projection Fallacy: by comparing the real world to some imagined better world, no matter how impossible it may be given the current constraints, the real world suddenly seems to fall short of a mark that the universe isn't actually paying any attention to. Well, the universe doesn't glitch and the mere ability to imagine some better alternative to the real world does not in fact imply an non-exhausted opportunity in the real system.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\" id=\"Do_Economists_Actually_Believe_in_Inefficiency_\">Do Economists Actually Believe in Inefficiency?</strong></p>\n<p style=\"font-weight: normal; \"><a href=\"http://opim.wharton.upenn.edu/~sok/papers/b/Bator-market-failure.pdf\">This is the first fundamental theorem of welfare economics, as stated by F.M. Bator:</a></p>\n<p style=\"font-weight: normal; \">\"It is the central theorem of modern welfare economics that under certain strong assumptions about technology, tastes, and producers' motivations, the equilibrium conditions which characterize a system of competitive markets will exactly correspond to the requirements of Paretian efficiency.\"</p>\n<p style=\"font-weight: normal; \">So yes, yes they do. They believe in inefficiency&nbsp;a lot. Look how strong the assumptions they think are necessary for efficiency to occur. We're a very long ways from \"Whatever is, is efficient,\" here.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\" id=\"Inefficiency__A_Fake_Explanation\">Inefficiency: A Fake Explanation</strong></p>\n<p style=\"font-weight: normal; \">How could you come to believe that the universe glitches occasionally?&nbsp;</p>\n<p style=\"font-weight: normal; \">You don't, obviously. But if the logical conclusion of what you do believe is that the universe glitches occasionally, that's a sign of a fake explanation. Suppose you posit God as an explanation for rainfall. Now, this could be a real, falsifiable hypothesis about a wizard who lives in the sky. It could also be a fake explanation. In the latter case, \"God\" is an empty word that just serves as an intellectual stop sign. It posits \"I don't know\" as an explanation, which is to say no explanation as an explanation. If you took the fake answer seriously, you would think that everyone who says that God causes rainfall thinks that rainfall makes no sense within the universe.</p>\n<p style=\"font-weight: normal; \">Inefficiency occupies the same role within economics. It doesn't explain anything. It's false, in its form as a falsifiable hypothesis, which claims that there exists foregone benefits that are not explained by any cost, and it's not even wrong in its form as a fake explanation, which claims that you can explain foregone benefits without explaining them.</p>\n<p style=\"font-weight: normal; \">If you aren't buying it, let's talk about transaction costs. Transaction costs, as the name implies, are the costs of transacting. More specifically, it's used by economists to refer to the cost of using the price system.&nbsp;<a href=\"http://weber.ucsd.edu/~jlbroz/Courses/POLI200C/syllabus/Coase_social%20cost.pdf\">In 1960, Ronald Coase applied the concept of transaction costs to externalities, with startling results</a>. Before 1960, everyone thought of externalities as inefficient, in both the sense of a system with externalities was supposed to have outstanding Pareto improvements, and in the sense that nobody knew what caused externalities and so \"explained\" them by calling them inefficient, with a few exceptions such as Frank Knight, who pointed out that economists didn't know how to explain the existence of externalities. Then Coase exploded both those ideas.</p>\n<p style=\"font-weight: normal; \">An externality is an external cost, where somebody does something that makes someone else worse off on the margin without suffering proportionally himself. The archetype is pollution. A polluter who pollutes in the process of producing some product makes other people worse off without having to pay himself for the harm, which means that on the margin he'll be hurting others more than his product benefits himself and others. That net harm was supposed to imply an outstanding Pareto improvement, since some people would benefit and others would not be harmed if the pollution was lessened and the polluter was then compensated for reducing his pollution. Why this didn't happen was unexplained, of course.</p>\n<p style=\"font-weight: normal; \">Coase pointed out that what explained why these exchanges weren't happening must be to do with some set of costs which prevent exchanges from happening. He called those costs \"transaction costs.\"</p>\n<p style=\"font-weight: normal; \">Essentially, the mystery is why people don't use the price system to internalize externalities. For example, suppose Abe builds a factory next to Barry's house. On the margin Abe benefits $5, and the pollution from his factory costs Barry $10.&nbsp;</p>\n<p style=\"font-weight: normal; \">The solution to this is obvious. Barry will pay Abe any amount of money from $5 to $10 to not pollute. Abe will agree to any amount of money over $5 to stop polluting and Barry will agree to pay any amount of money under $10 to prevent the pollution. They'll both benefit. So how can externalities possibly exist? Economic agents should automatically internalize them.</p>\n<p style=\"font-weight: normal; \">The only possible real answer, as Coase realized, is that the cost of Abe and Barry using the price system, the transaction cost, must exceed the range of payments they will both agree on. If it costs Abe and Barry $6 to agree to some price on the externality, then there will be no price they can both agree on, and the externality will persist.</p>\n<p style=\"font-weight: normal; \">Coase substituted a real explanation, transaction costs, for a fake explanation, inefficiency, and won a Nobel Prize for it. But in fact his argument applies to much more than pollution.&nbsp;All&nbsp;the various types of supposed economic inefficiency, such as monopoly and collective action problems, are really just externalities. In the case of monopoly, people would like to pay the monopoly to behave more competitively but can't due to the cost of using the price system, and in the case of collective action problems, people would like to pay others not to defect but can't again due to transaction costs.</p>\n<p style=\"font-weight: normal; \">All the standard forms of economic inefficiency are really just exchanges that don't take place due to transaction costs. But any cost, not just transaction costs, prevent exchanges. The cost of steel prevents some people from buying steel from people who want to sell them steel. I'd like to buy a lot of steel if it cost nothing, but that's not the case. Is that inefficiency? If not, then why are exchanges that don't happen because of the cost of transacting any different? This is really just the same point as above that any foregone benefit that is explained by a cost isn't inefficient.&nbsp;There's no inefficiency in the system anywhere.</p>\n<p style=\"font-weight: normal; \">It's especially important to notice that the explanation for externalities, i.e. all types of \"inefficiency,\" came well after the concept of inefficiency had become a firm part of economics. Fake explanations are what people naturally turn to in the absence of a ready real explanation. Economists did not know about transaction costs when they began to study externalities. They knew&nbsp;<span style=\"font-style: italic;\">absent some constraint</span>&nbsp;that externalities should not exist, and they had absolutely no idea what that constraint was. Their mistakes were failing to realize that economics always demands that everything be explained by some constraint, and failing to realize that \"inefficiency\" is a fake explanation.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\" id=\"The_New_Problem\">The New Problem</strong></p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\"></strong>So inefficiency is a fake explanation. Whatever is, is efficient. Great. Here's the problem.&nbsp;Coase proved that externalities, and by extension, everything, are efficient back in 1960. He won a Nobel Prize for it. It's the most cited economics paper of all time. The point is, economists already know that the transaction costs analysis proves the efficiency of externalities.&nbsp;So why do economists still call things inefficient? Check any textbook, read any blog, ask any economist, and they will all (with perhaps the exception of those trained by Armen Alchian) tell you that externalities, imperfect competition, public goods, and so on are all inefficient. Even though they know that all of these things are the efficient economic response to transaction costs, they still call them inefficient.</p>\n<p style=\"font-weight: normal; \">That's weird.</p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\" id=\"Inefficiency__More_Than_a_Fake_Explanation\">Inefficiency: More Than a Fake Explanation</strong></p>\n<p style=\"font-weight: normal; \"><strong style=\"font-weight: bold;\"></strong>Economists still use the term \"inefficiency,\" but not as an explanation. In fact, the term has two other purposes. They are:</p>\n<p style=\"font-weight: normal; \">1.&nbsp;As a substitute for \"bad\" that sounds more scientific and objective.</p>\n<p style=\"font-weight: normal; \">2. As a label for models lacking certain qualities.</p>\n<p style=\"font-weight: normal; \"><strong id=\"Inefficiency_Doesn_t_Mean_Bad__And_Efficiency_Doesn_t_Mean_Good_\">Inefficiency Doesn't Mean Bad (And Efficiency Doesn't Mean Good)</strong></p>\n<p style=\"font-weight: normal; \"><strong></strong>Economists use the terms this way, but they shouldn't. And it produces a great deal of confusion when economists don't realize that inefficiency does not imply bad and efficiency does not imply good (since everything is efficient, then so was the Holocaust).&nbsp;</p>\n<p style=\"font-weight: normal; \">Carl Dahlman, in \"The Problem of Externality, correctly goes through the application of transaction costs to externalities to conclude that \"it is not possible to specify any class of transaction costs that...generate externalities that constitute deviations form an&nbsp;attainable&nbsp;optimum....\" But he retreats from the conclusion that everything is efficient, saying \"If we include transaction costs in the constraints, this appears to be the unavoidable conundrum we end up in: externalities are irrelevant, monopoly problems do not exist, public goods present difficulties, and so on.\" Alas, \"It is difficult to see, then, how it is possible to prove analytically that the presence of externalities imply welfare problems.\"</p>\n<p style=\"font-weight: normal; \">Dahlman's argument is, I believe, the primary reason economists dislike the conclusion that everything is efficient. The argument to them implies that all is well with the world by definition. Tyler Cowen, in his paper \"The Importance of Defining the Feasible Set\" calls the conclusion that everything is efficient \"extreme\" and says that \"Virtually everyone rejects this view and many people scorn it.\" Dahlman calls the conclusion \"Unpalatable to many economists.\"</p>\n<p style=\"font-weight: normal; \">But of course it is very easy to show that externalities imply welfare problems without using welfare economics. If Abe's pollution on the margin benefits him $5 and costs Barry $10, that's a net loss. That's bad. It's certainly not inefficient--there's no outstanding Pareto improvement unexplainable by the laws of economics. But it is bad. Even an economist should be able to see that.</p>\n<p style=\"font-weight: normal; \">So what's interesting here is that economists don't seem entirely cognizant of the fact that they're using \"inefficient\" to mean \"bad.\"</p>\n<p style=\"font-weight: normal; \"><strong id=\"Inefficiency_as_a_Label\">Inefficiency as a Label</strong></p>\n<p style=\"font-weight: normal; \"><strong></strong>Let's go back to the first fundamental theorem of welfare economics:</p>\n<p style=\"font-weight: normal; \">\"It is the central theorem of modern welfare economics that under certain strong assumptions about technology, tastes, and producers' motivations, the equilibrium conditions which characterize a system of competitive markets will exactly correspond to the requirements of Paretian efficiency.\"</p>\n<p style=\"font-weight: normal; \">We know that this is wrong because any system which obeys the laws of economics will exactly correspond to the requirements of Paretian efficiency. We also know that economists have known that this is wrong since 1960. So why hasn't the first fundamental theorem of welfare economics been consigned to the trash heap?</p>\n<p style=\"font-weight: normal; \">The answer is that inefficiency has taken on a life of its own, a new life as a completely meaningless label.&nbsp;Let's go back to Bator's paper. Discussing the question of externalities, he says,</p>\n<p style=\"font-weight: normal; \">\"Yet is it possible that due to more or less arbitrary and accidental circumstances of institutions, laws, customs, or feasibility, competitive markets would not be Pareto-efficient....It is easy to show that if apple blossoms have a positive effect on honey production...a maximum-of-welfare solution, or any Pareto-efficient solution,\" will associate with apple blossoms a positive Langrangean shadow-price. If, then, apple producers are unable to protect their equity in apple-nectar and markets do not impute to apple blossoms their correct shadow value, profit-maximizing decisions will fail correctly to allocate resources at the margin.\"</p>\n<p style=\"font-weight: normal; \">Bator's right, of course, that resource allocation would be&nbsp;better&nbsp;in a world free of externalities, although I'm not sure why he thinks that that means resource allocation is incorrect in the actual world we live in. But more importantly, notice that Bator isnot using inefficiency as an explanation&nbsp;here. He&nbsp;has&nbsp;an explanation for the externality:\"institutions, laws, customs, or feasibility.\" He even goes on to say,</p>\n<p style=\"font-weight: normal; \">\"The important point is that the difficulties reside in institutional rrangements, the feasibility of keeping tab, etc....Apple nectar has a positive shadow price, which,&nbsp;<strong>if only payment were enforceable</strong>, cause nectar production in precisely the right amount and even distribution would be correctly rationed. The difficulty is due exclusively to the difficulty of keeping accounts....\" (emphasis added)</p>\n<p style=\"font-weight: normal; \">Bator's not confused about the externality. He's not looking for a fake explanation. He understand exactly how the laws of economics move through the economic agents over the constraints on the system to produce externalities. He doesn't think there's anything inefficient about externalities. When Bator calls externalities \"inefficient,\" by that he means&nbsp;<em>absolutely nothing at all</em>.&nbsp;He fully explains the phenomenon in question and then adds on an extra term that does nothing at all. It's like describing the laws of motion, and then saying that they tell the Motion Fairy how to move things, or like observing the order and lawfulness in the universe and attributing that to God.</p>\n<p style=\"font-weight: normal; \">Inefficiency in this sense is merely a label. Some economic systems are labeled efficient. These are the systems which meet \"certain&nbsp;strong assumptions about technology, tastes, and producers' motivations\" in \"the equilibrium conditions which characterize a system of competitive markets.\" Any system which does not meet that definition is labeled inefficient.</p>\n<p style=\"font-weight: normal; \"><strong id=\"How_a_Fake_Answer_Took_on_a_Life_of_Its_Own\">How a Fake Answer Took on a Life of Its Own</strong></p>\n<p style=\"font-weight: normal; \"><strong></strong>I can only speculate on how inefficiency transformed from a fake answer to a multi-function word that in addition to being a fake answer is also an objective-sounding substitute for \"bad\" and a label that means nothing. But it's easy to guess how inefficiency could come to mean \"bad\" since inefficiency&nbsp;is&nbsp;something bad. You would always want to move from an inefficient to an efficient system. And over time things slip bit by bit until inefficiency is just the word economists use to refer to bad things they want to change. As for how inefficiency became a label, that seems like it could be the consequence of pre-1960 economists noticing that all the efficient system had certain qualities and all the inefficient ones lacked those qualities. It's easy to imagine that over time economists began to think that it was that combination of qualities and only that combination of qualities which allow for the possibility of efficiency. Pre-1960, that's what they would have observed.</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal; \">&nbsp;</p>\n<p style=\"font-weight: normal;\"><strong style=\"font-weight: bold;\" id=\"Additional_reading\">Additional reading</strong></p>\n<p style=\"font-weight: normal; \"><a href=\"http://www.sfu.ca/~allen/DemsetzSocialCost.pdf\">In this paper</a>&nbsp;Demsetz makes much the same analysis that I do in terms of demonstrating that what economists call inefficient isn't. He does, however, claim that issues of \"strategic behavior\" might deserve to be called inefficient. He offers no justification for this view, and issues of strategic behavior seem just as well-explained by constraints and the laws of economics to me as anything else.</p>\n<p style=\"font-weight: normal; \">Those of you with access to JSTOR should look at \"The Problem of Externality\" by Carl J. Dahlman in The Journal of Law and Economics, Vol 22, No. 1 of April 1979, pages 141-162. He also goes through the application of transaction costs to externalities conclude that externalities are efficient. He retreats from the conclusion, however, citing both the general distaste economists have for saying that everything is efficient, and he wonders how one would know that externalities and the like are bad if they are not inefficient.</p>\n<p style=\"font-weight: normal; \">Anything and everything written by Ronald Coase is relevant and important.</p>\n<p style=\"font-weight: normal;\">&nbsp;</p>\n</div>", "sections": [{"title": "tl;dr:", "anchor": "tl_dr_", "level": 1}, {"title": "What About Social Efficiency?", "anchor": "What_About_Social_Efficiency_", "level": 1}, {"title": "Whatever Is, Is Efficient", "anchor": "Whatever_Is__Is_Efficient", "level": 1}, {"title": "Do Economists Actually Believe in Inefficiency?", "anchor": "Do_Economists_Actually_Believe_in_Inefficiency_", "level": 1}, {"title": "Inefficiency: A Fake Explanation", "anchor": "Inefficiency__A_Fake_Explanation", "level": 1}, {"title": "The New Problem", "anchor": "The_New_Problem", "level": 1}, {"title": "Inefficiency: More Than a Fake Explanation", "anchor": "Inefficiency__More_Than_a_Fake_Explanation", "level": 1}, {"title": "Inefficiency Doesn't Mean Bad (And Efficiency Doesn't Mean Good)", "anchor": "Inefficiency_Doesn_t_Mean_Bad__And_Efficiency_Doesn_t_Mean_Good_", "level": 1}, {"title": "Inefficiency as a Label", "anchor": "Inefficiency_as_a_Label", "level": 1}, {"title": "How a Fake Answer Took on a Life of Its Own", "anchor": "How_a_Fake_Answer_Took_on_a_Life_of_Its_Own", "level": 1}, {"title": "Additional reading", "anchor": "Additional_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "33 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T14:12:52.190Z", "modifiedAt": "2020-08-21T22:29:33.322Z", "url": null, "title": "Inferential credit history", "slug": "inferential-credit-history", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "RyanCarey", "user": {"username": "RyanCarey", "createdAt": "2011-04-27T00:19:14.586Z", "isAdmin": false, "displayName": "RyanCarey"}, "userId": "CBkbKSCEzEK2kLQww", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PL4hW4eNsfa4FZaKi/inferential-credit-history", "pageUrlRelative": "/posts/PL4hW4eNsfa4FZaKi/inferential-credit-history", "linkUrl": "https://www.lesswrong.com/posts/PL4hW4eNsfa4FZaKi/inferential-credit-history", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inferential%20credit%20history&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInferential%20credit%20history%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPL4hW4eNsfa4FZaKi%2Finferential-credit-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inferential%20credit%20history%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPL4hW4eNsfa4FZaKi%2Finferential-credit-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPL4hW4eNsfa4FZaKi%2Finferential-credit-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 853, "htmlBody": "<p>Here\u2019s an&nbsp;<a href=\"http://www.foxnews.com/on-air/oreilly/index.html#/v/1146350884001/alien-invasion-over-global-warming\">interview</a>&nbsp;with Seth Baum. Seth is an expert in risk analysis and a founder of the Global Catastrophic Risk Institute. As expected,&nbsp;<i>Bill O\u2019Reilly</i>&nbsp;caricatured Seth as extreme, and cut up his interview with dramatic and extreme events from alien films. As a professional provocateur, it is his job to lay the gauntlet down to his guests. Also as expected, Seth put on a calm and confident performance. Was the interview net-positive or negative? It\u2019s hard to say, even in retrospect. Getting any publicity for catastrophic risk reduction is good, and difficult. Still, I\u2019m not sure just<i>&nbsp;how bad&nbsp;</i>publicity has to be before it really<i>&nbsp;is&nbsp;</i>bad publicity\u2026</p><p>Explaining catastrophic risks to the audience of Fox News is perhaps equally difficult to explaining the risk of artificial intelligence to anyone. This is a task that frustrated Eliezer Yudkowsky so deeply that he was driven to write the epic&nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">LessWrong sequences</a>. In his view, the&nbsp;<i>inferential distance</i>&nbsp;was too large to be bridged by a single conversation. There were too many things that he knew that were prerequisites to understanding his current plan. So he wrote this&nbsp;<i>sequence</i>&nbsp;of online posts that set out everything he knew about cognitive science and probability theory, applied to help readers think more clearly and live out their scientific values. He had to write a thousand words per day for about two years before talking about AI explicitly. Perhaps surprisingly, and as an enormous credit to Eliezer\u2019s brain, these sequences formed the founding manifesto of the quickly growing rationality movement, many of whom now share his concerns about AI. Since he wrote these, his Machine Intelligence Research Institute (formely the singularity Institute) has grown precipitously and spun off the Center for Applied Rationality, a teaching facility and monument to the promotion of public rationality.</p><p>Why have Seth and Eliezer had such a hard time?&nbsp;<a href=\"/lw/kg/expecting_short_inferential_distances/\">Inferential distance&nbsp;</a>explains a lot, but I have a second explanation, Seth and Eliezer had to build an&nbsp;<i>inferential credit history</i>. By the time you get to the end of the sequences, you have seen Eliezer bridge many an inferential distance, and you trust him to span another! If each time I loan Eliezer some attention, and suspend my disbelief, he has paid me back (in the currency of interesting and useful insight), then I will listen to him saying things that I don\u2019t yet believe for a long time.</p><p>When I watch Seth on&nbsp;<i>The Factor</i>, his interview is coloured by his&nbsp;<i>Triple A credit rating</i>. We have talked before, and I have read his papers. For the rest of the audience, he had no time to build&nbsp;<i>intellectual rapport.&nbsp;</i>It\u2019s not just that the inferential distance was large, it\u2019s more that he didn\u2019t have a credit rating of sufficient quality to take out a loan of that magnitude!</p><p>I contend that if you want to explain something abstract and unfamiliar, first you have to give a bunch of small and challenging chunks of insight, some of which must be practically applicable, and ideally you will lead your audience on a trek across a series of inferential distances, each slightly bigger than the last. It\u2019ll sure be helpful fills in some of the steps toward understanding the bigger picture, but not necessary.</p><p>This proposal could explain why historical explanations are often effective. Explanations that go like:</p><p>Initially I wanted to help people. And then I read The Life You Can Save. And then I realised I had been neglecting to think about large numbers of people. And then I read about scope insensitivity, which made me think&nbsp;<i>this</i>, and then I read Bostrom\u2019s&nbsp;<i>Fable of the Dragon Tyrant</i>, which made me think&nbsp;<i>that,</i>&nbsp;<i>and so on\u2026</i></p><p>This kind of explanation is often disorganised, with frequent detours, and false turns \u2013 steps in your ideological history that turned out to be wrong or unhelpful. The good thing about historical explanations is that they are stories, and that they have a main character \u2013 you \u2013 and this all makes the story more compelling. I will argue that a further advantage is that they give you the opportunity to borrow lots of small amounts of your audience\u2019s attention, and accrete a good credit rating, that you will need to make your boldest claims.</p><p>Lastly, let me present an alternative philosophy to overcoming inferential distances. It will seem to contradict what I have said so far, although I find it also useful.</p><p>If you say that X idea is crazy, then this can often become a self-fulfilling prophesy.</p><p>On this view, those who publicise AI risk should never complain about, and rarely talk about the large inferential distance before them, least of all publicy. They should normalise their proposal by treating it as normal. I still think it\u2019s important for them to acknowledge any intuitive reluctance on the part of their audience to entertain an idea. It\u2019s like how if you don\u2019t appear embarrassed after committing a faux-pas, you\u2019re&nbsp;<a href=\"http://news.discovery.com/human/psychology/embarassed-trust-111004.htm\">seen as untrustworthy</a>. But&nbsp;after acknowledging this challenge, they had best get back to their subject material, as any normal person would!</p><p>So if you believe in&nbsp;<i>inferential distance</i>,&nbsp;<i>inferential credit history&nbsp;</i>(building trust), and&nbsp;<i>acting normal</i>, then explain hard things by first beginning with lots of easy things, build larger and larger bridges, and acknowledge, but beware overemphasising any difficulties.</p><p>[also posted on my <a href=\"http://www.careyryan.com/199/\">blog</a>]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 1, "fkABsGCJZ6y9qConW": 1, "WPkEd3et8f488w8LT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PL4hW4eNsfa4FZaKi", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 58, "extendedScore": null, "score": 0.000161, "legacy": true, "legacyId": "23480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A", "HLqWn5LASfhhArZ7w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T18:35:04.130Z", "modifiedAt": null, "url": null, "title": "An argument against indirect normativity", "slug": "an-argument-against-indirect-normativity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:07.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jGzWK43usCBzWnmqv/an-argument-against-indirect-normativity", "pageUrlRelative": "/posts/jGzWK43usCBzWnmqv/an-argument-against-indirect-normativity", "linkUrl": "https://www.lesswrong.com/posts/jGzWK43usCBzWnmqv/an-argument-against-indirect-normativity", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20argument%20against%20indirect%20normativity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20argument%20against%20indirect%20normativity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGzWK43usCBzWnmqv%2Fan-argument-against-indirect-normativity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20argument%20against%20indirect%20normativity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGzWK43usCBzWnmqv%2Fan-argument-against-indirect-normativity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGzWK43usCBzWnmqv%2Fan-argument-against-indirect-normativity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>I think I've found a new argument, which I'll call X, against Paul Christiano's \"indirect normativity\" approach to FAI goals. I just discussed X with Paul, who agreed that it's serious.</p>\n<p>This post won't describe X in detail because it's based on basilisks, which are a forbidden topic on LW, and I respect Eliezer's requests despite sometimes disagreeing with them. If you understand Paul's idea and understand basilisks, figuring out X should take you about five minutes (there's only one obvious way to combine the two ideas), so you might as well do it now. If you decide to discuss X here, please try to follow the spirit of LW policy.</p>\n<p>In conclusion, I'd like to ask Eliezer to rethink his position on secrecy. If more LWers understood basilisks, somebody might have come up with X earlier.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jGzWK43usCBzWnmqv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 5, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "23481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T22:02:55.576Z", "modifiedAt": null, "url": null, "title": "Making Rationality General-Interest", "slug": "making-rationality-general-interest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:32.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R38jwJcWKH6S7hpFJ/making-rationality-general-interest", "pageUrlRelative": "/posts/R38jwJcWKH6S7hpFJ/making-rationality-general-interest", "linkUrl": "https://www.lesswrong.com/posts/R38jwJcWKH6S7hpFJ/making-rationality-general-interest", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20Rationality%20General-Interest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20Rationality%20General-Interest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR38jwJcWKH6S7hpFJ%2Fmaking-rationality-general-interest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20Rationality%20General-Interest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR38jwJcWKH6S7hpFJ%2Fmaking-rationality-general-interest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR38jwJcWKH6S7hpFJ%2Fmaking-rationality-general-interest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1175, "htmlBody": "<p><strong>Introduction</strong></p>\n<p>Less Wrong currently represents a tiny, tiny, tiny segment of the population. In its current form, it might only <em>appeal </em>to a tiny, tiny segment of the population. Basically, the people who have a strong <a href=\"/\">need for </a><a href=\"http://en.wikipedia.org/wiki/Need_for_cognition\">cognition</a>, who are INTx on the Myers-Briggs (65% of us as per <a href=\"/lw/fp5/2012_survey_results/\">2012 survey data</a>), etc.</p>\n<p><a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the sanity waterline</a> seems like a generally good idea. Smart people who believe stupid things, and go on to&nbsp;invest resources in stupid ways because of it,&nbsp;are <em>frustrating. </em>Trying to learn rationality skills in my 20s, when a bunch of thought patterns are already overlearned, is even more frustrating.</p>\n<p>I have an intuition that&nbsp;a better future would be one&nbsp;where the concept of rationality (maybe called something different, but the same idea) is&nbsp;<em>normal</em>. Where it's as obvious as the idea that you shouldn't spend more money than you earn, or that you should live a healthy lifestyle, etc. The point isn't that <em>everyone</em> currently&nbsp;lives debt-free, eats decently well and exercises; that isn't the case; but they are normal things to do if you're a minimally proactive person who cares a bit about your future. No one has ever told me that doing taekwondo to stay fit is weird and culty, or that&nbsp;keeping a budget will make me unhappy because I'm overthinking thing.</p>\n<p>I think the questions of \"whether we should try to do this\" and \"if so, how do we do it in practice?\" are both valuable to discuss, and interesting.</p>\n<p>&nbsp;</p>\n<p><strong>Is making rationality general-interest a good goal?</strong></p>\n<p>My intuitions are far from 100% reliable. I can think of a few reasons why this might be a <em>bad </em>idea:</p>\n<p>1. A little bit of rationality can be <a href=\"/lw/h8m/being_halfrational_about_pascals_wager_is_even/\">damaging</a>; it might push people in the direction of <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">too much contrarianism</a>, or something else I haven't thought of. Since introspection is imperfect, knowing a bit about cognitive biases and the mistakes that <em>other people </em>make might make people actually less likely to change their mind&ndash;they see other people making those well-known mistakes, but not themselves. Likewise, rationality taught only as a tool or skill, without any kind of underlying philosophy of <a href=\"/lw/eqn/the_useful_idea_of_truth/\">why you should want to believe true things</a>, might cause problems of a similar nature to martial art skills taught without the traditional, often non-violent philosophies&ndash;it could result in people abusing the skill to win fights/debates, making the larger community worse off overall. (Credit to <a href=\"/user/krzhang/overview/\">Yan Zhang</a> for martial arts metaphor).&nbsp;</p>\n<p>2. Making the concepts general-interest, or just growing too fast, might involve <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">watering them down</a> or changing them in some way that the value of the LW microcommunity is lost. This could be worse for the people who currently enjoy LW even if it isn't worse overall. I don't know how easy it would be to avoid, or whether</p>\n<p>3. It turns out that rationalists don't <a href=\"/lw/7i/rationality_is_systematized_winning/\">actually</a> <a href=\"/lw/5f/bayesians_vs_barbarians/\">win</a>, and x-rationality, as Yvain terms it, <a href=\"/lw/9p/rationality_its_not_that_great/\">just isn't that amazing</a> over-and-above already being proactive and doing stuff like keeping a budget. Yeah, you can say stuff like \"the definition of rationality is that it helps you win\", but if in real life, all the people who deliberately try to increase their rationality do worse off overall, by their own standards (or even equally well, but with less time left over for other fun pursuits) than the people who aim for their life goals directly, I want to know that.&nbsp;</p>\n<p>4. Making rationality general-interest is a good idea, but not the best thing to be spending time and energy on right now because of Mysterious Reasons X, Y, Z. Maybe I only think it is because of my personal bias towards liking community stuff (and wishing all of my friends were also friends with each other and liked the same activities, which would simplify my social life, but probably shouldn't happen for good reasons).&nbsp;</p>\n<p>Obviously, if any of these are the case, I want to know about it. I also want to know about it if there are <em>other </em>reasons, off my radar, why this is a terrible idea.</p>\n<p>&nbsp;</p>\n<p><strong>What has to change for this to happen?<br /></strong></p>\n<p>I don't really know, or I would be doing those things already (maybe, akrasia allowing). I have some ideas, though.</p>\n<p>1. The <a href=\"/lw/g65/you_cant_signal_to_rubes/#more\">jargon</a> <a href=\"http://wiki.lesswrong.com/wiki/Jargon\">thing</a>. I'm currently trying to compile a list of LW/CFAR jargon as a project for CFAR, and there are lots of terms I don't know. There are terms that I've realized in retrospect that I was <a href=\"/lw/hvw/how_i_became_more_ambitious/9drr\">using incorrectly all along</a>. This presents both a large initial effort for someone interested in learning about rationality via the LW route, and also might contribute to the <a href=\"/lw/atm/cult_impressions_of_less_wrongsingularity/\">looking-like-a-cult </a>thing.</p>\n<p>2. The <a href=\"/lw/ap/of_gender_and_rationality/\">gender</a> <a href=\"/lw/134/sayeth_the_girl/\">ratio</a> thing. This has been discussed before, and it's a controversial thing to discuss, and I don't know how much arguing about it in comments will present any solutions. It seems pretty clear that if you want to appeal to the whole population, and a group that represents 50% of the general population only represents 10% of your participants (also as per 2012 survey data, see link above), there's going to be a problem somewhere down the road.</p>\n<p>My data point: as a female on LW, I haven't experienced any discrimination, and I'm a bit baffled as to why the gender ratio is so skewed in the first place. Then again, I've already been through the filter of not caring if I'm the only girl at a meetup group. And I do hang out in female-dominated groups (i.e. the entire field of nursing), and fit in okay, but I'm probably not all that good as a typical example to generalize from.&nbsp;</p>\n<p>3. LW currently appeals to intelligent people, or at least people who self-identify as intelligent; according to the 2012 survey data, the self-reported IQ median is 138. This wouldn't be surprising, and isn't a problem until you want to appeal to more than 1% of the population. But intelligence and rationality are, in theory, <a href=\"/lw/7hu/the_beginnings_of_a_test_for_rationality_quotient/\">orthogonal</a>, or at least not the same thing. If I suffered a brain injury that reduced my IQ significantly but didn't otherwise affects my likes and dislikes, I expect I would still be interested in improving my rationality and think it was important, perhaps even more so, but I also think I would find it frustrating. And I might feel horribly out of place.</p>\n<p>4. Rationality in general has a bad rap; specifically, the <a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Spock thing</a>. And this isn't just affecting whether or not people thing Less Wrong the site is weird; it's affecting whether they want to think about their own decision-making.</p>\n<p>This is only what I can think of in 5 minutes...</p>\n<p>&nbsp;</p>\n<p><strong>What's already happening? </strong></p>\n<p>Meetup groups are happening.<a href=\"http://rationality.org/\"> CFAR</a> is happening. And there are groups out there practicing skills similar or related to rationality, whether or not they call it the same thing.</p>\n<p>&nbsp;</p>\n<p><strong>Conclusion</strong></p>\n<p>Rationality, Less Wrong and CFAR have, gradually over the last 2-3 years, become a big part of my life. It's been fun, and I think it's made me stronger, and I would prefer a world where as many other people as possible have that. I'd like to know if people think that's a) a good idea, b) feasible, and c) how to do it practically.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R38jwJcWKH6S7hpFJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 45, "extendedScore": null, "score": 0.000131, "legacy": true, "legacyId": "23384", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>Less Wrong currently represents a tiny, tiny, tiny segment of the population. In its current form, it might only <em>appeal </em>to a tiny, tiny segment of the population. Basically, the people who have a strong <a href=\"/\">need for </a><a href=\"http://en.wikipedia.org/wiki/Need_for_cognition\">cognition</a>, who are INTx on the Myers-Briggs (65% of us as per <a href=\"/lw/fp5/2012_survey_results/\">2012 survey data</a>), etc.</p>\n<p><a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the sanity waterline</a> seems like a generally good idea. Smart people who believe stupid things, and go on to&nbsp;invest resources in stupid ways because of it,&nbsp;are <em>frustrating. </em>Trying to learn rationality skills in my 20s, when a bunch of thought patterns are already overlearned, is even more frustrating.</p>\n<p>I have an intuition that&nbsp;a better future would be one&nbsp;where the concept of rationality (maybe called something different, but the same idea) is&nbsp;<em>normal</em>. Where it's as obvious as the idea that you shouldn't spend more money than you earn, or that you should live a healthy lifestyle, etc. The point isn't that <em>everyone</em> currently&nbsp;lives debt-free, eats decently well and exercises; that isn't the case; but they are normal things to do if you're a minimally proactive person who cares a bit about your future. No one has ever told me that doing taekwondo to stay fit is weird and culty, or that&nbsp;keeping a budget will make me unhappy because I'm overthinking thing.</p>\n<p>I think the questions of \"whether we should try to do this\" and \"if so, how do we do it in practice?\" are both valuable to discuss, and interesting.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Is_making_rationality_general_interest_a_good_goal_\">Is making rationality general-interest a good goal?</strong></p>\n<p>My intuitions are far from 100% reliable. I can think of a few reasons why this might be a <em>bad </em>idea:</p>\n<p>1. A little bit of rationality can be <a href=\"/lw/h8m/being_halfrational_about_pascals_wager_is_even/\">damaging</a>; it might push people in the direction of <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">too much contrarianism</a>, or something else I haven't thought of. Since introspection is imperfect, knowing a bit about cognitive biases and the mistakes that <em>other people </em>make might make people actually less likely to change their mind\u2013they see other people making those well-known mistakes, but not themselves. Likewise, rationality taught only as a tool or skill, without any kind of underlying philosophy of <a href=\"/lw/eqn/the_useful_idea_of_truth/\">why you should want to believe true things</a>, might cause problems of a similar nature to martial art skills taught without the traditional, often non-violent philosophies\u2013it could result in people abusing the skill to win fights/debates, making the larger community worse off overall. (Credit to <a href=\"/user/krzhang/overview/\">Yan Zhang</a> for martial arts metaphor).&nbsp;</p>\n<p>2. Making the concepts general-interest, or just growing too fast, might involve <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">watering them down</a> or changing them in some way that the value of the LW microcommunity is lost. This could be worse for the people who currently enjoy LW even if it isn't worse overall. I don't know how easy it would be to avoid, or whether</p>\n<p>3. It turns out that rationalists don't <a href=\"/lw/7i/rationality_is_systematized_winning/\">actually</a> <a href=\"/lw/5f/bayesians_vs_barbarians/\">win</a>, and x-rationality, as Yvain terms it, <a href=\"/lw/9p/rationality_its_not_that_great/\">just isn't that amazing</a> over-and-above already being proactive and doing stuff like keeping a budget. Yeah, you can say stuff like \"the definition of rationality is that it helps you win\", but if in real life, all the people who deliberately try to increase their rationality do worse off overall, by their own standards (or even equally well, but with less time left over for other fun pursuits) than the people who aim for their life goals directly, I want to know that.&nbsp;</p>\n<p>4. Making rationality general-interest is a good idea, but not the best thing to be spending time and energy on right now because of Mysterious Reasons X, Y, Z. Maybe I only think it is because of my personal bias towards liking community stuff (and wishing all of my friends were also friends with each other and liked the same activities, which would simplify my social life, but probably shouldn't happen for good reasons).&nbsp;</p>\n<p>Obviously, if any of these are the case, I want to know about it. I also want to know about it if there are <em>other </em>reasons, off my radar, why this is a terrible idea.</p>\n<p>&nbsp;</p>\n<p><strong id=\"What_has_to_change_for_this_to_happen_\">What has to change for this to happen?<br></strong></p>\n<p>I don't really know, or I would be doing those things already (maybe, akrasia allowing). I have some ideas, though.</p>\n<p>1. The <a href=\"/lw/g65/you_cant_signal_to_rubes/#more\">jargon</a> <a href=\"http://wiki.lesswrong.com/wiki/Jargon\">thing</a>. I'm currently trying to compile a list of LW/CFAR jargon as a project for CFAR, and there are lots of terms I don't know. There are terms that I've realized in retrospect that I was <a href=\"/lw/hvw/how_i_became_more_ambitious/9drr\">using incorrectly all along</a>. This presents both a large initial effort for someone interested in learning about rationality via the LW route, and also might contribute to the <a href=\"/lw/atm/cult_impressions_of_less_wrongsingularity/\">looking-like-a-cult </a>thing.</p>\n<p>2. The <a href=\"/lw/ap/of_gender_and_rationality/\">gender</a> <a href=\"/lw/134/sayeth_the_girl/\">ratio</a> thing. This has been discussed before, and it's a controversial thing to discuss, and I don't know how much arguing about it in comments will present any solutions. It seems pretty clear that if you want to appeal to the whole population, and a group that represents 50% of the general population only represents 10% of your participants (also as per 2012 survey data, see link above), there's going to be a problem somewhere down the road.</p>\n<p>My data point: as a female on LW, I haven't experienced any discrimination, and I'm a bit baffled as to why the gender ratio is so skewed in the first place. Then again, I've already been through the filter of not caring if I'm the only girl at a meetup group. And I do hang out in female-dominated groups (i.e. the entire field of nursing), and fit in okay, but I'm probably not all that good as a typical example to generalize from.&nbsp;</p>\n<p>3. LW currently appeals to intelligent people, or at least people who self-identify as intelligent; according to the 2012 survey data, the self-reported IQ median is 138. This wouldn't be surprising, and isn't a problem until you want to appeal to more than 1% of the population. But intelligence and rationality are, in theory, <a href=\"/lw/7hu/the_beginnings_of_a_test_for_rationality_quotient/\">orthogonal</a>, or at least not the same thing. If I suffered a brain injury that reduced my IQ significantly but didn't otherwise affects my likes and dislikes, I expect I would still be interested in improving my rationality and think it was important, perhaps even more so, but I also think I would find it frustrating. And I might feel horribly out of place.</p>\n<p>4. Rationality in general has a bad rap; specifically, the <a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Spock thing</a>. And this isn't just affecting whether or not people thing Less Wrong the site is weird; it's affecting whether they want to think about their own decision-making.</p>\n<p>This is only what I can think of in 5 minutes...</p>\n<p>&nbsp;</p>\n<p><strong id=\"What_s_already_happening__\">What's already happening? </strong></p>\n<p>Meetup groups are happening.<a href=\"http://rationality.org/\"> CFAR</a> is happening. And there are groups out there practicing skills similar or related to rationality, whether or not they call it the same thing.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>Rationality, Less Wrong and CFAR have, gradually over the last 2-3 years, become a big part of my life. It's been fun, and I think it's made me stronger, and I would prefer a world where as many other people as possible have that. I'd like to know if people think that's a) a good idea, b) feasible, and c) how to do it practically.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Is making rationality general-interest a good goal?", "anchor": "Is_making_rationality_general_interest_a_good_goal_", "level": 1}, {"title": "What has to change for this to happen?", "anchor": "What_has_to_change_for_this_to_happen_", "level": 1}, {"title": "What's already happening? ", "anchor": "What_s_already_happening__", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "114 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x9FNKTEt68Rz6wQ6P", "XqmjdBKa4ZaXJtNmf", "ebiCeBHr7At8Yyq9R", "7FzD7pNm9X68Gp5ZC", "XqvnWFtRD2keJdwjX", "MLaSGq6A6bLTdt6r8", "4ARtkT3EYox3THYjF", "KsHmn6iJAEr9bACQW", "LgavAYtzFQZKg95WC", "CfTH84gGFCRqo8D7t", "xsyG7PkMekHud2DMK", "gsL6CLqjujPNSLL2o", "cgQcj48SRkkeoWNRy", "zuJmtSqt3TsnBTYyu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-24T23:39:08.266Z", "modifiedAt": null, "url": null, "title": "Meetup : London Meetup - Achieving Better Goals", "slug": "meetup-london-meetup-achieving-better-goals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:30.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kerspoon", "createdAt": "2011-12-27T09:53:49.512Z", "isAdmin": false, "displayName": "kerspoon"}, "userId": "XvZ9yyyJNeDwWhECW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jn6rkrPRFPczvD3Pe/meetup-london-meetup-achieving-better-goals", "pageUrlRelative": "/posts/jn6rkrPRFPczvD3Pe/meetup-london-meetup-achieving-better-goals", "linkUrl": "https://www.lesswrong.com/posts/jn6rkrPRFPczvD3Pe/meetup-london-meetup-achieving-better-goals", "postedAtFormatted": "Wednesday, July 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Meetup%20-%20Achieving%20Better%20Goals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Meetup%20-%20Achieving%20Better%20Goals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjn6rkrPRFPczvD3Pe%2Fmeetup-london-meetup-achieving-better-goals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Meetup%20-%20Achieving%20Better%20Goals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjn6rkrPRFPczvD3Pe%2Fmeetup-london-meetup-achieving-better-goals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjn6rkrPRFPczvD3Pe%2Fmeetup-london-meetup-achieving-better-goals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p1'>London Meetup - Achieving Better Goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 August 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, London, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London are having another awesome meetup and we would love for you to come!</p>\n\n<p>The plan is to meet at The Shakespeare Inn, 200m from Holborn Underground at 2pm on Sunday 4th August. We will officially finish at 4pm but honestly people tend to enjoy it so much they want to stay longer, and regularly do. We will have a sign with the LessWrong logo on it so you can find us easily.</p>\n\n<p>This meetup is all about creating goals and turning them into actionable objectives. If you went to Rikk's Meetup then hopefully you already have a few goals in mind but don't worry if you didn't.</p>\n\n<p>If you have any questions, or are thinking of coming, feel free to email me (James) at kerspoon+lw@gmail.com. Otherwise, just turn up!</p>\n\n<p>Hope to see you there,\nJames</p>\n\n<p>P.S err on the side of turning-up, we're friendly, and it's fun :)</p>\n\n<p>\"Through rationality we shall become awesome, and invent and test systematic methods for making\npeople awesome, and plot to optimize everything in sight, and have fun.\"</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p1'>London Meetup - Achieving Better Goals</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jn6rkrPRFPczvD3Pe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.2778753327591706e-06, "legacy": true, "legacyId": "23483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Meetup___Achieving_Better_Goals\">Discussion article for the meetup : <a href=\"/meetups/p1\">London Meetup - Achieving Better Goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 August 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, London, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London are having another awesome meetup and we would love for you to come!</p>\n\n<p>The plan is to meet at The Shakespeare Inn, 200m from Holborn Underground at 2pm on Sunday 4th August. We will officially finish at 4pm but honestly people tend to enjoy it so much they want to stay longer, and regularly do. We will have a sign with the LessWrong logo on it so you can find us easily.</p>\n\n<p>This meetup is all about creating goals and turning them into actionable objectives. If you went to Rikk's Meetup then hopefully you already have a few goals in mind but don't worry if you didn't.</p>\n\n<p>If you have any questions, or are thinking of coming, feel free to email me (James) at kerspoon+lw@gmail.com. Otherwise, just turn up!</p>\n\n<p>Hope to see you there,\nJames</p>\n\n<p>P.S err on the side of turning-up, we're friendly, and it's fun :)</p>\n\n<p>\"Through rationality we shall become awesome, and invent and test systematic methods for making\npeople awesome, and plot to optimize everything in sight, and have fun.\"</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Meetup___Achieving_Better_Goals1\">Discussion article for the meetup : <a href=\"/meetups/p1\">London Meetup - Achieving Better Goals</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Meetup - Achieving Better Goals", "anchor": "Discussion_article_for_the_meetup___London_Meetup___Achieving_Better_Goals", "level": 1}, {"title": "Discussion article for the meetup : London Meetup - Achieving Better Goals", "anchor": "Discussion_article_for_the_meetup___London_Meetup___Achieving_Better_Goals1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T01:31:13.076Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia informal meetup", "slug": "meetup-philadelphia-informal-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:25.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mare-of-night", "createdAt": "2013-04-06T13:26:03.532Z", "isAdmin": false, "displayName": "mare-of-night"}, "userId": "6thzLTpEnEpcZF8bf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BdXoZphRXFa4Bkhz9/meetup-philadelphia-informal-meetup", "pageUrlRelative": "/posts/BdXoZphRXFa4Bkhz9/meetup-philadelphia-informal-meetup", "linkUrl": "https://www.lesswrong.com/posts/BdXoZphRXFa4Bkhz9/meetup-philadelphia-informal-meetup", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia%20informal%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia%20informal%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdXoZphRXFa4Bkhz9%2Fmeetup-philadelphia-informal-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia%20informal%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdXoZphRXFa4Bkhz9%2Fmeetup-philadelphia-informal-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdXoZphRXFa4Bkhz9%2Fmeetup-philadelphia-informal-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p2'>Philadelphia informal meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 July 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1414 S Penn Square, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There has been some interest in reviving the Philadelphia LessWrong meetup group. We're going to start with an informal meetup at the coffee shop La Colombe, tentatively from 3:00 to 4:30 (but will likely extend past the planned time).</p>\n\n<p>Our plan for the meeting:\n1) Get to know each other\n2) Talk about our goals for the group, and what we can contribute\n3) Brainstorm ideas for future activities and discussions</p>\n\n<p>The discussion topic is \"optimizing working in groups\". We might or might not get to it, depending on how long the rest takes and what people feel like talking about.</p>\n\n<p>We have been using a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-philadelphia\">mailing list</a> to communicate. Feel free to join. If you'd like to come to future meetups, please mark which times are convenient for you on the <a href=\"http://doodle.com/nbpzvyknn479sa4v\" rel=\"nofollow\">calendar</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p2'>Philadelphia informal meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BdXoZphRXFa4Bkhz9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2779661913816378e-06, "legacy": true, "legacyId": "23485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Philadelphia_informal_meetup\">Discussion article for the meetup : <a href=\"/meetups/p2\">Philadelphia informal meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 July 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1414 S Penn Square, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There has been some interest in reviving the Philadelphia LessWrong meetup group. We're going to start with an informal meetup at the coffee shop La Colombe, tentatively from 3:00 to 4:30 (but will likely extend past the planned time).</p>\n\n<p>Our plan for the meeting:\n1) Get to know each other\n2) Talk about our goals for the group, and what we can contribute\n3) Brainstorm ideas for future activities and discussions</p>\n\n<p>The discussion topic is \"optimizing working in groups\". We might or might not get to it, depending on how long the rest takes and what people feel like talking about.</p>\n\n<p>We have been using a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-philadelphia\">mailing list</a> to communicate. Feel free to join. If you'd like to come to future meetups, please mark which times are convenient for you on the <a href=\"http://doodle.com/nbpzvyknn479sa4v\" rel=\"nofollow\">calendar</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Philadelphia_informal_meetup1\">Discussion article for the meetup : <a href=\"/meetups/p2\">Philadelphia informal meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Philadelphia informal meetup", "anchor": "Discussion_article_for_the_meetup___Philadelphia_informal_meetup", "level": 1}, {"title": "Discussion article for the meetup : Philadelphia informal meetup", "anchor": "Discussion_article_for_the_meetup___Philadelphia_informal_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T02:58:35.717Z", "modifiedAt": null, "url": null, "title": "Useful Questions Repository", "slug": "useful-questions-repository", "viewCount": null, "lastCommentedAt": "2015-08-12T14:18:13.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ghcCY5E4rmwsmTxjv/useful-questions-repository", "pageUrlRelative": "/posts/ghcCY5E4rmwsmTxjv/useful-questions-repository", "linkUrl": "https://www.lesswrong.com/posts/ghcCY5E4rmwsmTxjv/useful-questions-repository", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Useful%20Questions%20Repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUseful%20Questions%20Repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FghcCY5E4rmwsmTxjv%2Fuseful-questions-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Useful%20Questions%20Repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FghcCY5E4rmwsmTxjv%2Fuseful-questions-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FghcCY5E4rmwsmTxjv%2Fuseful-questions-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p><strong>See also:</strong> <a href=\"/lw/gx5/boring_advice_repository/\">Boring Advice Repository</a>, <a href=\"/lw/h2m/solved_problems_repository/\">Solved Problems Repository</a>, <a href=\"/lw/h7d/grad_student_advice_repository/\">Grad Student Advice Repository</a>, <a href=\"/lw/hhl/useful_concepts_repository/\">Useful Concepts Repository</a>, <a href=\"/lw/htw/bad_concepts_repository/\">Bad Concepts Repository</a></p>\n<p>I just got back from the <a href=\"http://rationality.org/workshops/\">July CFAR workshop</a>, where I was a guest instructor. One useful piece of rationality I started paying more attention to as a result of the workshop is the idea of <strong>useful questions</strong>&nbsp;to ask in various situations, particularly because I had been introduced to a new one:</p>\n<p>\"What skill am I actually training?\"</p>\n<p>This is a question that can be asked whenever you're practicing something, but more generally it can also be asked whenever you're doing something you do frequently, and it can help you notice when you're practicing a skill you weren't intending to train. Some examples of when to use this question:</p>\n<ul>\n<li>You practice a piece of music so quickly that you consistently make mistakes. What skill are you actually training? How to play with mistakes.</li>\n<li>You teach students math by putting them in a classroom and having them take notes while a lecturer talks about math. What skill are you actually training? How to take notes.&nbsp;</li>\n<li>A personal example: at the workshop, I noticed that I was more apprehensive about the idea of singing in public than I had previously thought I was. After walking outside and actually singing in public for a little, I had a hypothesis about why: for the past several years, I've been singing in public when I don't think anyone is around but stopping when I saw people because I didn't want to bother them. What skill was I actually training by doing that? How to not sing around people.&nbsp;</li>\n</ul>\n<p>Many of the lessons of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a> can also be packaged as useful questions, like \"what do I believe and why do I believe it?\" and \"what would I expect to see if this were true?\"&nbsp;</p>\n<p>I'd like to invite people to post other examples of useful questions in the comments, hopefully together with an explanation of why they're useful and some examples of when to use them. As usual, one useful question per comment for voting purposes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "2wjPMY34by2gXEXA2": 1, "Eha62RrqBtEbpcEza": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ghcCY5E4rmwsmTxjv", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 40, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "23493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HEn2qiMxk5BggN83J", "iTzvJ7kKK2TYJhYHB", "9iofKNvYKZe3T7MpS", "umzNiYpHLypdcXuEf", "RcMjekC7yDTBzKCij"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-07-25T02:58:35.717Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T03:03:40.490Z", "modifiedAt": null, "url": null, "title": "Meetup : Chicago: Discuss Thinking, Fast and Slow", "slug": "meetup-chicago-discuss-thinking-fast-and-slow", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SLyGtSFyiF6GcMqzv/meetup-chicago-discuss-thinking-fast-and-slow", "pageUrlRelative": "/posts/SLyGtSFyiF6GcMqzv/meetup-chicago-discuss-thinking-fast-and-slow", "linkUrl": "https://www.lesswrong.com/posts/SLyGtSFyiF6GcMqzv/meetup-chicago-discuss-thinking-fast-and-slow", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Chicago%3A%20Discuss%20Thinking%2C%20Fast%20and%20Slow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Chicago%3A%20Discuss%20Thinking%2C%20Fast%20and%20Slow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLyGtSFyiF6GcMqzv%2Fmeetup-chicago-discuss-thinking-fast-and-slow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Chicago%3A%20Discuss%20Thinking%2C%20Fast%20and%20Slow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLyGtSFyiF6GcMqzv%2Fmeetup-chicago-discuss-thinking-fast-and-slow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLyGtSFyiF6GcMqzv%2Fmeetup-chicago-discuss-thinking-fast-and-slow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p3'>Chicago: Discuss Thinking, Fast and Slow</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 August 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Corner Bakery, 360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be discussing the beginning of Kahneman's <em>Thinking, Fast and Slow</em> as part of a series of meetups for this book.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p3'>Chicago: Discuss Thinking, Fast and Slow</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SLyGtSFyiF6GcMqzv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2780411507202847e-06, "legacy": true, "legacyId": "23495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Chicago__Discuss_Thinking__Fast_and_Slow\">Discussion article for the meetup : <a href=\"/meetups/p3\">Chicago: Discuss Thinking, Fast and Slow</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 August 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Corner Bakery, 360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be discussing the beginning of Kahneman's <em>Thinking, Fast and Slow</em> as part of a series of meetups for this book.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Chicago__Discuss_Thinking__Fast_and_Slow1\">Discussion article for the meetup : <a href=\"/meetups/p3\">Chicago: Discuss Thinking, Fast and Slow</a></h2>", "sections": [{"title": "Discussion article for the meetup : Chicago: Discuss Thinking, Fast and Slow", "anchor": "Discussion_article_for_the_meetup___Chicago__Discuss_Thinking__Fast_and_Slow", "level": 1}, {"title": "Discussion article for the meetup : Chicago: Discuss Thinking, Fast and Slow", "anchor": "Discussion_article_for_the_meetup___Chicago__Discuss_Thinking__Fast_and_Slow1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T04:36:01.374Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 25, chapter 96", "slug": "harry-potter-and-the-methods-of-rationality-discussion-16", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:36.201Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ey8yGkFnT7Gcgnt5r/harry-potter-and-the-methods-of-rationality-discussion-16", "pageUrlRelative": "/posts/Ey8yGkFnT7Gcgnt5r/harry-potter-and-the-methods-of-rationality-discussion-16", "linkUrl": "https://www.lesswrong.com/posts/Ey8yGkFnT7Gcgnt5r/harry-potter-and-the-methods-of-rationality-discussion-16", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2025%2C%20chapter%2096&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2025%2C%20chapter%2096%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEy8yGkFnT7Gcgnt5r%2Fharry-potter-and-the-methods-of-rationality-discussion-16%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2025%2C%20chapter%2096%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEy8yGkFnT7Gcgnt5r%2Fharry-potter-and-the-methods-of-rationality-discussion-16", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEy8yGkFnT7Gcgnt5r%2Fharry-potter-and-the-methods-of-rationality-discussion-16", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;and anything related to it. This thread is intended for discussing&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/chapter/96\">chapter 96</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">The previous thread&nbsp;</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">is at almost 300 comments.&nbsp;</span></p>\n<p>There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/fyv/harry_potter_and_the_methods_of_rationality/\">17</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/g1q/harry_potter_and_the_methods_of_rationality/\">18</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/huq/harry_potter_and_the_methods_of_rationality/\">19</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hvg/harry_potter_and_the_methods_of_rationality/\">20</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hwf/harry_potter_and_the_methods_of_rationality/\">21</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hws/harry_potter_and_the_methods_of_rationality/\">22</a>,&nbsp; <a href=\"/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">23</a>,&nbsp; <a href=\"/r/discussion/lw/i19/harry_potter_and_the_methods_of_rationality/\">24</a>,&nbsp; .</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ey8yGkFnT7Gcgnt5r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.2781160297093487e-06, "legacy": true, "legacyId": "23499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 525, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bMxxf7Wtic298LcNx", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3", "QkhX5YeuYHzPW7Waz", "4sY9rqAqty8rHWGSW", "35GjH7tDvNJWSHQ3H", "Pxiu5SG8gjhCh2jYd", "CEd85FLRbQWsbkrmf", "CcnpbKuRaYMjpFmQq", "smKK6yrKBehxvQq5i", "uBpSaxteqitApiJJs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T08:39:45.663Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Miscellaneous Rationality", "slug": "meetup-moscow-miscellaneous-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wWxNnpZcdWbXaksPA/meetup-moscow-miscellaneous-rationality", "pageUrlRelative": "/posts/wWxNnpZcdWbXaksPA/meetup-moscow-miscellaneous-rationality", "linkUrl": "https://www.lesswrong.com/posts/wWxNnpZcdWbXaksPA/meetup-moscow-miscellaneous-rationality", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Miscellaneous%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Miscellaneous%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwWxNnpZcdWbXaksPA%2Fmeetup-moscow-miscellaneous-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Miscellaneous%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwWxNnpZcdWbXaksPA%2Fmeetup-moscow-miscellaneous-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwWxNnpZcdWbXaksPA%2Fmeetup-moscow-miscellaneous-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p4'>Moscow: Miscellaneous Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 August 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall. And we will also check the entrance at 16:10, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality exercises.</p></li>\n<li><p>Short presentations about useful discussions and regression to the mean.</p></li>\n<li><p>Discussion about setting goals.</p></li>\n<li><p>Game session: the Liar's dice or the Resistance.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_reports+20130804_meet_up&amp;utm_content=20130804_meet_up&amp;utm_campaign=moscow_meetups\">here, in Russian</a>, now with more photos!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p4'>Moscow: Miscellaneous Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wWxNnpZcdWbXaksPA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2783136982937905e-06, "legacy": true, "legacyId": "23504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Miscellaneous_Rationality\">Discussion article for the meetup : <a href=\"/meetups/p4\">Moscow: Miscellaneous Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 August 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door with the sign \u201cYandex Money\u201d in Russian. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall. And we will also check the entrance at 16:10, so please do not be late.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality exercises.</p></li>\n<li><p>Short presentations about useful discussions and regression to the mean.</p></li>\n<li><p>Discussion about setting goals.</p></li>\n<li><p>Game session: the Liar's dice or the Resistance.</p></li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_reports+20130804_meet_up&amp;utm_content=20130804_meet_up&amp;utm_campaign=moscow_meetups\">here, in Russian</a>, now with more photos!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Miscellaneous_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/p4\">Moscow: Miscellaneous Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Miscellaneous Rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Miscellaneous_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Miscellaneous Rationality", "anchor": "Discussion_article_for_the_meetup___Moscow__Miscellaneous_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T11:41:57.559Z", "modifiedAt": null, "url": null, "title": "The difference between Determinism & Pre-determination", "slug": "the-difference-between-determinism-and-pre-determination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:35.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RogerS", "createdAt": "2013-02-27T17:28:11.625Z", "isAdmin": false, "displayName": "RogerS"}, "userId": "xCQ7jDkbR33hGqyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5v35BEZRwNNy6oq9m/the-difference-between-determinism-and-pre-determination", "pageUrlRelative": "/posts/5v35BEZRwNNy6oq9m/the-difference-between-determinism-and-pre-determination", "linkUrl": "https://www.lesswrong.com/posts/5v35BEZRwNNy6oq9m/the-difference-between-determinism-and-pre-determination", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20difference%20between%20Determinism%20%26%20Pre-determination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20difference%20between%20Determinism%20%26%20Pre-determination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5v35BEZRwNNy6oq9m%2Fthe-difference-between-determinism-and-pre-determination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20difference%20between%20Determinism%20%26%20Pre-determination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5v35BEZRwNNy6oq9m%2Fthe-difference-between-determinism-and-pre-determination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5v35BEZRwNNy6oq9m%2Fthe-difference-between-determinism-and-pre-determination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2595, "htmlBody": "<p><span style=\"FONT-FAMILY: 'Times New Roman'; FONT-SIZE: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-GB; mso-fareast-language: EN-GB; mso-bidi-language: AR-SA\"><span style=\"FONT-FAMILY: 'Times New Roman'; FONT-SIZE: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-GB; mso-fareast-language: EN-GB; mso-bidi-language: AR-SA\"> </span></span></p>\n<h2><strong>1. Scope</strong></h2>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\"><strong>&nbsp;</strong></p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">There are two arm-waving views often expressed about the relationship between &ldquo;determinism/causality&rdquo; on the one hand and &ldquo;predetermination/predictability in principle&rdquo; on the other. The first treats them as essentially interchangeable: what is causally determined from instant to instant is thereby predetermined over any period - the Laplacian view. The second view is that this is a confusion, and they are two quite distinct concepts. What I have never seen thoroughly explored (and therefore propose to make a start on here) is the range of different cases which give rise to different relationships between determinism and predetermination. I will attempt to illustrate that, indeed, determinism is neither a necessary nor a sufficient condition for predetermination in the most general case.</p>\n<p>To make the main argument clear, I will relegate various pedantic qualifications, clarifications&nbsp;and comments to <sup>[footnotes]</sup>.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">Most of the argument relates to cases of a physically classical, pre-quantum world (which is not as straightforward as often assumed, and certainly not without relevance to the world we experience). The difference that quantum uncertainty makes will be considered briefly at the end.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<h2><strong style=\"mso-bidi-font-weight: normal\">2. Instantaneous determinism</strong></h2>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">To start with it is useful to define what exactly we mean by an (instantaneously) determinist system. In simple terms this means that how the system&nbsp;<em style=\"mso-bidi-font-style: normal\">changes</em>&nbsp;at any instant is fully determined by the <em>state</em> of the system at that instant <sup>[1]</sup>. This is how physical laws work in a Newtonian universe. The arm-waving argument says that if this is the case, we can derive the state of the system at any future instant by advancing through an infinite number of infinitesimal steps. Since each step is fully determined, the outcome must be as well. However, as it stands this is a mathematical over-simplification. It is well known that an infinite number of infinitesimals is indeterminate as such, and so we have to look at this process more carefully - and this is where there turn out to be significant differences between different cases.</p>\n<p>&nbsp;</p>\n<h2><strong>3. Convergent and divergent behaviour</strong></h2>\n<p>To illustrate the first difference that needs to be recognized, consider two simple cases - a snooker ball just about to collide with another snooker ball, and a snooker ball heading towards a pocket. In the first case, a small change in the starting position of the ball (assuming the direction of travel is unchanged) results in a steadily increasing change in the positions at successive instants after impact - that is,&nbsp;<em style=\"mso-bidi-font-style: normal\">neighbouring trajectories diverge</em>. In the second case, a small change in the starting position has no effect on the final position hanging in the pocket:&nbsp;<em style=\"mso-bidi-font-style: normal\">neighbouring trajectories converge</em>. So we can call these &ldquo;convergent&rdquo; and &ldquo;divergent&rdquo; cases respectively. <sup>[1.1]</sup></p>\n<p>Now consider what happens if we try to predict the state of some system (e.g. the position of the ball) after a finite time interval. Any attempt to find the starting position will involve a small error. The effect on the accuracy of prediction differs markedly in the two cases. In the convergent case, small initial errors will fade away with time. In the divergent case, by contrast, the error will grow and grow. Of course, if better instruments were available we could reduce the initial error and improve the prediction - but that would also increase the accuracy with which we could check the final error! So the notable fact about this case is that&nbsp;<em style=\"mso-bidi-font-style: normal\">no matter how accurately we know the initial state, we can never predict the final state to the same level of accuracy -&nbsp;</em>despite the perfect instantaneous determinism<span style=\"mso-spacerun: yes\">&nbsp;</span>assumed, the&nbsp;<em style=\"mso-bidi-font-style: normal\">last significant figure that we can measure&nbsp;</em>remains as unpredictable as ever. <sup>[2]</sup></p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">One possible objection that might be raised to this conclusion is that with &ldquo;perfect knowledge&rdquo; of the initial state, we can predict any subsequent state perfectly. This is philosophically contentious - rather analagous to arguments about what happens when an irresistable force meets an immovable object. For example, philosophers who believe in &ldquo;operational definitions&rdquo; may doubt whether there is any operation that could be performed to obtain &ldquo;the exact initial conditions&rdquo;. I prefer to follow the mathematical convention that says that exact, perfect, or infinite entities are properly understood as the limiting cases of<span style=\"mso-spacerun: yes\">&nbsp;&nbsp;</span>more mundane entities. On this convention, if the last significant figure of the most accurate measure we can make of an outcome remains unpredictable for&nbsp;<em style=\"mso-bidi-font-style: normal\">any finite degree of accuracy</em>, then we must say that the same is true for &ldquo;infinite accuracy&rdquo;.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p>The conclusion that there is always something unknown about the predicted outcome places a &ldquo;qualitative upper limit&rdquo;, so to speak, on the strength of predictability in this case, but we must also recognize a &ldquo;qualitative lower limit&rdquo; that is just as important, since in the snooker impact example&nbsp;<em style=\"mso-bidi-font-style: normal\">whatever the accuracy of prediction that is desired after whatever time period, we can always calculate an accuracy of initial measurement that would enable it</em>. (However, as we shall shortly see <sup>[3]</sup>, this does not apply in every case.)<span style=\"mso-spacerun: yes\">&nbsp;&nbsp;</span>The combination of predictability in principle to any degree, with necessary unpredictability to the precision of the best available measurement, might be termed &ldquo;truncated predictability&rdquo;.</p>\n<p>&nbsp;</p>\n<h2><strong>4. More general cases</strong></h2>\n<p>The two elemementary cases considered so far illustrate the importance of distinguishing convergent from divergent behaviour, and so provide a useful paradigm to be kept in mind, but of course, most real cases are more complicated than this.</p>\n<p>To take some examples, a system can have both divergent parts and convergent parts at any instant - such as different balls on the same snooker table; an element whose trajectory is behaving divergently at one instant may behave convergently at another instant; convergent movement along one axis may be accompanied by divergent movement relative to another; and, significantly, divergent behaviour at one scale may be accompanied by convergent behaviour at a different scale. Zoom out from that snooker table, round positions to the nearest metre or so, and the trajectories of all the balls follow that of the adjacent surface of the earth.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">There is also the possibility that a system can be potentially divergent at all times and places. A famous case of such behaviour is the chaotic behaviour of the atmosphere, first clearly understood by Edward Lorentz in 1961.&nbsp;This story comes in two parts, the second apparently much less well known than the first.</p>\n<p>&nbsp;</p>\n<h2><strong style=\"mso-bidi-font-weight: normal\">5. Chaotic case: discrete</strong></h2>\n<p>The equations normally used to describe the physical behaviour of the atmosphere formally describe a continuum, an infinitely divisible fluid. As there is no algebraic &ldquo;solution&rdquo; to these equations, approximate solutions have to be found numerically, which in turn require the equations to be &ldquo;discretised&rdquo;, that is adapted to describe the behaviour at, or averaged around, a suitably large number of discrete points.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">The well-known part of Lorenz&rsquo;s work <sup>[4]</sup> arose from an accidental observation, that a very small change in the rounding of the values at the start of a numerical simulation led in due course to an entirely different &ldquo;forecast&rdquo;. Thus this is a case of divergent trajectories from any starting point, or &ldquo;sensitivity to initial conditions&rdquo; as it has come to be known.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">The part of &ldquo;chaos theory&rdquo; that grew out of this initial insight describes the convergent trajectories from any starting point: they diverge exponentially, with a time constant known as the Kolmogorov constant for the particular problem case <sup>[5]</sup>. Thus we can still say, as we said for the snooker ball, that <em style=\"mso-bidi-font-style: normal;\">whatever the accuracy of prediction that is desired after whatever time period, we can always calculate an accuracy of initial measurement that would enable it</em>.</p>\n<p>&nbsp;</p>\n<h2><strong style=\"mso-bidi-font-weight: normal;\">6. Chaotic case: continuum</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">Other researchers might have dismissed the initial discovery of sensitivity to initial conditions as an artefact of the computation, but Lorenz realised that even if the computation had been perfect, exactly the same consequences would flow from disturbances in the fluid in the gaps between the discrete points of the numerical model.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This is often called the &ldquo;Butterfly Effect&rdquo; because of a conference editor's colourful summary that &ldquo;the beating of a butterfly&rsquo;s wings in Brazil could cause a tornado in Texas&rdquo;.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">It is important to note that the Butterfly Effect is <em style=\"mso-bidi-font-style: normal;\">not</em>&nbsp;strictly the same as &ldquo;Sensitivity to Initial Conditions&rdquo; as is often reported <sup>[6]</sup>, although they are closely related. Sensitivity to Initial Conditions is an attribute of some discretised numerical models. The Butterfly Effect describes an attribute of the equations describing a continuous fluid, so is better described as &ldquo;sensitivity to disturbances of minimal extent&rdquo;, or in practice, sensitivity to what falls between the discrete points modelled.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">Since, as noted above, there is no algebraic solution to the continuous equations, the only way to establish the divergent characteristics of the equations themselves is to repeatedly reduce the scale of discretisation (the typical distance between the points on the grid of measurements) and observe the trend. In fact, this was done for a very practical reason: to find out how much benefit would be obtained, in terms of the durability of the forecast <sup>[7]</sup>, by providing more weather stations. The result was highly significant: each doubling of the number of stations increased the durability of the forecast by a smaller amount, so that (by extrapolation) as the number of imaginary weather stations was increased <em style=\"mso-bidi-font-style: normal;\">without limit</em>, the forecast durability of the model converged to a <em style=\"mso-bidi-font-style: normal;\">finite value</em><sup>[8]</sup>. Thus, beyond this time limit, the equations that we use to describe the atmosphere give <em style=\"mso-bidi-font-style: normal;\">indeterminate</em> results, however much detail we have about the initial conditions. <sup>[9]</sup></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">Readers will doubtless have noticed that this result does not strictly apply to the earth&rsquo;s atmosphere, because that is not the infinitely divisible fluid that the equations assumed (and a butterfly is likewise finitely divisible). Nevertheless, the fact that there are perfectly well-formed, familiar equations which by their nature have unpredictable outcomes after a finite time interval vividly exposes the difference between determinism and predetermination.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">With hindsight, the diminishing returns in forecast durability from refining the scale of discretisation is not too surprising: it is much quicker for a disturbance on a 1 km scale to have effects on a 2 km scale than for a disturbance on a 100 km scale to have effects on a 200 km scale.</p>\n<p>&nbsp;</p>\n<h2><strong style=\"mso-bidi-font-weight: normal;\">7. Consequences of quantum uncertainty</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">It is often claimed that the the Uncertainty Principle of quantum mechanics <sup>[10]</sup> makes the future unpredictable <sup>[11]</sup>, but in the terms of the above analysis this is far from the whole story.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">The effect of quantum mechanics is that at the scale of fundamental particles <sup>[12]</sup> the laws of physical causality are probabilistic. As a consequence, there is certainly no basis, for example, to predict whether an unstable nucleus will disintegrate before or after the expiry of its half-life.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">However, in the case of a <em style=\"mso-bidi-font-style: normal;\">convergent</em> process at ordinary scales, the unpredictability at quantum scale is immaterial, and at the scale of interest predictability continues to hold sway. The snooker ball finishes up at the bottom of the pocket whatever the energy levels of its constituent electrons. <sup>[13]</sup></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">It is in the case of <em style=\"mso-bidi-font-style: normal;\">divergent</em> processes that quantum effects can make for unpredictability at large scales. In the case of the atmosphere, for example, the source of that tornado in Texas could be a cosmic ray in Colombia, and cosmic radiation is strictly non-deterministic. The atmosphere may not be the infinitely divisible fluid considered by Lorenz, but a molecular fluid subject to random quantum processes has just the same lack of predictability.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">[EDIT] How does this look in terms of the LW-preferred Many Worlds interpretation of quantum mechanics?<sup>[14]</sup> In this framework, exact \"objective prediction\" is possible in principle but the prediction is of an ever-growing array of equally real states. We can speak of the \"probability\" of a particular outcome in the sense of the probability of that outcome being present in any state chosen at random from the set. In a convergent process the cases become so similar that there appears to be only one outcome at the macro scale (despite continued differences on the micro scale); whereas in a divergent process the \"density of probability\" (in the above sense) becomes so vanishingly small for some states that at a macro scale the outcomes appear to split into separate branches. (They have become decoherent.) Any one such branch appears to an observer&nbsp;<em>within that branch</em>&nbsp;to be the only outcome, and so such an observer could not have known what to \"expect\" - only the probability distribution of what to expect. This can be described as a condition of subjective unpredictability, in the sense that there is&nbsp;<em>no subjective expectation that can be formed</em>&nbsp;before the divergent process which can be reliably expected to coincident with an observation made after the process. [END of EDIT]</p>\n<p>&nbsp;</p>\n<h2><strong style=\"mso-bidi-font-weight: normal;\">8. Conclusions</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">What has emerged from this review of different cases, it seems to me, is that it is the convergent/divergent dichotomy that has the greatest effect on the predictability of a system&rsquo;s behaviour, not the deterministic/quantised dichotomy at subatomic scales.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">More particularly, in short-hand:-</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Convergent + deterministic =&gt; full predictability</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Convergent + quantised =&gt; predictability at all super-atomic scales</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Divergent + deterministic + discrete =&gt; &ldquo;truncated predictability&rdquo;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Divergent + deterministic + continuous =&gt; unpredictability</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">[EDIT] Divergent + quantised =&gt; objective predictability of the multiverse but subjective unpredictability</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<h2><strong style=\"mso-bidi-font-weight: normal;\">Footnotes</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">1. The &ldquo;state&rdquo; may already include time derivatives of course, and in the case of a continuum, the state includes spatial gradients of all relevant properties.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">1.1 For simplicity I have ignored the case between the two where neighbouring trajectories are parallel. It should be obvious how the argument applies to this case. Convergence/divergence is clearly related to (in)stability, and less directly to other properties such as (non)-linearity and (a)periodicity, but as convergence defines the characteristic that matters in the present context it seems better to focus on that.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">2. In referring to a &ldquo;significant figure&rdquo; I am of course assuming that decimal notation is used, and that the initial error has diverged by at least a factor of 10.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">3. In section 6.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">4. For example, see Gleick, &ldquo;Chaos&rdquo;, \"The Butterfly Effect\" chapter.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">5. My source for this statement is a contribution by Eric Kvaalen to the New Scientist comment pages.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">6. E.G by Gleick or Wikipedia.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">7. By durability I mean the period over which the required degree of accuracy is maintained.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">8. This account is based on my recollection, and notes made at the time, of an article in New Scientist, volume 42, p290. If anybody has access to this or knows of an equivalent source available on-line, I would be interested to hear!</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">9. I am referring to predictions of the conditions at particular locations and times. It is, of course, possible to predict average conditions over an area on a probabilistic basis, whether based on seasonal data, or the position of the jetstream etc. These are further examples of how divergence at one scale can be accompanied by something nearer to convergence on another scale.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">10. I am using &ldquo;quantum mechanics&rdquo; as a generic term to include its later derivatives such as quantum chromodynamics. As far as I understand it these later developments do not affect the points made here. However, this is certainly well outside my<span style=\"mso-spacerun: yes;\">&nbsp; </span>professional expertise in aspects of Newtonian mechanics, so I will gladly stand corrected by more specialist contributors!</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">11. E.G. by Karl Popper in an appendix to The Poverty of Historicism.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">12. To be pedantic, I&rsquo;m aware that this also applies to greater scales, but to a vanishingly small extent.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">13. In such cases we could perhaps say that predictability is effectively an &ldquo;<a class=\"description\" title=\"emergent property\" href=\"/r/discussion/lw/h7j/the_real_difference_between_reductionism_and/\" target=\"_blank\">emergent property</a>&rdquo; that is not present in the reductionist laws of the ultimate ingredients but only appears in the solution space of large scale aggregates.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">14. Thanks to the contributors of the comments below as at 30 July 2013 which I have tried to take into account. The online preview of \"The Emergent Multiverse: Quantum Theory  According to the Everett Interpretation\" by David  Wallace has also been helpful to understanding the implications of Many Worlds.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p><span style=\"FONT-FAMILY: 'Times New Roman'; FONT-SIZE: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-GB; mso-fareast-language: EN-GB; mso-bidi-language: AR-SA\"> </span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5v35BEZRwNNy6oq9m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 1.2784614941809694e-06, "legacy": true, "legacyId": "23427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"FONT-FAMILY: 'Times New Roman'; FONT-SIZE: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-GB; mso-fareast-language: EN-GB; mso-bidi-language: AR-SA\"><span style=\"FONT-FAMILY: 'Times New Roman'; FONT-SIZE: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-GB; mso-fareast-language: EN-GB; mso-bidi-language: AR-SA\"> </span></span></p>\n<h2 id=\"1__Scope\"><strong>1. Scope</strong></h2>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\"><strong>&nbsp;</strong></p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">There are two arm-waving views often expressed about the relationship between \u201cdeterminism/causality\u201d on the one hand and \u201cpredetermination/predictability in principle\u201d on the other. The first treats them as essentially interchangeable: what is causally determined from instant to instant is thereby predetermined over any period - the Laplacian view. The second view is that this is a confusion, and they are two quite distinct concepts. What I have never seen thoroughly explored (and therefore propose to make a start on here) is the range of different cases which give rise to different relationships between determinism and predetermination. I will attempt to illustrate that, indeed, determinism is neither a necessary nor a sufficient condition for predetermination in the most general case.</p>\n<p>To make the main argument clear, I will relegate various pedantic qualifications, clarifications&nbsp;and comments to <sup>[footnotes]</sup>.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">Most of the argument relates to cases of a physically classical, pre-quantum world (which is not as straightforward as often assumed, and certainly not without relevance to the world we experience). The difference that quantum uncertainty makes will be considered briefly at the end.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<h2 id=\"2__Instantaneous_determinism\"><strong style=\"mso-bidi-font-weight: normal\">2. Instantaneous determinism</strong></h2>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">To start with it is useful to define what exactly we mean by an (instantaneously) determinist system. In simple terms this means that how the system&nbsp;<em style=\"mso-bidi-font-style: normal\">changes</em>&nbsp;at any instant is fully determined by the <em>state</em> of the system at that instant <sup>[1]</sup>. This is how physical laws work in a Newtonian universe. The arm-waving argument says that if this is the case, we can derive the state of the system at any future instant by advancing through an infinite number of infinitesimal steps. Since each step is fully determined, the outcome must be as well. However, as it stands this is a mathematical over-simplification. It is well known that an infinite number of infinitesimals is indeterminate as such, and so we have to look at this process more carefully - and this is where there turn out to be significant differences between different cases.</p>\n<p>&nbsp;</p>\n<h2 id=\"3__Convergent_and_divergent_behaviour\"><strong>3. Convergent and divergent behaviour</strong></h2>\n<p>To illustrate the first difference that needs to be recognized, consider two simple cases - a snooker ball just about to collide with another snooker ball, and a snooker ball heading towards a pocket. In the first case, a small change in the starting position of the ball (assuming the direction of travel is unchanged) results in a steadily increasing change in the positions at successive instants after impact - that is,&nbsp;<em style=\"mso-bidi-font-style: normal\">neighbouring trajectories diverge</em>. In the second case, a small change in the starting position has no effect on the final position hanging in the pocket:&nbsp;<em style=\"mso-bidi-font-style: normal\">neighbouring trajectories converge</em>. So we can call these \u201cconvergent\u201d and \u201cdivergent\u201d cases respectively. <sup>[1.1]</sup></p>\n<p>Now consider what happens if we try to predict the state of some system (e.g. the position of the ball) after a finite time interval. Any attempt to find the starting position will involve a small error. The effect on the accuracy of prediction differs markedly in the two cases. In the convergent case, small initial errors will fade away with time. In the divergent case, by contrast, the error will grow and grow. Of course, if better instruments were available we could reduce the initial error and improve the prediction - but that would also increase the accuracy with which we could check the final error! So the notable fact about this case is that&nbsp;<em style=\"mso-bidi-font-style: normal\">no matter how accurately we know the initial state, we can never predict the final state to the same level of accuracy -&nbsp;</em>despite the perfect instantaneous determinism<span style=\"mso-spacerun: yes\">&nbsp;</span>assumed, the&nbsp;<em style=\"mso-bidi-font-style: normal\">last significant figure that we can measure&nbsp;</em>remains as unpredictable as ever. <sup>[2]</sup></p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">One possible objection that might be raised to this conclusion is that with \u201cperfect knowledge\u201d of the initial state, we can predict any subsequent state perfectly. This is philosophically contentious - rather analagous to arguments about what happens when an irresistable force meets an immovable object. For example, philosophers who believe in \u201coperational definitions\u201d may doubt whether there is any operation that could be performed to obtain \u201cthe exact initial conditions\u201d. I prefer to follow the mathematical convention that says that exact, perfect, or infinite entities are properly understood as the limiting cases of<span style=\"mso-spacerun: yes\">&nbsp;&nbsp;</span>more mundane entities. On this convention, if the last significant figure of the most accurate measure we can make of an outcome remains unpredictable for&nbsp;<em style=\"mso-bidi-font-style: normal\">any finite degree of accuracy</em>, then we must say that the same is true for \u201cinfinite accuracy\u201d.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p>The conclusion that there is always something unknown about the predicted outcome places a \u201cqualitative upper limit\u201d, so to speak, on the strength of predictability in this case, but we must also recognize a \u201cqualitative lower limit\u201d that is just as important, since in the snooker impact example&nbsp;<em style=\"mso-bidi-font-style: normal\">whatever the accuracy of prediction that is desired after whatever time period, we can always calculate an accuracy of initial measurement that would enable it</em>. (However, as we shall shortly see <sup>[3]</sup>, this does not apply in every case.)<span style=\"mso-spacerun: yes\">&nbsp;&nbsp;</span>The combination of predictability in principle to any degree, with necessary unpredictability to the precision of the best available measurement, might be termed \u201ctruncated predictability\u201d.</p>\n<p>&nbsp;</p>\n<h2 id=\"4__More_general_cases\"><strong>4. More general cases</strong></h2>\n<p>The two elemementary cases considered so far illustrate the importance of distinguishing convergent from divergent behaviour, and so provide a useful paradigm to be kept in mind, but of course, most real cases are more complicated than this.</p>\n<p>To take some examples, a system can have both divergent parts and convergent parts at any instant - such as different balls on the same snooker table; an element whose trajectory is behaving divergently at one instant may behave convergently at another instant; convergent movement along one axis may be accompanied by divergent movement relative to another; and, significantly, divergent behaviour at one scale may be accompanied by convergent behaviour at a different scale. Zoom out from that snooker table, round positions to the nearest metre or so, and the trajectories of all the balls follow that of the adjacent surface of the earth.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">There is also the possibility that a system can be potentially divergent at all times and places. A famous case of such behaviour is the chaotic behaviour of the atmosphere, first clearly understood by Edward Lorentz in 1961.&nbsp;This story comes in two parts, the second apparently much less well known than the first.</p>\n<p>&nbsp;</p>\n<h2 id=\"5__Chaotic_case__discrete\"><strong style=\"mso-bidi-font-weight: normal\">5. Chaotic case: discrete</strong></h2>\n<p>The equations normally used to describe the physical behaviour of the atmosphere formally describe a continuum, an infinitely divisible fluid. As there is no algebraic \u201csolution\u201d to these equations, approximate solutions have to be found numerically, which in turn require the equations to be \u201cdiscretised\u201d, that is adapted to describe the behaviour at, or averaged around, a suitably large number of discrete points.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">The well-known part of Lorenz\u2019s work <sup>[4]</sup> arose from an accidental observation, that a very small change in the rounding of the values at the start of a numerical simulation led in due course to an entirely different \u201cforecast\u201d. Thus this is a case of divergent trajectories from any starting point, or \u201csensitivity to initial conditions\u201d as it has come to be known.</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">The part of \u201cchaos theory\u201d that grew out of this initial insight describes the convergent trajectories from any starting point: they diverge exponentially, with a time constant known as the Kolmogorov constant for the particular problem case <sup>[5]</sup>. Thus we can still say, as we said for the snooker ball, that <em style=\"mso-bidi-font-style: normal;\">whatever the accuracy of prediction that is desired after whatever time period, we can always calculate an accuracy of initial measurement that would enable it</em>.</p>\n<p>&nbsp;</p>\n<h2 id=\"6__Chaotic_case__continuum\"><strong style=\"mso-bidi-font-weight: normal;\">6. Chaotic case: continuum</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">Other researchers might have dismissed the initial discovery of sensitivity to initial conditions as an artefact of the computation, but Lorenz realised that even if the computation had been perfect, exactly the same consequences would flow from disturbances in the fluid in the gaps between the discrete points of the numerical model.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This is often called the \u201cButterfly Effect\u201d because of a conference editor's colourful summary that \u201cthe beating of a butterfly\u2019s wings in Brazil could cause a tornado in Texas\u201d.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">It is important to note that the Butterfly Effect is <em style=\"mso-bidi-font-style: normal;\">not</em>&nbsp;strictly the same as \u201cSensitivity to Initial Conditions\u201d as is often reported <sup>[6]</sup>, although they are closely related. Sensitivity to Initial Conditions is an attribute of some discretised numerical models. The Butterfly Effect describes an attribute of the equations describing a continuous fluid, so is better described as \u201csensitivity to disturbances of minimal extent\u201d, or in practice, sensitivity to what falls between the discrete points modelled.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">Since, as noted above, there is no algebraic solution to the continuous equations, the only way to establish the divergent characteristics of the equations themselves is to repeatedly reduce the scale of discretisation (the typical distance between the points on the grid of measurements) and observe the trend. In fact, this was done for a very practical reason: to find out how much benefit would be obtained, in terms of the durability of the forecast <sup>[7]</sup>, by providing more weather stations. The result was highly significant: each doubling of the number of stations increased the durability of the forecast by a smaller amount, so that (by extrapolation) as the number of imaginary weather stations was increased <em style=\"mso-bidi-font-style: normal;\">without limit</em>, the forecast durability of the model converged to a <em style=\"mso-bidi-font-style: normal;\">finite value</em><sup>[8]</sup>. Thus, beyond this time limit, the equations that we use to describe the atmosphere give <em style=\"mso-bidi-font-style: normal;\">indeterminate</em> results, however much detail we have about the initial conditions. <sup>[9]</sup></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">Readers will doubtless have noticed that this result does not strictly apply to the earth\u2019s atmosphere, because that is not the infinitely divisible fluid that the equations assumed (and a butterfly is likewise finitely divisible). Nevertheless, the fact that there are perfectly well-formed, familiar equations which by their nature have unpredictable outcomes after a finite time interval vividly exposes the difference between determinism and predetermination.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">With hindsight, the diminishing returns in forecast durability from refining the scale of discretisation is not too surprising: it is much quicker for a disturbance on a 1 km scale to have effects on a 2 km scale than for a disturbance on a 100 km scale to have effects on a 200 km scale.</p>\n<p>&nbsp;</p>\n<h2 id=\"7__Consequences_of_quantum_uncertainty\"><strong style=\"mso-bidi-font-weight: normal;\">7. Consequences of quantum uncertainty</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">It is often claimed that the the Uncertainty Principle of quantum mechanics <sup>[10]</sup> makes the future unpredictable <sup>[11]</sup>, but in the terms of the above analysis this is far from the whole story.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">The effect of quantum mechanics is that at the scale of fundamental particles <sup>[12]</sup> the laws of physical causality are probabilistic. As a consequence, there is certainly no basis, for example, to predict whether an unstable nucleus will disintegrate before or after the expiry of its half-life.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">However, in the case of a <em style=\"mso-bidi-font-style: normal;\">convergent</em> process at ordinary scales, the unpredictability at quantum scale is immaterial, and at the scale of interest predictability continues to hold sway. The snooker ball finishes up at the bottom of the pocket whatever the energy levels of its constituent electrons. <sup>[13]</sup></p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">It is in the case of <em style=\"mso-bidi-font-style: normal;\">divergent</em> processes that quantum effects can make for unpredictability at large scales. In the case of the atmosphere, for example, the source of that tornado in Texas could be a cosmic ray in Colombia, and cosmic radiation is strictly non-deterministic. The atmosphere may not be the infinitely divisible fluid considered by Lorenz, but a molecular fluid subject to random quantum processes has just the same lack of predictability.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">[EDIT] How does this look in terms of the LW-preferred Many Worlds interpretation of quantum mechanics?<sup>[14]</sup> In this framework, exact \"objective prediction\" is possible in principle but the prediction is of an ever-growing array of equally real states. We can speak of the \"probability\" of a particular outcome in the sense of the probability of that outcome being present in any state chosen at random from the set. In a convergent process the cases become so similar that there appears to be only one outcome at the macro scale (despite continued differences on the micro scale); whereas in a divergent process the \"density of probability\" (in the above sense) becomes so vanishingly small for some states that at a macro scale the outcomes appear to split into separate branches. (They have become decoherent.) Any one such branch appears to an observer&nbsp;<em>within that branch</em>&nbsp;to be the only outcome, and so such an observer could not have known what to \"expect\" - only the probability distribution of what to expect. This can be described as a condition of subjective unpredictability, in the sense that there is&nbsp;<em>no subjective expectation that can be formed</em>&nbsp;before the divergent process which can be reliably expected to coincident with an observation made after the process. [END of EDIT]</p>\n<p>&nbsp;</p>\n<h2 id=\"8__Conclusions\"><strong style=\"mso-bidi-font-weight: normal;\">8. Conclusions</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">What has emerged from this review of different cases, it seems to me, is that it is the convergent/divergent dichotomy that has the greatest effect on the predictability of a system\u2019s behaviour, not the deterministic/quantised dichotomy at subatomic scales.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">More particularly, in short-hand:-</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Convergent + deterministic =&gt; full predictability</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Convergent + quantised =&gt; predictability at all super-atomic scales</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Divergent + deterministic + discrete =&gt; \u201ctruncated predictability\u201d</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">Divergent + deterministic + continuous =&gt; unpredictability</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt 36pt;\">[EDIT] Divergent + quantised =&gt; objective predictability of the multiverse but subjective unpredictability</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<h2 id=\"Footnotes\"><strong style=\"mso-bidi-font-weight: normal;\">Footnotes</strong></h2>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">1. The \u201cstate\u201d may already include time derivatives of course, and in the case of a continuum, the state includes spatial gradients of all relevant properties.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">1.1 For simplicity I have ignored the case between the two where neighbouring trajectories are parallel. It should be obvious how the argument applies to this case. Convergence/divergence is clearly related to (in)stability, and less directly to other properties such as (non)-linearity and (a)periodicity, but as convergence defines the characteristic that matters in the present context it seems better to focus on that.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">2. In referring to a \u201csignificant figure\u201d I am of course assuming that decimal notation is used, and that the initial error has diverged by at least a factor of 10.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">3. In section 6.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">4. For example, see Gleick, \u201cChaos\u201d, \"The Butterfly Effect\" chapter.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">5. My source for this statement is a contribution by Eric Kvaalen to the New Scientist comment pages.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">6. E.G by Gleick or Wikipedia.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">7. By durability I mean the period over which the required degree of accuracy is maintained.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">8. This account is based on my recollection, and notes made at the time, of an article in New Scientist, volume 42, p290. If anybody has access to this or knows of an equivalent source available on-line, I would be interested to hear!</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">9. I am referring to predictions of the conditions at particular locations and times. It is, of course, possible to predict average conditions over an area on a probabilistic basis, whether based on seasonal data, or the position of the jetstream etc. These are further examples of how divergence at one scale can be accompanied by something nearer to convergence on another scale.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">10. I am using \u201cquantum mechanics\u201d as a generic term to include its later derivatives such as quantum chromodynamics. As far as I understand it these later developments do not affect the points made here. However, this is certainly well outside my<span style=\"mso-spacerun: yes;\">&nbsp; </span>professional expertise in aspects of Newtonian mechanics, so I will gladly stand corrected by more specialist contributors!</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">11. E.G. by Karl Popper in an appendix to The Poverty of Historicism.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">12. To be pedantic, I\u2019m aware that this also applies to greater scales, but to a vanishingly small extent.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">13. In such cases we could perhaps say that predictability is effectively an \u201c<a class=\"description\" title=\"emergent property\" href=\"/r/discussion/lw/h7j/the_real_difference_between_reductionism_and/\" target=\"_blank\">emergent property</a>\u201d that is not present in the reductionist laws of the ultimate ingredients but only appears in the solution space of large scale aggregates.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">14. Thanks to the contributors of the comments below as at 30 July 2013 which I have tried to take into account. The online preview of \"The Emergent Multiverse: Quantum Theory  According to the Everett Interpretation\" by David  Wallace has also been helpful to understanding the implications of Many Worlds.</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0cm 0cm 0pt;\">&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"MARGIN: 0cm 0cm 0pt\">&nbsp;</p>\n<p><span style=\"FONT-FAMILY: 'Times New Roman'; FONT-SIZE: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-GB; mso-fareast-language: EN-GB; mso-bidi-language: AR-SA\"> </span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "1. Scope", "anchor": "1__Scope", "level": 1}, {"title": "2. Instantaneous determinism", "anchor": "2__Instantaneous_determinism", "level": 1}, {"title": "3. Convergent and divergent behaviour", "anchor": "3__Convergent_and_divergent_behaviour", "level": 1}, {"title": "4. More general cases", "anchor": "4__More_general_cases", "level": 1}, {"title": "5. Chaotic case: discrete", "anchor": "5__Chaotic_case__discrete", "level": 1}, {"title": "6. Chaotic case: continuum", "anchor": "6__Chaotic_case__continuum", "level": 1}, {"title": "7. Consequences of quantum uncertainty", "anchor": "7__Consequences_of_quantum_uncertainty", "level": 1}, {"title": "8. Conclusions", "anchor": "8__Conclusions", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wewdE655v7ibvw4zM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T18:46:13.486Z", "modifiedAt": null, "url": null, "title": "The Robots, AI, and Unemployment Anti-FAQ", "slug": "the-robots-ai-and-unemployment-anti-faq", "viewCount": null, "lastCommentedAt": "2015-11-02T12:21:52.918Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq", "pageUrlRelative": "/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq", "linkUrl": "https://www.lesswrong.com/posts/ZiRKzx3yv7NyA5rjF/the-robots-ai-and-unemployment-anti-faq", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Robots%2C%20AI%2C%20and%20Unemployment%20Anti-FAQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Robots%2C%20AI%2C%20and%20Unemployment%20Anti-FAQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiRKzx3yv7NyA5rjF%2Fthe-robots-ai-and-unemployment-anti-faq%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Robots%2C%20AI%2C%20and%20Unemployment%20Anti-FAQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiRKzx3yv7NyA5rjF%2Fthe-robots-ai-and-unemployment-anti-faq", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiRKzx3yv7NyA5rjF%2Fthe-robots-ai-and-unemployment-anti-faq", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6517, "htmlBody": "<p>Q. &nbsp;Are the current high levels of unemployment being caused by advances in Artificial Intelligence automating away human jobs?</p>\n<p>A. &nbsp;Conventional economic theory says this shouldn't happen. &nbsp;Suppose it costs 2 units of labor to produce a hot dog and 1 unit of labor to produce a bun, and that 30 units of labor are producing 10 hot dogs in 10 buns. &nbsp;If automation makes it possible to produce a hot dog using 1 unit of labor instead, conventional economics says that some people should shift from making hot dogs to buns, and the new equilibrium should be 15 hot dogs in 15 buns. &nbsp;On standard economic theory, improved productivity - including from automating away some jobs - should produce increased standards of living, not long-term unemployment.</p>\n<p>Q. &nbsp;Sounds like a lovely theory. &nbsp;As the proverb goes, the tragedy of science is a beautiful theory slain by an ugly fact. &nbsp;Experiment trumps theory and in reality, unemployment is rising.</p>\n<p>A. &nbsp;Sure. &nbsp;Except that the happy equilibrium with 15 hot dogs in buns, is <em>exactly </em>what happened over the last four centuries where we went from 95% of the population being farmers to 2% of the population being farmers (in agriculturally self-sufficient developed countries). &nbsp;We don't live in a world where 93% of the people are unemployed because 93% of the jobs went away. &nbsp;The first thought of automation removing a job, and thus the economy having one fewer job, has <em>not </em>been the way the world has worked since the Industrial Revolution. &nbsp;The parable of the hot dog in the bun is how economies really, actually worked in real life for centuries. &nbsp;Automation followed by re-employment went on for literally centuries in exactly the way that the standard&nbsp;lovely&nbsp;economic model said it should. &nbsp;The idea that there's a limited amount of work which is destroyed by automation is known in economics as the \"<a href=\"http://en.wikipedia.org/wiki/Lump_of_labour_fallacy\">lump of labour fallacy</a>\".</p>\n<p>Q. &nbsp;But now people <em>aren't </em>being reemployed. &nbsp;The jobs that went away in the Great Recession aren't coming back, even as the stock market and corporate profits rise again.</p>\n<p>A. &nbsp;Yes. &nbsp;And that's a <em>new </em>problem. &nbsp;We didn't get that when the Model T automobile mechanized the entire horse-and-buggy industry out of existence. &nbsp;The difficulty with supposing that automation is producing unemployment is that automation isn't new, so how can you use it to explain this new phenomenon of increasing long-term unemployment?</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Baxter-robot.jpg\" alt=\"Baxter robot\" align=\"center\" /><a id=\"more\"></a></p>\n<p>Q. &nbsp;Maybe we've finally reached the point where there's no work left to be done, or where all the jobs that people can easily be retrained into can be even more easily automated.</p>\n<p>A. &nbsp;You talked about jobs going away in the Great Recession and then not coming back. &nbsp;Well, the Great Recession wasn't produced by a sudden increase in productivity, it was produced by... I don't want to use fancy terms like \"aggregate demand shock\" so let's just call it problems in the financial system. &nbsp;The point is, in previous recessions the jobs came back strongly once NGDP rose again. &nbsp;(Nominal Gross Domestic Product - roughly the total amount of money being spent in face-value dollars.) &nbsp;<em>Now </em>there's been a recession and the jobs <em>aren't </em>coming back (in the US and EU), even though NGDP has risen back to its previous level (at least in the US). &nbsp;If the problem is automation, and we didn't experience any sudden leap in automation in 2008, then why can't people get back at least the jobs they used to have, as they did in previous recessions? &nbsp;Something has gone wrong with the engine of reemployment.</p>\n<p>Q. &nbsp;And you don't think that what's gone wrong with the engine of reemployment is that it's easier to automate the lost jobs than to hire someone new?</p>\n<p>A. &nbsp;No. &nbsp;That's something you could say just as easily about the 'lost' jobs from hand-weaving when mechanical looms came along. &nbsp;Some new obstacle is preventing jobs lost in the 2008 recession from coming back. &nbsp;Which may indeed mean that jobs eliminated by automation are <em>also </em>not coming back.&nbsp;&nbsp;And new high school and college graduates entering the labor market, likewise usually a good thing for an economy, will just end up being sad and unemployed.&nbsp;&nbsp; But this must mean something new and awful is happening to the processes of employment - it's not because the kind of automation that's happening today is different from automation in the 1990s, 1980s, 1920s, or 1870s; there were skilled jobs lost then, too. &nbsp;It should also be noted that automation has been a comparatively small force this decade next to shifts in global trade - which have <em>also </em>been going on for centuries and have <em>also </em>previously been a hugely positive economic force. &nbsp;But if something is generally wrong with reemployment, then it might be possible for increased trade with China to result in permanently lost jobs within the US, in <em>direct contrast</em> to the way it's worked over all previous economic history. &nbsp;But just like new college graduates ending up unemployed, something else must be going very wrong - that <em>wasn't </em>going wrong in 1960 - for anything so unusual to happen!</p>\n<p>Q. &nbsp;What if what's changed is that we're out of new jobs to create? &nbsp;What if we've already got enough hot dog buns, for every kind of hot dog bun there is in the labor market, and now AI is automating away the&nbsp;<em>last&nbsp;</em>jobs and the&nbsp;<em>last&nbsp;</em>of the demand for labor?</p>\n<p>A. &nbsp;This does not square with our being unable to recover the jobs that existed before the Great Recession. &nbsp;Or with lots of the world living in poverty. &nbsp;If we imagine the situation being much more extreme than it actually is, there was a time when professionals usually had personal cooks and maids - as Agatha Christie said, \"When I was young I never expected to be so poor that I could not afford a servant, or so rich that I could afford a motor car.\" \u2028\u2028 &nbsp;Many people would hire personal cooks or maids if we could afford them, which is the sort of new service that ought to come into existence if other jobs were eliminated - the reason maids became less common is that they were offered better jobs, not because demand for that form of human labor stopped existing. &nbsp;Or to be less extreme, there are lots of businesses who'd take nearly-free employees at various occupations, if those employees could be hired literally at minimum wage and legal liability wasn't an issue. &nbsp;<em>Right now</em>&nbsp;we haven't run out of&nbsp;<em>want</em>&nbsp;or&nbsp;<em>use</em>&nbsp;for human labor, so how could \"The End of Demand\" be producing unemployment&nbsp;<em>right now?</em>&nbsp;&nbsp;The fundamental fact that's driven employment over the course of previous human history is that it is a very strange state of affairs for somebody sitting around doing nothing, to have nothing better to do. &nbsp;We do not literally have nothing better for unemployed workers to do. &nbsp;Our civilization is not&nbsp;that&nbsp;advanced. &nbsp;So we must be doing something wrong (which we weren't doing wrong in 1950).</p>\n<p>Q. &nbsp;So what <em>is </em>wrong with \"reemployment\", then?</p>\n<p>A. &nbsp;I know less about macroeconomics than I know about AI, but even I can see <em>all sorts</em> of changed circumstances which are much more plausible sources of novel employment dysfunction than the relatively steady progress of automation. &nbsp;In terms of developed countries that seem to be doing okay on reemployment, Australia hasn't had any drops in employment and their monetary policy has kept <em>nominal</em>&nbsp;GDP growth on a much steadier keel - using their central bank to regularize the number of face-value Australian dollars being spent - which an increasing number of influential econbloggers think the US and even more so the EU have been getting catastrophically wrong. &nbsp;Though that's a <a href=\"http://en.wikipedia.org/wiki/Market_monetarism\">long story</a>.[1] &nbsp;Germany saw unemployment drop from 11% to 5% from 2006-2012 after implementing a series of labor market reforms, though there were other things going on during that time. &nbsp;(Germany has <a href=\"http://www.nytimes.com/2013/01/24/technology/robot-makers-spread-global-gospel-of-automation.html?_r=1&amp;\">twice the number of robots per capita</a> as the US, which probably isn't significant to their larger macroeconomic trends, but would be a strange fact if robots were the leading cause of unemployment.) &nbsp;Labor markets and monetary policy are both major, obvious, widely-discussed candidates for what could've changed between now and the 1950s that might make reemployment harder. &nbsp;And though I'm not a leading econblogger, some other obvious-seeming thoughts that occur to me are:</p>\n<p>* Many industries that would otherwise be accessible to relatively less skilled labor, have much higher barriers to entry now than in 1950. &nbsp;Taxi medallions, governments saving us from the terror of unlicensed haircuts, fees and regulatory burdens associated with new businesses - all things that could've plausibly changed between now and the previous four centuries. &nbsp;This doesn't apply only to unskilled labor, either; in 1900 it was a lot easier, legally speaking, to set up shop as a doctor. &nbsp;(Yes, the average doctor was substantially worse back then. &nbsp;But ask yourself whether some simple, repetitive medical surgery should really, truly require 11 years of medical school and residency, rather than a 2-year vocational training program for someone with high dexterity and good focus.) &nbsp;These sorts of barriers to entry allow people who are currently employed in that field to extract value from people trying to get jobs in that field (and from the general population too, of course). &nbsp;In any one sector this wouldn't hurt the whole economy too much, but if it happens everywhere at once, that could be the problem.</p>\n<p>* <a href=\"http://www.bloomberg.com/news/2012-11-25/the-working-poor-pay-high-taxes-too.html\">True effective marginal tax rates on low-income families</a> have gone up today compared to the 1960s, after all phasing-out benefits are taken into account, counting federal and state taxes, city sales taxes, and so on. &nbsp;I've seen figures tossed around like 70% and worse, and this seems like the sort of thing that could easily trash reemployment.[2]</p>\n<p>* Perhaps companies are, for some reason, less willing to hire previously unskilled people and train them on the job. &nbsp;Empirically this seems to be something that is more true today than in the 1950s. &nbsp;If I were to guess at why, I would say that employees moving more from job to job, and fewer life-long jobs, makes it less rewarding for employers to invest in training an employee; and also college is more universal now than then. &nbsp;Which means that employers might try to <em>rely on</em> colleges to train employees, and this is a function colleges can't actually handle because:</p>\n<p>* The US educational system is either getting worse at training people to handle new jobs, or getting so much more expensive that people can't afford retraining, for various other reasons. &nbsp;(Plus, we are really stunningly stupid about matching educational supply to labor demand. &nbsp;How completely ridiculous is it to ask high school students to decide what they want to do with the rest of their lives and give them nearly no support in doing so? &nbsp;Support like, say, spending a day apiece watching twenty different jobs and then another week at their top three choices, with salary charts and projections and probabilities of graduating that subject given their test scores? &nbsp;The more so considering this is a central allocation question for the entire economy? &nbsp;But I have no particular reason to believe this part has gotten <em>worse </em>since 1960.)</p>\n<p>* The financial system is staring much more at the inside of its eyelids now than in the 1980s. &nbsp;This could be making it harder for expanding businesses to get loans at terms they would find acceptable, or making it harder for expanding businesses to access capital markets at acceptable terms, or interfering with central banks' attempts to regularize nominal demand, or acting as a brake on the system in some other fashion.</p>\n<p>* Hiring a new employee now exposes an employer to more downside risk of being sued, or risk of being unable to fire the new employee if it turns out to be a bad decision. &nbsp;Human beings, including employers, are very averse to downside risk, so this could plausibly be a major obstacle to reemployment. &nbsp;Such risks are a plausible major factor in making the decision to hire someone <em>hedonically unpleasant</em>&nbsp;for the person who has to make that decision, which could've changed between now and 1950. &nbsp;(If your sympathies are with employees rather than employers, please consider that, nonetheless, if you pass any protective measure that makes the decision to hire somebody <em>less pleasant</em> for the hirer, fewer people will be hired and this is not good for people seeking employment. &nbsp;Many labor market regulations transfer wealth or job security to the already-employed at the expense of the unemployed, and these have been increasing over time.)</p>\n<p>* Tyler Cowen's&nbsp;<a href=\"http://marginalrevolution.com/marginalrevolution/2010/07/zero-marginal-product-workers.html\">Zero Marginal Product Workers</a>&nbsp;hypothesis: &nbsp;Anyone long-term-unemployed has now been swept into a group of people who have less than zero&nbsp;<em>average</em>&nbsp;marginal productivity, due to some of the people in this pool being negative-marginal-product workers who will destroy value, and employers not being able to tell the difference. &nbsp;We need some new factor to explain why this wasn't true in 1950, and obvious candidates would be (1) legal liability making past-employer references unreliable and (2) expanded use of college credentialing sweeping up more of the positive-product workers so that the average product of the uncredentialed workers drops.</p>\n<p>* There's a thesis (whose most notable proponent I know is Peter Thiel, though this is not exactly how Thiel phrases it) that real, material technological change has been dying. &nbsp;If you can build a feature-app and flip it to Google for $20M in an acqui-hire, why bother trying to invent the next Model T? &nbsp;Maybe working on hard technology problems using math and science until you can build a liquid fluoride thorium reactor, has been made to seem less attractive to brilliant young kids than flipping a $20M company to Google or becoming a hedge-fund trader (and this is truer today relative to 1950).[3]</p>\n<p>* Closely related to the above: &nbsp;Maybe change in atoms instead of bits has been regulated out of existence. &nbsp;The expected biotech revolution never happened because the FDA is just too much of a roadblock (it adds a great deal of expense, significant risk, and most of all, delays the returns beyond venture capital time horizons). &nbsp;It's plausible we'll never see a city with a high-speed all-robotic all-electric car fleet because the government, after lobbying from various industries, will require human attendants on every car - for safety reasons, of course! &nbsp;If cars were invented nowadays, the horse-and-saddle industry would surely <em>try </em>to arrange for them to be regulated out of existence, or sued out of existence, or limited to the same speed as horses to ensure existing buggies remained safe. &nbsp;Patents are also an increasing drag on innovation in its most fragile stages, and may shortly bring an end to the remaining life in software startups as well. &nbsp;(But note that this thesis, like the one above, seems hard-pressed to account for jobs not coming back after the Great Recession. &nbsp;It is not conventional macroeconomics that re-employment after a recession requires macro sector shifts or new kinds of technology jobs. &nbsp; The above is more of a Great Stagnation thesis of \"What happened to productivity growth?\" than a Great Recession thesis of \"Why aren't the jobs coming back?\"[4])</p>\n<p>Q. &nbsp;Some of those ideas sounded more plausible than others, I have to say.</p>\n<p>A. &nbsp;Well, it's not like they could all be true simultaneously. &nbsp;There's only a fixed effect size of unemployment to be explained, so the more likely it is that any one of these factors played a big role, the less we need to suppose that all the other factors were important; and perhaps what's Really Going On is something else entirely. &nbsp;Furthermore, the 'real cause' isn't always the factor you want to fix. &nbsp;If the European Union's unemployment problems were 'originally caused' by labor market regulation, there's no rule saying that those problems couldn't be mostly fixed by instituting an NGDP level targeting regime. &nbsp;This might or might not work, but the point is that there's no law saying that to fix a problem you have to fix its original historical cause.</p>\n<p>Q. &nbsp;Regardless, if the engine of re-employment is broken <em>for whatever reason, </em>then AI really is killing jobs - a marginal job automated away by advances in AI algorithms won't come back.</p>\n<p>A. &nbsp;Then it's odd to see so many news articles talking about AI killing jobs, when plain old non-AI computer programming and the Internet have affected many more jobs than that. &nbsp;The buyer ordering books over the Internet, the spreadsheet replacing the accountant - these processes are not strongly relying on the sort of algorithms that we would usually call 'AI' or 'machine learning' or 'robotics'. &nbsp;The main role I can think of for actual AI algorithms being involved, is in computer vision enabling more automation. &nbsp;And many manufacturing jobs were already automated by robotic arms even before robotic vision came along. &nbsp;Most computer programming is not AI programming, and most automation is not AI-driven. &nbsp;And then on near-term scales, like changes over the last five years, trade shifts and financial shocks and new labor market entrants are more powerful economic forces than the slow continuing march of computer programming. &nbsp;(Automation is a weak economic force in any given year, but cumulative and directional over decades. &nbsp;Trade shifts and financial shocks are stronger forces in any single year, but might go in the opposite direction the next decade. &nbsp;Thus, even generalized automation via computer programming is still an unlikely culprit for any sudden drop in employment as occurred in the Great Recession.)</p>\n<p>Q. &nbsp;Okay, you've persuaded me that it's ridiculous to point to AI while talking about modern-day unemployment. &nbsp;What about <em>future</em>&nbsp;unemployment?</p>\n<p>A. &nbsp;Like after the next ten years? &nbsp;We might or might not see robot-driven cars, which would be genuinely based in improved AI algorithms, and would automate away another bite of human labor. &nbsp;Even then, the total number of people driving cars for money would just be a small part of the total global economy; most humans are not paid to drive cars most of the time. &nbsp;Also again: for AI or productivity growth or increased trade or immigration or graduating students to increase unemployment, instead of resulting in more hot dogs and buns for everyone, you must be doing something terribly wrong that you weren't doing wrong in 1950.</p>\n<p>Q. &nbsp;How about timescales longer&nbsp;than ten years? &nbsp;There was one class of laborers permanently unemployed by the automobile revolution, namely horses. &nbsp;There are a lot fewer horses nowadays because there is literally nothing left for horses to do that machines can't do better; horses' marginal labor productivity dropped below their cost of living. &nbsp;Could that happen to humans too, if AI advanced far enough that it could do <em>all</em>&nbsp;the labor?</p>\n<p>A. &nbsp;If we imagine that in future decades machine intelligence is slowly going past the equivalent of IQ 70, 80, 90, eating up more and more jobs along the way... then I defer to Robin Hanson's analysis in <a href=\"http://hanson.gmu.edu/aigrow.pdf\">Economic Growth Given Machine Intelligence</a>, in which, as the abstract says, \"Machines complement human labor when [humans] become more productive at the jobs they perform, but machines also substitute for human labor by taking over human jobs. At \ufb01rst, complementary e\ufb00ects dominate, and human wages rise with computer productivity. But eventually substitution can dominate, making wages fall as fast as computer prices now do.\"</p>\n<p>Q. &nbsp;Could we already be in this substitution regime -</p>\n<p>A. &nbsp;No, no, a dozen times no, for the dozen reasons already mentioned. &nbsp;That sentence in Hanson's paper has <em>nothing to do </em>with what is going on <em>right now</em>. &nbsp;The future cannot be a cause of the past. &nbsp;Future scenarios, even if they seem to associate the concept of AI with the concept of unemployment, cannot rationally increase the probability that current AI is responsible for current unemployment.</p>\n<p>Q. &nbsp;But AI will inevitably&nbsp;become a problem later?</p>\n<p>A. &nbsp;Not necessarily. &nbsp;We only get the Hansonian scenario if AI is <em>broadly, steadily&nbsp;</em>going past IQ 70, 80, 90, etc., making an increasingly large portion of the population fully obsolete in the sense that there is literally no job anywhere on Earth for them to do instead of nothing, because for <em>every </em>task they could do there is an AI algorithm or robot which does it more cheaply. &nbsp;That scenario isn't the only possibility.</p>\n<p>Q. &nbsp;What other possibilities are there?</p>\n<p>A. &nbsp;Lots, since what Hanson is talking about is a <em>new unprecedented phenomenon&nbsp;</em>extrapolated over&nbsp;<em>new future circumstances which have never been seen before</em>&nbsp;and there are all kinds of things which could potentially go differently within that. &nbsp;Hanson's <a href=\"http://hanson.gmu.edu/aigrow.pdf\">paper</a> may be the first obvious extrapolation from conventional macroeconomics and steady AI trendlines, but that's hardly a sure bet. &nbsp;Accurate prediction is hard, especially about the future, and I'm pretty sure Hanson would agree with that.</p>\n<p>Q. &nbsp;I see. &nbsp;Yeah, when you put it that way, there are other possibilities. &nbsp;Like, Ray Kurzweil would predict that brain-computer interfaces would let humans keep up with computers, and then we wouldn't get mass unemployment.</p>\n<p>A. &nbsp;The future would be more uncertain than that, even granting Kurzweil's hypotheses - it's not as simple as picking one futurist and assuming that their favorite assumptions correspond to their favorite outcome. &nbsp;You might get mass unemployment <em>anyway </em>if humans with brain-computer interfaces are more expensive&nbsp;or less effective&nbsp;than pure automated systems. &nbsp;With today's technology we could design robotic rigs to amplify a horse's muscle power - maybe, we're still working on that tech for humans - but it took around an extra century after the Model T to get to that point, and a plain old car is much cheaper.</p>\n<p>Q. &nbsp;Bah, anyone can nod wisely and say \"Uncertain, the future is.\" &nbsp;Stick your neck out, Yoda, and state your opinion clearly enough that you can later be proven wrong. &nbsp;Do <em>you</em>&nbsp;think&nbsp;we will eventually get to the point where AI produces mass unemployment?</p>\n<p>A. &nbsp;My own guess is a moderately strong 'No', but for reasons that would sound like a complete subject change relative to all the macroeconomic phenomena we've been discussing so far. &nbsp;In particular I refer you to \"<a href=\"http://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics: Returns on cognitive reinvestment</a>\", a paper <a href=\"http://www.themoneyillusion.com/?p=21486\">recently referenced</a> on Scott Sumner's blog as relevant to this issue.</p>\n<p>Q. &nbsp;Hold on, let me read the abstract and... what the heck is this?</p>\n<p>A. &nbsp;It's an argument that you don't get the Hansonian scenario <em>or </em>the Kurzweilian scenario, because if you look at the historical course of hominid evolution and try to assess the inputs of marginally increased cumulative evolutionary selection pressure versus the cognitive outputs of hominid brains, and infer the corresponding curve of returns, then ask about a reinvestment scenario -</p>\n<p>Q. &nbsp;English.</p>\n<p>A. &nbsp;Arguably, what you get is I. J. Good's scenario where once an AI goes over some threshold of sufficient intelligence, it can self-improve and increase in intelligence far&nbsp;past the human level. &nbsp;This scenario is formally termed an 'intelligence explosion', informally 'hard takeoff' or 'AI-go-FOOM'. &nbsp;The resulting predictions are strongly distinct from traditional economic models of accelerating technological growth (we're <em>not </em>talking about Moore's Law here). &nbsp;Since it should take advanced general AI to automate away <em>most or all&nbsp;</em>humanly possible labor, my guess is that AI will intelligence-explode to superhuman intelligence <em>before</em>&nbsp;there's time for moderately-advanced AIs to crowd humans out of the global economy. &nbsp;(See also section 3.10 of the aforementioned&nbsp;<a href=\"http://intelligence.org/files/IEM.pdf\">paper</a>.) &nbsp;Widespread economic adoption of a technology comes with a delay factor&nbsp;that wouldn't slow down an AI rewriting its own source code. &nbsp;This means we <em>don't </em>see the scenario of human programmers gradually improving broad AI technology past the 90, 100, 110-IQ threshold. &nbsp;An explosion of AI self-improvement utterly derails that scenario, and sends us onto a completely different track which confronts us with wholly dissimilar questions.</p>\n<p>Q. &nbsp;Okay. &nbsp;What effect do you think a superhumanly intelligent self-improving AI would have on unemployment, especially the bottom 25% who are already struggling now? &nbsp;Should we really be trying to create this technological wonder of self-improving AI, if the end result is to make the world's poor even poorer? &nbsp;How is someone with a high-school education supposed to compete with a machine superintelligence for jobs?</p>\n<p>A. &nbsp;I think you're asking an overly narrow question there.</p>\n<p>Q. &nbsp;How so?</p>\n<p>A. &nbsp;You might be thinking about 'intelligence' in terms of the contrast between a human college professor and a human janitor, rather than the contrast between a human and a chimpanzee. &nbsp;Human intelligence more or less created the entire modern world, including our invention of money; twenty thousand years ago we were just running around with bow and arrows. &nbsp;And yet on a biological level, human intelligence has stayed roughly the same since the invention of agriculture. &nbsp;Going past human-level intelligence is change on a scale much larger than the Industrial Revolution, or even the Agricultural Revolution, which both took place at a constant level of intelligence; human nature didn't change. &nbsp;As Vinge observed, building something <em>smarter than you</em>&nbsp;implies a future that is <em>fundamentally</em>&nbsp;different in a way that you wouldn't get from&nbsp;better medicine or interplanetary travel.</p>\n<p>Q. &nbsp;But what <em>does</em>&nbsp;happen to people who were already economically disadvantaged, who don't have investments in the stock market and who aren't sharing in the profits of the corporations that own these superintelligences?</p>\n<p>A. &nbsp;Um... we appear to be using substantially different background assumptions. &nbsp;The notion of a 'superintelligence' is not that it sits around in Goldman Sachs's basement trading stocks for its corporate masters. &nbsp;The concrete illustration I often use is that a superintelligence asks itself what the fastest possible route is to increasing its real-world power, and then, rather than bothering with the digital counters that humans call money, the superintelligence solves the <a href=\"http://en.wikipedia.org/wiki/Protein_structure_prediction\">protein structure prediction problem</a>, emails some DNA sequences to online peptide synthesis labs, and gets back a batch of proteins which it can mix together to create an acoustically controlled equivalent of an artificial ribosome which it can use to make second-stage nanotechnology which manufactures third-stage nanotechnology which manufactures diamondoid molecular nanotechnology and then... well, it doesn't really matter from our perspective what comes after that, because from a human perspective any technology more advanced than molecular nanotech is just overkill. &nbsp;A superintelligence with molecular nanotech does not wait for you to buy things from it in order for it to acquire money. &nbsp;It just moves atoms around into whatever molecular structures or large-scale structures it wants.</p>\n<p>Q. &nbsp;How would it get the energy to move those atoms, if not by buying electricity from existing power plants? &nbsp;Solar power?</p>\n<p>A. &nbsp;Indeed, one popular speculation is that optimal use of a star system's resources is to disassemble local gas giants (Jupiter in our case) for the raw materials to build a Dyson Sphere, an enclosure that captures all of a star's energy output. &nbsp;This does not involve buying solar panels from human manufacturers, rather it involves self-replicating machinery which builds copies of itself on a rapid exponential curve -</p>\n<p>Q. &nbsp;Yeah, I think I'm starting to get a picture of your background assumptions. &nbsp;So let me expand the question. &nbsp;If we grant that scenario rather than the Hansonian scenario or the Kurzweilian scenario, what sort of effect does&nbsp;<em>that </em>have on humans?</p>\n<p>A. &nbsp;That depends on the <em>exact</em> initial design of the first AI which undergoes an intelligence explosion. &nbsp;Imagine a vast space containing all possible mind designs. &nbsp;Now imagine that humans, who all have a brain with a cerebellum, thalamus, a cerebral cortex organized into roughly the same areas, neurons firing at a top speed of 200 spikes per second, and so on, are one tiny little dot within this space of all possible minds. &nbsp;Different kinds of AIs can be vastly more different from each other than you are different from a chimpanzee. &nbsp;What happens after AI, depends on what kind of AI you build - the exact selected point in mind design space. &nbsp;If you can solve the technical problems and wisdom problems associated with building an AI that is nice to humans, or nice to sentient beings in general, then we all live <a href=\"/lw/xy/the_fun_theory_sequence/\">happily ever afterward</a>.&nbsp; If you build the AI incorrectly... well, the AI is unlikely to end up with a specific hate for humans. &nbsp;But such an AI won't attach a positive value to us either.&nbsp;&nbsp;\"The AI does not hate you, nor does it love you, but you are made of atoms which it can use for something else.\" &nbsp;The human species would end up disassembled for spare atoms, after which human unemployment would be zero. &nbsp;In <em>neither </em>alternative do we end up with poverty-stricken unemployed humans hanging around being sad because they can't get jobs as janitors now that star-striding nanotech-wielding superintelligences are taking all the janitorial jobs. &nbsp;And so I conclude that advanced AI causing mass human unemployment is, all things considered, unlikely.</p>\n<p>Q. &nbsp;Some of the background assumptions you used to arrive at that conclusion strike me as requiring additional support beyond the arguments you listed here.</p>\n<p>A. &nbsp;I recommend <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a> for an overview of the general issues and literature, <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Artificial Intelligence as a positive and negative factor in global risk</a> for a summary of some of the issues around building AI correctly or incorrectly, and the aforementioned <a href=\"http://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a> for some ideas about analyzing the scenario of an AI investing cognitive labor in improving its own cognition. &nbsp;The last in particular is an important open problem in economics if you're a smart young economist reading this, although since the fate of the entire human species could well depend on the answer, you would be foolish to expect there'd be as many papers published about that as squirrel migration patterns.&nbsp;&nbsp;Nonetheless, bright young economists who want to say something important about AI should consider analyzing the microeconomics of returns on cognitive (re)investments, rather than post-AI macroeconomics which may not actually <em>exist</em>&nbsp;depending on the answer to the first question. &nbsp;Oh, and&nbsp;<a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> at the&nbsp;<a href=\"http://www.fhi.ox.ac.uk/\">Oxford Future of Humanity Institute</a>&nbsp;is supposed to have a forthcoming book on the intelligence explosion; that book isn't out yet so I can't link to it, but&nbsp;<a href=\"http://www.nickbostrom.com/\">Bostrom</a>&nbsp;personally and&nbsp;<a href=\"http://www.fhi.ox.ac.uk/\">FHI</a>&nbsp;generally have published some excellent academic papers already.</p>\n<p>Q. &nbsp;But to sum up, you think that AI is definitely not the issue we should be talking about with respect to unemployment.</p>\n<p>A. &nbsp;Right. &nbsp;From an economic perspective, AI is a completely odd place to focus your concern about modern-day unemployment. &nbsp;From an AI perspective, modern-day unemployment trends are a moderately odd reason to be worried about AI. &nbsp;Still, it is scarily true that increased automation, like increased global trade or new graduates or&nbsp;<em>anything else</em>&nbsp;that ought properly to produce a stream of employable labor to the benefit of all, might perversely operate to increase unemployment&nbsp;<em>if&nbsp;the broken reemployment engine is not fixed</em>.</p>\n<p>Q. &nbsp;And with respect to future AI... what is it you think, exactly?</p>\n<p>A. &nbsp;I think that with respect to moderately more advanced AI, we probably won't see <em>intrinsic unavoidable&nbsp;</em>mass unemployment in the economic world as we know it.&nbsp; <em>If </em>re-employment stays broken and new college graduates continue to have trouble finding jobs, then there are plausible stories where future&nbsp;AI advances far enough (but not <em>too</em>&nbsp;far)&nbsp;to be a significant part of what's freeing up new employable labor which bizarrely cannot be employed. &nbsp;I wouldn't consider this my main-line, average-case guess; I wouldn't expect to see it in the next 15 years or as the result of just robotic cars; and if it did happen, I wouldn't call AI the 'problem' while central banks still hadn't adopted NGDP level targeting. &nbsp;And then with respect to <em>very </em>advanced&nbsp;AI, the sort that might be produced by AI self-improving and going FOOM, asking about the effect of&nbsp;<em>machine superintelligence</em> on&nbsp;the conventional human labor market is like asking how US-Chinese trade patterns would be affected by the Moon crashing into the Earth. &nbsp;There would indeed be effects, but you'd be missing the point.</p>\n<p>Q. &nbsp;Thanks for clearing that up.</p>\n<p>A. &nbsp;No problem.</p>\n<hr />\n<p>ADDED 8/30/13: &nbsp;Tyler Cowen's reply to this was one I hadn't listed:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">Think of <a href=\"http://mruniversity.com/courses/great-economists-classical-economics-and-its-forerunners/machinery-question\">the machines of the industrial revolution</a> as getting underway sometime in the 1770s or 1780s. &nbsp;The big wage gains for British workers <a href=\"http://mruniversity.com/courses/great-economists-classical-economics-and-its-forerunners/living-standards-during-industrial\">don&rsquo;t really come until the 1840s</a>. &nbsp;Depending on your exact starting point, that is over fifty years of labor market problems from automation.</p>\n</blockquote>\n<p>See <a href=\"http://marginalrevolution.com/marginalrevolution/2013/08/eliezer-yudkowsky-asks-about-automation.html\">here</a> for the rest of Tyler's reply.</p>\n<p>Taken at face value this might suggest that if we wait 50 years everything will be all right. &nbsp;<a href=\"http://www.motherjones.com/kevin-drum/2013/08/robot-revolution-employment-economy\">Kevin Drum</a> replies that in 50 years there might be no human jobs left, which is possible but wouldn't be an effect we've seen <em>already</em>,&nbsp;rather a prediction of novel things yet to come.</p>\n<p>Though Tyler also says, \"A second point is that now we have a much more extensive network of government benefits and also regulations which increase the fixed cost of hiring labor\" and this of course was already on my list of things that could be trashing modern reemployment unlike-in-the-1840s.</p>\n<p>'Brett' in MR's comments section also counter-claims:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">The spread of steam-powered machinery and industrialization from textiles/mining/steel to all manner of British industries didn&rsquo;t really get going until the 1830s and 1840s. Before that, it was mostly piece-meal, with some areas picking up the technology faster than others, while the overall economy didn&rsquo;t change that drastically (hence the minimal changes in overall wages).</p>\n</blockquote>\n<hr />\n<p>[1] &nbsp;The core idea in market monetarism is <em>very</em>&nbsp;roughly something like this: &nbsp;A central bank can control the total amount of money and thereby control any single economic variable measured in money, i.e., control one <em>nominal</em>&nbsp;variable. &nbsp;A central bank can't directly control how many people are employed, because that's a real variable. &nbsp;You could, however, try to control Nominal Gross Domestic Income (NGDI) or the total amount that people have available to spend (as measured in your currency). &nbsp;If the central bank commits to an NGDI <em>level target</em>&nbsp;then any shortfalls are made up the next year - if your NGDI growth target is 5% and you only get 4% in one year then you try for 6% the year after that. &nbsp;NGDI level targeting would mean that all the companies would know that, collectively, all the customers in the country would have 5% more money (measured in dollars) to spend in the next year than the previous year. &nbsp;This is usually called \"NGDP level targeting\" for historical reasons (NGDP is the other side of the equation, what the earned dollars are being spent on) but the most advanced modern form of the idea is probably \"Level-targeting a market forecast of per-capita NGDI\". &nbsp;Why this is the <em>best</em>&nbsp;nominal variable for central banks to control is a longer story and for that you'll have to <a href=\"https://www.google.com/search?q=market+monetarism\">read up on market monetarism</a>. &nbsp;I will note that if you were worried about hyperinflation back when the Federal Reserve started dropping US interest rates to almost zero and buying government bonds by printing money... well, you really should note that (a) most economists said this wouldn't happen, (b) the market spreads on inflation-protected Treasuries said that the market was anticipating very low inflation, and that (c) we then <em>actually got</em>&nbsp;inflation <em>below </em>the Fed's 2% target. &nbsp;You can argue with economists. &nbsp;You can even argue with the market forecast, though in this case you ought to bet money on your beliefs. &nbsp;But when your fears of hyperinflation are disagreed with by economists, the market forecast&nbsp;<em>and observed reality,</em>&nbsp;it's time to give up on the theory that generated the false prediction. &nbsp;In this case, market monetarists would have told you not to expect hyperinflation because NGDP/NGDI was collapsing and this constituted (overly) tight money regardless of what interest rates or the monetary base looked like.</p>\n<p>[2] &nbsp;Call me a wacky utopian idealist, but I wonder if it might be genuinely politically feasible to reduce marginal taxes on the bottom 20%, if economists on both sides of the usual political divide got together behind the idea that income taxes (including payroll taxes) on the bottom 20% are (a) immoral and (b) do economic harm far out of proportion to government revenue generated. &nbsp;This would also require some amount of decreased taxes on the next quintile in order to avoid high<em>&nbsp;marginal</em>&nbsp;tax rates, i.e., if you suddenly start paying $2000/year in taxes as soon as your income goes from $19,000/year to $20,000/year then that was a 200% tax rate on that particular extra $1000 earned. &nbsp;The lost tax revenue must be made up somewhere else. &nbsp;In the current political environment this probably requires higher income taxes on higher wealth brackets rather than anything more creative. &nbsp;But if we allow ourselves to discuss economic dreamworlds, then income taxes, corporate income taxes, and capital-gains taxes are all very inefficient compared to consumption taxes, land taxes, and basically <em>anything but</em>&nbsp;income and corporate taxes. &nbsp;This is true even from the perspective of equality; a rich person who earns lots of money, but invests it all instead of spending it, is benefiting the economy rather than themselves and should not be taxed until they try to <em>spend</em>&nbsp;the money on a yacht, at which point you charge a consumption tax or luxury tax (even if that yacht is listed as a business expense, which should make no difference; consumption is not more moral when done by businesses instead of individuals). &nbsp;If I were given unlimited powers to try to fix the unemployment thing, I'd be reforming the entire tax code from scratch to present the minimum possible obstacles to exchanging one's labor for money, and as a second priority minimize obstacles to compound reinvestment of wealth. &nbsp;But trying to change anything on this scale is probably not politically feasible relative to a simpler, more understandable crusade to \"Stop taxing the bottom 20%, it <em>harms our economy</em>&nbsp;because they're customers of all those other companies and it's <em>immoral</em>&nbsp;because they get a raw enough deal already.\"</p>\n<p>[3] &nbsp;Two possible&nbsp;forces for significant technological change in the 21st century would be robotic cars and electric cars. &nbsp;Imagine a city with an all-robotic all-electric car fleet, dispatching light cars with only the battery sizes needed for the journey, traveling at much higher speeds with no crash risk and much lower fuel costs... <em>and </em>lowering rents by greatly extending the effective area of a city, i.e., extending the physical distance you can live from the center of the action while still getting to work on time because your average speed is 75mph. &nbsp;What comes to mind when you think of robotic cars? &nbsp;Google's prototype robotic cars. &nbsp;What comes to mind when you think of electric cars? &nbsp;Tesla. &nbsp;In both cases we're talking about ascended, post-exit Silicon Valley moguls trying to create industrial progress out of the goodness of their hearts, using money they earned from Internet startups. &nbsp;Can you sustain a whole economy based on what Elon Musk and Larry Page decide are cool?</p>\n<p>[4] &nbsp; Currently the conversation among economists is more like \"Why has total factor productivity growth slowed down in developed countries?\" than \"Is productivity growing so fast due to automation that we'll run out of jobs?\" &nbsp;Ask them the latter question and they will, with justice, give you very strange looks. &nbsp;Productivity <em>isn't</em>&nbsp;growing at high rates, and if it <em>were </em>that ought to cause employment rather than unemployment. &nbsp;This is why the Great Stagnation in productivity is one possible explanatory factor in unemployment, albeit (as mentioned) not a very good explanation for why we can't get back the jobs lost in the Great Recession. &nbsp;The idea would have to be that some natural rate of productivity growth and sectoral shift is necessary for <em>re</em>-employment to happen after recessions, and we've lost that natural rate; but so far as I know this is not conventional macroeconomics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HkiwLtMRLxpBa6zs5": 2, "bFi5fzkCzBWoQSeiB": 2, "PDJ6KqJBRzvKPfuS3": 1, "fuZZ64fNz24BLrXnY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZiRKzx3yv7NyA5rjF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 76, "baseScore": 81, "extendedScore": null, "score": 0.000196, "legacy": true, "legacyId": "22648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 268, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-07-25T18:46:13.486Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-25T20:29:17.051Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta LessWrong: August Meetup ", "slug": "meetup-atlanta-lesswrong-august-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8ujQjLbteCeKc455F/meetup-atlanta-lesswrong-august-meetup", "pageUrlRelative": "/posts/8ujQjLbteCeKc455F/meetup-atlanta-lesswrong-august-meetup", "linkUrl": "https://www.lesswrong.com/posts/8ujQjLbteCeKc455F/meetup-atlanta-lesswrong-august-meetup", "postedAtFormatted": "Thursday, July 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20LessWrong%3A%20August%20Meetup%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20LessWrong%3A%20August%20Meetup%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ujQjLbteCeKc455F%2Fmeetup-atlanta-lesswrong-august-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20LessWrong%3A%20August%20Meetup%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ujQjLbteCeKc455F%2Fmeetup-atlanta-lesswrong-august-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ujQjLbteCeKc455F%2Fmeetup-atlanta-lesswrong-august-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p5'>Atlanta LessWrong: August Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 August 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For the first of our two planned August meetups, we will be continuing our work on group self-improvement and Bayesian reasoning, as well as having super fun social times with games and banter!</p>\n\n<p>Agenda:</p>\n\n<ul>\n<li><p>Introductions and Welcome to New Members!</p></li>\n<li><p>Meta-Meetup Stuff: Creating a regularly scheduled, twice monthly, meeting time. Mini-presentations by members. Currently we have a series going on epistemology.</p></li>\n<li><p>Self-improvement brainstorming: Each meetup, we will invite one member to volunteer for our \"self-improvement group hacking/brainstorming.\" The volunteer will discuss an issue with their lives, be it physical, emotional, intellectual, or philosophical, and invite feedback and brainstorming from other members to improve their lives.</p></li>\n<li><p>Meetup Educational Experiences! We've done meditation, and we've been working on a series of Bayesian Reasoning Preliminaries on Probability and Confidence Intervals. We will continue with conditional probability and move towards rockin' us some Bayes' Theorem. Games, Social Time, Free Discussion</p></li>\n</ul>\n\n<p>Please contact me if you have allergies to cats, as our meeting space has two. As always, vegan, gluten-free snacks will be provided. Feel free to bring additional snacks and drink!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p5'>Atlanta LessWrong: August Meetup </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8ujQjLbteCeKc455F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.2788894208167396e-06, "legacy": true, "legacyId": "23508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_LessWrong__August_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/p5\">Atlanta LessWrong: August Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 August 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For the first of our two planned August meetups, we will be continuing our work on group self-improvement and Bayesian reasoning, as well as having super fun social times with games and banter!</p>\n\n<p>Agenda:</p>\n\n<ul>\n<li><p>Introductions and Welcome to New Members!</p></li>\n<li><p>Meta-Meetup Stuff: Creating a regularly scheduled, twice monthly, meeting time. Mini-presentations by members. Currently we have a series going on epistemology.</p></li>\n<li><p>Self-improvement brainstorming: Each meetup, we will invite one member to volunteer for our \"self-improvement group hacking/brainstorming.\" The volunteer will discuss an issue with their lives, be it physical, emotional, intellectual, or philosophical, and invite feedback and brainstorming from other members to improve their lives.</p></li>\n<li><p>Meetup Educational Experiences! We've done meditation, and we've been working on a series of Bayesian Reasoning Preliminaries on Probability and Confidence Intervals. We will continue with conditional probability and move towards rockin' us some Bayes' Theorem. Games, Social Time, Free Discussion</p></li>\n</ul>\n\n<p>Please contact me if you have allergies to cats, as our meeting space has two. As always, vegan, gluten-free snacks will be provided. Feel free to bring additional snacks and drink!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_LessWrong__August_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/p5\">Atlanta LessWrong: August Meetup </a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta LessWrong: August Meetup ", "anchor": "Discussion_article_for_the_meetup___Atlanta_LessWrong__August_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta LessWrong: August Meetup ", "anchor": "Discussion_article_for_the_meetup___Atlanta_LessWrong__August_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T00:51:00.460Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup\u2014Introduction to Anthropics", "slug": "meetup-west-la-meetup-introduction-to-anthropics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LNqgSSvzL499LyvYy/meetup-west-la-meetup-introduction-to-anthropics", "pageUrlRelative": "/posts/LNqgSSvzL499LyvYy/meetup-west-la-meetup-introduction-to-anthropics", "linkUrl": "https://www.lesswrong.com/posts/LNqgSSvzL499LyvYy/meetup-west-la-meetup-introduction-to-anthropics", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%E2%80%94Introduction%20to%20Anthropics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%E2%80%94Introduction%20to%20Anthropics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNqgSSvzL499LyvYy%2Fmeetup-west-la-meetup-introduction-to-anthropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%E2%80%94Introduction%20to%20Anthropics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNqgSSvzL499LyvYy%2Fmeetup-west-la-meetup-introduction-to-anthropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLNqgSSvzL499LyvYy%2Fmeetup-west-la-meetup-introduction-to-anthropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p6'>West LA Meetup\u2014Introduction to Anthropics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 July 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you apply some skullduggery.</p>\n\n<p><strong>Discussion</strong>: The study of observer selection effects is called \"anthropics\" or \"anthropic reasoning\". As I am not an expert in anthropics, I will introduce the topic and some of its famous problems, clear up basic confusions, and caution against more advanced confusions.</p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://www.anthropic-principle.com/?q=anthropic_principle/primer\" rel=\"nofollow\">Anthropic Principle Primer</a></li>\n<li><a href=\"http://www.acceleratingfuture.com/steven/?cat=22\" rel=\"nofollow\">Black Belt Bayesian posts tagged &quot;anthropics&quot;</a></li>\n<li><a href=\"https://meteuphoric.wordpress.com/anthropic-principles/\" rel=\"nofollow\">Katja Grace&#39;s Anthropic Principles</a></li>\n<li><a href=\"http://lesswrong.com/lw/17d/forcing_anthropics_boltzmann_brains/\">Forcing Anthropics: Boltzmann Brains</a>, <a href=\"http://lesswrong.com/lw/17c/anthropic_updatelessness/\">Outlawing Anthropics</a>, <a href=\"http://lesswrong.com/lw/19d/the_anthropic_trilemma/\">The Anthropic Trilemma</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/emt/no_anthropic_evidence/\">No Anthropic Evidence</a>, in which Vladimir Nesov proves that you can't update your priors with anthropic observations.</li>\n</ul>\n\n<p>Prior exposure to Less Wrong is <strong>mandatory</strong>. If you haven't memorized the core sequences, don't even <strong>bother</strong> showing up.</p>\n\n<p>There will probably be a highly visible whiteboard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p6'>West LA Meetup\u2014Introduction to Anthropics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LNqgSSvzL499LyvYy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.2791019047860331e-06, "legacy": true, "legacyId": "23509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_Introduction_to_Anthropics\">Discussion article for the meetup : <a href=\"/meetups/p6\">West LA Meetup\u2014Introduction to Anthropics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 July 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you apply some skullduggery.</p>\n\n<p><strong>Discussion</strong>: The study of observer selection effects is called \"anthropics\" or \"anthropic reasoning\". As I am not an expert in anthropics, I will introduce the topic and some of its famous problems, clear up basic confusions, and caution against more advanced confusions.</p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://www.anthropic-principle.com/?q=anthropic_principle/primer\" rel=\"nofollow\">Anthropic Principle Primer</a></li>\n<li><a href=\"http://www.acceleratingfuture.com/steven/?cat=22\" rel=\"nofollow\">Black Belt Bayesian posts tagged \"anthropics\"</a></li>\n<li><a href=\"https://meteuphoric.wordpress.com/anthropic-principles/\" rel=\"nofollow\">Katja Grace's Anthropic Principles</a></li>\n<li><a href=\"http://lesswrong.com/lw/17d/forcing_anthropics_boltzmann_brains/\">Forcing Anthropics: Boltzmann Brains</a>, <a href=\"http://lesswrong.com/lw/17c/anthropic_updatelessness/\">Outlawing Anthropics</a>, <a href=\"http://lesswrong.com/lw/19d/the_anthropic_trilemma/\">The Anthropic Trilemma</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/emt/no_anthropic_evidence/\">No Anthropic Evidence</a>, in which Vladimir Nesov proves that you can't update your priors with anthropic observations.</li>\n</ul>\n\n<p>Prior exposure to Less Wrong is <strong>mandatory</strong>. If you haven't memorized the core sequences, don't even <strong>bother</strong> showing up.</p>\n\n<p>There will probably be a highly visible whiteboard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_Introduction_to_Anthropics1\">Discussion article for the meetup : <a href=\"/meetups/p6\">West LA Meetup\u2014Introduction to Anthropics</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup\u2014Introduction to Anthropics", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_Introduction_to_Anthropics", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup\u2014Introduction to Anthropics", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_Introduction_to_Anthropics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LubwxZHKKvCivYGzx", "ZTEkZNLrmycNuCNYq", "y7jZ9BLEeuNTzgAE5", "tzjWC9Lvqfe454Ttc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T02:35:56.816Z", "modifiedAt": null, "url": null, "title": "Welcome to Less Wrong! (6th thread, July 2013)", "slug": "welcome-to-less-wrong-6th-thread-july-2013", "viewCount": null, "lastCommentedAt": "2020-04-06T19:45:00.735Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KnaveOfAllTrades", "createdAt": "2012-07-20T02:08:23.538Z", "isAdmin": false, "displayName": "KnaveOfAllTrades"}, "userId": "FuACexYrBpyrMmz5C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pMDiDTZWB2yZpcozy/welcome-to-less-wrong-6th-thread-july-2013", "pageUrlRelative": "/posts/pMDiDTZWB2yZpcozy/welcome-to-less-wrong-6th-thread-july-2013", "linkUrl": "https://www.lesswrong.com/posts/pMDiDTZWB2yZpcozy/welcome-to-less-wrong-6th-thread-july-2013", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Welcome%20to%20Less%20Wrong!%20(6th%20thread%2C%20July%202013)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWelcome%20to%20Less%20Wrong!%20(6th%20thread%2C%20July%202013)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpMDiDTZWB2yZpcozy%2Fwelcome-to-less-wrong-6th-thread-july-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Welcome%20to%20Less%20Wrong!%20(6th%20thread%2C%20July%202013)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpMDiDTZWB2yZpcozy%2Fwelcome-to-less-wrong-6th-thread-july-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpMDiDTZWB2yZpcozy%2Fwelcome-to-less-wrong-6th-thread-july-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1663, "htmlBody": "<div>If you've recently joined the <a href=\"/lw/1/about_less_wrong\">Less Wrong community</a>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, <a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as an aspiring rationalist</a> or how you found us. You can <a href=\"/lw/h3p/welcome_to_less_wrong_5th_thread_march_2013/#comments\">skip right to that</a> if you like; the rest of this post consists of a few things you might find helpful. More can be found at the <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a>.</div>\n<p>&nbsp;</p>\n<h4><a id=\"more\"></a>A few notes about the site mechanics<br /></h4>\n<div><strong>To post your first comment</strong>, you must have carried out the e-mail confirmation: When you signed up to create your account, an e-mail was sent to the address you provided with a link that you need to follow to confirm your e-mail address. You must do this before you can post!</div>\n<div><br /></div>\n<div>Less Wrong&nbsp;<strong>comments are threaded</strong>&nbsp;for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp;(you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div><br /></div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to <strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning&mdash; not just that they disagree with you!<strong> If you have any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong><br /></strong></div>\n<div class=\"md\"><strong>Replies</strong> to your comments across the site, plus <strong>private messages</strong> from other users, will show up in your <a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">It's definitely worth your time <strong>commenting on old posts</strong>; veteran users look through the <a href=\"/comments\">recent comments thread</a> quite often (there's a separate <a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of <a href=\"/r/discussion/tag/open_thread/\">open</a> comment <a href=\"/tag/open_thread\">threads</a> for discussion of anything remotely related to rationality.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply <strong>\"tap out\" of a discussion</strong> that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.</div>\n<div class=\"md\"><br /></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br /></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong> This is our attempt to make searching by topic feasible, as well as to store information like <a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a> and idioms. It's a good place to look if someone's speaking Greek to you.<br /></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong> This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong> A <em>huge</em> corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br /></div>\n<h4>A few notes about the community<br /></h4>\n<div>If you've come to Less Wrong to&nbsp; <strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><br /></div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp;<strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)</div>\n<div><br /></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp;<strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma&mdash;honestly, you don't know what you don't know about the community norms here.)</div>\n<div><br /></div>\n<div>Alternatively, if you're still unsure where to submit a post, whether to submit it at all, would like some feedback before submitting, or want to gauge interest, you can ask / provide your draft / summarize your submission in the latest <a href=\"/r/discussion/tag/open_thread/\">open</a> comment <a href=\"/tag/open_thread\">thread</a>. In fact, Open Threads are intended for <strong>anything 'worth saying, but not worth its own post'</strong>, so please do dive in! Informally, there is also the unofficial <a href=\"http://wiki.lesswrong.com/wiki/IRC\">Less Wrong IRC chat room</a>, and you might also like to take a look at some of the other regular <a href=\"http://wiki.lesswrong.com/wiki/Special_threads\">special threads</a>; they're a great way to get involved with the community!</div>\n<div><br /></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp; <strong>meetups&nbsp;</strong> in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>. There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>. If you have your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address. <br /> * <a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a> <br /> * <a href=\"/user/Randaly\">Randaly</a> <br /> * <a href=\"/user/shokwave\">shokwave</a> <br /> * <a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4>A list of some posts that are pretty awesome<br /></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp;to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">The Worst Argument in the World</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site!</p>\n<p>&nbsp;</p>\n<p><sub>Once a post gets over 500 comments, the site stops showing them all by default. If this post has 500 comments and you have 20 karma, please do start the next welcome post; a new post is a good perennial way to encourage newcomers and lurkers to introduce themselves. (Step-by-step, foolproof instructions <a href=\"http://pastebin.com/vZjt59F0\">here</a>; takes &lt;180seconds.)</sub></p>\n<p><sub> </sub></p>\n<p><sub>If there's anything I should add or update on this post (especially broken links), please send me a private message&mdash;I may not notice a comment on the post.</sub></p>\n<p><sub> </sub></p>\n<p><sub>Finally, a big thank you to everyone that helped write this post via its <a href=\"/tag/welcome/\">predecessors</a>!</sub></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pMDiDTZWB2yZpcozy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 30, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "23507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div>If you've recently joined the <a href=\"/lw/1/about_less_wrong\">Less Wrong community</a>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, <a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as an aspiring rationalist</a> or how you found us. You can <a href=\"/lw/h3p/welcome_to_less_wrong_5th_thread_march_2013/#comments\">skip right to that</a> if you like; the rest of this post consists of a few things you might find helpful. More can be found at the <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a>.</div>\n<p>&nbsp;</p>\n<h4 id=\"A_few_notes_about_the_site_mechanics\"><a id=\"more\"></a>A few notes about the site mechanics<br></h4>\n<div><strong>To post your first comment</strong>, you must have carried out the e-mail confirmation: When you signed up to create your account, an e-mail was sent to the address you provided with a link that you need to follow to confirm your e-mail address. You must do this before you can post!</div>\n<div><br></div>\n<div>Less Wrong&nbsp;<strong>comments are threaded</strong>&nbsp;for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp;(you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div><br></div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to <strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning\u2014 not just that they disagree with you!<strong> If you have any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong><br></strong></div>\n<div class=\"md\"><strong>Replies</strong> to your comments across the site, plus <strong>private messages</strong> from other users, will show up in your <a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">It's definitely worth your time <strong>commenting on old posts</strong>; veteran users look through the <a href=\"/comments\">recent comments thread</a> quite often (there's a separate <a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of <a href=\"/r/discussion/tag/open_thread/\">open</a> comment <a href=\"/tag/open_thread\">threads</a> for discussion of anything remotely related to rationality.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply <strong>\"tap out\" of a discussion</strong> that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.</div>\n<div class=\"md\"><br></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong> This is our attempt to make searching by topic feasible, as well as to store information like <a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a> and idioms. It's a good place to look if someone's speaking Greek to you.<br></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong> This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong> A <em>huge</em> corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br></div>\n<h4 id=\"A_few_notes_about_the_community\">A few notes about the community<br></h4>\n<div>If you've come to Less Wrong to&nbsp; <strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><br></div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp;<strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)</div>\n<div><br></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp;<strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma\u2014honestly, you don't know what you don't know about the community norms here.)</div>\n<div><br></div>\n<div>Alternatively, if you're still unsure where to submit a post, whether to submit it at all, would like some feedback before submitting, or want to gauge interest, you can ask / provide your draft / summarize your submission in the latest <a href=\"/r/discussion/tag/open_thread/\">open</a> comment <a href=\"/tag/open_thread\">thread</a>. In fact, Open Threads are intended for <strong>anything 'worth saying, but not worth its own post'</strong>, so please do dive in! Informally, there is also the unofficial <a href=\"http://wiki.lesswrong.com/wiki/IRC\">Less Wrong IRC chat room</a>, and you might also like to take a look at some of the other regular <a href=\"http://wiki.lesswrong.com/wiki/Special_threads\">special threads</a>; they're a great way to get involved with the community!</div>\n<div><br></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp; <strong>meetups&nbsp;</strong> in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>. There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>. If you have your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address. <br> * <a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a> <br> * <a href=\"/user/Randaly\">Randaly</a> <br> * <a href=\"/user/shokwave\">shokwave</a> <br> * <a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4 id=\"A_list_of_some_posts_that_are_pretty_awesome\">A list of some posts that are pretty awesome<br></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp;to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">The Worst Argument in the World</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site!</p>\n<p>&nbsp;</p>\n<p><sub>Once a post gets over 500 comments, the site stops showing them all by default. If this post has 500 comments and you have 20 karma, please do start the next welcome post; a new post is a good perennial way to encourage newcomers and lurkers to introduce themselves. (Step-by-step, foolproof instructions <a href=\"http://pastebin.com/vZjt59F0\">here</a>; takes &lt;180seconds.)</sub></p>\n<p><sub> </sub></p>\n<p><sub>If there's anything I should add or update on this post (especially broken links), please send me a private message\u2014I may not notice a comment on the post.</sub></p>\n<p><sub> </sub></p>\n<p><sub>Finally, a big thank you to everyone that helped write this post via its <a href=\"/tag/welcome/\">predecessors</a>!</sub></p>", "sections": [{"title": "A few notes about the site mechanics", "anchor": "A_few_notes_about_the_site_mechanics", "level": 1}, {"title": "A few notes about the community", "anchor": "A_few_notes_about_the_community", "level": 1}, {"title": "A list of some posts that are pretty awesome", "anchor": "A_list_of_some_posts_that_are_pretty_awesome", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "518 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 522, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2om7AHEHtbogJmT5s", "BHMBBFupzb4s8utts", "yCWPkLi8wJvewPbEp", "5wMcKNAwB6X4mp9og", "6FmqiAgS8h4EJm86s", "msJA6B9ZjiiZxT6EZ", "Psp8ZpYLCDJjshpRb", "CPm5LTwHrvBJCa9h5", "ZiQqsgGX6a42Sfpii", "2ftJ38y9SRBCBsCzy", "zJZvoiwydJ5zvzTHK", "zNcLnqHF5rvrTsQJx", "knpAQ4F3gmguxy39z", "buixYfcXBah9hbSNZ", "neQ7eXuaXpiYw7SBy", "erGipespbbzdG5zYb", "xgicQnkrdA5FehhnQ", "6ddcsdA2c2XpNpE5x", "HFyWNBnDNEDsDNLrZ", "QsMJQSFj7WfoTMNgW", "PeSzc9JTBxhaYRp9b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T11:08:53.942Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-10", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pf2wu8eZYCFg2b5o3/meetup-brussels-meetup-10", "pageUrlRelative": "/posts/Pf2wu8eZYCFg2b5o3/meetup-brussels-meetup-10", "linkUrl": "https://www.lesswrong.com/posts/Pf2wu8eZYCFg2b5o3/meetup-brussels-meetup-10", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPf2wu8eZYCFg2b5o3%2Fmeetup-brussels-meetup-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPf2wu8eZYCFg2b5o3%2Fmeetup-brussels-meetup-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPf2wu8eZYCFg2b5o3%2Fmeetup-brussels-meetup-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p7'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 August 2013 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9e' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform?pli=1\" rel=\"nofollow\">this</a> one minute form, to share your contact information.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p7'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pf2wu8eZYCFg2b5o3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2796037964213332e-06, "legacy": true, "legacyId": "23521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/p7\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 August 2013 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9e' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform?pli=1\" rel=\"nofollow\">this</a> one minute form, to share your contact information.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/p7\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T13:30:17.215Z", "modifiedAt": null, "url": null, "title": "The Argument From Marginal Cases", "slug": "the-argument-from-marginal-cases", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.644Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2rit2BKzWSWsP2yKT/the-argument-from-marginal-cases", "pageUrlRelative": "/posts/2rit2BKzWSWsP2yKT/the-argument-from-marginal-cases", "linkUrl": "https://www.lesswrong.com/posts/2rit2BKzWSWsP2yKT/the-argument-from-marginal-cases", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Argument%20From%20Marginal%20Cases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Argument%20From%20Marginal%20Cases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rit2BKzWSWsP2yKT%2Fthe-argument-from-marginal-cases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Argument%20From%20Marginal%20Cases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rit2BKzWSWsP2yKT%2Fthe-argument-from-marginal-cases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rit2BKzWSWsP2yKT%2Fthe-argument-from-marginal-cases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>The <a href=\"http://en.wikipedia.org/wiki/Argument_from_marginal_cases\">argument from marginal cases</a> claims that you can't both think that humans <a href=\"http://www.jefftk.com/news/2012-04-30\">matter</a> morally and that animals don't, because no reasonable set of criteria for moral worth cleanly separates all humans from all animals. For example, perhaps someone says that suffering only matters when it happens to something that has some bundle of capabilities like linguistic ability, compassion, and/or abstract reasoning. If livestock don't have these capabilities, however, then some people such as very young children probably don't either.</p>\n<p>This is a strong argument, and it avoids the <a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">noncentral fallacy</a>. Any set of qualities you value are going to vary over people and animals, and if you make a continuum there's not going to be a place you can draw a line that will fall above all animals and below all people. So why do I treat humans as the only entities that count morally?</p>\n<p>If you asked me how many chickens I would be willing to kill to save your life, the answer is effectively \"all of them\". [1] This pins down two points on the continuum that I'm clear on: you and chickens. While I'm uncertain where along there things start getting up to significant levels, I think it's probably somewhere that includes no or almost no animals but nearly all humans. Making this distinction among humans, however, would be incredibly socially destructive, especially given how unsure I am about where the line should go, and so I think we end up with a much better society if we treat all humans as morally equal. This means I end up saying things like \"value all humans equally; don't value animals\" when that's not my real distinction, just the closest <a href=\"http://en.wikipedia.org/wiki/Focal_point_(game_theory)\">schelling point</a>.</p>\n<p>&nbsp;</p>\n<p>[1] Chicken extinction would make life worse for many other people, so I wouldn't actually do that, but not because of the effect on the chickens.</p>\n<p><small><em>I also posted this&nbsp;<a href=\"http://www.jefftk.com/news/2013-07-25\">on my blog</a>.</em></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2rit2BKzWSWsP2yKT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 20, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "23522", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yCWPkLi8wJvewPbEp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T15:21:44.726Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Southeast Michigan", "slug": "new-lw-meetup-southeast-michigan", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZcBLLh6K9QsDGB2Di/new-lw-meetup-southeast-michigan", "pageUrlRelative": "/posts/ZcBLLh6K9QsDGB2Di/new-lw-meetup-southeast-michigan", "linkUrl": "https://www.lesswrong.com/posts/ZcBLLh6K9QsDGB2Di/new-lw-meetup-southeast-michigan", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Southeast%20Michigan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Southeast%20Michigan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcBLLh6K9QsDGB2Di%2Fnew-lw-meetup-southeast-michigan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Southeast%20Michigan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcBLLh6K9QsDGB2Di%2Fnew-lw-meetup-southeast-michigan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcBLLh6K9QsDGB2Di%2Fnew-lw-meetup-southeast-michigan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<p><strong>This summary was posted to LW main on July 19th. The following week's summary is <a href=\"/lw/i5f/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/oo\">Southeast Michigan:&nbsp;<span class=\"date\">21 July 2013 02:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/os\">Atlanta Lesswrong: The Math of Bayes' Theorem:&nbsp;<span class=\"date\">21 July 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/om\">Bratislava Meetup V.:&nbsp;<span class=\"date\">22 July 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/or\">Cincinnati: Financial optimisation:&nbsp;<span class=\"date\">28 July 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/oe\">Helsinki LW meetup:&nbsp;<span class=\"date\">20 July 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/oj\">[Moscow] The Meet up:&nbsp;<span class=\"date\">21 July 2013 04:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">27 July 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ot\">Durham NC/Triangle Area: Meetup!:&nbsp;<span class=\"date\">01 August 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ol\">London Social - Direct Sunlight 2, Vitamin Boogaloo - July 21st :&nbsp;<span class=\"date\">21 July 2013 11:54AM</span></a></li>\n<li><a href=\"/meetups/ou\">Melbourne Excursion: Comfort Zone Expansion:&nbsp;<span class=\"date\">20 July 2013 10:30AM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZcBLLh6K9QsDGB2Di", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.2798092752770445e-06, "legacy": true, "legacyId": "23402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["75nn7krhxd8WKZa4E", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T17:57:52.410Z", "modifiedAt": null, "url": null, "title": "System Administrator Appreciation Day - Thanks Trike!", "slug": "system-administrator-appreciation-day-thanks-trike", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.574Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6TiTLshGiXF5X7cQA/system-administrator-appreciation-day-thanks-trike", "pageUrlRelative": "/posts/6TiTLshGiXF5X7cQA/system-administrator-appreciation-day-thanks-trike", "linkUrl": "https://www.lesswrong.com/posts/6TiTLshGiXF5X7cQA/system-administrator-appreciation-day-thanks-trike", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20System%20Administrator%20Appreciation%20Day%20-%20Thanks%20Trike!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASystem%20Administrator%20Appreciation%20Day%20-%20Thanks%20Trike!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6TiTLshGiXF5X7cQA%2Fsystem-administrator-appreciation-day-thanks-trike%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=System%20Administrator%20Appreciation%20Day%20-%20Thanks%20Trike!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6TiTLshGiXF5X7cQA%2Fsystem-administrator-appreciation-day-thanks-trike", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6TiTLshGiXF5X7cQA%2Fsystem-administrator-appreciation-day-thanks-trike", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>In honor of <a href=\"http://sysadminday.com/\">System Administrator Appreciation Day</a>, this is a post to thank <a href=\"http://trikeapps.com/\">Trike Apps</a>&nbsp;for creating &amp; maintaining Less Wrong. &nbsp;A lot of the time when they are mentioned on Less Wrong, it is to complain about bugs or request new features. &nbsp;So this is the time of year: thanks for everything that continues to silently go right!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "a65Lgr7Q5jqRWHtM6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6TiTLshGiXF5X7cQA", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 71, "baseScore": 110, "extendedScore": null, "score": 1.2799361839051412e-06, "legacy": true, "legacyId": "23524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-05-11T21:00:32.000Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-26T17:58:16.824Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 77-78, Part 2", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TkANx4rSJ3YBXWsgu/meetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2", "pageUrlRelative": "/posts/TkANx4rSJ3YBXWsgu/meetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2", "linkUrl": "https://www.lesswrong.com/posts/TkANx4rSJ3YBXWsgu/meetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2", "postedAtFormatted": "Friday, July 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2077-78%2C%20Part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2077-78%2C%20Part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTkANx4rSJ3YBXWsgu%2Fmeetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2077-78%2C%20Part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTkANx4rSJ3YBXWsgu%2Fmeetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTkANx4rSJ3YBXWsgu%2Fmeetup-durham-rtlw-hpmor-discussion-ch-77-78-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p8'>Durham/RTLW HPMoR discussion, ch. 77-78, Part 2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 July 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Fullsteam for discussion of HPMoR chapters 77-78, Electric Boogaloo.</p>\n\n<p>Feel free to bring coffee from Cocoa Cinnamon (420 <em>West</em> Geer St., right around the corner), food from any convenient food truck, friends, neighbors, etc.</p>\n\n<p>12:00 -- collect consumables, introductions, chat <br />\n12:30 -- discussion! <br />\n2:00 ish -- disperse and/or go play ping pong</p>\n\n<p>If you have a chance to do the reading (or have already done the reading!) think about a few possible discussion questions.  If you don't have a chance to do the reading, as always, you're welcome to join anyway!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p8'>Durham/RTLW HPMoR discussion, ch. 77-78, Part 2</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TkANx4rSJ3YBXWsgu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "23525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_78__Part_2\">Discussion article for the meetup : <a href=\"/meetups/p8\">Durham/RTLW HPMoR discussion, ch. 77-78, Part 2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 July 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Fullsteam for discussion of HPMoR chapters 77-78, Electric Boogaloo.</p>\n\n<p>Feel free to bring coffee from Cocoa Cinnamon (420 <em>West</em> Geer St., right around the corner), food from any convenient food truck, friends, neighbors, etc.</p>\n\n<p>12:00 -- collect consumables, introductions, chat <br>\n12:30 -- discussion! <br>\n2:00 ish -- disperse and/or go play ping pong</p>\n\n<p>If you have a chance to do the reading (or have already done the reading!) think about a few possible discussion questions.  If you don't have a chance to do the reading, as always, you're welcome to join anyway!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_78__Part_21\">Discussion article for the meetup : <a href=\"/meetups/p8\">Durham/RTLW HPMoR discussion, ch. 77-78, Part 2</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 77-78, Part 2", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_78__Part_2", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 77-78, Part 2", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__77_78__Part_21", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T01:08:17.385Z", "modifiedAt": null, "url": null, "title": "A Personal Record, and a Call For Advice:", "slug": "a-personal-record-and-a-call-for-advice", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ambition", "createdAt": "2011-07-20T20:17:59.390Z", "isAdmin": false, "displayName": "Ambition"}, "userId": "WRenqmXnWfRwX7EE6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MigLq7ZnjBny3MmxC/a-personal-record-and-a-call-for-advice", "pageUrlRelative": "/posts/MigLq7ZnjBny3MmxC/a-personal-record-and-a-call-for-advice", "linkUrl": "https://www.lesswrong.com/posts/MigLq7ZnjBny3MmxC/a-personal-record-and-a-call-for-advice", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Personal%20Record%2C%20and%20a%20Call%20For%20Advice%3A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Personal%20Record%2C%20and%20a%20Call%20For%20Advice%3A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMigLq7ZnjBny3MmxC%2Fa-personal-record-and-a-call-for-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Personal%20Record%2C%20and%20a%20Call%20For%20Advice%3A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMigLq7ZnjBny3MmxC%2Fa-personal-record-and-a-call-for-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMigLq7ZnjBny3MmxC%2Fa-personal-record-and-a-call-for-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1850, "htmlBody": "<p><strong>Introduction:</strong></p>\n<p>This is my first post here on LessWrong, so I'll start off by introducing myself.</p>\n<p><span style=\"white-space: pre;\"> </span>My name is Caleb, and I've been reading here on LessWrong for quite some time now. I suspect nobody will be surprised when I say it was HPMOR that brought me here, as it has so many people. I am a Musician, I play the Violin, Piano and somewhat Guitar, (which I need to work on), and my hobbies include recursive self-improvement, reading books, and discovering new ways to improve my life.</p>\n<p><span style=\"white-space: pre;\"> </span>Now, it should be clear from the title that I'm not really offering new insight here, (not quite there yet), but am instead asking the community for advice. I know there are a lot of people out there who stand to benefit from hearing applicable information that doesn't require too much base knowledge of rationality. I'm going to be talking quite a bit about my personal history, mostly for a personal record. I recommend skipping directly to the <strong>\"A Call for Advice\"</strong>&nbsp;section, to get to the part where I actually talk about what information I'm looking for, if you could care less about my history.</p>\n<p>&nbsp;</p>\n<p><strong><span style=\"white-space: pre;\"> </span>My Childhood</strong></p>\n<p><span style=\"white-space: pre;\"> </span>When I was born, my parents were both Christian. My mother came from a family that took the Bible literally. Thus it was a patriarchy, and she grew up having been taught that a \"woman's purpose\" was to serve man. My father came from a \"cherry-picking\" Christian family, and had three separate stepfathers before he turned 10. They were both 21 when I came into existence, and still trying to achieve their goals.</p>\n<p>I was very young, (not quite sure how old), when they became atheists, or at least stopped believing in a god. Because of this, it wasn't particularly hard for me to change perspectives, and I could somewhat follow my father's, \"Just because somebody says he's Superman, that doesn't mean he is\" explanation for me. My brother, Pascal, was born when I was 4, and Atlas soon followed.</p>\n<p><strong><span style=\"white-space: pre;\"> </span>My Education</strong></p>\n<p><span style=\"white-space: pre;\"> </span>We moved often, and so I was taught by my mother until we settled in Hollywood, where my father pursued his dream of becoming a filmmaker. Because of this, I made my first encounter with public schooling in the third grade, where I was quickly made aware of how regular children behaved. My mother had not neglected my education, and I already knew nearly everything my teacher had to offer. It wasn't long before she told me to stop raising my hand, and to not ask any more questions. This did not sit well with my family, and before long they had a little talk with my teacher. I of course still refrained from asking questions, since I was taught to always be considerate of others.</p>\n<p><span style=\"white-space: pre;\"> </span>I hesitate to say things were tough for me, when I know compared to some situations it was hardly difficult at all, but it certainly wasn't easy for me. My social skills were practically non-existent, formed on the basis of being unfailingly polite and friendly to people, and the idea of hurting others for your own gain was alien to me. I would do anything people asked for, which inevitably lead to me being exploited quite a bit. I also stuck out like a sore thumb, standing was several inches taller than my classmates. Unsurprisingly, my lack of religious faith was very disturbing to the children, (and teachers). Luckily, before long I was given a test to measure my aptitude for academic success. I was deemed highly gifted, and transferred to a school with special programs for me to take.</p>\n<p><span style=\"white-space: pre;\"> </span>The new school was much cleaner, with bigger classrooms and more teachers, but the kids did not change much at all. An attempted, \"Hello, I'm Caleb. Would you like to be friends?\", got me a swift shoving from the girl I was talking to, who proceeded to confront me for the remainder of my time there. While my relations with fellow classmates was less than desired, I got along very well with my math teacher, who quickly made it his goal to stretch my limits, which was an annoying change from breezing through school with minimal effort. Every week was met with a new math puzzle, a new time trial, or various unique assignments that trained me to be flexible in language and mathematics.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>While I was shining in one area, I was doing painfully bad in another. Math was like a puzzle for me, with multiple different pieces to combine in order to get the picture I wanted. History was a long boring monologue of information I saw little purpose in, and required mind numbing hours of study. You see, I had been relying on my quick and flexible understanding of concepts when doing work before. History had little use for these things, really only needing a long attention span, and a willingness to work on it for a while. I had neither of these things at the time, and scraped by mainly on special projects and papers.</p>\n<p><span style=\"white-space: pre;\"> </span>Eventually we had to move again, and found a new solution to my schooling, an online course program called k12, where I could work independently online, and work ahead as much as I chose. At this point I very much stopped caring about school at all, avoiding difficult work wherever possible, and cutting corners as much as I could. It was thanks to the intervention of my parents, and their choice of difficult classes for me, that made it difficult for me to simply take the easy road, something I am very grateful for.</p>\n<p>&nbsp;</p>\n<p><strong><span style=\"white-space: pre;\"> </span>Becoming Rational</strong></p>\n<p><span style=\"white-space: pre;\"> </span>Several things were ingrained into me by my family, two of which being my desire for learning, and my never ending quest for self improvement. \"Be the best you can be.\" This is what my father would tell me whenever I'd feel without direction. Unfortunately these ideals were often overridden by simpler desires, such as to avoid work, and experience a good story. My desire for learning ended up being filled by Video Games, where I could immediately excel and feel the enjoyment that comes with achieving things, without the pain of hard work being present.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>One day, my father sent me a PDF, titled \"Harry Potter and the Methods of Rationality\", telling me that it would be like nothing I had ever read before. It went to be one of my favorite novels ever written, and continues to be, but more importantly, it gave me a new perspective. Rationality, the science of winning at life. Finding the hidden rules behind the turnings of the world, the thoughts that run under the surface of consciousness, the ways that one could understand, and apply knowledge to everyday life. If there was ever one event, or thought that truly changed me, it was this.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>Up until this point I had fluctuated from concept to concept. My family would support me in anything that piqued my interest, and so I collected a wide range of talents. Acting, Musical Composing, Drawing, I can't really say what it was that pulled me to these things, but those were what I pursued. A firm believer in the power of research, I could improve at an increased rate, simply by taking advice from people who had already done what I sought to accomplish, a tool too few people used in my opinion; then, there was this. It was like the final piece, to a puzzle I had never fully solved.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>It was this that got me interested in psychology, which I pursued with a passion, and continue to today.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>My Current State</strong></p>\n<p><span style=\"white-space: pre;\"> </span>A lot has changed since that day, and I can happily say that I have improved. My personality type was INFP, although at this point the INTP description fits me much better. In any case, the INFP personality has a very nasty tendency to mentally block off unwanted information. Any thoughts that caused me discomfort, be it the topic of death, or the paper I needed to write, my mind would push back, and then forget, leaving me free to enjoy life without such burdens on my mind. Obviously this lead to major problems, and it was the very first thing that I fixed. (I have very good recall at this point, and I decided to just keep the habit of writing things down when I want to remember them.)</p>\n<p><span style=\"white-space: pre;\"> </span>I also Workout regularly, and also meditate as a part of my daily routine. Thanks to a comment on Cold Thermogenesis, I now take the coldest showers I can, which I have come to enjoy. Non-Fiction reading takes place on a daily basis, and I have resolved myself to take the hardest courses K12 High School will let me take. I study and write music, (with mixed success), and also continue to read here on LessWrong.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>A Call for Advice</strong></p>\n<p><span style=\"white-space: pre;\"> </span>Finally to the actual topic of this post. Now, on August 4th, I will be turning 15, and I've decided to make this the real turning point of my life, through a project I'm calling \"The Grand Plan\" for lack of a better name. (need to work on that)&nbsp;</p>\n<p>The Grand Plan is me researching an enormous multitude of life improvements, useful skills, worthwhile areas of study, and organizing them into a general plan of where to go from here. What things give me a boost in utility the most and the quickest? What time spent on short term gain should be diverted into starting long term goals? What makes a long term goal more worthwhile to start now? What benefits should I be going for first? Should I first improve myself to improve my money making abilities, or should I first improve my money in order to improve myself? Should I start by learning skills that make you better at learning skills? Or should I divert some of that time into learning a skill early?</p>\n<p>It should be pretty clear why this is going to take a while. The sheer scope of the project and amount of information to sort through would have daunted me before, but I've taught myself control, and it will have been a worthwhile experience even if I fail. Besides, at the same time I'm doing useful research on life improvements, and that's always a plus.</p>\n<p>Now, what I'm asking for is quite simple. I'd like to hear from LessWrong what things are of high importance to learn at a young age, or even to learn in general. My ultimate goal is of course to upgrade my wealth, intelligence, and social status in as little time as possible, as much as possible. Anything the community can suggest that is cohesive to that, I would appreciate.&nbsp;</p>\n<p>I'd also like to mention that I am NOT trying to plan out my whole life, that would be suicide. I'm simply trying to figure out what makes sense to do right now. There are too many variables and things I don't know for me to get the perfect answer, but I'd at least like a good one, and working towards that end strikes me as an effective use of my time at this point.</p>\n<p>All suggestions are appreciated, thank you for your time.</p>\n<p>&nbsp;</p>\n<p>-Ambition</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MigLq7ZnjBny3MmxC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "23526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong id=\"Introduction_\">Introduction:</strong></p>\n<p>This is my first post here on LessWrong, so I'll start off by introducing myself.</p>\n<p><span style=\"white-space: pre;\"> </span>My name is Caleb, and I've been reading here on LessWrong for quite some time now. I suspect nobody will be surprised when I say it was HPMOR that brought me here, as it has so many people. I am a Musician, I play the Violin, Piano and somewhat Guitar, (which I need to work on), and my hobbies include recursive self-improvement, reading books, and discovering new ways to improve my life.</p>\n<p><span style=\"white-space: pre;\"> </span>Now, it should be clear from the title that I'm not really offering new insight here, (not quite there yet), but am instead asking the community for advice. I know there are a lot of people out there who stand to benefit from hearing applicable information that doesn't require too much base knowledge of rationality. I'm going to be talking quite a bit about my personal history, mostly for a personal record. I recommend skipping directly to the <strong>\"A Call for Advice\"</strong>&nbsp;section, to get to the part where I actually talk about what information I'm looking for, if you could care less about my history.</p>\n<p>&nbsp;</p>\n<p><strong id=\"_My_Childhood\"><span style=\"white-space: pre;\"> </span>My Childhood</strong></p>\n<p><span style=\"white-space: pre;\"> </span>When I was born, my parents were both Christian. My mother came from a family that took the Bible literally. Thus it was a patriarchy, and she grew up having been taught that a \"woman's purpose\" was to serve man. My father came from a \"cherry-picking\" Christian family, and had three separate stepfathers before he turned 10. They were both 21 when I came into existence, and still trying to achieve their goals.</p>\n<p>I was very young, (not quite sure how old), when they became atheists, or at least stopped believing in a god. Because of this, it wasn't particularly hard for me to change perspectives, and I could somewhat follow my father's, \"Just because somebody says he's Superman, that doesn't mean he is\" explanation for me. My brother, Pascal, was born when I was 4, and Atlas soon followed.</p>\n<p><strong id=\"_My_Education\"><span style=\"white-space: pre;\"> </span>My Education</strong></p>\n<p><span style=\"white-space: pre;\"> </span>We moved often, and so I was taught by my mother until we settled in Hollywood, where my father pursued his dream of becoming a filmmaker. Because of this, I made my first encounter with public schooling in the third grade, where I was quickly made aware of how regular children behaved. My mother had not neglected my education, and I already knew nearly everything my teacher had to offer. It wasn't long before she told me to stop raising my hand, and to not ask any more questions. This did not sit well with my family, and before long they had a little talk with my teacher. I of course still refrained from asking questions, since I was taught to always be considerate of others.</p>\n<p><span style=\"white-space: pre;\"> </span>I hesitate to say things were tough for me, when I know compared to some situations it was hardly difficult at all, but it certainly wasn't easy for me. My social skills were practically non-existent, formed on the basis of being unfailingly polite and friendly to people, and the idea of hurting others for your own gain was alien to me. I would do anything people asked for, which inevitably lead to me being exploited quite a bit. I also stuck out like a sore thumb, standing was several inches taller than my classmates. Unsurprisingly, my lack of religious faith was very disturbing to the children, (and teachers). Luckily, before long I was given a test to measure my aptitude for academic success. I was deemed highly gifted, and transferred to a school with special programs for me to take.</p>\n<p><span style=\"white-space: pre;\"> </span>The new school was much cleaner, with bigger classrooms and more teachers, but the kids did not change much at all. An attempted, \"Hello, I'm Caleb. Would you like to be friends?\", got me a swift shoving from the girl I was talking to, who proceeded to confront me for the remainder of my time there. While my relations with fellow classmates was less than desired, I got along very well with my math teacher, who quickly made it his goal to stretch my limits, which was an annoying change from breezing through school with minimal effort. Every week was met with a new math puzzle, a new time trial, or various unique assignments that trained me to be flexible in language and mathematics.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>While I was shining in one area, I was doing painfully bad in another. Math was like a puzzle for me, with multiple different pieces to combine in order to get the picture I wanted. History was a long boring monologue of information I saw little purpose in, and required mind numbing hours of study. You see, I had been relying on my quick and flexible understanding of concepts when doing work before. History had little use for these things, really only needing a long attention span, and a willingness to work on it for a while. I had neither of these things at the time, and scraped by mainly on special projects and papers.</p>\n<p><span style=\"white-space: pre;\"> </span>Eventually we had to move again, and found a new solution to my schooling, an online course program called k12, where I could work independently online, and work ahead as much as I chose. At this point I very much stopped caring about school at all, avoiding difficult work wherever possible, and cutting corners as much as I could. It was thanks to the intervention of my parents, and their choice of difficult classes for me, that made it difficult for me to simply take the easy road, something I am very grateful for.</p>\n<p>&nbsp;</p>\n<p><strong id=\"_Becoming_Rational\"><span style=\"white-space: pre;\"> </span>Becoming Rational</strong></p>\n<p><span style=\"white-space: pre;\"> </span>Several things were ingrained into me by my family, two of which being my desire for learning, and my never ending quest for self improvement. \"Be the best you can be.\" This is what my father would tell me whenever I'd feel without direction. Unfortunately these ideals were often overridden by simpler desires, such as to avoid work, and experience a good story. My desire for learning ended up being filled by Video Games, where I could immediately excel and feel the enjoyment that comes with achieving things, without the pain of hard work being present.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>One day, my father sent me a PDF, titled \"Harry Potter and the Methods of Rationality\", telling me that it would be like nothing I had ever read before. It went to be one of my favorite novels ever written, and continues to be, but more importantly, it gave me a new perspective. Rationality, the science of winning at life. Finding the hidden rules behind the turnings of the world, the thoughts that run under the surface of consciousness, the ways that one could understand, and apply knowledge to everyday life. If there was ever one event, or thought that truly changed me, it was this.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>Up until this point I had fluctuated from concept to concept. My family would support me in anything that piqued my interest, and so I collected a wide range of talents. Acting, Musical Composing, Drawing, I can't really say what it was that pulled me to these things, but those were what I pursued. A firm believer in the power of research, I could improve at an increased rate, simply by taking advice from people who had already done what I sought to accomplish, a tool too few people used in my opinion; then, there was this. It was like the final piece, to a puzzle I had never fully solved.&nbsp;</p>\n<p><span style=\"white-space: pre;\"> </span>It was this that got me interested in psychology, which I pursued with a passion, and continue to today.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>My Current State</strong></p>\n<p><span style=\"white-space: pre;\"> </span>A lot has changed since that day, and I can happily say that I have improved. My personality type was INFP, although at this point the INTP description fits me much better. In any case, the INFP personality has a very nasty tendency to mentally block off unwanted information. Any thoughts that caused me discomfort, be it the topic of death, or the paper I needed to write, my mind would push back, and then forget, leaving me free to enjoy life without such burdens on my mind. Obviously this lead to major problems, and it was the very first thing that I fixed. (I have very good recall at this point, and I decided to just keep the habit of writing things down when I want to remember them.)</p>\n<p><span style=\"white-space: pre;\"> </span>I also Workout regularly, and also meditate as a part of my daily routine. Thanks to a comment on Cold Thermogenesis, I now take the coldest showers I can, which I have come to enjoy. Non-Fiction reading takes place on a daily basis, and I have resolved myself to take the hardest courses K12 High School will let me take. I study and write music, (with mixed success), and also continue to read here on LessWrong.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>A Call for Advice</strong></p>\n<p><span style=\"white-space: pre;\"> </span>Finally to the actual topic of this post. Now, on August 4th, I will be turning 15, and I've decided to make this the real turning point of my life, through a project I'm calling \"The Grand Plan\" for lack of a better name. (need to work on that)&nbsp;</p>\n<p>The Grand Plan is me researching an enormous multitude of life improvements, useful skills, worthwhile areas of study, and organizing them into a general plan of where to go from here. What things give me a boost in utility the most and the quickest? What time spent on short term gain should be diverted into starting long term goals? What makes a long term goal more worthwhile to start now? What benefits should I be going for first? Should I first improve myself to improve my money making abilities, or should I first improve my money in order to improve myself? Should I start by learning skills that make you better at learning skills? Or should I divert some of that time into learning a skill early?</p>\n<p>It should be pretty clear why this is going to take a while. The sheer scope of the project and amount of information to sort through would have daunted me before, but I've taught myself control, and it will have been a worthwhile experience even if I fail. Besides, at the same time I'm doing useful research on life improvements, and that's always a plus.</p>\n<p>Now, what I'm asking for is quite simple. I'd like to hear from LessWrong what things are of high importance to learn at a young age, or even to learn in general. My ultimate goal is of course to upgrade my wealth, intelligence, and social status in as little time as possible, as much as possible. Anything the community can suggest that is cohesive to that, I would appreciate.&nbsp;</p>\n<p>I'd also like to mention that I am NOT trying to plan out my whole life, that would be suicide. I'm simply trying to figure out what makes sense to do right now. There are too many variables and things I don't know for me to get the perfect answer, but I'd at least like a good one, and working towards that end strikes me as an effective use of my time at this point.</p>\n<p>All suggestions are appreciated, thank you for your time.</p>\n<p>&nbsp;</p>\n<p>-Ambition</p>", "sections": [{"title": "Introduction:", "anchor": "Introduction_", "level": 1}, {"title": " My Childhood", "anchor": "_My_Childhood", "level": 1}, {"title": " My Education", "anchor": "_My_Education", "level": 1}, {"title": " Becoming Rational", "anchor": "_Becoming_Rational", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T02:45:45.320Z", "modifiedAt": null, "url": null, "title": "A Call For Advice:", "slug": "a-call-for-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.462Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ambition", "createdAt": "2011-07-20T20:17:59.390Z", "isAdmin": false, "displayName": "Ambition"}, "userId": "WRenqmXnWfRwX7EE6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jkrJrGTAAf3kfs3Aj/a-call-for-advice", "pageUrlRelative": "/posts/jkrJrGTAAf3kfs3Aj/a-call-for-advice", "linkUrl": "https://www.lesswrong.com/posts/jkrJrGTAAf3kfs3Aj/a-call-for-advice", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Call%20For%20Advice%3A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Call%20For%20Advice%3A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjkrJrGTAAf3kfs3Aj%2Fa-call-for-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Call%20For%20Advice%3A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjkrJrGTAAf3kfs3Aj%2Fa-call-for-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjkrJrGTAAf3kfs3Aj%2Fa-call-for-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 399, "htmlBody": "<p>I'll start off by introducing myself.</p>\n<p>My name is Caleb, I'm currently 14 years old and taking High School. I'm an INTP, I stand 6'2\" tall, and enjoy composing music in my spare time.</p>\n<p>Now, the purpose of this post is not to bring new research or ideas to the table from me, (probably going to take a while before that happens), but rather to request it, for a specific project I'm doing.</p>\n<p>I've been reading LessWrong for about a year and a half now, and in that time I've made a multitude of improvements to my life. Since I came here I've started meditating, working out regularly, reading a number of books on psychology, and becoming more aware of my own biases. Rationality has become my passion, and I work on it daily.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>The Plan</strong><span style=\"white-space: pre;\"> </span></p>\n<p>Onto the purpose of this post. On August 4th, I will be turning 15, and I've decided to initiate a very large project, which for lack of a better name, I will dub \"The Plan\".&nbsp;</p>\n<p>I intend to spend the days leading up to my 15th Birthday by taking information from an enormous variety of sources on what life improvements can be made, what skills are most useful, and what areas should be studied, to reach the ultimate goal of gaining as much benefit possible, as quick as possible.</p>\n<p>There's tons of things to consider, even assuming I have a tireless work ethic and can implement this immediately. What types of utility increases are there? Which are more important? Should all time be devoted to the quickest increases in utility, or should energy be set aside for starting some long term goals early? Does it make more sense to improve yourself, so you can make more money? Or to make some money, and use it to improve yourself?&nbsp;</p>\n<p>Obviously I'm not going to find a perfect answer, and attempting to plan out my whole life is doomed to fail, but I'd at least like to have a better idea of where to go from here. (Besides, even if I fail, I'll still have learned a lots of useful information.)</p>\n<p><span style=\"white-space: pre;\"> </span><strong>The Question</strong></p>\n<p>So, I pose this question to the LessWrong Community:</p>\n<p>If you were me, and turning 15, what would you recommend that I do over the next year, to give me the biggest utility bonus the fastest, both in intelligence and wealth?</p>\n<p>&nbsp;</p>\n<p>Hopefully, even if it proves an impossible question, we'll see some interesting discussion.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jkrJrGTAAf3kfs3Aj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "23528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T04:51:40.244Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup: Vancouver", "slug": "meetup-meetup-vancouver", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hfBXTjMW7eMYH75ez/meetup-meetup-vancouver", "pageUrlRelative": "/posts/hfBXTjMW7eMYH75ez/meetup-meetup-vancouver", "linkUrl": "https://www.lesswrong.com/posts/hfBXTjMW7eMYH75ez/meetup-meetup-vancouver", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%3A%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%3A%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfBXTjMW7eMYH75ez%2Fmeetup-meetup-vancouver%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%3A%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfBXTjMW7eMYH75ez%2Fmeetup-meetup-vancouver", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfBXTjMW7eMYH75ez%2Fmeetup-meetup-vancouver", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/p9'>Meetup: Vancouver</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 July 2013 01:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 W Broadway, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow we will be discussing the Landmark Education forum, a self-improvement workshop attended by some of our members, who reflect positively on it and would like to discuss its merits.\n<a href=\"http://www.landmarkworldwide.com/\" rel=\"nofollow\">http://www.landmarkworldwide.com/</a></p>\n\n<p>Also, two of us have recently returned from the July CFAR workshop, and there will hopefully be discussion of how great that was.</p>\n\n<p>Anyways, please join us for rationality discussion and learning at Benny's Bagels on west Broadway. Note that tomorrow will be meeting at the earlier time of 1:30 PM, instead of the usual 3:30, but we will still be there for the full time we usually chill out there.</p>\n\n<p>Join the mailing list if you're new or haven't already:\nhttps://groups.google.com/forum/#!forum/vancouver-rationalists</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/p9'>Meetup: Vancouver</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hfBXTjMW7eMYH75ez", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.280467864666003e-06, "legacy": true, "legacyId": "23529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup__Vancouver\">Discussion article for the meetup : <a href=\"/meetups/p9\">Meetup: Vancouver</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 July 2013 01:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 W Broadway, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow we will be discussing the Landmark Education forum, a self-improvement workshop attended by some of our members, who reflect positively on it and would like to discuss its merits.\n<a href=\"http://www.landmarkworldwide.com/\" rel=\"nofollow\">http://www.landmarkworldwide.com/</a></p>\n\n<p>Also, two of us have recently returned from the July CFAR workshop, and there will hopefully be discussion of how great that was.</p>\n\n<p>Anyways, please join us for rationality discussion and learning at Benny's Bagels on west Broadway. Note that tomorrow will be meeting at the earlier time of 1:30 PM, instead of the usual 3:30, but we will still be there for the full time we usually chill out there.</p>\n\n<p>Join the mailing list if you're new or haven't already:\nhttps://groups.google.com/forum/#!forum/vancouver-rationalists</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup__Vancouver1\">Discussion article for the meetup : <a href=\"/meetups/p9\">Meetup: Vancouver</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup: Vancouver", "anchor": "Discussion_article_for_the_meetup___Meetup__Vancouver", "level": 1}, {"title": "Discussion article for the meetup : Meetup: Vancouver", "anchor": "Discussion_article_for_the_meetup___Meetup__Vancouver1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T07:20:54.373Z", "modifiedAt": null, "url": null, "title": "Meetup : (Improv?) games meetup", "slug": "meetup-improv-games-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ztjMjM7rXxaqgWP7e/meetup-improv-games-meetup", "pageUrlRelative": "/posts/ztjMjM7rXxaqgWP7e/meetup-improv-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/ztjMjM7rXxaqgWP7e/meetup-improv-games-meetup", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20(Improv%3F)%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20(Improv%3F)%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FztjMjM7rXxaqgWP7e%2Fmeetup-improv-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20(Improv%3F)%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FztjMjM7rXxaqgWP7e%2Fmeetup-improv-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FztjMjM7rXxaqgWP7e%2Fmeetup-improv-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pa'>(Improv?) games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 July 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to do improv games! However, because I will not actually be there and I'm not sure anyone (including me, actually) knows how to run such an event, as a backup it will be a normal games meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pa'>(Improv?) games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ztjMjM7rXxaqgWP7e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2805892804544515e-06, "legacy": true, "legacyId": "23535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Improv___games_meetup\">Discussion article for the meetup : <a href=\"/meetups/pa\">(Improv?) games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 July 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to do improv games! However, because I will not actually be there and I'm not sure anyone (including me, actually) knows how to run such an event, as a backup it will be a normal games meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Improv___games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/pa\">(Improv?) games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : (Improv?) games meetup", "anchor": "Discussion_article_for_the_meetup____Improv___games_meetup", "level": 1}, {"title": "Discussion article for the meetup : (Improv?) games meetup", "anchor": "Discussion_article_for_the_meetup____Improv___games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T14:13:46.502Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Practical Meetup", "slug": "meetup-helsinki-practical-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:29.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cKhAXLZvgZPasiDMG/meetup-helsinki-practical-meetup", "pageUrlRelative": "/posts/cKhAXLZvgZPasiDMG/meetup-helsinki-practical-meetup", "linkUrl": "https://www.lesswrong.com/posts/cKhAXLZvgZPasiDMG/meetup-helsinki-practical-meetup", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Practical%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Practical%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKhAXLZvgZPasiDMG%2Fmeetup-helsinki-practical-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Practical%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKhAXLZvgZPasiDMG%2Fmeetup-helsinki-practical-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKhAXLZvgZPasiDMG%2Fmeetup-helsinki-practical-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pb'>Helsinki Practical Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 August 2013 03:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Kirjasto 10, Elielinaukio, Helsinki</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019ll tackle the theme of <a href=\"https://en.wikipedia.org/wiki/Quantified_Self\" rel=\"nofollow\">Quantified Self</a> \u2013 can measurement and tracking of personal statistics be fun and useful, and how? We\u2019ll go through examples of quantifying, and we\u2019ll learn some very easy methods for doing statistical analysis.</p>\n\n<p>We\u2019re meeting in room 40 in Kirjasto 10. It\u2019s reserved for three hours (15-18), after which we can relocate to either <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a> or the <a href=\"http://www.musiikkitalo.fi\" rel=\"nofollow\">Helsinki Music Centre</a>.</p>\n\n<p>In the future, meetups will probably happen every two weeks.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pb'>Helsinki Practical Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cKhAXLZvgZPasiDMG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2809252911945995e-06, "legacy": true, "legacyId": "23538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Practical_Meetup\">Discussion article for the meetup : <a href=\"/meetups/pb\">Helsinki Practical Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 August 2013 03:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Kirjasto 10, Elielinaukio, Helsinki</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019ll tackle the theme of <a href=\"https://en.wikipedia.org/wiki/Quantified_Self\" rel=\"nofollow\">Quantified Self</a> \u2013 can measurement and tracking of personal statistics be fun and useful, and how? We\u2019ll go through examples of quantifying, and we\u2019ll learn some very easy methods for doing statistical analysis.</p>\n\n<p>We\u2019re meeting in room 40 in Kirjasto 10. It\u2019s reserved for three hours (15-18), after which we can relocate to either <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a> or the <a href=\"http://www.musiikkitalo.fi\" rel=\"nofollow\">Helsinki Music Centre</a>.</p>\n\n<p>In the future, meetups will probably happen every two weeks.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Practical_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/pb\">Helsinki Practical Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Practical Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Practical_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Practical Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Practical_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T16:24:54.515Z", "modifiedAt": null, "url": null, "title": "Meetup : Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases", "slug": "meetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jtyzWKNfWyjT72ZcL/meetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive", "pageUrlRelative": "/posts/jtyzWKNfWyjT72ZcL/meetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive", "linkUrl": "https://www.lesswrong.com/posts/jtyzWKNfWyjT72ZcL/meetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Israel%20(Tel%20Aviv)%20Meetup%3A%20Fun%20and%20Games%20with%20Cognitive%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Israel%20(Tel%20Aviv)%20Meetup%3A%20Fun%20and%20Games%20with%20Cognitive%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjtyzWKNfWyjT72ZcL%2Fmeetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Israel%20(Tel%20Aviv)%20Meetup%3A%20Fun%20and%20Games%20with%20Cognitive%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjtyzWKNfWyjT72ZcL%2Fmeetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjtyzWKNfWyjT72ZcL%2Fmeetup-israel-tel-aviv-meetup-fun-and-games-with-cognitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pc'>Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 August 2013 08:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Adventures Garden (Gan Harpatkaot), Tel Aviv, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is going to be about Cognitive Biases, a whole bunch of socializing, and a rump session.</p>\n\n<p>We will be meeting next to the Adventure Park in Park Hayarkon in Tel Aviv at 8pm.</p>\n\n<p>Here: https://ssl.panoramio.com/photo/19706559</p>\n\n<p>Its next to \"Theatre in the park\".</p>\n\n<p>The main talk will be:</p>\n\n<p>Fun and Games with Cognitive Biases / Gal Hochberg\nExploring and brainstorming cognitive biases, how to recognize them, how to avoid them and how to use them to win. Based on an NYC LW meetup session.</p>\n\n<p>A rump session is (to those unfamiliar with the concept):\nEach participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes.</p>\n\n<p>The schedule:</p>\n\n<p>| Start | End | What |</p>\n\n<p>|---------+-------+--------------------------------------|</p>\n\n<p>| 20:00 | 20:15 | Assembly |</p>\n\n<p>| 20:15 | 21:00 | Main Talk |</p>\n\n<p>| 21:00 | 22:00 | Dinner &amp; Discussion |</p>\n\n<p>| 22:00 | 23:00 | Rump Session (minitalks) |</p>\n\n<p>| 23:00 | ??:?? | End of official programming |</p>\n\n<p>See you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pc'>Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jtyzWKNfWyjT72ZcL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2810320460615586e-06, "legacy": true, "legacyId": "23539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Israel__Tel_Aviv__Meetup__Fun_and_Games_with_Cognitive_Biases\">Discussion article for the meetup : <a href=\"/meetups/pc\">Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 August 2013 08:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Adventures Garden (Gan Harpatkaot), Tel Aviv, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is going to be about Cognitive Biases, a whole bunch of socializing, and a rump session.</p>\n\n<p>We will be meeting next to the Adventure Park in Park Hayarkon in Tel Aviv at 8pm.</p>\n\n<p>Here: https://ssl.panoramio.com/photo/19706559</p>\n\n<p>Its next to \"Theatre in the park\".</p>\n\n<p>The main talk will be:</p>\n\n<p>Fun and Games with Cognitive Biases / Gal Hochberg\nExploring and brainstorming cognitive biases, how to recognize them, how to avoid them and how to use them to win. Based on an NYC LW meetup session.</p>\n\n<p>A rump session is (to those unfamiliar with the concept):\nEach participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes.</p>\n\n<p>The schedule:</p>\n\n<p>| Start | End | What |</p>\n\n<p>|---------+-------+--------------------------------------|</p>\n\n<p>| 20:00 | 20:15 | Assembly |</p>\n\n<p>| 20:15 | 21:00 | Main Talk |</p>\n\n<p>| 21:00 | 22:00 | Dinner &amp; Discussion |</p>\n\n<p>| 22:00 | 23:00 | Rump Session (minitalks) |</p>\n\n<p>| 23:00 | ??:?? | End of official programming |</p>\n\n<p>See you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Israel__Tel_Aviv__Meetup__Fun_and_Games_with_Cognitive_Biases1\">Discussion article for the meetup : <a href=\"/meetups/pc\">Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases</a></h2>", "sections": [{"title": "Discussion article for the meetup : Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases", "anchor": "Discussion_article_for_the_meetup___Israel__Tel_Aviv__Meetup__Fun_and_Games_with_Cognitive_Biases", "level": 1}, {"title": "Discussion article for the meetup : Israel (Tel Aviv) Meetup: Fun and Games with Cognitive Biases", "anchor": "Discussion_article_for_the_meetup___Israel__Tel_Aviv__Meetup__Fun_and_Games_with_Cognitive_Biases1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T18:02:19.802Z", "modifiedAt": null, "url": null, "title": "Norbert Wiener on automation and unemployment", "slug": "norbert-wiener-on-automation-and-unemployment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:06.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2NqaBJjtyMEWCmvhq/norbert-wiener-on-automation-and-unemployment", "pageUrlRelative": "/posts/2NqaBJjtyMEWCmvhq/norbert-wiener-on-automation-and-unemployment", "linkUrl": "https://www.lesswrong.com/posts/2NqaBJjtyMEWCmvhq/norbert-wiener-on-automation-and-unemployment", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Norbert%20Wiener%20on%20automation%20and%20unemployment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANorbert%20Wiener%20on%20automation%20and%20unemployment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NqaBJjtyMEWCmvhq%2Fnorbert-wiener-on-automation-and-unemployment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Norbert%20Wiener%20on%20automation%20and%20unemployment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NqaBJjtyMEWCmvhq%2Fnorbert-wiener-on-automation-and-unemployment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NqaBJjtyMEWCmvhq%2Fnorbert-wiener-on-automation-and-unemployment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1055, "htmlBody": "<p>As a part of my work on the&nbsp;<a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a>&nbsp;project for MIRI, I looked at mathematician <a href=\"http://en.wikipedia.org/wiki/Norbert_Wiener\">Norbert Wiener's</a>&nbsp;predictions about the impact of automation on society, and how he acted based on these predictions.</p>\n<p>I already <a href=\"/lw/i2g/norbert_wieners_paper_some_moral_and_technical/\">wrote about</a>&nbsp;Wiener's <a href=\"http://en.wikipedia.org/wiki/The_Sorcerer's_Apprentice\">\"Sorcerer's Apprentice\"</a>&nbsp;type concerns about the dangers of automation, which parallel MIRI's concerns and Eliezer's concerns about AI risk. But Wiener was also concerned that automation would cause unemployment.</p>\n<p>Coincidentally, immediately after I investigated this, Eliezer wrote <a href=\"/lw/hh4/the_robots_ai_and_unemployment_antifaq/\">The Robots, AI, and Unemployment Anti-FAQ</a>, which argues against the position that Wiener held.</p>\n<p><a id=\"more\"></a></p>\n<p>I found a recent paper titled <a href=\"http://21stcenturywiener.org.previewdns.com/wp-content/uploads/2013/07/NotesonWienersConcernsAbout_SocialImpactOfCybernetics.beta_2.pdf\">Some Notes on Wiener&rsquo;s Concerns about the Social Impact of Cybernetics, the Effects of Automation on Labor, and &ldquo;the Human Use of Human Beings&rdquo;</a>&nbsp;which summarizes Wiener's views on automation and unemployment and how he acted based on them.</p>\n<p><strong>Wiener's prediction</strong></p>\n<p>Wiener believed that unless countermeasures were taken, automation would render low-skilled workers unemployable, precipitating an economic depression of far greater magnitude than the Great Depression of the 1930s:</p>\n<blockquote>\n<p>Wiener voiced his reservations about the potential impact of the new technology on working people in every business and industry, [saying] the second, cybernetic, industrial revolution &ldquo;is [bound] to devalue the human brain, at least in its simpler and more routine decisions [until] the average human being of mediocre attainments or less has nothing to sell that it is worth anyone&rsquo;s money to buy.&rdquo;</p>\n<p>[...]</p>\n&ldquo;It is perfectly clear that [automation] will produce an unemployment situation, in comparison with which&hellip;the depression of the [nineteen] thirties will seem a pleasant joke. This depression will ruin many industries&mdash;possibly even the industries which have taken advantage of the new potentialities.&hellip;&rdquo;</blockquote>\n<p><strong>What Wiener did based on his prediction</strong></p>\n<p>Wiener believed that the problem that he foresaw was so great that he considered giving top priority to mitigating it:</p>\n<blockquote>\n<p>&ldquo;I wondered whether I had not got into a moral situation in which my first duty might be to speak to others concerning material which could be socially harmful.&rdquo;</p>\n</blockquote>\n<p>He attempted to network with labor unions so as to mitigate the problem:</p>\n<blockquote>\n<p>Early in the postwar period, Wiener began an active outreach to organized labor. He made contact with union leaders, but he could not impress union officials the seriousness of the challenges posed by automation. The experience left him frustrated and strongly suspecting that labor leaders had a limited view of the coming realities of automation and few tools for dealing with his larger questions about the future of labor itself.&nbsp;</p>\n<p>[...]</p>\n<p>Wiener&rsquo;s outreach to labor never resulted in the coordinated planning and struggle he knew would be needed to prepare societies for the coming age of automation.</p>\n</blockquote>\n<p><strong>Assessing Wiener's prediction</strong></p>\n<p>Wiener was correct that automation would put some workers out of work in the near term:</p>\n<blockquote>\n<p>And, beginning in the 1960s, the numbers of American industrial workers began their historic decline. In the first wave, more than a million factory workers lost their jobs to automation, including 160,000 members of Walter Reuther&rsquo;s United Automobile Workers union.</p>\n</blockquote>\n<p>However, at a macro-level, and over large time scales, automation doesn't seem to have increased unemployment nearly as much as Wiener seems to have believed. The graph of&nbsp;<a href=\"http://www.tradingeconomics.com/united-states/unemployment-rate\">unemployment from 1950 until present</a>&nbsp;gives the impression that at the time when Wiener expressed his concerns, unemployment hovered around 6%, whereas in later decades it's hovered around 7%. It's interesting that there appears to have been a slight increase, but this could be attributable to outsourcing to foreign countries rather than to automation, and the absolute unemployment rate is very low compared with the 20% unemployment rate from the Great Depression.&nbsp;</p>\n<p>As Eliezer said <a href=\"/lw/hh4/the_robots_ai_and_unemployment_antifaq/\">in his recent post</a></p>\n<blockquote>\n<p>Conventional economic theory says [long term unemployment due to automation] shouldn't happen. Suppose it costs 2 units of labor to produce a hot dog and 1 unit of labor to produce a bun, and that 30 units of labor are producing 10 hot dogs in 10 buns. If automation makes it possible to produce a hot dog using 1 unit of labor instead, conventional economics says that some people should shift from making hot dogs to buns, and the new equilibrium should be 15 hot dogs in 15 buns. On standard economic theory, improved productivity - including from automating away some jobs - should produce increased standards of living, not long-term unemployment.</p>\n</blockquote>\n<p>Wiener seems not to have assimilated conventional economic theory. Specifically, he seems to have been unattuned to the existence of equilibrating influences. As Robin Hanson wrote in his post on <a href=\"http://www.overcomingbias.com/2012/05/eventual-futures.html\">Eventual Futures</a>:</p>\n<blockquote>\n<p>[A] common pattern [is]: project forward a current trend to an extreme, while assuming other things don&rsquo;t change much, and then recommend an action which might make sense if this extreme change were to happen all at once soon.</p>\n<p>This is usually a mistake. The trend may not continue indefinitely. Or, by the time a projected extreme is reached, other changes may have changed the appropriate response. Or, the best response may be to do nothing for a long time, until closer to big consequences. Or, the best response may be to do nothing, ever &ndash; not all negative changes can be profitably resisted.</p>\n</blockquote>\n<p>At least with the benefit of hindsight, Wiener's predictions appear naive.</p>\n<p><strong>Where did Wiener go wrong?</strong></p>\n<p>Wiener seems to have gone wrong in relying on <a href=\"/lesswrong.com/lw/hmb/many_weak_arguments_vs_one_relatively_strong\">one relatively strong argument as opposed to many weak arguments</a>. The argument \"if machines start doing the labor that low skilled workers are qualified to do, then their employers will fire them, and they won't have jobs\" may have seemed strong from the inside. But arguments that feel strong from the inside are often wrong.</p>\n<p>Wiener could have done better by giving more weight to conventional wisdom, by trying harder to understand why others didn't share his concerns (which might have resulted in more exposure to conventional economic theory), and by doing a more detailed study of the historical impact of technological innovations on employment.&nbsp;</p>\n<p>In Chapter 2 of <a href=\"/lw/hxx/some_highlights_from_nate_silvers_the_signal_and/\">Nate Silver's book \"The Signal and the Noise\"</a>, Silver describes <a href=\"http://en.wikipedia.org/wiki/Philip_E._Tetlock\">Philip Tetlock's</a> study of expert political judgment, and Tetlock's findings about characteristics of relatively successful political expert predictors. Two characteristics that he highlights as helpful are</p>\n<ul>\n<li><strong>Being multidisciplinary</strong>&nbsp;&mdash; Incorporating ideas from different disciplines.</li>\n<li><strong>Being empirical</strong>&nbsp;&mdash;&nbsp;Relying more on observation than on theory.</li>\n</ul>\n<p>If Wiener had approached the question of whether automation will lead to large scale unemployment in a more multidisciplinary and empirical way, he might have made a better prediction.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2NqaBJjtyMEWCmvhq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "23540", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As a part of my work on the&nbsp;<a href=\"/r/discussion/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">\"Can we know what to do about AI?\"</a>&nbsp;project for MIRI, I looked at mathematician <a href=\"http://en.wikipedia.org/wiki/Norbert_Wiener\">Norbert Wiener's</a>&nbsp;predictions about the impact of automation on society, and how he acted based on these predictions.</p>\n<p>I already <a href=\"/lw/i2g/norbert_wieners_paper_some_moral_and_technical/\">wrote about</a>&nbsp;Wiener's <a href=\"http://en.wikipedia.org/wiki/The_Sorcerer's_Apprentice\">\"Sorcerer's Apprentice\"</a>&nbsp;type concerns about the dangers of automation, which parallel MIRI's concerns and Eliezer's concerns about AI risk. But Wiener was also concerned that automation would cause unemployment.</p>\n<p>Coincidentally, immediately after I investigated this, Eliezer wrote <a href=\"/lw/hh4/the_robots_ai_and_unemployment_antifaq/\">The Robots, AI, and Unemployment Anti-FAQ</a>, which argues against the position that Wiener held.</p>\n<p><a id=\"more\"></a></p>\n<p>I found a recent paper titled <a href=\"http://21stcenturywiener.org.previewdns.com/wp-content/uploads/2013/07/NotesonWienersConcernsAbout_SocialImpactOfCybernetics.beta_2.pdf\">Some Notes on Wiener\u2019s Concerns about the Social Impact of Cybernetics, the Effects of Automation on Labor, and \u201cthe Human Use of Human Beings\u201d</a>&nbsp;which summarizes Wiener's views on automation and unemployment and how he acted based on them.</p>\n<p><strong id=\"Wiener_s_prediction\">Wiener's prediction</strong></p>\n<p>Wiener believed that unless countermeasures were taken, automation would render low-skilled workers unemployable, precipitating an economic depression of far greater magnitude than the Great Depression of the 1930s:</p>\n<blockquote>\n<p>Wiener voiced his reservations about the potential impact of the new technology on working people in every business and industry, [saying] the second, cybernetic, industrial revolution \u201cis [bound] to devalue the human brain, at least in its simpler and more routine decisions [until] the average human being of mediocre attainments or less has nothing to sell that it is worth anyone\u2019s money to buy.\u201d</p>\n<p>[...]</p>\n\u201cIt is perfectly clear that [automation] will produce an unemployment situation, in comparison with which\u2026the depression of the [nineteen] thirties will seem a pleasant joke. This depression will ruin many industries\u2014possibly even the industries which have taken advantage of the new potentialities.\u2026\u201d</blockquote>\n<p><strong id=\"What_Wiener_did_based_on_his_prediction\">What Wiener did based on his prediction</strong></p>\n<p>Wiener believed that the problem that he foresaw was so great that he considered giving top priority to mitigating it:</p>\n<blockquote>\n<p>\u201cI wondered whether I had not got into a moral situation in which my first duty might be to speak to others concerning material which could be socially harmful.\u201d</p>\n</blockquote>\n<p>He attempted to network with labor unions so as to mitigate the problem:</p>\n<blockquote>\n<p>Early in the postwar period, Wiener began an active outreach to organized labor. He made contact with union leaders, but he could not impress union officials the seriousness of the challenges posed by automation. The experience left him frustrated and strongly suspecting that labor leaders had a limited view of the coming realities of automation and few tools for dealing with his larger questions about the future of labor itself.&nbsp;</p>\n<p>[...]</p>\n<p>Wiener\u2019s outreach to labor never resulted in the coordinated planning and struggle he knew would be needed to prepare societies for the coming age of automation.</p>\n</blockquote>\n<p><strong id=\"Assessing_Wiener_s_prediction\">Assessing Wiener's prediction</strong></p>\n<p>Wiener was correct that automation would put some workers out of work in the near term:</p>\n<blockquote>\n<p>And, beginning in the 1960s, the numbers of American industrial workers began their historic decline. In the first wave, more than a million factory workers lost their jobs to automation, including 160,000 members of Walter Reuther\u2019s United Automobile Workers union.</p>\n</blockquote>\n<p>However, at a macro-level, and over large time scales, automation doesn't seem to have increased unemployment nearly as much as Wiener seems to have believed. The graph of&nbsp;<a href=\"http://www.tradingeconomics.com/united-states/unemployment-rate\">unemployment from 1950 until present</a>&nbsp;gives the impression that at the time when Wiener expressed his concerns, unemployment hovered around 6%, whereas in later decades it's hovered around 7%. It's interesting that there appears to have been a slight increase, but this could be attributable to outsourcing to foreign countries rather than to automation, and the absolute unemployment rate is very low compared with the 20% unemployment rate from the Great Depression.&nbsp;</p>\n<p>As Eliezer said <a href=\"/lw/hh4/the_robots_ai_and_unemployment_antifaq/\">in his recent post</a></p>\n<blockquote>\n<p>Conventional economic theory says [long term unemployment due to automation] shouldn't happen. Suppose it costs 2 units of labor to produce a hot dog and 1 unit of labor to produce a bun, and that 30 units of labor are producing 10 hot dogs in 10 buns. If automation makes it possible to produce a hot dog using 1 unit of labor instead, conventional economics says that some people should shift from making hot dogs to buns, and the new equilibrium should be 15 hot dogs in 15 buns. On standard economic theory, improved productivity - including from automating away some jobs - should produce increased standards of living, not long-term unemployment.</p>\n</blockquote>\n<p>Wiener seems not to have assimilated conventional economic theory. Specifically, he seems to have been unattuned to the existence of equilibrating influences. As Robin Hanson wrote in his post on <a href=\"http://www.overcomingbias.com/2012/05/eventual-futures.html\">Eventual Futures</a>:</p>\n<blockquote>\n<p>[A] common pattern [is]: project forward a current trend to an extreme, while assuming other things don\u2019t change much, and then recommend an action which might make sense if this extreme change were to happen all at once soon.</p>\n<p>This is usually a mistake. The trend may not continue indefinitely. Or, by the time a projected extreme is reached, other changes may have changed the appropriate response. Or, the best response may be to do nothing for a long time, until closer to big consequences. Or, the best response may be to do nothing, ever \u2013 not all negative changes can be profitably resisted.</p>\n</blockquote>\n<p>At least with the benefit of hindsight, Wiener's predictions appear naive.</p>\n<p><strong id=\"Where_did_Wiener_go_wrong_\">Where did Wiener go wrong?</strong></p>\n<p>Wiener seems to have gone wrong in relying on <a href=\"/lesswrong.com/lw/hmb/many_weak_arguments_vs_one_relatively_strong\">one relatively strong argument as opposed to many weak arguments</a>. The argument \"if machines start doing the labor that low skilled workers are qualified to do, then their employers will fire them, and they won't have jobs\" may have seemed strong from the inside. But arguments that feel strong from the inside are often wrong.</p>\n<p>Wiener could have done better by giving more weight to conventional wisdom, by trying harder to understand why others didn't share his concerns (which might have resulted in more exposure to conventional economic theory), and by doing a more detailed study of the historical impact of technological innovations on employment.&nbsp;</p>\n<p>In Chapter 2 of <a href=\"/lw/hxx/some_highlights_from_nate_silvers_the_signal_and/\">Nate Silver's book \"The Signal and the Noise\"</a>, Silver describes <a href=\"http://en.wikipedia.org/wiki/Philip_E._Tetlock\">Philip Tetlock's</a> study of expert political judgment, and Tetlock's findings about characteristics of relatively successful political expert predictors. Two characteristics that he highlights as helpful are</p>\n<ul>\n<li><strong>Being multidisciplinary</strong>&nbsp;\u2014 Incorporating ideas from different disciplines.</li>\n<li><strong>Being empirical</strong>&nbsp;\u2014&nbsp;Relying more on observation than on theory.</li>\n</ul>\n<p>If Wiener had approached the question of whether automation will lead to large scale unemployment in a more multidisciplinary and empirical way, he might have made a better prediction.</p>", "sections": [{"title": "Wiener's prediction", "anchor": "Wiener_s_prediction", "level": 1}, {"title": "What Wiener did based on his prediction", "anchor": "What_Wiener_did_based_on_his_prediction", "level": 1}, {"title": "Assessing Wiener's prediction", "anchor": "Assessing_Wiener_s_prediction", "level": 1}, {"title": "Where did Wiener go wrong?", "anchor": "Where_did_Wiener_go_wrong_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t7gKX9Av5zggsQsYr", "2rWfmahhqASnFcYLr", "ZiRKzx3yv7NyA5rjF", "rGj2K8vu5qQCTWCar"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-27T19:26:59.792Z", "modifiedAt": null, "url": null, "title": "Alternative to Bayesian Score", "slug": "alternative-to-bayesian-score", "viewCount": null, "lastCommentedAt": "2021-06-11T13:47:45.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FZ8SfWktLM4sg833g/alternative-to-bayesian-score", "pageUrlRelative": "/posts/FZ8SfWktLM4sg833g/alternative-to-bayesian-score", "linkUrl": "https://www.lesswrong.com/posts/FZ8SfWktLM4sg833g/alternative-to-bayesian-score", "postedAtFormatted": "Saturday, July 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alternative%20to%20Bayesian%20Score&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlternative%20to%20Bayesian%20Score%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFZ8SfWktLM4sg833g%2Falternative-to-bayesian-score%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alternative%20to%20Bayesian%20Score%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFZ8SfWktLM4sg833g%2Falternative-to-bayesian-score", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFZ8SfWktLM4sg833g%2Falternative-to-bayesian-score", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 951, "htmlBody": "<p>I am starting to wonder whether or not Bayes Score is what I want to maximize for my epistemic rationality. I started thinking about by trying to design a board game to teach calibration of probabilities, so I will use that as my example:</p>\n<p>I wanted a scoring mechanism which motivates honest reporting of probabilities and rewards players who are better calibrated. For simplicity, lets assume that we only have to deal with true/false questions for now. A player is given a question which they believe is true with probability p. They then name a real number x between 0 and 1. Then, they receive a score which is a function of x and whether or not the problem is true. We want the expected score to be maximized exactly when x=p. Let f(x) be the output if the question is true, and let g(x) be the output if the question is false. Then, my expected utility is (p)f(x)+(1-p)g(x). If we assume f and g are smooth, then in order to have a maximum at x=p, we want (p)f'(p)+(1-p)g'(p)=0, which still leaves us with a large class of functions. It would also be nice to have symmetry by having f(x)=g(1-x). If we further require this, we get (p)f'(p)+(1-p)(-1)f'(1-p)=0, or equivalently (p)f'(p)=(1-p)f'(1-p). One way to achieve this is to set (x)f'(x) to be a constant So then, f'(x)=c/x, so f(x)=log x. This scoring mechanism is referred to as \"Bayesian Score\".</p>\n<p>However, another natural way to to achieve this is by setting f'(p)/(1-p) equal to a constant. If we set this constant equal to 2, we get f'(x)=2-2x, which gives us f(x)=2x-x<sup>2</sup>=1-(1-x)<sup>2</sup>. I will call this the \"Squared Error Score.\"</p>\n<p>There are many other functions which satisfy the desired conditions, but these two are the simplest, so I will focus on these two.</p>\n<p>Eliezer argues for Bayesian Score in <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>, which I recommend reading. The reason he prefers Bayesian Score is that he wants the sum of the scores associated with determining P(A) and P(B|A) to equal the score for determining P(A&amp;B). In other words he wants it to not matter whether you break a problem up into one experiment or two experiments. This is a legitimate virtue of this scoring mechanism, but I think that many people think it is a lot more valuable than it is. This doesn't eliminate the problem of we don't know what questions to ask. It gives us the same answer regardless of how we break up an experiment into smaller experiments, but our score is still dependent on what questions are asked, and this cannot be fixed by just saying, \"Ask all questions.\" There are infinitely many of them. The sum does not converge. Because the score is still a function of what questions are asked, the fact that it gives the same answer for some related sets of questions is not a huge benefit.</p>\n<p>One nice thing about the&nbsp;Squared Error&nbsp;Score is that it always gives a score between 0 and 1, which means we can actually use it in real life. For example, we could ask someone to construct a spinner that comes up either true or false, and then spin it twice. They win if either of the two spins comes up with the true answer. In this case, the best strategy is to assign probability p to true. There is no way to do anything similar for the Bayesian Score, in fact it is questionable whether or not arbitrary low utilities even make sense.</p>\n<p>The Bayesian Score is slightly easier to generalize to multiple choice questions. The&nbsp;Squared Error&nbsp;Score can also be generalized, but it unfortunately has to make your score a function not only of the probability you assigned to the correct solution. For example, If A is the correct answer, you get more points for 80%A 10%B 10%C than from 80%A 20%B 0%C. The function you want for multiple values is if you assign probabilities x<sub>1</sub>, through x<sub>n</sub>, and the first option is correct you get output 2x<sub>1</sub>-x<span style=\"font-size: 10.857142448425293px;\">1</span><sup>2</sup>-x<sub>2</sub><sup>2</sup>-...-x<sub>n</sub><sup>2</sup>. I do not think this is as bad as it seems. It kind of makes sense that when the answer is A, you get penalized slightly for saying that you are much more confident in B than in C, since making such a claim is a waste of information. To view this as a spinner, you construct a spinner, spin it twice, and you win if either spin gets the correct answer, or if the first spin comes lexicographically strictly before the second spin.</p>\n<p>For the purpose of my calibration game, I will almost certainly use&nbsp;Squared Error&nbsp;Scoring, because log is not feasible. But it got me thinking about why I am not thinking in terms of&nbsp;Squared Error&nbsp;Score in real life.</p>\n<p>You might ask what is the experimental difference between the two, since they are both maximized by honest probabilities. Well If I have two questions and I want to maximize my (possibly weighted) average score, and I have a limited amount of time to research and improve my answers for them, then it matters how much the scoring mechanism penalizes various errors. Bayesian Scoring penalizes so much for being sure of one false thing that none of the other scores really matter, while&nbsp;Squared Error&nbsp;is much more forgiving. If we normalize to say that 50/50 gives 0 points while true certainty gives 1 points, then&nbsp;Squared Error&nbsp;gives -3 points for false certainty while Bayesian gives negative infinity.</p>\n<p>I view maximizing Bayesian Score as the Golden Rule of epistemic rationality, so even a small chance that something else might be better is worth investigating. Even if you are fully committed to Bayesian Score, I would love to hear any pros or cons you can think of in either direction.</p>\n<p>(Edited for formatting)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XtnxF4TsAEyWBRJZq": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FZ8SfWktLM4sg833g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "23542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-28T02:04:48.937Z", "modifiedAt": null, "url": null, "title": "Pareto improvement in gym norms: Spread the word!", "slug": "pareto-improvement-in-gym-norms-spread-the-word", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:09.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q9Kk73R2vg6m4bQwB/pareto-improvement-in-gym-norms-spread-the-word", "pageUrlRelative": "/posts/Q9Kk73R2vg6m4bQwB/pareto-improvement-in-gym-norms-spread-the-word", "linkUrl": "https://www.lesswrong.com/posts/Q9Kk73R2vg6m4bQwB/pareto-improvement-in-gym-norms-spread-the-word", "postedAtFormatted": "Sunday, July 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pareto%20improvement%20in%20gym%20norms%3A%20Spread%20the%20word!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APareto%20improvement%20in%20gym%20norms%3A%20Spread%20the%20word!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9Kk73R2vg6m4bQwB%2Fpareto-improvement-in-gym-norms-spread-the-word%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pareto%20improvement%20in%20gym%20norms%3A%20Spread%20the%20word!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9Kk73R2vg6m4bQwB%2Fpareto-improvement-in-gym-norms-spread-the-word", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9Kk73R2vg6m4bQwB%2Fpareto-improvement-in-gym-norms-spread-the-word", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 635, "htmlBody": "<p>This article is in a superposition of tongue-in-cheek and tongue straight in the mouth. (That's a Norwegian expression meaning \"to concentrate on something difficult\".) If you read it, please report your experimental observation of which it is, so that we can determine the amplitudes of the two states. However, I am actually making a serious point: Why do we have this non-optimal norm, and can we change it?&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Gyms, at least the ones I've been in, seem to have a norm that each user should wipe his own sweat off the machine he just used. This is <em>obviously&nbsp;</em>inefficient. Consider that there are two kinds of users: Sensible, rational people (SRPs) who don't give a damn about other people's sweat on the machine; and finicky fussbudget frumpy failures (4Fs) (names chosen at random out of a hat, and completely unrelated to my own opinion on the point) who are too precious to have anyone else's sweat in their immediate vicinity; it's not as though they're going to shower after their exercise, right? Anyway. Under the existing norm, everyone has to clean once per machine use, but only the 4Fs are getting any utilons. Clearly, if we switch to a norm that everyone <em>optionally</em> cleans the machine they're <em>about to use,</em>&nbsp;then the SRPs are saved some work, while the 4Fs still get to use clean machines. This is an obvious Pareto improvement. Moreover, it's also a Nash equilibrium (and, incidentally, the current norm is a puzzling failure of the usual rule of thumb that social arrangements are Nash equilibria - why have we chosen this particular activity as one where we put effort into pushing people away from the equilibrium?) since nobody can improve his situation by cleaning the machine after using it, or failing to clean beforehand.&nbsp;</p>\n<p>Please spread the word of this obvious improvement in gym-users' quality of life! Also, please push society towards the Nash equilibrium by defecting from the current norm: Either clean your machine before, not after, using it, or else don't clean it at all. If anyone challenges you, give them a quick lecture on economics - this has the added benefit of making you popular with the opposite sex.&nbsp;</p>\n<p>Some possible objections:</p>\n<p>1. <em>My mother taught me to clean up after myself.</em></p>\n<p>And imagine how much more pleasant your childhood would have been, if only you'd known about Nash equilibria and Pareto improvements! However, not all is lost: You can still try to convince your SO or roommate that the one who cares most about mess should be the one to clean it up.&nbsp;</p>\n<p>2. <em>My utility function has a term for not making others do work.</em></p>\n<p>Also, apparently, for signalling your concern for others. The total amount of work done is rather less in my proposed new equilibrium. Suggest you update accordingly.&nbsp;</p>\n<p>3. <em>I prefer cleaning up my own sweat to cleaning that of others.</em></p>\n<p>Have you considered the benefits of self-modifying to be more masochistic? Today's society offers all kinds of opportunities for turning yourself on, if only you could take advantage! This could actually be more efficient than taking a pill that makes you bisexual, since you can only sleep with so many people in one lifetime anyway. Repeat after me: <em>Thank you for making me clean the machine, Master! Please may I clean another? </em>There, do you feel the surge of hormones?&nbsp;</p>\n<p>4. <em>If I have to clean the machine, everyone else should too!</em></p>\n<p>Until the rest of society has self-modified to be sufficiently masochistic to derive pleasure from your dominance, you should not attempt to impose it on them. This aside, have you considered the benefits of suggesting suitable punishments for anyone who <em>doesn't</em> clean their machine? Aren't they being rather <em>naughty</em>? Many exciting encounters may result from this handy ice-breaker!&nbsp;</p>\n<p>5. <em>My gym doesn't have that norm.</em></p>\n<p>Excellent! Please spread the word. Today your gym, tomorrow mine!&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q9Kk73R2vg6m4bQwB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -13, "extendedScore": null, "score": 1.2815043329384443e-06, "legacy": true, "legacyId": "23451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-28T09:07:19.427Z", "modifiedAt": null, "url": null, "title": "Anthropics and a cosmic immune system", "slug": "anthropics-and-a-cosmic-immune-system", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SxRPHizoALmfS5m4R/anthropics-and-a-cosmic-immune-system", "pageUrlRelative": "/posts/SxRPHizoALmfS5m4R/anthropics-and-a-cosmic-immune-system", "linkUrl": "https://www.lesswrong.com/posts/SxRPHizoALmfS5m4R/anthropics-and-a-cosmic-immune-system", "postedAtFormatted": "Sunday, July 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropics%20and%20a%20cosmic%20immune%20system&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropics%20and%20a%20cosmic%20immune%20system%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSxRPHizoALmfS5m4R%2Fanthropics-and-a-cosmic-immune-system%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropics%20and%20a%20cosmic%20immune%20system%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSxRPHizoALmfS5m4R%2Fanthropics-and-a-cosmic-immune-system", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSxRPHizoALmfS5m4R%2Fanthropics-and-a-cosmic-immune-system", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 381, "htmlBody": "<p>Some people like to assume that the cosmos is ours for the taking, even though this could make us special to the order of 1 in 10<sup>80</sup>. The argument is that the cosmos <em>could be</em> transformed by technology - engineered on astronomical scales - but <em>hasn't been</em> thus transformed.</p>\n<p>The most common alternative hypothesis is that \"we are in a simulation\". Perhaps we are. But there are other possibilities too.</p>\n<p>One is that technological life usually destroys, not just its homeworld, but its whole bubble of space-time, by using high-energy physics to cause a <a href=\"http://en.wikipedia.org/wiki/False_vacuum\">\"vacuum decay\"</a>, in which physics changes in a way that makes space uninhabitable. For example, the mass of an elementary particle is essentially equal to the energy density of the Higgs field, times a quantity called a \"yukawa coupling\". If the Higgs field increased its energy density by orders of magnitude, but the yukawas stayed the same, matter as we know it would be destroyed, everywhere that the change spread.</p>\n<p>Here I want to highlight a different possibility. The idea is that the universe contains very large lifeforms and very small lifeforms. We are among the small. The large ones are, let's say, mostly dark matter, galactic in scale, and stars and planets for them are like biomolecules for us; tiny functional elements which go together to make up the whole. And - the crucial part - they have immune systems which automatically crush anything which interferes with the natural celestial order.</p>\n<p>This is why the skies are full of untamed stars rather than Dyson spheres - any small life which begins to act on that scale is destroyed by dark-matter antibodies. And it explains anthropically why you're human-size rather than galactic-size: small life is more numerous than large life, just not so numerous as cosmic colonization would imply.</p>\n<p>Two questions arise - how did large life evolve, and, shouldn't anthropics favor universes which have no large life, just space-colonizing small life? I could spin a story about cosmological natural selection, and <a href=\"http://evodevouniverse.com/wiki/Meduso-anthropic_principle\">large life which uses small life to reproduce</a>, but it doesn't really answer the second question, in particular. Still, I feel that this is a huge unexplored topic - the anthropic consequences of \"biocosmic\" ecology and evolution - and who knows what else is lurking here, waiting to be discovered?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SxRPHizoALmfS5m4R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": -10, "extendedScore": null, "score": 1.2818486253556904e-06, "legacy": true, "legacyId": "23544", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-28T18:24:58.354Z", "modifiedAt": "2019-12-04T05:51:14.972Z", "url": null, "title": "Arguments Against Speciesism", "slug": "arguments-against-speciesism", "viewCount": null, "lastCommentedAt": "2021-01-17T18:09:59.390Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lukas_Gloor", "createdAt": "2012-06-10T19:33:54.240Z", "isAdmin": false, "displayName": "Lukas_Gloor"}, "userId": "c8sYrDHyjxX8MNoxi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tm2i5r2rNSfeam5or/arguments-against-speciesism", "pageUrlRelative": "/posts/tm2i5r2rNSfeam5or/arguments-against-speciesism", "linkUrl": "https://www.lesswrong.com/posts/tm2i5r2rNSfeam5or/arguments-against-speciesism", "postedAtFormatted": "Sunday, July 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arguments%20Against%20Speciesism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArguments%20Against%20Speciesism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftm2i5r2rNSfeam5or%2Farguments-against-speciesism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arguments%20Against%20Speciesism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftm2i5r2rNSfeam5or%2Farguments-against-speciesism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftm2i5r2rNSfeam5or%2Farguments-against-speciesism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2657, "htmlBody": "<p>There have been some posts about animals lately, for instance <a title=\"here\" href=\"/lw/hox/effective_altruism_through_advertising\" target=\"_blank\">here</a>&nbsp;and <a title=\"here\" href=\"/lw/i3s/why_eat_less_meat/\" target=\"_blank\">here</a>. While normative assumptions about the treatment of nonhumans played an important role in the articles and were debated at length in the comment sections, I was missing a concise summary of these arguments. <a title=\"This\" href=\"/lw/757/the_ethical_status_of_nonhuman_animals/\" target=\"_blank\">This</a>&nbsp;post from over a year ago comes closest to what I have in mind, but I want to focus on some of the issues in more detail.</p>\n<p>A while back, I read the following comment in a LessWrong <a title=\"discussion on uploads\" href=\"/lw/d80/malthusian_copying_mass_death_of_unhappy\" target=\"_blank\">discussion on uploads</a>:</p>\n<blockquote>\n<p>I do not at all understand this PETA-like obsession with ethical treatment of bits.</p>\n</blockquote>\n<p>Aside from (carbon-based) humans, which other beings deserve moral consideration? Nonhuman animals? Intelligent aliens? Uploads? Nothing else?</p>\n<p>This article is intended to shed light on these questions; it is however not the intent of this post to advocate a specific ethical framework. Instead, I'll try to show that some ethical principles held by a lot of people are inconsistent with some of their other attitudes -- an argument that doesn't rely on ethics being universal or objective.&nbsp;</p>\n<p>More precisely, I will develop the arguments behind anti-speciesism (and the rejection of analogous forms of discrimination, such as discrimination against uploads) to point out common inconsistencies in some people's values. This will also provide an illustrative example of how <a title=\"coherentist ethical reasoning\" href=\"http://plato.stanford.edu/entries/reflective-equilibrium/\" target=\"_blank\">coherentist ethical reasoning</a>&nbsp;can be applied to shared intuitions. If there are no shared intuitions, ethical discourse will likely be unfruitful, so it is likely that not everyone will draw the same conclusions from the arguments here.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>What Is Speciesism?</h2>\n<p>Speciesism, a term popularized (but not coined) by the philosopher Peter Singer, is meant to be analogous to sexism or racism. It refers to a discriminatory attitude against a being where less ethical consideration i.e. caring less about a being's welfare or interests is given solely because of the \"wrong\" species membership. The \"solely\" here is crucial, and it's misunderstood often enough to warrant the redundant emphasis.</p>\n<p>For instance, it is not speciesist to deny pigs the right to vote, just like it is not sexist to deny men the right to have an abortion performed on their body. Treating beings of different species differently is not speciesist if there are <em>relevant criteria</em> for doing so.&nbsp;</p>\n<p>Singer summarized his case against speciesism in <a title=\"this essay\" href=\"http://www.animal-rights-library.com/texts-m/singer02.htm\" target=\"_blank\">this essay</a>. The argument that does most of the work is often referred to as the&nbsp;<a title=\"argument from marginal cases\" href=\"http://www.iep.utm.edu/anim-eth/#SSH3ai\" target=\"_blank\">argument from marginal cases</a>. A perhaps less anthropocentric, more fitting name would be argument from species overlap, as some philosophers (e.g. Oscar <a title=\"Horta\" href=\"http://masalladelaespecie.wordpress.com/2010/05/22/what-is-speciesism/\" target=\"_blank\">Horta</a>)&nbsp;have pointed out.&nbsp;</p>\n<p>The argument boils down to the question of choosing relevant criteria for moral concern. What properties do human beings possess that makes us think that it is wrong to torture them? Or to kill them? (Note that these are two different questions.) The argument from species overlap points out that all the typical or plausible suggestions for relevant criteria apply equally to dogs, pigs or chickens as they do to human infants or late-stage Alzheimer patients. Therefore, giving less ethical consideration to the former would be based merely on species membership, which is just as arbitrary as choosing race or sex as relevant criterion (further justification for that claim follows below).</p>\n<p>Here are some examples for commonly suggested criteria. Those who want may pause at this point and think about the criteria they consult for whether it is wrong to inflict suffering on a being (and separately, those that are relevant for the wrongness of killing).</p>\n<p>&nbsp;</p>\n<p>The suggestions are:</p>\n<p>A: Capacity for moral reasoning</p>\n<p>B: Being able to reciprocate</p>\n<p>C: (Human-like) intelligence</p>\n<p>D: Self-awareness</p>\n<p>E: Future-related preferences; future plans</p>\n<p>E': Preferences / interests (in general)</p>\n<p>F: Sentience (capacity for suffering and happiness)</p>\n<p>G: Life / biological complexity</p>\n<p>H: What I care about / feel sympathy or loyalty towards</p>\n<p>&nbsp;</p>\n<p>The argument from species overlap points out that not all humans are equal. The sentiment behind \"all humans are equal\" is not that they are literally equal, but that equal interests/capacities deserve equal consideration. None of the above criteria except (in some empirical cases) H imply that human infants or late stage demented people should be given more ethical consideration than cows, pigs or chickens.</p>\n<p>While H is an unlikely criterion for direct ethical consideration (it could justify genocide in specific circumstances!), it is an important indirect factor. Most humans have much more empathy for fellow humans than for nonhuman animals. While this is not a criterion for giving humans more ethical consideration per se, it is nevertheless a factor that strongly influences ethical decision-making in real-life.</p>\n<p>However, such factors can't apply for ethical reasoning at a theoretical/normative level, where all the relevant variables are looked at in isolation in order to come up with a consistent ethical framework that covers all possible cases.</p>\n<p>If there were no intrinsic reasons for giving moral consideration to babies, then a society in which some babies were (factory-)farmed would be totally fine as long as the people are okay with it. If we consider this implication to be unacceptable, then the same must apply for the situations nonhuman animals find themselves in on farms.</p>\n<p><em>Side note</em>: The question whether killing a given being is wrong, and if so, \"why\" and \"how wrong exactly\", is complex and outside the scope of this article. Instead of on killing, the focus will be on suffering, and by suffering I mean something like wanting to get out of one's current conscious state, or wanting to change some aspect about it. The empirical issue of which beings are capable of suffering is a different matter that I will (only briefly) discuss below. So in this context, giving a being moral consideration means that we don't want it to suffer, leaving open the question whether killing it painlessly is bad/neutral/good or prohibited/permissible/obligatory.&nbsp;</p>\n<p>The main conclusion so far is that if we care about all the suffering of members of the human species, and if we reject question-begging reasoning that could also be used to justify racism or other forms of discrimination, then we must also care fully about suffering happening in nonhuman animals. This would imply that x amount of suffering is just as bad, i.e. that we care about it just as much, in nonhuman animals as in humans, or in aliens or in uploads. (Though admittedly the latter wouldn't be anti-speciesist but rather anti-\"substratist\", or anti-\"fleshist\".)</p>\n<p>The claim is that there is no way to block this conclusion without:</p>\n<p><span style=\"white-space: pre;\"> </span>1. using reasoning that could analogically be used to justify racism or sexism<br /><span style=\"white-space: pre;\"> </span>or<br /><span style=\"white-space: pre;\"> </span>2. using reasoning that allows for hypothetical circumstances where it would be okay (or even called for) to torture babies in<span style=\"white-space: pre;\"> </span>cases where utilitarian calculations <em>prohibit</em> it.</p>\n<p>I've tried and have asked others to try -- without success.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Caring about suffering</h2>\n<p>I have not given a reason why torturing babies or racism is bad or wrong. I'm hoping that the vast majority of people will share that intuition/value of mine, that they want to be the sort of person who would have been amongst those challenging racist or sexist prejudices, had they lived in the past.&nbsp;</p>\n<p>Some might be willing to bite the bullet at this point, trusting some strongly held ethical principle of theirs (e.g. A, B, C, D, or E above), to the conclusion of excluding humans who lack certain cognitive capacities from moral concern. One could point out that people's empathy and indirect considerations about human rights, societal stability and so on, will ensure that this \"loophole\" in such an ethical view almost certainly remains without consequences for beings with human DNA. It is a convenient Schelling point after all to care about all humans (or at least all humans outside their mother's womb). However, I don't see why absurd conclusions that will likely remain hypothetical would be significantly less bad than other absurd conclusions. Their mere possibility undermines the whole foundation one's decisional algorithm is grounded in. (Compare hypothetical problems for specific decision theories.)&nbsp;</p>\n<p>Furthermore, while D and E seem plausible candidates for reasons against killing a being with these properties (E is in fact Peter Singer's view on the matter), none of the criteria from A to E seem relevant to suffering, to whether a being can be harmed or benefitted. The case for these being bottom-up morally relevant criteria for the relevance of suffering (or happiness) is very weak, to say the least.&nbsp;</p>\n<p>Maybe that's <a title=\"the speciesist's central confusion\" href=\"http://faculty.smu.edu/jkazez/animal%20rights/norcross-4.pdf\" target=\"_blank\">the speciesist's central confusion</a>, that the rationality/sapience of a being is somehow relevant for whether its suffering matters morally. Clearly, for us ourselves, this does not seem to be the case. If I was told that some evil scientist would first operate on my brain to (temporarily) lower my IQ and cognitive abilities, and then torture me afterwards, it is not like I will be less afraid of the torture or care less about averting it!&nbsp;</p>\n<p>Those who do consider biting the bullet should ask themselves whether they would have defended that view in all contexts, or whether they might be driven towards such a conclusion by a self-serving bias. There seems to be a strange and sudden increase in the frequency of people who are willing to claim that there is nothing intrinsically wrong with torturing babies when the subject is animal rights, or more specifically, the steak they intend to have for dinner.</p>\n<p>It is an entirely <em>different matter</em> if people genuinely think that animals or human infants or late-stage demented people are not sentient. To be clear about what is meant by sentience:&nbsp;</p>\n<blockquote>\n<p><span style=\"white-space: pre;\"> </span>A <strong>sentient</strong> being is one for whom \"it feels like something to be that being\".&nbsp;</p>\n</blockquote>\n<p>I find it highly implausible that only self-aware or \"sapient\" beings are sentient, but if true, this would constitute a compelling reason against caring for at least most nonhuman animals, for the same reason that it would pointless to care about pebbles for the pebbles' sake. If all nonhumans truly weren't sentient, then obviously singling out humans for the sphere of moral concern would not be speciesist.</p>\n<p>What irritates me, however, is that anyone advocating such a view should, it seems to me, still have to factor in a significant probability of being wrong, given that both philosophy of mind and the neuroscience that goes with it are hard and, as far as I'm aware, not quite settled yet. The issue matters because of the huge numbers of nonhuman animals at stake and because of the terrible conditions these beings live in.&nbsp;</p>\n<p>I rarely see this uncertainty acknowledged. If we imagine the torture-scenario outlined above, how confident would we really be that the torture \"won't matter\" if our own advanced cognitive capacities are temporarily suspended?&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Why species membership really is an absurd criterion</h2>\n<p>In the beginning of the article, I wrote that I'd get back to this for those not convinced. Some readers may still feel that there is something special about being a member of the human species. Some may be tempted to think about the concept of \"species\" as if it were a fundamental concept, a Platonic form.&nbsp;</p>\n<p>The following likely isn't news to most of the LW audience, but it is worth spelling it out anyway: There exists a continuum of \"species\" in thing-space as well as in the actual evolutionary timescale. The species boundaries seem obvious just because the intermediates kept evolving or went extinct. And even if that were not the case, we could imagine it. The theoretical possibility is enough to make the philosophical case, even though psychologically, actualities are more convincing.</p>\n<p>We can imagine a continuous line-up of ancestors, always daughter and mother, from modern humans back to the common ancestor of humans and, say, cows, and then forward in time again to modern cows. How would we then divide this line up into distinct species? Morally significant lines would have to be drawn between mother and daughter, but that seems absurd! There are several different definitions of \"species\" used in biology. A common <a title=\"criterion\" href=\"http://evolution.berkeley.edu/evosite/evo101/VA1BioSpeciesConcept.shtml\" target=\"_blank\">criterion</a>&nbsp;-- for sexually reproducing organisms anyway -- is whether groups of beings (of different sex) can have fertile offspring together. If so, they belong to the same species.&nbsp;</p>\n<p>That is a rather odd way of determining whether one cares about the suffering of some hominid creature in the line-up of ancestors -- why should that for instance be relevant in regard to determining whether some instance of suffering matters to us?&nbsp;</p>\n<p>Moreover, is that really the terminal value of people who claim they only care about humans, or could it be that they would, upon reflection, revoke such statements?</p>\n<p>And what about transhumanism? I remember that a couple of years ago, I thought I had found a decisive argument against human enhancement. I thought it would likely lead to speciation, and somehow the thought of that directly implied that posthumans would treat the remaining humans badly, and so the whole thing became immoral in my mind. Obviously this is absurd; there is nothing wrong with speciation per se, and if posthumans will be anti-speciesist, then the remaining humans would have nothing to fear! But given the speciesism in today's society, it is all too understandable that people would be concerned about this. If we imagine the huge extent to which a posthuman, or not to mention a strong AI, would be superior compared to current humans, isn't that a bit like comparing chickens to us?</p>\n<p>A last possible objection I can think of: Suppose one held the belief that group averages are what matters, and that all members of the human species deserve equal protection because of the group average for a criterion that is considered relevant and that would, without the group average rule, deny moral consideration to some sentient humans.&nbsp;</p>\n<p>This defense too doesn't work. Aside from seeming suspiciously arbitrary, such a view would imply absurd conclusions. A thought experiment for illustration: A pig with a macro-mutation is born, she develops child-like intelligence and the ability to speak. Do we refuse to allow her to live unharmed -- or even let her go to school -- simply because she belongs to a group (defined presumably by snout shape, or DNA, or whatever the criteria for \"pigness\" are) with an average that is too low?</p>\n<p>Or imagine you are the head of an architecture bureau and looking to hire a new aspiring architect. Is tossing out an application written by a brilliant woman going to increase the expected success of your firm, assuming that women are, on average, less skilled at spatial imagination than men? Surely not!</p>\n<p>Moreover, taking group averages as our ethical criterion requires us to first define the relevant groups. Why even take species-groups instead of groups defined by skin color, weight or height? Why single out one property and not others?&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Summary</h2>\n<p>Our speciesism is an anthropocentric bias without any reasonable foundation. It would be completely arbitrary to give special consideration to a being simply because of its species membership. Doing so would lead to a number of implications that most people clearly reject. A strong case can be made that suffering is bad in virtue of being suffering, regardless of where it happens. If the suffering or deaths of nonhuman animals deserve no ethical consideration, then human beings with the same relevant properties (of which all plausible ones seem to come down to having similar levels of awareness) deserve no intrinsic ethical consideration either, barring speciesism.&nbsp;</p>\n<p>Assuming that we would feel uncomfortable giving justifications or criteria for our scope of ethical concern that can analogously be used to defend racism or sexism, those not willing to bite the bullet about torturing babies are forced by considerations of consistency to care about animal suffering just as much as they care about human suffering.&nbsp;</p>\n<p>Such a view leaves room for probabilistic discounting in cases where we are empirically uncertain whether beings are capable of suffering, but we should be on the lookout for biases in our assessments.&nbsp;<br /><br />Edit: As Carl Shulman has pointed out, discounting may also apply for \"intensity of sentience\", because it seems at least plausible that shrimps (for instance), if they are sentient, can experience less suffering than e.g. a whale.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q9ASuEEoJWxT3RLMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tm2i5r2rNSfeam5or", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 27, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "23547", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>There have been some posts about animals lately, for instance <a title=\"here\" href=\"/lw/hox/effective_altruism_through_advertising\" target=\"_blank\">here</a>&nbsp;and <a title=\"here\" href=\"/lw/i3s/why_eat_less_meat/\" target=\"_blank\">here</a>. While normative assumptions about the treatment of nonhumans played an important role in the articles and were debated at length in the comment sections, I was missing a concise summary of these arguments. <a title=\"This\" href=\"/lw/757/the_ethical_status_of_nonhuman_animals/\" target=\"_blank\">This</a>&nbsp;post from over a year ago comes closest to what I have in mind, but I want to focus on some of the issues in more detail.</p>\n<p>A while back, I read the following comment in a LessWrong <a title=\"discussion on uploads\" href=\"/lw/d80/malthusian_copying_mass_death_of_unhappy\" target=\"_blank\">discussion on uploads</a>:</p>\n<blockquote>\n<p>I do not at all understand this PETA-like obsession with ethical treatment of bits.</p>\n</blockquote>\n<p>Aside from (carbon-based) humans, which other beings deserve moral consideration? Nonhuman animals? Intelligent aliens? Uploads? Nothing else?</p>\n<p>This article is intended to shed light on these questions; it is however not the intent of this post to advocate a specific ethical framework. Instead, I'll try to show that some ethical principles held by a lot of people are inconsistent with some of their other attitudes -- an argument that doesn't rely on ethics being universal or objective.&nbsp;</p>\n<p>More precisely, I will develop the arguments behind anti-speciesism (and the rejection of analogous forms of discrimination, such as discrimination against uploads) to point out common inconsistencies in some people's values. This will also provide an illustrative example of how <a title=\"coherentist ethical reasoning\" href=\"http://plato.stanford.edu/entries/reflective-equilibrium/\" target=\"_blank\">coherentist ethical reasoning</a>&nbsp;can be applied to shared intuitions. If there are no shared intuitions, ethical discourse will likely be unfruitful, so it is likely that not everyone will draw the same conclusions from the arguments here.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"What_Is_Speciesism_\">What Is Speciesism?</h2>\n<p>Speciesism, a term popularized (but not coined) by the philosopher Peter Singer, is meant to be analogous to sexism or racism. It refers to a discriminatory attitude against a being where less ethical consideration i.e. caring less about a being's welfare or interests is given solely because of the \"wrong\" species membership. The \"solely\" here is crucial, and it's misunderstood often enough to warrant the redundant emphasis.</p>\n<p>For instance, it is not speciesist to deny pigs the right to vote, just like it is not sexist to deny men the right to have an abortion performed on their body. Treating beings of different species differently is not speciesist if there are <em>relevant criteria</em> for doing so.&nbsp;</p>\n<p>Singer summarized his case against speciesism in <a title=\"this essay\" href=\"http://www.animal-rights-library.com/texts-m/singer02.htm\" target=\"_blank\">this essay</a>. The argument that does most of the work is often referred to as the&nbsp;<a title=\"argument from marginal cases\" href=\"http://www.iep.utm.edu/anim-eth/#SSH3ai\" target=\"_blank\">argument from marginal cases</a>. A perhaps less anthropocentric, more fitting name would be argument from species overlap, as some philosophers (e.g. Oscar <a title=\"Horta\" href=\"http://masalladelaespecie.wordpress.com/2010/05/22/what-is-speciesism/\" target=\"_blank\">Horta</a>)&nbsp;have pointed out.&nbsp;</p>\n<p>The argument boils down to the question of choosing relevant criteria for moral concern. What properties do human beings possess that makes us think that it is wrong to torture them? Or to kill them? (Note that these are two different questions.) The argument from species overlap points out that all the typical or plausible suggestions for relevant criteria apply equally to dogs, pigs or chickens as they do to human infants or late-stage Alzheimer patients. Therefore, giving less ethical consideration to the former would be based merely on species membership, which is just as arbitrary as choosing race or sex as relevant criterion (further justification for that claim follows below).</p>\n<p>Here are some examples for commonly suggested criteria. Those who want may pause at this point and think about the criteria they consult for whether it is wrong to inflict suffering on a being (and separately, those that are relevant for the wrongness of killing).</p>\n<p>&nbsp;</p>\n<p>The suggestions are:</p>\n<p>A: Capacity for moral reasoning</p>\n<p>B: Being able to reciprocate</p>\n<p>C: (Human-like) intelligence</p>\n<p>D: Self-awareness</p>\n<p>E: Future-related preferences; future plans</p>\n<p>E': Preferences / interests (in general)</p>\n<p>F: Sentience (capacity for suffering and happiness)</p>\n<p>G: Life / biological complexity</p>\n<p>H: What I care about / feel sympathy or loyalty towards</p>\n<p>&nbsp;</p>\n<p>The argument from species overlap points out that not all humans are equal. The sentiment behind \"all humans are equal\" is not that they are literally equal, but that equal interests/capacities deserve equal consideration. None of the above criteria except (in some empirical cases) H imply that human infants or late stage demented people should be given more ethical consideration than cows, pigs or chickens.</p>\n<p>While H is an unlikely criterion for direct ethical consideration (it could justify genocide in specific circumstances!), it is an important indirect factor. Most humans have much more empathy for fellow humans than for nonhuman animals. While this is not a criterion for giving humans more ethical consideration per se, it is nevertheless a factor that strongly influences ethical decision-making in real-life.</p>\n<p>However, such factors can't apply for ethical reasoning at a theoretical/normative level, where all the relevant variables are looked at in isolation in order to come up with a consistent ethical framework that covers all possible cases.</p>\n<p>If there were no intrinsic reasons for giving moral consideration to babies, then a society in which some babies were (factory-)farmed would be totally fine as long as the people are okay with it. If we consider this implication to be unacceptable, then the same must apply for the situations nonhuman animals find themselves in on farms.</p>\n<p><em>Side note</em>: The question whether killing a given being is wrong, and if so, \"why\" and \"how wrong exactly\", is complex and outside the scope of this article. Instead of on killing, the focus will be on suffering, and by suffering I mean something like wanting to get out of one's current conscious state, or wanting to change some aspect about it. The empirical issue of which beings are capable of suffering is a different matter that I will (only briefly) discuss below. So in this context, giving a being moral consideration means that we don't want it to suffer, leaving open the question whether killing it painlessly is bad/neutral/good or prohibited/permissible/obligatory.&nbsp;</p>\n<p>The main conclusion so far is that if we care about all the suffering of members of the human species, and if we reject question-begging reasoning that could also be used to justify racism or other forms of discrimination, then we must also care fully about suffering happening in nonhuman animals. This would imply that x amount of suffering is just as bad, i.e. that we care about it just as much, in nonhuman animals as in humans, or in aliens or in uploads. (Though admittedly the latter wouldn't be anti-speciesist but rather anti-\"substratist\", or anti-\"fleshist\".)</p>\n<p>The claim is that there is no way to block this conclusion without:</p>\n<p><span style=\"white-space: pre;\"> </span>1. using reasoning that could analogically be used to justify racism or sexism<br><span style=\"white-space: pre;\"> </span>or<br><span style=\"white-space: pre;\"> </span>2. using reasoning that allows for hypothetical circumstances where it would be okay (or even called for) to torture babies in<span style=\"white-space: pre;\"> </span>cases where utilitarian calculations <em>prohibit</em> it.</p>\n<p>I've tried and have asked others to try -- without success.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Caring_about_suffering\">Caring about suffering</h2>\n<p>I have not given a reason why torturing babies or racism is bad or wrong. I'm hoping that the vast majority of people will share that intuition/value of mine, that they want to be the sort of person who would have been amongst those challenging racist or sexist prejudices, had they lived in the past.&nbsp;</p>\n<p>Some might be willing to bite the bullet at this point, trusting some strongly held ethical principle of theirs (e.g. A, B, C, D, or E above), to the conclusion of excluding humans who lack certain cognitive capacities from moral concern. One could point out that people's empathy and indirect considerations about human rights, societal stability and so on, will ensure that this \"loophole\" in such an ethical view almost certainly remains without consequences for beings with human DNA. It is a convenient Schelling point after all to care about all humans (or at least all humans outside their mother's womb). However, I don't see why absurd conclusions that will likely remain hypothetical would be significantly less bad than other absurd conclusions. Their mere possibility undermines the whole foundation one's decisional algorithm is grounded in. (Compare hypothetical problems for specific decision theories.)&nbsp;</p>\n<p>Furthermore, while D and E seem plausible candidates for reasons against killing a being with these properties (E is in fact Peter Singer's view on the matter), none of the criteria from A to E seem relevant to suffering, to whether a being can be harmed or benefitted. The case for these being bottom-up morally relevant criteria for the relevance of suffering (or happiness) is very weak, to say the least.&nbsp;</p>\n<p>Maybe that's <a title=\"the speciesist's central confusion\" href=\"http://faculty.smu.edu/jkazez/animal%20rights/norcross-4.pdf\" target=\"_blank\">the speciesist's central confusion</a>, that the rationality/sapience of a being is somehow relevant for whether its suffering matters morally. Clearly, for us ourselves, this does not seem to be the case. If I was told that some evil scientist would first operate on my brain to (temporarily) lower my IQ and cognitive abilities, and then torture me afterwards, it is not like I will be less afraid of the torture or care less about averting it!&nbsp;</p>\n<p>Those who do consider biting the bullet should ask themselves whether they would have defended that view in all contexts, or whether they might be driven towards such a conclusion by a self-serving bias. There seems to be a strange and sudden increase in the frequency of people who are willing to claim that there is nothing intrinsically wrong with torturing babies when the subject is animal rights, or more specifically, the steak they intend to have for dinner.</p>\n<p>It is an entirely <em>different matter</em> if people genuinely think that animals or human infants or late-stage demented people are not sentient. To be clear about what is meant by sentience:&nbsp;</p>\n<blockquote>\n<p><span style=\"white-space: pre;\"> </span>A <strong>sentient</strong> being is one for whom \"it feels like something to be that being\".&nbsp;</p>\n</blockquote>\n<p>I find it highly implausible that only self-aware or \"sapient\" beings are sentient, but if true, this would constitute a compelling reason against caring for at least most nonhuman animals, for the same reason that it would pointless to care about pebbles for the pebbles' sake. If all nonhumans truly weren't sentient, then obviously singling out humans for the sphere of moral concern would not be speciesist.</p>\n<p>What irritates me, however, is that anyone advocating such a view should, it seems to me, still have to factor in a significant probability of being wrong, given that both philosophy of mind and the neuroscience that goes with it are hard and, as far as I'm aware, not quite settled yet. The issue matters because of the huge numbers of nonhuman animals at stake and because of the terrible conditions these beings live in.&nbsp;</p>\n<p>I rarely see this uncertainty acknowledged. If we imagine the torture-scenario outlined above, how confident would we really be that the torture \"won't matter\" if our own advanced cognitive capacities are temporarily suspended?&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Why_species_membership_really_is_an_absurd_criterion\">Why species membership really is an absurd criterion</h2>\n<p>In the beginning of the article, I wrote that I'd get back to this for those not convinced. Some readers may still feel that there is something special about being a member of the human species. Some may be tempted to think about the concept of \"species\" as if it were a fundamental concept, a Platonic form.&nbsp;</p>\n<p>The following likely isn't news to most of the LW audience, but it is worth spelling it out anyway: There exists a continuum of \"species\" in thing-space as well as in the actual evolutionary timescale. The species boundaries seem obvious just because the intermediates kept evolving or went extinct. And even if that were not the case, we could imagine it. The theoretical possibility is enough to make the philosophical case, even though psychologically, actualities are more convincing.</p>\n<p>We can imagine a continuous line-up of ancestors, always daughter and mother, from modern humans back to the common ancestor of humans and, say, cows, and then forward in time again to modern cows. How would we then divide this line up into distinct species? Morally significant lines would have to be drawn between mother and daughter, but that seems absurd! There are several different definitions of \"species\" used in biology. A common <a title=\"criterion\" href=\"http://evolution.berkeley.edu/evosite/evo101/VA1BioSpeciesConcept.shtml\" target=\"_blank\">criterion</a>&nbsp;-- for sexually reproducing organisms anyway -- is whether groups of beings (of different sex) can have fertile offspring together. If so, they belong to the same species.&nbsp;</p>\n<p>That is a rather odd way of determining whether one cares about the suffering of some hominid creature in the line-up of ancestors -- why should that for instance be relevant in regard to determining whether some instance of suffering matters to us?&nbsp;</p>\n<p>Moreover, is that really the terminal value of people who claim they only care about humans, or could it be that they would, upon reflection, revoke such statements?</p>\n<p>And what about transhumanism? I remember that a couple of years ago, I thought I had found a decisive argument against human enhancement. I thought it would likely lead to speciation, and somehow the thought of that directly implied that posthumans would treat the remaining humans badly, and so the whole thing became immoral in my mind. Obviously this is absurd; there is nothing wrong with speciation per se, and if posthumans will be anti-speciesist, then the remaining humans would have nothing to fear! But given the speciesism in today's society, it is all too understandable that people would be concerned about this. If we imagine the huge extent to which a posthuman, or not to mention a strong AI, would be superior compared to current humans, isn't that a bit like comparing chickens to us?</p>\n<p>A last possible objection I can think of: Suppose one held the belief that group averages are what matters, and that all members of the human species deserve equal protection because of the group average for a criterion that is considered relevant and that would, without the group average rule, deny moral consideration to some sentient humans.&nbsp;</p>\n<p>This defense too doesn't work. Aside from seeming suspiciously arbitrary, such a view would imply absurd conclusions. A thought experiment for illustration: A pig with a macro-mutation is born, she develops child-like intelligence and the ability to speak. Do we refuse to allow her to live unharmed -- or even let her go to school -- simply because she belongs to a group (defined presumably by snout shape, or DNA, or whatever the criteria for \"pigness\" are) with an average that is too low?</p>\n<p>Or imagine you are the head of an architecture bureau and looking to hire a new aspiring architect. Is tossing out an application written by a brilliant woman going to increase the expected success of your firm, assuming that women are, on average, less skilled at spatial imagination than men? Surely not!</p>\n<p>Moreover, taking group averages as our ethical criterion requires us to first define the relevant groups. Why even take species-groups instead of groups defined by skin color, weight or height? Why single out one property and not others?&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Summary\">Summary</h2>\n<p>Our speciesism is an anthropocentric bias without any reasonable foundation. It would be completely arbitrary to give special consideration to a being simply because of its species membership. Doing so would lead to a number of implications that most people clearly reject. A strong case can be made that suffering is bad in virtue of being suffering, regardless of where it happens. If the suffering or deaths of nonhuman animals deserve no ethical consideration, then human beings with the same relevant properties (of which all plausible ones seem to come down to having similar levels of awareness) deserve no intrinsic ethical consideration either, barring speciesism.&nbsp;</p>\n<p>Assuming that we would feel uncomfortable giving justifications or criteria for our scope of ethical concern that can analogously be used to defend racism or sexism, those not willing to bite the bullet about torturing babies are forced by considerations of consistency to care about animal suffering just as much as they care about human suffering.&nbsp;</p>\n<p>Such a view leaves room for probabilistic discounting in cases where we are empirically uncertain whether beings are capable of suffering, but we should be on the lookout for biases in our assessments.&nbsp;<br><br>Edit: As Carl Shulman has pointed out, discounting may also apply for \"intensity of sentience\", because it seems at least plausible that shrimps (for instance), if they are sentient, can experience less suffering than e.g. a whale.&nbsp;</p>", "sections": [{"title": "What Is Speciesism?", "anchor": "What_Is_Speciesism_", "level": 1}, {"title": "Caring about suffering", "anchor": "Caring_about_suffering", "level": 1}, {"title": "Why species membership really is an absurd criterion", "anchor": "Why_species_membership_really_is_an_absurd_criterion", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "478 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 479, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["grP8nTMWm67RbKZwg", "LbbyQhLkcwAwWmBoj", "ikQArh7yRE47bsokR", "ynyemLY8YWX8rQ84f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-07-28T18:24:58.354Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-28T22:59:02.955Z", "modifiedAt": null, "url": null, "title": "Repository repository", "slug": "repository-repository", "viewCount": null, "lastCommentedAt": "2014-12-11T16:42:41.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sEaDmtwrmTC7kTqcf/repository-repository", "pageUrlRelative": "/posts/sEaDmtwrmTC7kTqcf/repository-repository", "linkUrl": "https://www.lesswrong.com/posts/sEaDmtwrmTC7kTqcf/repository-repository", "postedAtFormatted": "Sunday, July 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Repository%20repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARepository%20repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEaDmtwrmTC7kTqcf%2Frepository-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Repository%20repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEaDmtwrmTC7kTqcf%2Frepository-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEaDmtwrmTC7kTqcf%2Frepository-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 291, "htmlBody": "<p>A few weeks ago, <a href=\"/lw/hyu/meta_open_threads_and_repository_threads_are/\">Adele_L suggested</a> that the repositories were underutilized and looked for suggestions on how to improve that. In that spirit, I added the following links to the <a href=\"http://wiki.lesswrong.com/wiki/Special_threads#Repository_Threads\">Special Threads wiki page</a>.</p>\n<p><a href=\"/lw/h2m/solved_problems_repository/ \">Solved Problems Repository</a> - A collection of \"solved problems in instrumental rationality.\"<br /><a href=\"/lw/hhl/useful_concepts_repository/\">Useful Concepts Repository</a> - A collection of concepts that Less Wrong users have \"found particularly useful for understanding the world.\"<br /><a href=\"/lw/gx5/boring_advice_repository/\">Boring Advice Repository</a> - A collection of advice that is optimized for helpfulness rather than depth of insight.<br /><a href=\"/lw/i4l/useful_questions_repository/\">Useful Questions Repository</a> - Questions that are useful to keep in mind in various situations.<br /><a href=\"/lw/htw/bad_concepts_repository/\">Bad Concepts Repository</a> - A collection of useless or harmful concepts<br /><a href=\"/lw/h7d/grad_student_advice_repository/\">Grad Student Advice Repository</a> - A collection of advice for graduate students.<br /><a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">Textbook Repository</a> - The Best Textbooks on Every Subject<br /><a href=\"/lw/2un/references_resources_for_lesswrong/\">Reference repository</a> - List of references and resources for LessWrong<br /><a href=\"/lw/453/procedural_knowledge_gaps/\">Procedural Knowledge Gaps</a> - How to do things that are \"common sense\" but that you may not know.<br /><a href=\"/r/discussion/lw/ikn/mistakes_repository/\">Mistakes Repository</a>&nbsp;- A list of life-course altering mistakes that LW members have made.<br /><a href=\"/r/discussion/lw/lch/good_things_to_have_learned/\">Good things to have learned</a>&nbsp;- A collection of skills and life lessons LWers have learned<br /><a href=\"/r/discussion/lw/la6/financial_effectiveness_repository/\">Financial Effectiveness Repository</a> - Tips for maximizing financial returns on (not necessarily market) investments.</p>\n<p>In a similar vein, there is also a wiki page for the <a href=\"http://wiki.lesswrong.com/wiki/Lesswrong_Community%27s_How-Tos_and_Recommendations\">LessWrong Communities How-To's and Recommendations</a>.</p>\n<p>If there are other repositories that I've missed or a better way to collect these things, please link to it in a top level comment so that I get a direct message. A year and a half after this was originally posted, I still get suggestions and still add them or explain why I don't add them.</p>\n<p>Edit: Added a few more to the list.<a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\"></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 12, "Eha62RrqBtEbpcEza": 1, "GQyPQcdEQF4zXhJBq": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sEaDmtwrmTC7kTqcf", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 62, "extendedScore": null, "score": 0.000153, "legacy": true, "legacyId": "23548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["P8Qc7te5qcEnpD3nv", "iTzvJ7kKK2TYJhYHB", "umzNiYpHLypdcXuEf", "HEn2qiMxk5BggN83J", "ghcCY5E4rmwsmTxjv", "RcMjekC7yDTBzKCij", "9iofKNvYKZe3T7MpS", "xg3hXCYQPJkwHyik2", "TNHQLZK5pHbxdnz4e", "ka8eveZpT7hXLhRTM", "KLiJPDFHCRYcftQnq", "zfZNkAWQxTbm5tLz5", "W4P9snvsgtXvybcLg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-07-28T22:59:02.955Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T09:09:27.464Z", "modifiedAt": null, "url": null, "title": "Why I'm Skeptical About Unproven Causes (And You Should Be Too)", "slug": "why-i-m-skeptical-about-unproven-causes-and-you-should-be", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:37.710Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XiN948y5QDgNbuTXP/why-i-m-skeptical-about-unproven-causes-and-you-should-be", "pageUrlRelative": "/posts/XiN948y5QDgNbuTXP/why-i-m-skeptical-about-unproven-causes-and-you-should-be", "linkUrl": "https://www.lesswrong.com/posts/XiN948y5QDgNbuTXP/why-i-m-skeptical-about-unproven-causes-and-you-should-be", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20I'm%20Skeptical%20About%20Unproven%20Causes%20(And%20You%20Should%20Be%20Too)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20I'm%20Skeptical%20About%20Unproven%20Causes%20(And%20You%20Should%20Be%20Too)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiN948y5QDgNbuTXP%2Fwhy-i-m-skeptical-about-unproven-causes-and-you-should-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20I'm%20Skeptical%20About%20Unproven%20Causes%20(And%20You%20Should%20Be%20Too)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiN948y5QDgNbuTXP%2Fwhy-i-m-skeptical-about-unproven-causes-and-you-should-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiN948y5QDgNbuTXP%2Fwhy-i-m-skeptical-about-unproven-causes-and-you-should-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3257, "htmlBody": "<p>Since living in Oxford, one of the centers of the \"effective altruism\" movement, I've been spending a lot of time discussing the classic &ldquo;effective altruism&rdquo; topic -- where it would be best to focus our time and money.</p>\n<p>Some people here seem to think that the most important thing we should be focusing our time and money on are speculative projects, or projects that promise a very high impact, but involve a lot of uncertainty. &nbsp;One such very common example is \"existential risk reduction\", or attempts to make a long-term far future for humans more likely, say by reducing the chance of things that would cause human extinction.</p>\n<p>I do agree that the far future is the most important thing to consider, by far (see papers by <a href=\"http://www.existential-risk.org/concept.pdf\">Nick Bostrom</a>&nbsp;and <a href=\"https://sites.google.com/site/nbeckstead/research\">Nick Beckstead</a>). &nbsp;And I do think we can influence the far future. &nbsp;I just don't think we can do it in a <em>reliable</em>&nbsp;way. &nbsp;All we have are guesses about what the far future will be like and guesses about how we can affect it. All of these ideas are unproven, speculative projects, and I don't think they deserve the main focus of our funding.</p>\n<p>While I waffled in cause indecision for a while, I'm now going to resume donating to <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's top charities</a>, except when I have an opportunity to use a donation to learn more about impact. &nbsp;Why? &nbsp;My case is that speculative causes, or any cause with high uncertainty (reducing nonhuman animal suffering, reducing existential risk, etc.) requires that we rely on our commonsense to evaluate them with na\u012bve cost-effectiveness calculations, and this is (1) demonstrably unreliable with a bad track record, (2) plays right into common biases, and (3) doesn&rsquo;t make sense based on how we ideally make decisions. &nbsp;While it&rsquo;s unclear what long-term impact a donation to a GiveWell top charity will have, the near-term benefit is quite clear and worth investing in.</p>\n<p>&nbsp;</p>\n<h2>Focusing on Speculative Causes Requires Unreliable Commonsense</h2>\n<p>How can we reduce the chance of human extinction? It just makes sense that if we fund cultural exchange programs between the US and China, there will be more goodwill for the other within each country, and therefore the countries will be less likely to nuke each other. Since nuclear war would likely be very bad, it's of high value to fund cultural exchange programs, right?</p>\n<p>Let's try another. The <a href=\"http://intelligence.org/\">Machine Intelligence Research Institute</a>&nbsp;(MIRI) thinks that someday artificial intelligent agents will become better than humans at making AIs. At this point, AI will build a smarter AI which will build an even smarter AI, and -- FOOM! -- we have a superintelligence. It's important that this superintelligence be programmed to be benevolent, or things will likely be very bad. And we can stop this bad event by funding MIRI to <a href=\"http://intelligence.org/research/\">write more papers about AI</a>, right?</p>\n<p>Or how about this one? It seems like there will be challenges in the far future that will be very daunting, and if humanity handles them wrong, things will be very bad. But if people were better educated and had more resources, surely they'd be better at handling those problems, whatever they may be. Therefore we should focus on speeding up economic development, right?</p>\n<p>These three examples are very common appeals to commonsense. &nbsp;But commonsense hasn't worked very well in the domain of finding optimal causes.</p>\n<p>&nbsp;</p>\n<p><strong>Can You Pick the Winning Social Program?</strong></p>\n<p>Benjamin Todd makes this point well in <a href=\"http://80000hours.org/blog/66-social-interventions-gone-wrong\">\"Social Interventions Gone Wrong\"</a>, where he provides a quiz with eight social programs and asks readers to guess whether they succeeded or failed.</p>\n<p><em>I'll wait for you to take the quiz first... doo doo doo... la la la...</em></p>\n<p>Ok, welcome back. I don't know how well you did, but success on this quiz is very rare, and this poses problems for commonsense. &nbsp;Sure, I'll grant you that Scared Straight sounds pretty suspicious. But the Even Start Family Literacy Program? It just makes sense that providing education to boost literacy skills and promote parent-child literacy activities should boost literacy rates, right? Unfortunately, it was wrong. Wrong in a very counter-intuitive way. There wasn't an effect. &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>GiveWell and Commonsense's Track Record of Failure</strong></p>\n<p>Commonsense actually has a track record of failure. GiveWell has been talking about this for ages. &nbsp;Every time GiveWell has found an intervention hyped by commonsense notions of high-impact and they've looked at it further, they've ended up disappointed.</p>\n<p><strong>The first was the Fred Hollows Foundation.</strong> A lot of people had been repeating the figure that the Fred Hollows Foundation could cure blindness for $50. But GiveWell <a href=\"http://blog.givewell.org/2008/07/21/preventing-blindness/\">found that number suspect</a>.</p>\n<p><strong>The second was VillageReach.</strong>&nbsp;GiveWell originally put them as their top charity and estimated them as <a href=\"http://www.givewell.org/international/top-charities/villagereach/December-2009-review#Whatdoyougetforyourdollar\">saving a life for under $1000</a>. But further investigation kept leading them to revise their estimate until ultimately they <a href=\"http://blog.givewell.org/2012/07/26/rethinking-villagereachs-pilot-project/\">weren't even sure if VillageReach had an impact at all</a>.</p>\n<p><strong>Third, there is deworming.</strong> Originally, deworming was announced as saving a year of healthy life (DALY) for every $3.41 spent. But when GiveWell dove into the spreadsheets that resulted in that number, they <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/\">found five errors</a>. When the dust settled, the $3.41 figure was found to actually be off by a factor of 100. It was revised to $326.43.</p>\n<p>Why shouldn't we expect this trend to not be the case in other areas where calculations are even looser and numbers are even less settled, like efforts devoted to speculative causes? Our only recourse is to fall back on interventions that are actually studied.</p>\n<p>&nbsp;</p>\n<p><strong>People Are Notoriously Bad At Predicting the (Far) Future</strong></p>\n<p>Cost-effectiveness estimates also frequently require making predictions about the future. Existential risk reduction, for example, requires making predictions about what will happen in the far future, and how your actions are likely to effect events hundreds of years down the road. Yet, experts are notoriously bad at making these kinds of predictions.</p>\n<p>James Shanteau found in <a href=\"http://www.sciencedirect.com/science/article/pii/074959789290064E\">\"Competence in Experts: The Role of Task Characteristics\"</a>&nbsp;(see also Kahneman and Klein's <a href=\"http://psycnet.apa.org/journals/amp/64/6/515/\">\"Conditions for Intuitive Expertise: A Failure to Disagree\"</a>) that experts perform well when thinking about static stimuli, thinking about things, and when there is feedback and objective analysis available. Furthermore, experts perform pretty badly when thinking about dynamic stimuli, thinking about behavior, and feedback and objective analysis are unavailable.</p>\n<p>Predictions about existential risk reduction and the far future are firmly in the second category. So how can we trust our predictions about our impact on the far future? Our only recourse is to fall back on interventions that we can reliably predict, until we get better at prediction (or invest money in getting better at making predictions).</p>\n<p>&nbsp;</p>\n<p><strong>Even Broad Effects Require Specific Attempts</strong></p>\n<p>One potential resolution to this problem is to argue for &ldquo;broad effects&rdquo; rather than &ldquo;specific attempts&rdquo;. &nbsp;Perhaps it&rsquo;s difficult to know whether a particular intervention will go well or mistaken to focus entirely on Friendly AI, but surely if we improved incentives and norms in academic work to better advance human knowledge (meta-research), improved education, or advocated for effective altruism, the far future would be much better equipped to handle threats.</p>\n<p>I agree that these broad effects would make the far future better and I agree that it&rsquo;s possible to implement these broad effects and change the far future. &nbsp;The problem, however, is it can&rsquo;t be done in an easy or well understood way. &nbsp;Any attempt to implement a broad effect would require a specific action that has an unknown expectation of success and unknown cost-effectiveness. &nbsp;It&rsquo;s definitely beneficial to advocate for effective altruism, but could this be done in a cost-effective way? &nbsp;A way that&rsquo;s more cost-effective at producing welfare than AMF? &nbsp;How would you know?</p>\n<p>In order to accomplish these broad effects, you&rsquo;d need specific organizations and interventions to channel your time and money into. &nbsp;And by picking these specific organizations and interventions, you&rsquo;re losing the advantage of broad effects and tying yourself to particular things with poorly understood impact and no track record to evaluate.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Focusing on Speculative Causes Plays Into Our Biases</h2>\n<p>We've now known for quite a long time that people are not all that rational. Instead, human thinking fails in very predictable and systematic ways. &nbsp;Some of these ways make us less likely to take speculative causes seriously, such as ambiguity aversion, the absurdity heuristic, scope neglect, and overconfidence bias.</p>\n<p>But there&rsquo;s also a different side of the coin, with biases that might make people think badly about existential risk:</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Optimism_bias\"><strong>Optimism bias</strong></a><strong>.</strong> People generally think things will turn out better than they actually will. This could lead people to think that their projects will have a higher impact than they actually will, which would lead to higher estimates of cost-effectiveness than is reasonable.</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Illusion_of_control\">Control bias</a>.</strong> People like to think they have more control over things than they actually do. This plausibly also includes control over the far future. Therefore, people are probably biased into thinking they have more control over the far future than they actually do, leading to higher estimates of ability to influence the future than is reasonable.</p>\n<p><strong>\"Wow factor\" bias.</strong> People seem attracted to more impressive claims. Saving a life for $2500 through a malaria bed net seems much more boring compared to the chance of saving the entire world by averting a global catastrophe. Within the Effective Altruist / LessWrong community, existential risk reduction is cool and high status, whereas averting global poverty is not. This might lead to more endorsement of existential risk reduction than is reasonable.</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">Conjunction fallacy</a>.</strong> &nbsp;People have a problem assessing probability properly when there are many steps involved, each of which has a chance of not happening. Ten steps, each with an independent 90% success rate, has only a 35% chance of success. &nbsp;Focusing on the far future seems to involve that a lot of largely independent events happen the way that is predicted. This would mean people are worse at estimating their chances of helping the far future, creating higher cost-effectiveness estimates than is reasonable.</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Selection_bias\">Selection bias</a>.</strong> &nbsp;When trying to find trends in history that are favorable for affecting the far future, some examples can be provided. &nbsp;However, this is because we usually hear about the interventions that end up working, whereas all the failed attempts to influence the far future are never heard of again. &nbsp;This creates a very skewed sample that can negatively bias our thinking about our success of influencing the far future.</p>\n<p>&nbsp;</p>\n<p>It&rsquo;s concerning there are numerous biases both weighted in favor and weighted against speculative causes, and this means we must tread carefully when assessing their merits. &nbsp;However, I would strongly expect biases to be even worse in favor of speculative causes rather than against them, because speculative causes lack the available feedback and objective evidence needed to help insulate against bias, whereas a focus on global health does not.</p>\n<p>&nbsp;</p>\n<h2>Focusing on Speculative Causes Uses Bad Decision Theory</h2>\n<p>Furthermore, not only is the case for speculative causes undermined by a bad track record and possible cognitive biases, but the underlying decision theory seems suspect in a way that's difficult to place. &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Would you play a lottery with no stated odds?</strong></p>\n<p>Imagine another thought experiment -- you're asked to play a lottery. You have to pay $2 to play, but you have a chance at winning $100. Do you play?</p>\n<p>Of course, you don't know, because you're not given odds. Rationally, it makes sense to play any lottery where you expect to come out ahead more often than not. If the lottery is a coin flip, it makes sense to pay $2 to have a 50/50 shot to win $100, since you'd expect to win $50 on average, and come ahead $48 each time. With a sufficiently high reward, even a one in a million chance is worth it. Pay $2 for a 1/1M chance of winning $1B, and you'd expect to come out ahead by $998 each time.</p>\n<p>But $2 for the chance to win $100, without knowing what the chance is? Even if you had some sort of bounds, like you knew the odds had to be at least 1/150 and at most 1/10, though you could be off by a little bit. Would you accept that bet?</p>\n<p>Such a bet seems intuitively uninviting to me, yet this is the bet that speculative causes offer me.</p>\n<p>&nbsp;</p>\n<p><strong>\"Conservative Orders of Magnitude\" Arguments</strong></p>\n<p>In response to these considerations, I've seen people endorsing speculative causes look at their calculations and remark that even if their estimate were off by 1000x, or three orders of magnitude, they still would be on solid ground for high impact, and there's no way they're actually off by three orders of magnitude. However, Nate Silver's <a href=\"http://www.amazon.com/dp/159420411X\"><strong>The Signal and the Noise: Why So Many Predictions Fail &mdash; but Some Don't</strong></a>&nbsp;offers a cautionary tale:</p>\n<blockquote>\n<p>Moody&rsquo;s, for instance, went through a period of making ad hoc adjustments to its model in which it increased the default probability assigned to AAA-rated securities by 50 percent. That might seem like a very prudent attitude: surely a 50 percent buffer will suffice to account for any slack in one&rsquo;s assumptions? It might have been fine had the potential for error in their forecasts been linear and arithmetic. But leverage, or investments financed by debt, can make the error in a forecast compound many times over, and introduces the potential of highly geometric and nonlinear mistakes.</p>\n<p>Moody&rsquo;s 50 percent adjustment was like applying sunscreen and claiming it protected you from a nuclear meltdown&mdash;wholly inadequate to the scale of the problem. It wasn&rsquo;t just a possibility that their estimates of default risk could be 50 percent too low: they might just as easily have underestimated it by 500 percent or 5,000 percent. In practice, defaults were two hundred times more likely than the ratings agencies claimed, meaning that their model was off by a mere 20,000 percent.</p>\n</blockquote>\n<p>Silver points out that when estimating how safe mortgage backed securities were, the difference between assuming defaults are perfectly uncorrelated and defaults are perfectly correlated is a difference of 160,000x in your risk estimate -- or five orders of magnitude.</p>\n<p>If these kinds of five-orders-of-magnitude errors are possible in a realm that has actual feedback and is moderately understood, how do we know the estimates for cost-effectiveness are safe for speculative causes that are poorly understood and offer no feedback? &nbsp;Again, our only recourse is to fall back on interventions that we can reliably predict, until we get better at prediction.</p>\n<p>&nbsp;</p>\n<h2>Value of Information, Exploring, and Exploiting</h2>\n<p>Of course, there still is one important aspect of this problem that has not been discussed -- <a href=\"http://en.wikipedia.org/wiki/Value_of_information\">value of information</a>&nbsp;-- or the idea that sometimes it&rsquo;s worth doing something just to learn more about how the world works. &nbsp;This is important in effective altruism too, where we focus specifically on &ldquo;giving to learn&rdquo;, or using our resources to figure out more about the impact of various causes.</p>\n<p>I think this is actually really important and is not a victim to any of my previous arguments, because we&rsquo;re not talking about impact, but rather learning value. &nbsp;Perhaps one could look to an \"<a href=\"http://en.wikipedia.org/wiki/Multi-armed_bandit\">explore-exploit model</a>\", or the idea that we achieve the best outcome when we spend a lot of time exploring first (learning more about how to achieve better outcomes) before exploiting (focusing resources on achieving the best outcome we can). &nbsp;Therefore, whenever we have an opportunity to &ldquo;explore&rdquo; further or learn more about what causes have high impact, we should take it.</p>\n<p>&nbsp;</p>\n<p><strong>Learning in Practice</strong></p>\n<p>Unfortunately, in practice, I think these opportunities are very rare. &nbsp;Many organizations that I think are &ldquo;promising&rdquo; and worth funding further to see what their impact looks like do not have sufficiently good self-measurement in place to actually assess their impact or sufficient transparency to provide that information, therefore making it difficult to actually learn from them. &nbsp;And on the other side of things, many very promising opportunities to learn more are already fully funded. &nbsp;One must be careful to ensure that it&rsquo;s actually one&rsquo;s marginal dollar that is getting marginal information.</p>\n<p>&nbsp;</p>\n<p><strong>The Typical Donor</strong></p>\n<p>Additionally, I don&rsquo;t think the typical donor is in a very good position to assess where there is high value of information or have the time and knowledge to act upon this information once it is acquired. &nbsp;I think there&rsquo;s a good argument for people in the &ldquo;effective altruist&rdquo; movement to perhaps make small investments in EA organizations and encourage transparency and good measurement in their operations to see if they&rsquo;re successfully doing what they claim (or potentially create an EA startup themselves to see if it would work, though this carries large risks of further splitting the resources of the movement).</p>\n<p>But even that would take a very savvy and involved effective altruist to pull off. &nbsp;Assessing the value of information on more massive investments like large-scale research or innovation efforts would be significantly more difficult, beyond the talent and resources of nearly all effective altruists, and are probably left to full-time foundations or subject-matter experts.</p>\n<p>&nbsp;</p>\n<p><strong>GiveWell&rsquo;s Top Charities Also Have High Value of Information</strong></p>\n<p>As Luke Muehlhauser mentions in <a href=\"/lw/hsd/start_under_the_streetlight_then_push_into_the/\">\"Start Under the Streetlight, Then Push Into the Shadows\"</a>, lots of lessons can be learned only by focusing on the easiest causes first, even if we have strong theoretical reasons to expect that they won&rsquo;t end up being the highest impact causes once we have more complete knowledge.</p>\n<p>We can use global health cost-effectiveness considerations as practice for slowly and carefully moving into the more complex and less understood domains. &nbsp;There even are some very natural transitions, such as beginning to look at \"<a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow through effects</a>\" of reducing disease in the third-world and beginning to look at how more esoteric things affect the disease burden, like climate change. &nbsp;Therefore, even additional funding for GiveWell&rsquo;s top charities has high value of information. &nbsp;And notably, GiveWell is beginning this \"push\" through <a href=\"http://blog.givewell.org/category/givewell-labs/\">GiveWell Labs</a>.</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>The bottom line is that sometimes things look too good to be true. &nbsp;Therefore, I should expect that the actual impact of speculative causes that make large promises, upon a thorough investigation, will be much lower.</p>\n<p>And this has been true in other domains. People are notoriously bad at estimating the effects of causes in both the developed world and developing world, and those are the causes that are near to us, provide us with feedback, and are easy to predict. Yet, from the Even Start Family Literacy Program to deworming estimates, our commonsense has failed us.</p>\n<p>Add to that the fact that we should expect ourselves to perform even worse at predicting the far future. Add to that optimism bias, control bias, \"wow factor\" bias, and the conjunction fallacy, which make it difficult for us to think realistically about speculative causes. And then add to that considerations in decision theory, and whether we would bet on a lottery with no stated odds.</p>\n<p>When all is said and done, I'm very skeptical of speculative projects. &nbsp;Therefore, I think we should be focused on exploring and exploiting. &nbsp;We should do whatever we can to fund projects aimed at learning more, when those are available, but be careful to make sure they actually have learning value. &nbsp;And when exploring isn&rsquo;t available, we should exploit what opportunities we have and fund proven interventions.</p>\n<p>But don&rsquo;t confuse these two concepts and fund causes intended for learning because of their actual impact value. &nbsp;I&rsquo;m skeptical about these causes actually being high impact, though I&rsquo;m open to the idea that they might be and look forward to funding them in the future when they become better proven. &nbsp; &nbsp;&nbsp;</p>\n<p>-</p>\n<p><strong>Followed up in: </strong><a href=\"/r/discussion/lw/i9u/what_would_it_take_to_prove_a_speculative_cause/\">\"What Would It Take To 'Prove' A Skeptical Cause\"</a>&nbsp;and <a href=\"/r/discussion/lw/ic0/where_ive_changed_my_mind_on_my_approach_to/\">\"Where I've Changed My Mind on My Approach to Speculative Causes\"</a>.</p>\n<p><em>This was also cross-posted <a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">to my blog</a> and <a href=\"http://effective-altruism.com/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">to effective-altruism.com</a>.</em></p>\n<p><em>I'd like to thank Nick Beckstead, Joey Savoie, Xio Kikauka, Carl Shulman, Ryan Carey, &nbsp;Tom Ash, Pablo Stafforini, Eliezer Yudkowsky, and Ben Hoskin for providing feedback on this essay, even if some of them might strongly disagree with it's conclusion.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 2, "EeSkeTcT4wtW2fWsL": 2, "xexCWMyds6QLWognu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XiN948y5QDgNbuTXP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 42, "extendedScore": null, "score": 0.000117, "legacy": true, "legacyId": "23559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Since living in Oxford, one of the centers of the \"effective altruism\" movement, I've been spending a lot of time discussing the classic \u201ceffective altruism\u201d topic -- where it would be best to focus our time and money.</p>\n<p>Some people here seem to think that the most important thing we should be focusing our time and money on are speculative projects, or projects that promise a very high impact, but involve a lot of uncertainty. &nbsp;One such very common example is \"existential risk reduction\", or attempts to make a long-term far future for humans more likely, say by reducing the chance of things that would cause human extinction.</p>\n<p>I do agree that the far future is the most important thing to consider, by far (see papers by <a href=\"http://www.existential-risk.org/concept.pdf\">Nick Bostrom</a>&nbsp;and <a href=\"https://sites.google.com/site/nbeckstead/research\">Nick Beckstead</a>). &nbsp;And I do think we can influence the far future. &nbsp;I just don't think we can do it in a <em>reliable</em>&nbsp;way. &nbsp;All we have are guesses about what the far future will be like and guesses about how we can affect it. All of these ideas are unproven, speculative projects, and I don't think they deserve the main focus of our funding.</p>\n<p>While I waffled in cause indecision for a while, I'm now going to resume donating to <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's top charities</a>, except when I have an opportunity to use a donation to learn more about impact. &nbsp;Why? &nbsp;My case is that speculative causes, or any cause with high uncertainty (reducing nonhuman animal suffering, reducing existential risk, etc.) requires that we rely on our commonsense to evaluate them with na\u012bve cost-effectiveness calculations, and this is (1) demonstrably unreliable with a bad track record, (2) plays right into common biases, and (3) doesn\u2019t make sense based on how we ideally make decisions. &nbsp;While it\u2019s unclear what long-term impact a donation to a GiveWell top charity will have, the near-term benefit is quite clear and worth investing in.</p>\n<p>&nbsp;</p>\n<h2 id=\"Focusing_on_Speculative_Causes_Requires_Unreliable_Commonsense\">Focusing on Speculative Causes Requires Unreliable Commonsense</h2>\n<p>How can we reduce the chance of human extinction? It just makes sense that if we fund cultural exchange programs between the US and China, there will be more goodwill for the other within each country, and therefore the countries will be less likely to nuke each other. Since nuclear war would likely be very bad, it's of high value to fund cultural exchange programs, right?</p>\n<p>Let's try another. The <a href=\"http://intelligence.org/\">Machine Intelligence Research Institute</a>&nbsp;(MIRI) thinks that someday artificial intelligent agents will become better than humans at making AIs. At this point, AI will build a smarter AI which will build an even smarter AI, and -- FOOM! -- we have a superintelligence. It's important that this superintelligence be programmed to be benevolent, or things will likely be very bad. And we can stop this bad event by funding MIRI to <a href=\"http://intelligence.org/research/\">write more papers about AI</a>, right?</p>\n<p>Or how about this one? It seems like there will be challenges in the far future that will be very daunting, and if humanity handles them wrong, things will be very bad. But if people were better educated and had more resources, surely they'd be better at handling those problems, whatever they may be. Therefore we should focus on speeding up economic development, right?</p>\n<p>These three examples are very common appeals to commonsense. &nbsp;But commonsense hasn't worked very well in the domain of finding optimal causes.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Can_You_Pick_the_Winning_Social_Program_\">Can You Pick the Winning Social Program?</strong></p>\n<p>Benjamin Todd makes this point well in <a href=\"http://80000hours.org/blog/66-social-interventions-gone-wrong\">\"Social Interventions Gone Wrong\"</a>, where he provides a quiz with eight social programs and asks readers to guess whether they succeeded or failed.</p>\n<p><em>I'll wait for you to take the quiz first... doo doo doo... la la la...</em></p>\n<p>Ok, welcome back. I don't know how well you did, but success on this quiz is very rare, and this poses problems for commonsense. &nbsp;Sure, I'll grant you that Scared Straight sounds pretty suspicious. But the Even Start Family Literacy Program? It just makes sense that providing education to boost literacy skills and promote parent-child literacy activities should boost literacy rates, right? Unfortunately, it was wrong. Wrong in a very counter-intuitive way. There wasn't an effect. &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"GiveWell_and_Commonsense_s_Track_Record_of_Failure\">GiveWell and Commonsense's Track Record of Failure</strong></p>\n<p>Commonsense actually has a track record of failure. GiveWell has been talking about this for ages. &nbsp;Every time GiveWell has found an intervention hyped by commonsense notions of high-impact and they've looked at it further, they've ended up disappointed.</p>\n<p><strong>The first was the Fred Hollows Foundation.</strong> A lot of people had been repeating the figure that the Fred Hollows Foundation could cure blindness for $50. But GiveWell <a href=\"http://blog.givewell.org/2008/07/21/preventing-blindness/\">found that number suspect</a>.</p>\n<p><strong>The second was VillageReach.</strong>&nbsp;GiveWell originally put them as their top charity and estimated them as <a href=\"http://www.givewell.org/international/top-charities/villagereach/December-2009-review#Whatdoyougetforyourdollar\">saving a life for under $1000</a>. But further investigation kept leading them to revise their estimate until ultimately they <a href=\"http://blog.givewell.org/2012/07/26/rethinking-villagereachs-pilot-project/\">weren't even sure if VillageReach had an impact at all</a>.</p>\n<p><strong>Third, there is deworming.</strong> Originally, deworming was announced as saving a year of healthy life (DALY) for every $3.41 spent. But when GiveWell dove into the spreadsheets that resulted in that number, they <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/\">found five errors</a>. When the dust settled, the $3.41 figure was found to actually be off by a factor of 100. It was revised to $326.43.</p>\n<p>Why shouldn't we expect this trend to not be the case in other areas where calculations are even looser and numbers are even less settled, like efforts devoted to speculative causes? Our only recourse is to fall back on interventions that are actually studied.</p>\n<p>&nbsp;</p>\n<p><strong id=\"People_Are_Notoriously_Bad_At_Predicting_the__Far__Future\">People Are Notoriously Bad At Predicting the (Far) Future</strong></p>\n<p>Cost-effectiveness estimates also frequently require making predictions about the future. Existential risk reduction, for example, requires making predictions about what will happen in the far future, and how your actions are likely to effect events hundreds of years down the road. Yet, experts are notoriously bad at making these kinds of predictions.</p>\n<p>James Shanteau found in <a href=\"http://www.sciencedirect.com/science/article/pii/074959789290064E\">\"Competence in Experts: The Role of Task Characteristics\"</a>&nbsp;(see also Kahneman and Klein's <a href=\"http://psycnet.apa.org/journals/amp/64/6/515/\">\"Conditions for Intuitive Expertise: A Failure to Disagree\"</a>) that experts perform well when thinking about static stimuli, thinking about things, and when there is feedback and objective analysis available. Furthermore, experts perform pretty badly when thinking about dynamic stimuli, thinking about behavior, and feedback and objective analysis are unavailable.</p>\n<p>Predictions about existential risk reduction and the far future are firmly in the second category. So how can we trust our predictions about our impact on the far future? Our only recourse is to fall back on interventions that we can reliably predict, until we get better at prediction (or invest money in getting better at making predictions).</p>\n<p>&nbsp;</p>\n<p><strong id=\"Even_Broad_Effects_Require_Specific_Attempts\">Even Broad Effects Require Specific Attempts</strong></p>\n<p>One potential resolution to this problem is to argue for \u201cbroad effects\u201d rather than \u201cspecific attempts\u201d. &nbsp;Perhaps it\u2019s difficult to know whether a particular intervention will go well or mistaken to focus entirely on Friendly AI, but surely if we improved incentives and norms in academic work to better advance human knowledge (meta-research), improved education, or advocated for effective altruism, the far future would be much better equipped to handle threats.</p>\n<p>I agree that these broad effects would make the far future better and I agree that it\u2019s possible to implement these broad effects and change the far future. &nbsp;The problem, however, is it can\u2019t be done in an easy or well understood way. &nbsp;Any attempt to implement a broad effect would require a specific action that has an unknown expectation of success and unknown cost-effectiveness. &nbsp;It\u2019s definitely beneficial to advocate for effective altruism, but could this be done in a cost-effective way? &nbsp;A way that\u2019s more cost-effective at producing welfare than AMF? &nbsp;How would you know?</p>\n<p>In order to accomplish these broad effects, you\u2019d need specific organizations and interventions to channel your time and money into. &nbsp;And by picking these specific organizations and interventions, you\u2019re losing the advantage of broad effects and tying yourself to particular things with poorly understood impact and no track record to evaluate.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Focusing_on_Speculative_Causes_Plays_Into_Our_Biases\">Focusing on Speculative Causes Plays Into Our Biases</h2>\n<p>We've now known for quite a long time that people are not all that rational. Instead, human thinking fails in very predictable and systematic ways. &nbsp;Some of these ways make us less likely to take speculative causes seriously, such as ambiguity aversion, the absurdity heuristic, scope neglect, and overconfidence bias.</p>\n<p>But there\u2019s also a different side of the coin, with biases that might make people think badly about existential risk:</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Optimism_bias\"><strong>Optimism bias</strong></a><strong>.</strong> People generally think things will turn out better than they actually will. This could lead people to think that their projects will have a higher impact than they actually will, which would lead to higher estimates of cost-effectiveness than is reasonable.</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Illusion_of_control\">Control bias</a>.</strong> People like to think they have more control over things than they actually do. This plausibly also includes control over the far future. Therefore, people are probably biased into thinking they have more control over the far future than they actually do, leading to higher estimates of ability to influence the future than is reasonable.</p>\n<p><strong>\"Wow factor\" bias.</strong> People seem attracted to more impressive claims. Saving a life for $2500 through a malaria bed net seems much more boring compared to the chance of saving the entire world by averting a global catastrophe. Within the Effective Altruist / LessWrong community, existential risk reduction is cool and high status, whereas averting global poverty is not. This might lead to more endorsement of existential risk reduction than is reasonable.</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">Conjunction fallacy</a>.</strong> &nbsp;People have a problem assessing probability properly when there are many steps involved, each of which has a chance of not happening. Ten steps, each with an independent 90% success rate, has only a 35% chance of success. &nbsp;Focusing on the far future seems to involve that a lot of largely independent events happen the way that is predicted. This would mean people are worse at estimating their chances of helping the far future, creating higher cost-effectiveness estimates than is reasonable.</p>\n<p><strong><a href=\"http://en.wikipedia.org/wiki/Selection_bias\">Selection bias</a>.</strong> &nbsp;When trying to find trends in history that are favorable for affecting the far future, some examples can be provided. &nbsp;However, this is because we usually hear about the interventions that end up working, whereas all the failed attempts to influence the far future are never heard of again. &nbsp;This creates a very skewed sample that can negatively bias our thinking about our success of influencing the far future.</p>\n<p>&nbsp;</p>\n<p>It\u2019s concerning there are numerous biases both weighted in favor and weighted against speculative causes, and this means we must tread carefully when assessing their merits. &nbsp;However, I would strongly expect biases to be even worse in favor of speculative causes rather than against them, because speculative causes lack the available feedback and objective evidence needed to help insulate against bias, whereas a focus on global health does not.</p>\n<p>&nbsp;</p>\n<h2 id=\"Focusing_on_Speculative_Causes_Uses_Bad_Decision_Theory\">Focusing on Speculative Causes Uses Bad Decision Theory</h2>\n<p>Furthermore, not only is the case for speculative causes undermined by a bad track record and possible cognitive biases, but the underlying decision theory seems suspect in a way that's difficult to place. &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Would_you_play_a_lottery_with_no_stated_odds_\">Would you play a lottery with no stated odds?</strong></p>\n<p>Imagine another thought experiment -- you're asked to play a lottery. You have to pay $2 to play, but you have a chance at winning $100. Do you play?</p>\n<p>Of course, you don't know, because you're not given odds. Rationally, it makes sense to play any lottery where you expect to come out ahead more often than not. If the lottery is a coin flip, it makes sense to pay $2 to have a 50/50 shot to win $100, since you'd expect to win $50 on average, and come ahead $48 each time. With a sufficiently high reward, even a one in a million chance is worth it. Pay $2 for a 1/1M chance of winning $1B, and you'd expect to come out ahead by $998 each time.</p>\n<p>But $2 for the chance to win $100, without knowing what the chance is? Even if you had some sort of bounds, like you knew the odds had to be at least 1/150 and at most 1/10, though you could be off by a little bit. Would you accept that bet?</p>\n<p>Such a bet seems intuitively uninviting to me, yet this is the bet that speculative causes offer me.</p>\n<p>&nbsp;</p>\n<p><strong id=\"_Conservative_Orders_of_Magnitude__Arguments\">\"Conservative Orders of Magnitude\" Arguments</strong></p>\n<p>In response to these considerations, I've seen people endorsing speculative causes look at their calculations and remark that even if their estimate were off by 1000x, or three orders of magnitude, they still would be on solid ground for high impact, and there's no way they're actually off by three orders of magnitude. However, Nate Silver's <a href=\"http://www.amazon.com/dp/159420411X\"><strong>The Signal and the Noise: Why So Many Predictions Fail \u2014 but Some Don't</strong></a>&nbsp;offers a cautionary tale:</p>\n<blockquote>\n<p>Moody\u2019s, for instance, went through a period of making ad hoc adjustments to its model in which it increased the default probability assigned to AAA-rated securities by 50 percent. That might seem like a very prudent attitude: surely a 50 percent buffer will suffice to account for any slack in one\u2019s assumptions? It might have been fine had the potential for error in their forecasts been linear and arithmetic. But leverage, or investments financed by debt, can make the error in a forecast compound many times over, and introduces the potential of highly geometric and nonlinear mistakes.</p>\n<p>Moody\u2019s 50 percent adjustment was like applying sunscreen and claiming it protected you from a nuclear meltdown\u2014wholly inadequate to the scale of the problem. It wasn\u2019t just a possibility that their estimates of default risk could be 50 percent too low: they might just as easily have underestimated it by 500 percent or 5,000 percent. In practice, defaults were two hundred times more likely than the ratings agencies claimed, meaning that their model was off by a mere 20,000 percent.</p>\n</blockquote>\n<p>Silver points out that when estimating how safe mortgage backed securities were, the difference between assuming defaults are perfectly uncorrelated and defaults are perfectly correlated is a difference of 160,000x in your risk estimate -- or five orders of magnitude.</p>\n<p>If these kinds of five-orders-of-magnitude errors are possible in a realm that has actual feedback and is moderately understood, how do we know the estimates for cost-effectiveness are safe for speculative causes that are poorly understood and offer no feedback? &nbsp;Again, our only recourse is to fall back on interventions that we can reliably predict, until we get better at prediction.</p>\n<p>&nbsp;</p>\n<h2 id=\"Value_of_Information__Exploring__and_Exploiting\">Value of Information, Exploring, and Exploiting</h2>\n<p>Of course, there still is one important aspect of this problem that has not been discussed -- <a href=\"http://en.wikipedia.org/wiki/Value_of_information\">value of information</a>&nbsp;-- or the idea that sometimes it\u2019s worth doing something just to learn more about how the world works. &nbsp;This is important in effective altruism too, where we focus specifically on \u201cgiving to learn\u201d, or using our resources to figure out more about the impact of various causes.</p>\n<p>I think this is actually really important and is not a victim to any of my previous arguments, because we\u2019re not talking about impact, but rather learning value. &nbsp;Perhaps one could look to an \"<a href=\"http://en.wikipedia.org/wiki/Multi-armed_bandit\">explore-exploit model</a>\", or the idea that we achieve the best outcome when we spend a lot of time exploring first (learning more about how to achieve better outcomes) before exploiting (focusing resources on achieving the best outcome we can). &nbsp;Therefore, whenever we have an opportunity to \u201cexplore\u201d further or learn more about what causes have high impact, we should take it.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Learning_in_Practice\">Learning in Practice</strong></p>\n<p>Unfortunately, in practice, I think these opportunities are very rare. &nbsp;Many organizations that I think are \u201cpromising\u201d and worth funding further to see what their impact looks like do not have sufficiently good self-measurement in place to actually assess their impact or sufficient transparency to provide that information, therefore making it difficult to actually learn from them. &nbsp;And on the other side of things, many very promising opportunities to learn more are already fully funded. &nbsp;One must be careful to ensure that it\u2019s actually one\u2019s marginal dollar that is getting marginal information.</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Typical_Donor\">The Typical Donor</strong></p>\n<p>Additionally, I don\u2019t think the typical donor is in a very good position to assess where there is high value of information or have the time and knowledge to act upon this information once it is acquired. &nbsp;I think there\u2019s a good argument for people in the \u201ceffective altruist\u201d movement to perhaps make small investments in EA organizations and encourage transparency and good measurement in their operations to see if they\u2019re successfully doing what they claim (or potentially create an EA startup themselves to see if it would work, though this carries large risks of further splitting the resources of the movement).</p>\n<p>But even that would take a very savvy and involved effective altruist to pull off. &nbsp;Assessing the value of information on more massive investments like large-scale research or innovation efforts would be significantly more difficult, beyond the talent and resources of nearly all effective altruists, and are probably left to full-time foundations or subject-matter experts.</p>\n<p>&nbsp;</p>\n<p><strong id=\"GiveWell_s_Top_Charities_Also_Have_High_Value_of_Information\">GiveWell\u2019s Top Charities Also Have High Value of Information</strong></p>\n<p>As Luke Muehlhauser mentions in <a href=\"/lw/hsd/start_under_the_streetlight_then_push_into_the/\">\"Start Under the Streetlight, Then Push Into the Shadows\"</a>, lots of lessons can be learned only by focusing on the easiest causes first, even if we have strong theoretical reasons to expect that they won\u2019t end up being the highest impact causes once we have more complete knowledge.</p>\n<p>We can use global health cost-effectiveness considerations as practice for slowly and carefully moving into the more complex and less understood domains. &nbsp;There even are some very natural transitions, such as beginning to look at \"<a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow through effects</a>\" of reducing disease in the third-world and beginning to look at how more esoteric things affect the disease burden, like climate change. &nbsp;Therefore, even additional funding for GiveWell\u2019s top charities has high value of information. &nbsp;And notably, GiveWell is beginning this \"push\" through <a href=\"http://blog.givewell.org/category/givewell-labs/\">GiveWell Labs</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>The bottom line is that sometimes things look too good to be true. &nbsp;Therefore, I should expect that the actual impact of speculative causes that make large promises, upon a thorough investigation, will be much lower.</p>\n<p>And this has been true in other domains. People are notoriously bad at estimating the effects of causes in both the developed world and developing world, and those are the causes that are near to us, provide us with feedback, and are easy to predict. Yet, from the Even Start Family Literacy Program to deworming estimates, our commonsense has failed us.</p>\n<p>Add to that the fact that we should expect ourselves to perform even worse at predicting the far future. Add to that optimism bias, control bias, \"wow factor\" bias, and the conjunction fallacy, which make it difficult for us to think realistically about speculative causes. And then add to that considerations in decision theory, and whether we would bet on a lottery with no stated odds.</p>\n<p>When all is said and done, I'm very skeptical of speculative projects. &nbsp;Therefore, I think we should be focused on exploring and exploiting. &nbsp;We should do whatever we can to fund projects aimed at learning more, when those are available, but be careful to make sure they actually have learning value. &nbsp;And when exploring isn\u2019t available, we should exploit what opportunities we have and fund proven interventions.</p>\n<p>But don\u2019t confuse these two concepts and fund causes intended for learning because of their actual impact value. &nbsp;I\u2019m skeptical about these causes actually being high impact, though I\u2019m open to the idea that they might be and look forward to funding them in the future when they become better proven. &nbsp; &nbsp;&nbsp;</p>\n<p>-</p>\n<p><strong>Followed up in: </strong><a href=\"/r/discussion/lw/i9u/what_would_it_take_to_prove_a_speculative_cause/\">\"What Would It Take To 'Prove' A Skeptical Cause\"</a>&nbsp;and <a href=\"/r/discussion/lw/ic0/where_ive_changed_my_mind_on_my_approach_to/\">\"Where I've Changed My Mind on My Approach to Speculative Causes\"</a>.</p>\n<p><em>This was also cross-posted <a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">to my blog</a> and <a href=\"http://effective-altruism.com/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">to effective-altruism.com</a>.</em></p>\n<p><em>I'd like to thank Nick Beckstead, Joey Savoie, Xio Kikauka, Carl Shulman, Ryan Carey, &nbsp;Tom Ash, Pablo Stafforini, Eliezer Yudkowsky, and Ben Hoskin for providing feedback on this essay, even if some of them might strongly disagree with it's conclusion.</em></p>", "sections": [{"title": "Focusing on Speculative Causes Requires Unreliable Commonsense", "anchor": "Focusing_on_Speculative_Causes_Requires_Unreliable_Commonsense", "level": 1}, {"title": "Can You Pick the Winning Social Program?", "anchor": "Can_You_Pick_the_Winning_Social_Program_", "level": 2}, {"title": "GiveWell and Commonsense's Track Record of Failure", "anchor": "GiveWell_and_Commonsense_s_Track_Record_of_Failure", "level": 2}, {"title": "People Are Notoriously Bad At Predicting the (Far) Future", "anchor": "People_Are_Notoriously_Bad_At_Predicting_the__Far__Future", "level": 2}, {"title": "Even Broad Effects Require Specific Attempts", "anchor": "Even_Broad_Effects_Require_Specific_Attempts", "level": 2}, {"title": "Focusing on Speculative Causes Plays Into Our Biases", "anchor": "Focusing_on_Speculative_Causes_Plays_Into_Our_Biases", "level": 1}, {"title": "Focusing on Speculative Causes Uses Bad Decision Theory", "anchor": "Focusing_on_Speculative_Causes_Uses_Bad_Decision_Theory", "level": 1}, {"title": "Would you play a lottery with no stated odds?", "anchor": "Would_you_play_a_lottery_with_no_stated_odds_", "level": 2}, {"title": "\"Conservative Orders of Magnitude\" Arguments", "anchor": "_Conservative_Orders_of_Magnitude__Arguments", "level": 2}, {"title": "Value of Information, Exploring, and Exploiting", "anchor": "Value_of_Information__Exploring__and_Exploiting", "level": 1}, {"title": "Learning in Practice", "anchor": "Learning_in_Practice", "level": 2}, {"title": "The Typical Donor", "anchor": "The_Typical_Donor", "level": 2}, {"title": "GiveWell\u2019s Top Charities Also Have High Value of Information", "anchor": "GiveWell_s_Top_Charities_Also_Have_High_Value_of_Information", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "98 comments"}], "headingsCount": 16}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hRAzDwwMuu8CZSTvN", "JvvGJxrfCgCiRLraq", "tX93igpzE5spGkpmp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T12:39:04.481Z", "modifiedAt": null, "url": null, "title": "Valuing Sentience: Can They Suffer?", "slug": "valuing-sentience-can-they-suffer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.194Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9zQBiqcsQa7kFoccr/valuing-sentience-can-they-suffer", "pageUrlRelative": "/posts/9zQBiqcsQa7kFoccr/valuing-sentience-can-they-suffer", "linkUrl": "https://www.lesswrong.com/posts/9zQBiqcsQa7kFoccr/valuing-sentience-can-they-suffer", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Valuing%20Sentience%3A%20Can%20They%20Suffer%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValuing%20Sentience%3A%20Can%20They%20Suffer%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zQBiqcsQa7kFoccr%2Fvaluing-sentience-can-they-suffer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Valuing%20Sentience%3A%20Can%20They%20Suffer%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zQBiqcsQa7kFoccr%2Fvaluing-sentience-can-they-suffer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zQBiqcsQa7kFoccr%2Fvaluing-sentience-can-they-suffer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>In the <a href=\"/lw/i3s/why_eat_less_meat/\">recent</a> <a href=\"/lw/i5e/the_argument_from_marginal_cases/\">discussions</a> <a href=\"/lw/i63/arguments_against_speciesism/\">here</a> about the value of animals several people have argued that what matters is \"sentience\", or the ability to feel. This goes back to at least Bentham with \"The question is not, Can they reason? nor, Can they talk? but, Can they suffer?\"</p>\n<p>Is \"can they feel pain\" or \"can they feel pleasure\" really the right question, though? Let's say we research the biological correlates of pleasure until we understand how to make a compact and efficient network of neurons that constantly experiences maximum pleasure. Because we've thrown out nearly everything else a brain does, this has the potential for orders of magnitude more sentience per gram of neurons than anything currently existing. A group of altruists intend to create a \"happy neuron farm\" of these: is this valuable? &nbsp;How valuable?</p>\n<p>(Or say a supervillian is creating a \"sad neuron farm\". How important is it that we stop them? &nbsp;Does it matter at all?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 2, "Q9ASuEEoJWxT3RLMT": 2, "nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9zQBiqcsQa7kFoccr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 1.2831961713778616e-06, "legacy": true, "legacyId": "23560", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LbbyQhLkcwAwWmBoj", "2rit2BKzWSWsP2yKT", "tm2i5r2rNSfeam5or"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T13:19:32.677Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes from people associated with LessWrong", "slug": "rationality-quotes-from-people-associated-with-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:05.879Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iWTZj26MfR8e8b9nm/rationality-quotes-from-people-associated-with-lesswrong", "pageUrlRelative": "/posts/iWTZj26MfR8e8b9nm/rationality-quotes-from-people-associated-with-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/iWTZj26MfR8e8b9nm/rationality-quotes-from-people-associated-with-lesswrong", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20from%20people%20associated%20with%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20from%20people%20associated%20with%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWTZj26MfR8e8b9nm%2Frationality-quotes-from-people-associated-with-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20from%20people%20associated%20with%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWTZj26MfR8e8b9nm%2Frationality-quotes-from-people-associated-with-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWTZj26MfR8e8b9nm%2Frationality-quotes-from-people-associated-with-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>The other rationality quotes thread operates under the rule:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 17.27272605895996px; text-align: justify;\">Do not quote from Less Wrong itself, Overcoming Bias, or HPMoR.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 17.27272605895996px; text-align: justify;\">Lately it seems that every&nbsp;</span>MIRI or CFAR employee is excempt from being quoted.</p>\n<p>As there are still interesting quotes that happen on LessWrong, Overcoming Bias, HPMoR and MIRI/CFAR employee in general, I think it makes sense to open this thread to provide a place for those quotes.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iWTZj26MfR8e8b9nm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 35, "extendedScore": null, "score": 1.283228497134922e-06, "legacy": true, "legacyId": "23561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T13:47:55.320Z", "modifiedAt": null, "url": null, "title": "[link] Book review: Mindmelding: Consciousness, Neuroscience, and the Mind\u2019s Privacy", "slug": "link-book-review-mindmelding-consciousness-neuroscience-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:19.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e56rwjzsc4tAuy8aw/link-book-review-mindmelding-consciousness-neuroscience-and", "pageUrlRelative": "/posts/e56rwjzsc4tAuy8aw/link-book-review-mindmelding-consciousness-neuroscience-and", "linkUrl": "https://www.lesswrong.com/posts/e56rwjzsc4tAuy8aw/link-book-review-mindmelding-consciousness-neuroscience-and", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Book%20review%3A%20Mindmelding%3A%20Consciousness%2C%20Neuroscience%2C%20and%20the%20Mind%E2%80%99s%20Privacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Book%20review%3A%20Mindmelding%3A%20Consciousness%2C%20Neuroscience%2C%20and%20the%20Mind%E2%80%99s%20Privacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe56rwjzsc4tAuy8aw%2Flink-book-review-mindmelding-consciousness-neuroscience-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Book%20review%3A%20Mindmelding%3A%20Consciousness%2C%20Neuroscience%2C%20and%20the%20Mind%E2%80%99s%20Privacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe56rwjzsc4tAuy8aw%2Flink-book-review-mindmelding-consciousness-neuroscience-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe56rwjzsc4tAuy8aw%2Flink-book-review-mindmelding-consciousness-neuroscience-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 852, "htmlBody": "<p><a href=\"http://kajsotala.fi/2013/07/book-review-mindmelding-consciousness-neuroscience-and-the-minds-privacy/\">http://kajsotala.fi/2013/07/book-review-mindmelding-consciousness-neuroscience-and-the-minds-privacy/</a></p>\n<p>I review <a href=\"http://en.wikipedia.org/wiki/William_Hirstein\">William Hirstein's</a> book <a href=\"http://www.amazon.com/Mindmelding-Consciousness-Neuroscience-Minds-Privacy/dp/0199231907/\">Mindmelding: Consciousness, Neuroscience, and the Mind&rsquo;s Privacy</a>, which he proposes a way of connecting the brains of two different people together so that when person A has a conscious experience, person B may also have the same experience. In particular, I compare it to my and Harri Valpola's earlier paper <a href=\"http://kajsotala.fi/Papers/CoalescingMinds.pdf\">Coalescing Minds</a>, in which we argued that it would be possible to join the brains of two people together in such a way that they'd become a single mind.</p>\n<blockquote>\n<p>Fortunately, it turns out that the book and the paper are actually  rather nicely complementary. To briefly summarize the main differences,  we intentionally skimmed over many neuroscientific details in order to  establish mindmelding as a possible future trend, while Hirstein  extensively covers the neuroscience but is mostly interested in  mindmelding as a thought experiment. We seek to predict a possible  future trend, while Hirstein seeks to argue a philosophical position:  Hirstein focuses on philosophical implications while we focus on  societal implications. Hirstein talks extensively about the possibility  of one person perceiving another&rsquo;s mental states while both remaining  distinct individuals, while we mainly discuss the possibility of two  distinct individuals coalescing together into one.</p>\n</blockquote>\n<p>I expect that LW readers might be particularly interested in some of the possible implications of Hirstein's argument, which he himself didn't discuss in the book, but which I speculated on in the review:</p>\n<blockquote>\n<p>Most obviously, if another person&rsquo;s conscious states could be  recorded and replayed, it would open the doors for using this as  entertainment. Were it the case that you couldn&rsquo;t just record and replay  anyone&rsquo;s conscious experience, but learning to correctly interpret the  data from another brain would require time and practice, then individual  method actors capable of immersing themselves in a wide variety of  emotional states might become the new movie stars. Once your brain  learned to interpret their conscious states, you could follow them in a  wide variety of movie-equivalents, with new actors being hampered by the  fact that learning to interpret the conscious states of someone who had  only appeared in one or two productions wouldn&rsquo;t be worth the effort.  If mind uploading was available, this might give considerable power to a  copy clan consisting of copies of the same actor, each participating in  different productions but each having a similar enough brain that  learning to interpret one&rsquo;s conscious states would be enough to give  access to the conscious states of all the others.</p>\n<p>The ability to perceive various drug- or meditation-induced states of  altered consciousness while still having one&rsquo;s executive processes  unhindered and functional would probably be fascinating for  consciousness researchers and the general public alike. At the same  time, the ability for anyone to experience happiness or pleasure by just  replaying another person&rsquo;s experience of it might finally bring <a href=\"http://en.wikipedia.org/wiki/Wirehead_%28science_fiction%29\">wireheading</a> within easy reach, with all the dangers associated with that.</p>\n<p>A Hirstein-style mind meld might possibly also be used as an uploading technique. <a href=\"http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf\">Some upload proposals</a> suggest compiling a rich database of information about a specific  person, and then later using that information to construct a virtual  mind whose behavior would be consistent with the information about that  person. While creating such a mind based on just behavioral data makes  questionable the extent to which the new person would really be a copy  of the original, the skeptical argument loses some of its force if we  can also include in the data a recording of all the original&rsquo;s conscious  states during various points in their life. If we are able to use the  data to construct a mind that would react to the same sensory inputs  with the same conscious states as the original did, whose executive  processes would manipulate those states in the same ways as the  original, and who would take the same actions as the original did, would  that mind then not essentially <em>be</em> the same mind as the original mind?</p>\n<p>Hirstein&rsquo;s argumentation is also relevant for our speculations  concerning the evolution of mind coalescences. We spoke abstractly about  the &rdquo;preferences&rdquo; of a mind, suggesting that it might be possible for  one mind to extract the knowledge from another mind without inherting  its preferences, and noting that conflicting preferences would be one  reason for two minds to avoid coalescing together. However, we did not  say much about where in the brain preferences are produced, and what  would be actually required for e.g. one mind to extract another&rsquo;s  knowledge without also acquiring its preferences. As the above  discussion hopefully shows, some of our preferences are implicit in our  automatic habits (the things that we show we value with our daily  routines), some in the preprocessing of sensory data that our brains  carry out (the things and ideas that are &rdquo;painted with&rdquo; positive  associations or feelings), and some in the configuration of our  executive processes (the actions we actually end up doing in response to  novel or conflicting situations). (<a href=\"http://lesswrong.com/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">See</a> <a href=\"http://lesswrong.com/lw/9jh/the_humans_hidden_utility_function_maybe/\">also</a>.)  This kind of a breakdown seems like very promising material for some  neuroscience-aware philosopher to tackle in an attempt to figure out  just what exactly preferences <em>are</em>; maybe someone has already done so.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2, "XSryTypw5Hszpa4TS": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e56rwjzsc4tAuy8aw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.2832523934090928e-06, "legacy": true, "legacyId": "23562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hN2aRnu798yas5b2k", "fa5o2tg9EfJE77jEQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T19:19:34.038Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 79-81", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-79-81", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m5cRN2MfosSGDwz5c/meetup-durham-rtlw-hpmor-discussion-ch-79-81", "pageUrlRelative": "/posts/m5cRN2MfosSGDwz5c/meetup-durham-rtlw-hpmor-discussion-ch-79-81", "linkUrl": "https://www.lesswrong.com/posts/m5cRN2MfosSGDwz5c/meetup-durham-rtlw-hpmor-discussion-ch-79-81", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2079-81&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2079-81%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5cRN2MfosSGDwz5c%2Fmeetup-durham-rtlw-hpmor-discussion-ch-79-81%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2079-81%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5cRN2MfosSGDwz5c%2Fmeetup-durham-rtlw-hpmor-discussion-ch-79-81", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5cRN2MfosSGDwz5c%2Fmeetup-durham-rtlw-hpmor-discussion-ch-79-81", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pd'>Durham/RTLW HPMoR discussion, ch. 79-81</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 August 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion of HPMoR 79-81 will occur at Fullsteam on Saturday 8/10.</p>\n\n<p>We'll gather around 12 and discuss from 12:30 til 2, or whenever we run out of things to say (whichever comes first.)</p>\n\n<p>Feel free to bring non-brewery comestibles (i.e., coffee and lunch/brunch/breakfast.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pd'>Durham/RTLW HPMoR discussion, ch. 79-81</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m5cRN2MfosSGDwz5c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__79_81\">Discussion article for the meetup : <a href=\"/meetups/pd\">Durham/RTLW HPMoR discussion, ch. 79-81</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 August 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion of HPMoR 79-81 will occur at Fullsteam on Saturday 8/10.</p>\n\n<p>We'll gather around 12 and discuss from 12:30 til 2, or whenever we run out of things to say (whichever comes first.)</p>\n\n<p>Feel free to bring non-brewery comestibles (i.e., coffee and lunch/brunch/breakfast.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__79_811\">Discussion article for the meetup : <a href=\"/meetups/pd\">Durham/RTLW HPMoR discussion, ch. 79-81</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 79-81", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__79_81", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 79-81", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__79_811", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T21:10:02.906Z", "modifiedAt": null, "url": null, "title": "Optimize Your Settings", "slug": "optimize-your-settings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:31.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WLwFq5QGZ9K727Brx/optimize-your-settings", "pageUrlRelative": "/posts/WLwFq5QGZ9K727Brx/optimize-your-settings", "linkUrl": "https://www.lesswrong.com/posts/WLwFq5QGZ9K727Brx/optimize-your-settings", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimize%20Your%20Settings&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimize%20Your%20Settings%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLwFq5QGZ9K727Brx%2Foptimize-your-settings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimize%20Your%20Settings%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLwFq5QGZ9K727Brx%2Foptimize-your-settings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLwFq5QGZ9K727Brx%2Foptimize-your-settings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 950, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a></p>\n<p>Perhaps the most significant teaching social psychology has to offer is that most of our behaviors are determined by <a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">situational factors</a> inherent to our settings, not by our personal qualities.<sup>[1] </sup></p>\n<p>Some consider this depressing-- for instance, the <a href=\"http://en.wikipedia.org/wiki/Milgram_experiment\">Milgram experiments in obedience to authority</a> and <a href=\"http://en.wikipedia.org/wiki/Stanford_prison_experiment\">Stanford prison experiment</a> are often cited as examples of how settings can cause otherwise-good people to participate in and even support unethical and dangerous behavior. However, as lukeprog points out in <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a>, this principle can also be considered uplifting. After all, if our settings have such an effect on our behavior, they are thus a powerful tool that we can employ to make ourselves <a href=\"http://wiki.lesswrong.com/wiki/Instrumental_rationality#Instrumental_rationality\">more effective</a>.<sup>[2]</sup></p>\n<p>&nbsp;</p>\n<p><strong>Changing Your Physical Settings<br /></strong></p>\n<p>One relatively easy place to start making such changes is in your personal life. I have found that great productivity increases can be gained through relatively minor changes in lifestyle-- or even seemingly-trivial matters such as the position of physical (or sometimes digital) objects in your environment!</p>\n<p>For instance, I recently noticed a tendency in myself to \"wake up\" and then waste the next twenty or thirty minutes aimlessly browsing the Internet on my laptop in bed before actually getting up and eating breakfast, showering, going to work, etc. Since I value time, especially morning time, substantially, I decided that action should be taken to avoid this.</p>\n<p>At first, I figured that once I had noticed the problem I could simply apply willpower and avoid it, but this proved less than effective-- it turns out that my willpower is not at its strongest when I first wake up and am still a little groggy!<sup>[3]</sup> I then decided to apply the principles of situational psychology to the situation. The most obvious setting contributing to the problem was that I was using an alarm app on my computer to wake up in the morning, and turning off this alarm caused me to interact with the computer.</p>\n<p>So I picked up an IKEA alarm clock, turned off my alarm app, and moved my computer to the kitchen instead of my room-- problem solved. In my new settings, browsing in bed was outright ridiculous-- I'd have to wake up, go downstairs to the kitchen, pick up my computer, and bring it back up to my room with me. Not a likely course of events!</p>\n<p>&nbsp;</p>\n<p><strong>Changing Your Mental Settings<br /></strong></p>\n<p>While physical environments can certainly produce changes in behavior,<sup>[4]</sup> social and intellectual environments can too.</p>\n<p>For instance, one of my friends from undergrad took an interesting approach when choosing what major to take. He knew that he wanted a solid private-sector income that would allow him to support a family, but didn't particularly care what field it was in. Overall, he wanted to ensure that whatever major he chose would have the highest possible chance of getting him a good job without unusual effort or circumstances.</p>\n<p>Therefore, during winter term of his sophomore year, prior to declaring, he went around to all the seniors he could get to talk to him and asked them what their major was, what they were doing post-graduation, and how much money they anticipated making. He found that the CS majors tended to have more private-sector job prospects and higher average starting salaries than students in other fields, so he decided to declare a CS major.<sup>[5]</sup></p>\n<p>While I don't think my friend's approach is necessarily the <em>best</em> possible option for determining what to do with your life, it certainly beats the sort of unstructured guessing that I've seen many others do. By considering academic majors as settings and examining what setting produced the best result on average, my friend managed to find a field and career that he's by all indications quite happy in-- and with a minimal amount of risk and stress involved.</p>\n<p>&nbsp;</p>\n<p><strong>Conclusion</strong></p>\n<p>Human psychology is greatly influenced by situational factors, and in more ways than a naive reasoner might expect. If you're looking to improve your life across any particular axis, one good way to start is by examining your current physical, social, and intellectual settings and paying close attention to how changes in those settings might help accomplish your goals.</p>\n<p>&nbsp;</p>\n<p>[1] If you don't believe that this is true, I advise simulating that you do and going on anyway. I find this method effective enough for me and others and easy enough to implement that it seems well worth testing, even if you don't fully believe in the claims behind it. At worst, it might become a potential <a href=\"/lw/hgv/epistemic_and_instrumental_tradeoffs/\">epistemic/instrumental tradeoff</a>.</p>\n<p>[2] See for instance Joseph Heath and Joel Anderson, <a href=\"http://www.phil.uu.nl/%7Ejoel/research/publications/Procrastination-ExtendedWill%28Heath-Anderson%29Feb2009.pdf\">Procrastination and the Extended Will</a> (2009).</p>\n<p>[3] In the course of researching and writing this post, I encountered some objections to the resource expenditure theory of willpower (many of which have already been <a href=\"/lw/2y2/willpower_not_a_limited_resource/\">summarized </a><a href=\"/lw/2y2/willpower_not_a_limited_resource/\">here</a> by Jess_Riedel). I believe my beliefs regarding willpower loss while tired/just awakening may be limiting in the same sense that believing willpower is a limited resource appears limiting, but have yet to test at the time of this writing.</p>\n<p>[4] If you're interested in seeing other examples of ways in which we can structure the physical objects around us in order to become more productive, you may wish to check out Alicorn's <a href=\"/lw/eyt/how_to_have_things_correctly/\">How to </a><a href=\"/lw/eyt/how_to_have_things_correctly/\">Have Things Correctly</a> and fowlertm's related <a href=\"/lw/hi2/how_to_have_space_correctly/\">How to Have Space Correctly</a>. Several of Alyssa Vance's <a href=\"http://rationalconspiracy.com/2012/12/02/random-life-tips/\">Random Life Tips</a> also relate to this matter.</p>\n<p>[5] The friend in question is now employed as a software engineer at a tech company and by all indications loves his job. Note though that this post <em>isn't</em> saying \"you should be a CS major.\" Things change over time, and what was a good choice for one person and one time may not be a good choice for another person or another time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WLwFq5QGZ9K727Brx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 23, "extendedScore": null, "score": 1.2836135463006028e-06, "legacy": true, "legacyId": "22933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to:</strong> <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a></p>\n<p>Perhaps the most significant teaching social psychology has to offer is that most of our behaviors are determined by <a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">situational factors</a> inherent to our settings, not by our personal qualities.<sup>[1] </sup></p>\n<p>Some consider this depressing-- for instance, the <a href=\"http://en.wikipedia.org/wiki/Milgram_experiment\">Milgram experiments in obedience to authority</a> and <a href=\"http://en.wikipedia.org/wiki/Stanford_prison_experiment\">Stanford prison experiment</a> are often cited as examples of how settings can cause otherwise-good people to participate in and even support unethical and dangerous behavior. However, as lukeprog points out in <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a>, this principle can also be considered uplifting. After all, if our settings have such an effect on our behavior, they are thus a powerful tool that we can employ to make ourselves <a href=\"http://wiki.lesswrong.com/wiki/Instrumental_rationality#Instrumental_rationality\">more effective</a>.<sup>[2]</sup></p>\n<p>&nbsp;</p>\n<p><strong id=\"Changing_Your_Physical_Settings\">Changing Your Physical Settings<br></strong></p>\n<p>One relatively easy place to start making such changes is in your personal life. I have found that great productivity increases can be gained through relatively minor changes in lifestyle-- or even seemingly-trivial matters such as the position of physical (or sometimes digital) objects in your environment!</p>\n<p>For instance, I recently noticed a tendency in myself to \"wake up\" and then waste the next twenty or thirty minutes aimlessly browsing the Internet on my laptop in bed before actually getting up and eating breakfast, showering, going to work, etc. Since I value time, especially morning time, substantially, I decided that action should be taken to avoid this.</p>\n<p>At first, I figured that once I had noticed the problem I could simply apply willpower and avoid it, but this proved less than effective-- it turns out that my willpower is not at its strongest when I first wake up and am still a little groggy!<sup>[3]</sup> I then decided to apply the principles of situational psychology to the situation. The most obvious setting contributing to the problem was that I was using an alarm app on my computer to wake up in the morning, and turning off this alarm caused me to interact with the computer.</p>\n<p>So I picked up an IKEA alarm clock, turned off my alarm app, and moved my computer to the kitchen instead of my room-- problem solved. In my new settings, browsing in bed was outright ridiculous-- I'd have to wake up, go downstairs to the kitchen, pick up my computer, and bring it back up to my room with me. Not a likely course of events!</p>\n<p>&nbsp;</p>\n<p><strong id=\"Changing_Your_Mental_Settings\">Changing Your Mental Settings<br></strong></p>\n<p>While physical environments can certainly produce changes in behavior,<sup>[4]</sup> social and intellectual environments can too.</p>\n<p>For instance, one of my friends from undergrad took an interesting approach when choosing what major to take. He knew that he wanted a solid private-sector income that would allow him to support a family, but didn't particularly care what field it was in. Overall, he wanted to ensure that whatever major he chose would have the highest possible chance of getting him a good job without unusual effort or circumstances.</p>\n<p>Therefore, during winter term of his sophomore year, prior to declaring, he went around to all the seniors he could get to talk to him and asked them what their major was, what they were doing post-graduation, and how much money they anticipated making. He found that the CS majors tended to have more private-sector job prospects and higher average starting salaries than students in other fields, so he decided to declare a CS major.<sup>[5]</sup></p>\n<p>While I don't think my friend's approach is necessarily the <em>best</em> possible option for determining what to do with your life, it certainly beats the sort of unstructured guessing that I've seen many others do. By considering academic majors as settings and examining what setting produced the best result on average, my friend managed to find a field and career that he's by all indications quite happy in-- and with a minimal amount of risk and stress involved.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>Human psychology is greatly influenced by situational factors, and in more ways than a naive reasoner might expect. If you're looking to improve your life across any particular axis, one good way to start is by examining your current physical, social, and intellectual settings and paying close attention to how changes in those settings might help accomplish your goals.</p>\n<p>&nbsp;</p>\n<p>[1] If you don't believe that this is true, I advise simulating that you do and going on anyway. I find this method effective enough for me and others and easy enough to implement that it seems well worth testing, even if you don't fully believe in the claims behind it. At worst, it might become a potential <a href=\"/lw/hgv/epistemic_and_instrumental_tradeoffs/\">epistemic/instrumental tradeoff</a>.</p>\n<p>[2] See for instance Joseph Heath and Joel Anderson, <a href=\"http://www.phil.uu.nl/%7Ejoel/research/publications/Procrastination-ExtendedWill%28Heath-Anderson%29Feb2009.pdf\">Procrastination and the Extended Will</a> (2009).</p>\n<p>[3] In the course of researching and writing this post, I encountered some objections to the resource expenditure theory of willpower (many of which have already been <a href=\"/lw/2y2/willpower_not_a_limited_resource/\">summarized </a><a href=\"/lw/2y2/willpower_not_a_limited_resource/\">here</a> by Jess_Riedel). I believe my beliefs regarding willpower loss while tired/just awakening may be limiting in the same sense that believing willpower is a limited resource appears limiting, but have yet to test at the time of this writing.</p>\n<p>[4] If you're interested in seeing other examples of ways in which we can structure the physical objects around us in order to become more productive, you may wish to check out Alicorn's <a href=\"/lw/eyt/how_to_have_things_correctly/\">How to </a><a href=\"/lw/eyt/how_to_have_things_correctly/\">Have Things Correctly</a> and fowlertm's related <a href=\"/lw/hi2/how_to_have_space_correctly/\">How to Have Space Correctly</a>. Several of Alyssa Vance's <a href=\"http://rationalconspiracy.com/2012/12/02/random-life-tips/\">Random Life Tips</a> also relate to this matter.</p>\n<p>[5] The friend in question is now employed as a software engineer at a tech company and by all indications loves his job. Note though that this post <em>isn't</em> saying \"you should be a CS major.\" Things change over time, and what was a good choice for one person and one time may not be a good choice for another person or another time.</p>", "sections": [{"title": "Changing Your Physical Settings", "anchor": "Changing_Your_Physical_Settings", "level": 1}, {"title": "Changing Your Mental Settings", "anchor": "Changing_Your_Mental_Settings", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q5CjE8pRiACqTvhRM", "hZud7CxcEqfYTL7YX", "NPxGwZGoyyrNzkjNw", "KT8Mf3ey6uwQAkWek", "uWDGzJME8FmnzrMgm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T21:38:41.929Z", "modifiedAt": null, "url": null, "title": "Leveling up...", "slug": "leveling-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:26.497Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertChange", "createdAt": "2013-01-17T09:34:41.865Z", "isAdmin": false, "displayName": "RobertChange"}, "userId": "c7jX3DDBqSMM9FwvK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zLFpX6ySm8ddiW8z3/leveling-up", "pageUrlRelative": "/posts/zLFpX6ySm8ddiW8z3/leveling-up", "linkUrl": "https://www.lesswrong.com/posts/zLFpX6ySm8ddiW8z3/leveling-up", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Leveling%20up...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeveling%20up...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLFpX6ySm8ddiW8z3%2Fleveling-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Leveling%20up...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLFpX6ySm8ddiW8z3%2Fleveling-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLFpX6ySm8ddiW8z3%2Fleveling-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p>I just figured out how to use the local banking system and I will be able for the first time to pay my rent from my actual salary received in this country (as opposed to savings from my previous life). Also I always hated shopping for groceries and now I can do it without much pain, because I've found a way that works for me. As I reflect on this, an expression comes to my mind: \"leveling up.\" I don't have the same problems any more which I used to have. I grow and face new challenges.</p>\n<p>Did you, fellow rationalists and transhumanists, ever have that feeling? Any particular accomplishments, big or small, that made you feel you're advancing? No matter fast or slow, in big steps or tiny, but firmly forward!?</p>\n<p>I'm thrilled to read your stories!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zLFpX6ySm8ddiW8z3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 1.2836369556833542e-06, "legacy": true, "legacyId": "23564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-29T22:26:36.505Z", "modifiedAt": null, "url": null, "title": "Open thread, July 29-August 4, 2013", "slug": "open-thread-july-29-august-4-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:00.147Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Yo2geey9fs5FRHue2/open-thread-july-29-august-4-2013", "pageUrlRelative": "/posts/Yo2geey9fs5FRHue2/open-thread-july-29-august-4-2013", "linkUrl": "https://www.lesswrong.com/posts/Yo2geey9fs5FRHue2/open-thread-july-29-august-4-2013", "postedAtFormatted": "Monday, July 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20July%2029-August%204%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20July%2029-August%204%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYo2geey9fs5FRHue2%2Fopen-thread-july-29-august-4-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20July%2029-August%204%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYo2geey9fs5FRHue2%2Fopen-thread-july-29-august-4-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYo2geey9fs5FRHue2%2Fopen-thread-july-29-august-4-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>\n<p>Of course, for \"every Monday\", the last one should have been dated July 22-28. <em>*cough*</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Yo2geey9fs5FRHue2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23565", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 390, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-30T04:08:01.527Z", "modifiedAt": null, "url": null, "title": "Freakonomics Study Investigates Decision-Making and Estimated Prior Probabilities", "slug": "freakonomics-study-investigates-decision-making-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:20.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "telms", "createdAt": "2012-09-03T03:28:43.729Z", "isAdmin": false, "displayName": "telms"}, "userId": "dS3ktpbDNffXTmDPv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PfupnyY23ABmTL7cQ/freakonomics-study-investigates-decision-making-and", "pageUrlRelative": "/posts/PfupnyY23ABmTL7cQ/freakonomics-study-investigates-decision-making-and", "linkUrl": "https://www.lesswrong.com/posts/PfupnyY23ABmTL7cQ/freakonomics-study-investigates-decision-making-and", "postedAtFormatted": "Tuesday, July 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Freakonomics%20Study%20Investigates%20Decision-Making%20and%20Estimated%20Prior%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFreakonomics%20Study%20Investigates%20Decision-Making%20and%20Estimated%20Prior%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPfupnyY23ABmTL7cQ%2Ffreakonomics-study-investigates-decision-making-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Freakonomics%20Study%20Investigates%20Decision-Making%20and%20Estimated%20Prior%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPfupnyY23ABmTL7cQ%2Ffreakonomics-study-investigates-decision-making-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPfupnyY23ABmTL7cQ%2Ffreakonomics-study-investigates-decision-making-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 427, "htmlBody": "<p>The Freakonomics web site is currently conducting online research that appears, to this properly hypothesis-blinded participant, to be investigating decision-making and estimated prior probability of success. You can participate yourself at&nbsp; <a href=\"http://www.freakonomics.com/experiments/\">http://www.freakonomics.com/experiments/</a>.</p>\n<p>The study asks the participant to choose a yes/no decision that they would be willing to commit to making on the basis of a random coin toss. (Well, actually, the random decay of an atomic nucleus, but they use coin flip graphics.) In my case, the only decision I was willing to make on such a random basis is something with very low risks: namely, the decision whether or not to quit twisting my hair. I accepted the obligation to change my behavior based on a coin toss, and the coin toss says I gotta change.</p>\n<p>Breaking a habit of such long standing will be difficult. Past behavior is the best predictor of future behavior, and all that, so when they asked how LIKELY I thought it would be that my hair-twisting habit would stick despite my best efforts to get rid of it, I estimated 90%. Yet I also claimed that I WILL PROBABLY (not certainly, but probably) conquer the habit.</p>\n<p>Yes, I recognize the dissonance between these two statements. It intrigues me. Is it perhaps the intent of the experiment to create explicit, conscious, cognitive dissonance like this in some participants, and see what difference it makes to outcomes?</p>\n<p>They could easily have phrased the odds question in the inverse form. They COULD have asked how likely I thought it was that I would SUCCEED in achieving my goal. That would align neatly with my statement of commitment and yield no dissonance. I could make the usual biased assumptions that strength of willpower is the same as odds of success, and over-estimate those success odds accordingly.</p>\n<p>I don't actually know that the study cares about this, but this is what I would care about if I were the researchers.</p>\n<p>The Freakonomics people will be following up over time by email. They're also checking on me through a friend, so there is every possibility that they expect to see an interaction between social involvement in the decision's outcome and the presence of cognitive dissonance, which is believed to drive SOCIAL behavior more strongly than it drives personal decisions kept to oneself.<br /><br />I'm posting this to increase my social commitment, of course. I also posted on Facebook. It's terrible to have a psychologically trained participant make assumptions about your research project and leverage those assumptions to the max for imaginary ends. But that's life in social science. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PfupnyY23ABmTL7cQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.2839551379069691e-06, "legacy": true, "legacyId": "23549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-30T09:27:14.981Z", "modifiedAt": null, "url": null, "title": "A thought on the value of \"rationality\" as a value", "slug": "a-thought-on-the-value-of-rationality-as-a-value", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:26.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/icT2q7EyPCtPkbtB8/a-thought-on-the-value-of-rationality-as-a-value", "pageUrlRelative": "/posts/icT2q7EyPCtPkbtB8/a-thought-on-the-value-of-rationality-as-a-value", "linkUrl": "https://www.lesswrong.com/posts/icT2q7EyPCtPkbtB8/a-thought-on-the-value-of-rationality-as-a-value", "postedAtFormatted": "Tuesday, July 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20thought%20on%20the%20value%20of%20%22rationality%22%20as%20a%20value&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20thought%20on%20the%20value%20of%20%22rationality%22%20as%20a%20value%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FicT2q7EyPCtPkbtB8%2Fa-thought-on-the-value-of-rationality-as-a-value%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20thought%20on%20the%20value%20of%20%22rationality%22%20as%20a%20value%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FicT2q7EyPCtPkbtB8%2Fa-thought-on-the-value-of-rationality-as-a-value", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FicT2q7EyPCtPkbtB8%2Fa-thought-on-the-value-of-rationality-as-a-value", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p>I read an interesting article today: [\"Your app makes me fat\"](<a href=\"http://seriouspony.com/blog/2013/7/24/your-app-makes-me-fat\">http://seriouspony.com/blog/2013/7/24/your-app-makes-me-fat</a>). Key quote:</p>\r\n<p>\"Researchers were astonished by a pile of experiments that led to one bizzare conclusion: Willpower and cognitive processing draw from the same pool of resources.\"</p>\r\n<p>Now, when we tell people to behave rationally, we often tend to ask them to consider short term sacrifices for long term gains and act to maximise the overall \"utility\"; to run through a process of evaluation and taking action that uses up both cognitive processing and willpower at once.</p>\r\n<p>I observed on many occasions that it is easy to make the \"right' choice when you value the fact that you are trying to live your life in the right manner. The nice feels that you get when making the right choice compensate for the willpower expended in taking the corresponding actions.</p>\r\n<p>And perhaps this is the value of \"rationality\" as a value.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "icT2q7EyPCtPkbtB8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 1.2842161322757139e-06, "legacy": true, "legacyId": "23576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-30T21:16:12.077Z", "modifiedAt": null, "url": null, "title": "Meetup : [Boston] Goal Factoring", "slug": "meetup-boston-goal-factoring", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ben_LandauTaylor", "createdAt": "2013-07-25T17:40:48.283Z", "isAdmin": false, "displayName": "Ben_LandauTaylor"}, "userId": "ZvoQwr4zZjPuC2oNJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4GEsmciY5c3LSjMDp/meetup-boston-goal-factoring", "pageUrlRelative": "/posts/4GEsmciY5c3LSjMDp/meetup-boston-goal-factoring", "linkUrl": "https://www.lesswrong.com/posts/4GEsmciY5c3LSjMDp/meetup-boston-goal-factoring", "postedAtFormatted": "Tuesday, July 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BBoston%5D%20Goal%20Factoring&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BBoston%5D%20Goal%20Factoring%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GEsmciY5c3LSjMDp%2Fmeetup-boston-goal-factoring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BBoston%5D%20Goal%20Factoring%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GEsmciY5c3LSjMDp%2Fmeetup-boston-goal-factoring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GEsmciY5c3LSjMDp%2Fmeetup-boston-goal-factoring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pe'>[Boston] Goal Factoring</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 August 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/131852462/\" rel=\"nofollow\">Meetup event page</a></p>\n\n<p>Goal factoring is a method for figuring out why you do the things you do, and for evaluating different ways of achieving the same ends. This lesson is based on material developed by the Center for Applied Rationality and Leverage Research.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pe'>[Boston] Goal Factoring</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4GEsmciY5c3LSjMDp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.284796099477704e-06, "legacy": true, "legacyId": "23577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Boston__Goal_Factoring\">Discussion article for the meetup : <a href=\"/meetups/pe\">[Boston] Goal Factoring</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 August 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/131852462/\" rel=\"nofollow\">Meetup event page</a></p>\n\n<p>Goal factoring is a method for figuring out why you do the things you do, and for evaluating different ways of achieving the same ends. This lesson is based on material developed by the Center for Applied Rationality and Leverage Research.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Boston__Goal_Factoring1\">Discussion article for the meetup : <a href=\"/meetups/pe\">[Boston] Goal Factoring</a></h2>", "sections": [{"title": "Discussion article for the meetup : [Boston] Goal Factoring", "anchor": "Discussion_article_for_the_meetup____Boston__Goal_Factoring", "level": 1}, {"title": "Discussion article for the meetup : [Boston] Goal Factoring", "anchor": "Discussion_article_for_the_meetup____Boston__Goal_Factoring1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-31T04:58:40.069Z", "modifiedAt": null, "url": null, "title": "Internet Research (with tangent on intelligence analysis and collapse)", "slug": "internet-research-with-tangent-on-intelligence-analysis-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BfDfGPRfDZXN2AZF8/internet-research-with-tangent-on-intelligence-analysis-and", "pageUrlRelative": "/posts/BfDfGPRfDZXN2AZF8/internet-research-with-tangent-on-intelligence-analysis-and", "linkUrl": "https://www.lesswrong.com/posts/BfDfGPRfDZXN2AZF8/internet-research-with-tangent-on-intelligence-analysis-and", "postedAtFormatted": "Wednesday, July 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Internet%20Research%20(with%20tangent%20on%20intelligence%20analysis%20and%20collapse)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInternet%20Research%20(with%20tangent%20on%20intelligence%20analysis%20and%20collapse)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfDfGPRfDZXN2AZF8%2Finternet-research-with-tangent-on-intelligence-analysis-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Internet%20Research%20(with%20tangent%20on%20intelligence%20analysis%20and%20collapse)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfDfGPRfDZXN2AZF8%2Finternet-research-with-tangent-on-intelligence-analysis-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfDfGPRfDZXN2AZF8%2Finternet-research-with-tangent-on-intelligence-analysis-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 952, "htmlBody": "<p><strong>Want to save time? Skip down to \"I'm looking to compile a thread on Internet Research\"!</strong></p>\n<p><em>Opinionated Preamble:</em></p>\n<p>There is a lot of high level thinking on Less Wrong, which is great. It's done wonders to structure and optimize my own decisions. I think the political and futurology-related issues that Less Wrong cover can sometimes get out of sync with the reality and injustices of events in the immediate world. There are comprehensive treatments of how medical science is failing, or how academia cannot give unbiased results, and this is the milieu of programmers and philosophers in the middle-to-upper-class of the planet. I at least believe that this circle of awareness can be expanded, even if it's treading into mind-killing territory. If anything I want to give people a near-mode sense of the stakes aside from x-risk: all in all the x-risk scenarios I've seen Less Wrong fear the most, kill humanity <em>somewhat instantly</em>. A slower descent into violence and poverty is to me much more horrifying,<em> because I might have to live in it and I don't know how</em>. In a matter of fact, I have no idea of how to predict it.</p>\n<p>This is one reason why I'm drawn to the Intelligence Operations performed by the military and crime units, among other things. Intelligence product delivery is about raw and immediate *fact*, and there is a lot of it. The problems featured in IntelOps are <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">one of the few things rationality is good for</a> - highly uncertain scenarios with one-off executions and messy or noisy feedback. Facts get lost in translation as messages are passed through, and of course the feeding and receiving fake facts are all a part of the job - but nevertheless, knowing *everything* *everywhere* is in the job description, and&nbsp;<a href=\"https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/index.html\">some form</a> <a href=\"http://advat.blogspot.co.uk/\">of rationality</a> became a necessity.</p>\n<p>It gets ugly. The demand for these kinds of skills often lie in industries that are highly competitive, violent, and illegal. I believe that once a close look is taken on how force and power is applied in practice then there isn't any pretending anymore that human evils are an accident.</p>\n<p>Open Source Intelligence, or \"OSINT\", is the mining of data and facts from public information databases, news articles, codebases, journals. Although the amount of classified data dwarfs the unclassified, the size and scope of the unclassified is responsible for a majority of intelligence reports - and thus is involved in the great majority of executive decisions made by government entities. It's worth giving some thought as to how much that we know, that they do too. As illustrated <a href=\"http://sofrep.com/10470/fusion-analyst-all-source-intelligence-and-analysis/\">in this expose</a>, the processing of OSINT is a great big chunk of what modern intelligence is about aside from many other things. I think understanding how rationality as developed on Less Wrong can contribute to better IntelOps, and how IntelOps can feed the rationality community, would be awesome, but that's a post for another time.</p>\n<p>--</p>\n<h2>The Show</h2>\n<p>Through my investigations into IntelOps I've noticed the emphasis on <em>search</em>. Good search.</p>\n<p><strong>I'm looking to compile a thread on Internet Research.</strong><strong>&nbsp;</strong>I'm wondering if there is any wisdom on Less Wrong that can be taken advantage of here on how to become more effective searchers.&nbsp;&nbsp;<strong>Here are some questions that could be answered specifically, but they are just guidelines - feel free to voice associated thoughts, we're exploring here.</strong></p>\n<ul>\n<li>Before actually going out and searching, what would be the most effective way of drafting and optimizing a collection plan? Are there any formal optimization models that inform our distribution of time and attention? <a href=\"http://www.indigosim.com/tutorials/exploration/t0s1.htm\">Exploration vs exploitation</a>&nbsp;comes to mind, but it would be worth formulating something specific. I heard that the <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\">multi-armed bandit problem</a> is solved?</li>\n</ul>\n<ul>\n<li>Do you have any links or resources regarding more effective search?</li>\n</ul>\n<ul>\n<li>Do you have any experiences regarding internet research that you can share? Any patterns that you've noticed that have made you more effective at searching?</li>\n</ul>\n<ul>\n<li>What are examples of closed-source information that are low-hanging fruit in terms of access (e.g. academic journals)? What are possible strategies for acquiring closed source data (e.g. enrolling in small courses at universities, e-mailing researchers, <a href=\"https://groups.google.com/forum/#!topic/brain-training/5vVIhM2TTWE\">cohesion via the law/Freedom of Information Act</a>, social engineering etc)?</li>\n</ul>\n<ul>\n<li>I would like to hear from SEOs and software developers on what their interpretation of semantic web technologies and how they are going to affect end-users. I am somewhat unfamiliar with the <a href=\"http://www.w3.org/standards/semanticweb/\">semantic web</a>, but from my understanding information that could not be indexed is now indexed; and new ontologies will emerge as this information is mined. What should an end-user expect and what opportunities will there be that didn't exist in the current generation of search?</li>\n</ul>\n<p>That should be enough to get started. Below are some links that I have found useful with respect to Internet Research.</p>\n<p>--</p>\n<p><strong><em>Meta-Search Engines or Assisted Search:</em></strong></p>\n<ul>\n<li>Carrot -&nbsp;http://search.carrot2.org/stable/search (concept clustering search engine)</li>\n</ul>\n<p><em><strong>Summarizers:</strong></em></p>\n<ul>\n<li>TextTeaser - http://www.textteaser.com/ - SOURCE: https://github.com/MojoJolo/textteaser</li>\n<li>Copernic (Commercial Summarizing Feed Program) - http://www.copernic.com/en/products/summarizer/</li>\n</ul>\n<p><strong><em>Bots/Collectors/Automatic Filters:</em></strong></p>\n<ul>\n<li>Google Alerts - http://www.google.ca/alerts</li>\n<li>Change Detection - http://www.changedetection.com/</li>\n</ul>\n<p><em><strong>Compilations and Directories:</strong></em></p>\n<ul>\n<li>Directories and Search Engine Repository -&nbsp;<a style=\"font-size: 13px; margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; text-decoration: none; color: #6611cc; cursor: pointer; font-family: Arial, Helvetica, sans-serif;\" href=\"http://rr.reuser.biz/index.html\" target=\"_blank\">http://rr.reuser.biz/index.html</a>&nbsp;(probably the last one you'll ever need.)</li>\n</ul>\n<ul>\n<li>How to Perform Industry Research -&nbsp;http://businesslibrary.uflib.ufl.edu/industryresearch</li>\n</ul>\n<p><strong><em>Guides:</em></strong></p>\n<ul>\n<li>Google Guide -&nbsp;<a href=\"http://www.googleguide.com/\">http://www.googleguide.com/</a>&nbsp;(with practice and tutorials)</li>\n</ul>\n<ul>\n<li>From UC Berkeley - http://www.lib.berkeley.edu/TeachingLib/Guides/Internet/FindInfo.html&nbsp;</li>\n</ul>\n<ul>\n<li>\"How to Solve Impossible Problems\" - http://www.johntedesco.net/blog/2012/06/21/how-to-solve-impossible-problems-daniel-russells-awesome-google-search-techniques/&nbsp;</li>\n</ul>\n<ul>\n<li>The NSA Guide to \"Untangling the Web\"; Internet Research -&nbsp;http://www.nsa.gov/public_info/_files/Untangling_the_Web.pdf [C. 2007]</li>\n</ul>\n<ul>\n<li>Fravia's Learnings on searching (value in essays) - http://search.lores.eu/indexo.htm [C. 1990s - 2009]</li>\n</ul>\n<ul>\n<li>\"Power Searching With Google\" Course -&nbsp;http://www.powersearchingwithgoogle.com/</li>\n</ul>\n<p><strong><em>Practice:</em></strong></p>\n<ul>\n<li>SearchReSearch -&nbsp;<a href=\"http://searchresearch1.blogspot.ca/\">http://searchresearch1.blogspot.ca/</a></li>\n</ul>\n<ul>\n<li>A Google A Day -&nbsp;<a href=\"http://agoogleaday.com/\">http://agoogleaday.com/</a></li>\n</ul>\n<p>I don't really care how you use this information, but I hope I've jogged some thinking of why it could be important.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BfDfGPRfDZXN2AZF8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 1.2851746759980055e-06, "legacy": true, "legacyId": "23585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Want_to_save_time__Skip_down_to__I_m_looking_to_compile_a_thread_on_Internet_Research__\">Want to save time? Skip down to \"I'm looking to compile a thread on Internet Research\"!</strong></p>\n<p><em>Opinionated Preamble:</em></p>\n<p>There is a lot of high level thinking on Less Wrong, which is great. It's done wonders to structure and optimize my own decisions. I think the political and futurology-related issues that Less Wrong cover can sometimes get out of sync with the reality and injustices of events in the immediate world. There are comprehensive treatments of how medical science is failing, or how academia cannot give unbiased results, and this is the milieu of programmers and philosophers in the middle-to-upper-class of the planet. I at least believe that this circle of awareness can be expanded, even if it's treading into mind-killing territory. If anything I want to give people a near-mode sense of the stakes aside from x-risk: all in all the x-risk scenarios I've seen Less Wrong fear the most, kill humanity <em>somewhat instantly</em>. A slower descent into violence and poverty is to me much more horrifying,<em> because I might have to live in it and I don't know how</em>. In a matter of fact, I have no idea of how to predict it.</p>\n<p>This is one reason why I'm drawn to the Intelligence Operations performed by the military and crime units, among other things. Intelligence product delivery is about raw and immediate *fact*, and there is a lot of it. The problems featured in IntelOps are <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">one of the few things rationality is good for</a> - highly uncertain scenarios with one-off executions and messy or noisy feedback. Facts get lost in translation as messages are passed through, and of course the feeding and receiving fake facts are all a part of the job - but nevertheless, knowing *everything* *everywhere* is in the job description, and&nbsp;<a href=\"https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/index.html\">some form</a> <a href=\"http://advat.blogspot.co.uk/\">of rationality</a> became a necessity.</p>\n<p>It gets ugly. The demand for these kinds of skills often lie in industries that are highly competitive, violent, and illegal. I believe that once a close look is taken on how force and power is applied in practice then there isn't any pretending anymore that human evils are an accident.</p>\n<p>Open Source Intelligence, or \"OSINT\", is the mining of data and facts from public information databases, news articles, codebases, journals. Although the amount of classified data dwarfs the unclassified, the size and scope of the unclassified is responsible for a majority of intelligence reports - and thus is involved in the great majority of executive decisions made by government entities. It's worth giving some thought as to how much that we know, that they do too. As illustrated <a href=\"http://sofrep.com/10470/fusion-analyst-all-source-intelligence-and-analysis/\">in this expose</a>, the processing of OSINT is a great big chunk of what modern intelligence is about aside from many other things. I think understanding how rationality as developed on Less Wrong can contribute to better IntelOps, and how IntelOps can feed the rationality community, would be awesome, but that's a post for another time.</p>\n<p>--</p>\n<h2 id=\"The_Show\">The Show</h2>\n<p>Through my investigations into IntelOps I've noticed the emphasis on <em>search</em>. Good search.</p>\n<p><strong>I'm looking to compile a thread on Internet Research.</strong><strong>&nbsp;</strong>I'm wondering if there is any wisdom on Less Wrong that can be taken advantage of here on how to become more effective searchers.&nbsp;&nbsp;<strong>Here are some questions that could be answered specifically, but they are just guidelines - feel free to voice associated thoughts, we're exploring here.</strong></p>\n<ul>\n<li>Before actually going out and searching, what would be the most effective way of drafting and optimizing a collection plan? Are there any formal optimization models that inform our distribution of time and attention? <a href=\"http://www.indigosim.com/tutorials/exploration/t0s1.htm\">Exploration vs exploitation</a>&nbsp;comes to mind, but it would be worth formulating something specific. I heard that the <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\">multi-armed bandit problem</a> is solved?</li>\n</ul>\n<ul>\n<li>Do you have any links or resources regarding more effective search?</li>\n</ul>\n<ul>\n<li>Do you have any experiences regarding internet research that you can share? Any patterns that you've noticed that have made you more effective at searching?</li>\n</ul>\n<ul>\n<li>What are examples of closed-source information that are low-hanging fruit in terms of access (e.g. academic journals)? What are possible strategies for acquiring closed source data (e.g. enrolling in small courses at universities, e-mailing researchers, <a href=\"https://groups.google.com/forum/#!topic/brain-training/5vVIhM2TTWE\">cohesion via the law/Freedom of Information Act</a>, social engineering etc)?</li>\n</ul>\n<ul>\n<li>I would like to hear from SEOs and software developers on what their interpretation of semantic web technologies and how they are going to affect end-users. I am somewhat unfamiliar with the <a href=\"http://www.w3.org/standards/semanticweb/\">semantic web</a>, but from my understanding information that could not be indexed is now indexed; and new ontologies will emerge as this information is mined. What should an end-user expect and what opportunities will there be that didn't exist in the current generation of search?</li>\n</ul>\n<p>That should be enough to get started. Below are some links that I have found useful with respect to Internet Research.</p>\n<p>--</p>\n<p><strong id=\"Meta_Search_Engines_or_Assisted_Search_\"><em>Meta-Search Engines or Assisted Search:</em></strong></p>\n<ul>\n<li>Carrot -&nbsp;http://search.carrot2.org/stable/search (concept clustering search engine)</li>\n</ul>\n<p><em><strong>Summarizers:</strong></em></p>\n<ul>\n<li>TextTeaser - http://www.textteaser.com/ - SOURCE: https://github.com/MojoJolo/textteaser</li>\n<li>Copernic (Commercial Summarizing Feed Program) - http://www.copernic.com/en/products/summarizer/</li>\n</ul>\n<p><strong id=\"Bots_Collectors_Automatic_Filters_\"><em>Bots/Collectors/Automatic Filters:</em></strong></p>\n<ul>\n<li>Google Alerts - http://www.google.ca/alerts</li>\n<li>Change Detection - http://www.changedetection.com/</li>\n</ul>\n<p><em><strong>Compilations and Directories:</strong></em></p>\n<ul>\n<li>Directories and Search Engine Repository -&nbsp;<a style=\"font-size: 13px; margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; text-decoration: none; color: #6611cc; cursor: pointer; font-family: Arial, Helvetica, sans-serif;\" href=\"http://rr.reuser.biz/index.html\" target=\"_blank\">http://rr.reuser.biz/index.html</a>&nbsp;(probably the last one you'll ever need.)</li>\n</ul>\n<ul>\n<li>How to Perform Industry Research -&nbsp;http://businesslibrary.uflib.ufl.edu/industryresearch</li>\n</ul>\n<p><strong id=\"Guides_\"><em>Guides:</em></strong></p>\n<ul>\n<li>Google Guide -&nbsp;<a href=\"http://www.googleguide.com/\">http://www.googleguide.com/</a>&nbsp;(with practice and tutorials)</li>\n</ul>\n<ul>\n<li>From UC Berkeley - http://www.lib.berkeley.edu/TeachingLib/Guides/Internet/FindInfo.html&nbsp;</li>\n</ul>\n<ul>\n<li>\"How to Solve Impossible Problems\" - http://www.johntedesco.net/blog/2012/06/21/how-to-solve-impossible-problems-daniel-russells-awesome-google-search-techniques/&nbsp;</li>\n</ul>\n<ul>\n<li>The NSA Guide to \"Untangling the Web\"; Internet Research -&nbsp;http://www.nsa.gov/public_info/_files/Untangling_the_Web.pdf [C. 2007]</li>\n</ul>\n<ul>\n<li>Fravia's Learnings on searching (value in essays) - http://search.lores.eu/indexo.htm [C. 1990s - 2009]</li>\n</ul>\n<ul>\n<li>\"Power Searching With Google\" Course -&nbsp;http://www.powersearchingwithgoogle.com/</li>\n</ul>\n<p><strong id=\"Practice_\"><em>Practice:</em></strong></p>\n<ul>\n<li>SearchReSearch -&nbsp;<a href=\"http://searchresearch1.blogspot.ca/\">http://searchresearch1.blogspot.ca/</a></li>\n</ul>\n<ul>\n<li>A Google A Day -&nbsp;<a href=\"http://agoogleaday.com/\">http://agoogleaday.com/</a></li>\n</ul>\n<p>I don't really care how you use this information, but I hope I've jogged some thinking of why it could be important.</p>", "sections": [{"title": "Want to save time? Skip down to \"I'm looking to compile a thread on Internet Research\"!", "anchor": "Want_to_save_time__Skip_down_to__I_m_looking_to_compile_a_thread_on_Internet_Research__", "level": 2}, {"title": "The Show", "anchor": "The_Show", "level": 1}, {"title": "Meta-Search Engines or Assisted Search:", "anchor": "Meta_Search_Engines_or_Assisted_Search_", "level": 2}, {"title": "Bots/Collectors/Automatic Filters:", "anchor": "Bots_Collectors_Automatic_Filters_", "level": 2}, {"title": "Guides:", "anchor": "Guides_", "level": 2}, {"title": "Practice:", "anchor": "Practice_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7dRGYDqA2z6Zt7Q4h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-31T09:18:07.224Z", "modifiedAt": null, "url": null, "title": "More \"Stupid\" Questions", "slug": "more-stupid-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:05.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hKt3braRWySYKs5iD/more-stupid-questions", "pageUrlRelative": "/posts/hKt3braRWySYKs5iD/more-stupid-questions", "linkUrl": "https://www.lesswrong.com/posts/hKt3braRWySYKs5iD/more-stupid-questions", "postedAtFormatted": "Wednesday, July 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20%22Stupid%22%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20%22Stupid%22%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhKt3braRWySYKs5iD%2Fmore-stupid-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20%22Stupid%22%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhKt3braRWySYKs5iD%2Fmore-stupid-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhKt3braRWySYKs5iD%2Fmore-stupid-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>This is a thread where people can ask questions that they would ordinarily feel embarrassed for not knowing the answer to. The previous <a href=\"/lw/hzd/stupid_questions_thread/\">\"stupid\" questions thread</a> went to over 800 comments in two and a half weeks, so I think it's time for a new one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hKt3braRWySYKs5iD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 1.2853871511488026e-06, "legacy": true, "legacyId": "23586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 498, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["P42E5sSbCkDgxEqbn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-31T09:52:08.323Z", "modifiedAt": null, "url": null, "title": "Mutual Worth without default point (but with potential threats)", "slug": "mutual-worth-without-default-point-but-with-potential", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:24.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/soa45aMxGehwa2xGc/mutual-worth-without-default-point-but-with-potential", "pageUrlRelative": "/posts/soa45aMxGehwa2xGc/mutual-worth-without-default-point-but-with-potential", "linkUrl": "https://www.lesswrong.com/posts/soa45aMxGehwa2xGc/mutual-worth-without-default-point-but-with-potential", "postedAtFormatted": "Wednesday, July 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mutual%20Worth%20without%20default%20point%20(but%20with%20potential%20threats)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMutual%20Worth%20without%20default%20point%20(but%20with%20potential%20threats)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsoa45aMxGehwa2xGc%2Fmutual-worth-without-default-point-but-with-potential%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mutual%20Worth%20without%20default%20point%20(but%20with%20potential%20threats)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsoa45aMxGehwa2xGc%2Fmutual-worth-without-default-point-but-with-potential", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsoa45aMxGehwa2xGc%2Fmutual-worth-without-default-point-but-with-potential", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 550, "htmlBody": "<p><em>Though I planned to avoid posting anything more until well after baby, I found this refinement to MWBS yesterday, so I'm posting it while Miriam sleeps during a pause in contractions.</em></p>\n<p>The <a href=\"/lw/hv6/gains_from_trade_slug_versus_galaxy_how_much/\">mutual worth bargaining solution</a> was built from the idea that the true value of a trade is having your utility function access the decision points of the other player. This gave the idea of utopia points: what happens when you are granted complete control over the other person's decisions. This gave a natural 1 to normalise your utility function. But the 0 point is chosen according to a default point. This is arbitrary, and breaks the symmetry between the top and bottom point of the normalisation.</p>\n<p>We'd also want normalisations that function well when players have no idea what their opponents will be. This includes not knowing what their utility functions will be. Can we model what a 'generic' opposing utility function would be?</p>\n<p>It's tricky, in general, to know what 'value' to put on an opponent's utility function. It's unclear what kind of utilities would you like to see them have? That's because game theory comes into play, with Nash equilibriums, multiple solution concepts, bargaining and threats: there is no universal default to the result of a game between two agents. There are two situations, however, that are respectively better and worse than all others: the situation where your opponent shares your exact utility function, and the situations where they have the negative of that (they're essentially your 'anti-agent').</p>\n<p>If your opponent shares your utility function, then there is a clear ideal outcome: act as if you and the opponent were the same person, acting to maximise your joint utility. This is the utopia point for MWBS, which can be standardised to take value 1.</p>\n<p>If your opponent has the negative of your utility, then the game is zero-sum: any gain to you is a loss to your opponent, and there is no possibility for mutually pleasing compromise. But zero-sum games also have a <a href=\"http://en.wikipedia.org/wiki/Zero-sum_game#Solution\">single canonical outcome</a>! For zero-sum games, the concepts of Nash equilibrium, minimax, and maximin are all equivalent (and are generally mixed outcomes). The game has a single defined <a href=\"https://en.wikipedia.org/wiki/Minimax#Minimax_theorem\">value</a>: each player can guarantee they get as much utility as that value, and the other player can guarantee that they get no more.</p>\n<p>It seems natural to normalise that point to -1 (0 would be equivalent, but -1 feels more appropriate). Given this normalisation for each utility, the two utilities can then be summed and joint maximised in the usual way.</p>\n<p>This bargaining solution has a lot of attractive features - it's symmetric in minimal and maximal utilities, does not require a default point, reflects the relative power, and captures the spread of opponents utilities that could be encountered without needing to go into game theory. It is vulnerable to (implicit) threats, however! If I can (potentially) cause a lot of damage to you and your cause, then when you normalise your utility, you get penalised because of what your anti-agent could do if they controlled my decision nodes. So just by having the power do do bad stuff to you, I come out better than I would otherwise (and vice-versa, of course).</p>\n<p>I feel it's worth exploring further (especially what happens with multiple agents) - but for me, after the baby.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "soa45aMxGehwa2xGc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "23578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7kvBxG9ZmYb5rDRiq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-07-31T20:02:30.245Z", "modifiedAt": null, "url": null, "title": "Q for GiveWell: What is GiveDirectly's mechanism of action?", "slug": "q-for-givewell-what-is-givedirectly-s-mechanism-of-action", "viewCount": null, "lastCommentedAt": "2014-01-31T22:11:28.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eYCj6GnnSSmDKDZgG/q-for-givewell-what-is-givedirectly-s-mechanism-of-action", "pageUrlRelative": "/posts/eYCj6GnnSSmDKDZgG/q-for-givewell-what-is-givedirectly-s-mechanism-of-action", "linkUrl": "https://www.lesswrong.com/posts/eYCj6GnnSSmDKDZgG/q-for-givewell-what-is-givedirectly-s-mechanism-of-action", "postedAtFormatted": "Wednesday, July 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%20for%20GiveWell%3A%20What%20is%20GiveDirectly's%20mechanism%20of%20action%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%20for%20GiveWell%3A%20What%20is%20GiveDirectly's%20mechanism%20of%20action%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeYCj6GnnSSmDKDZgG%2Fq-for-givewell-what-is-givedirectly-s-mechanism-of-action%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%20for%20GiveWell%3A%20What%20is%20GiveDirectly's%20mechanism%20of%20action%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeYCj6GnnSSmDKDZgG%2Fq-for-givewell-what-is-givedirectly-s-mechanism-of-action", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeYCj6GnnSSmDKDZgG%2Fq-for-givewell-what-is-givedirectly-s-mechanism-of-action", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2331, "htmlBody": "<p>I first wrote up the following post, then happened to run into Holden Karnofsky in person and asked him a much-shortened form of the question verbally. &nbsp;My attempt to recount Holden's verbal reply is also given further below. &nbsp;I was moderately impressed by Holden's response because I had not thought of it when listing out possible replies, but I don't understand yet why Holden's response should be true. &nbsp;Since GiveWell has <a href=\"http://blog.givewell.org/2013/07/31/responses-to-objections-on-cash-transfers/\">recently posted</a> about objections to GiveDirectly and replies, I decided to go ahead and post this now.</p>\n<hr />\n<p>A question for GiveWell:</p>\n<p>Your current #2 top-rated charity is GiveDirectly, which gives one-time gifts of $1000 over 9 months, directly to poor recipients in Kenya via M-PESA.</p>\n<p>Givewell tries for high standards of evidence of efficacy and cost-effectiveness. &nbsp;As I understand it, you don't just want the charity to be arguably cost effective, you want a very high probability that the charity is cost-effective.</p>\n<p>The main evidence I've seen cited for direct giving is that the recipients who received the $1000 are then substantially better off 9 months later compared to people who aren't.</p>\n<p>While I can imagine arguments that could repair the obvious objection to this reasoning, I haven't seen yet how the resulting evidence about cost-effectiveness could rise again to the epistemic standards one would expect of Givewell's #2 evidence-based charity.</p>\n<p>The obvious objection is as follows: &nbsp;<strong>Suppose the Kenyan government simply printed new shillings and handed out $1000 of such shillings to the same recipients targeted by GiveDirectly. &nbsp;Although the recipients would be better off than non-recipients, this might not reflect any improvement in net utility in Kenya because no new resources were created by printing the money.</strong></p>\n<p>There are of course obvious replies to this obvious objection:</p>\n<p><em>(1)</em> &nbsp;Because the shillings handed out by GiveDirectly are purchased on the foreign currency exchange market using U. S. dollars, and would otherwise have been spent in Kenya in other ways, we should not expect any inflation of the shilling, and should expect an increase in Kenyan consumption of foreign goods corresponding to the increased price of shillings implied by GiveDirectly adding their marginal demand to the auction and thereby raising the marginal price of all shillings sold. &nbsp;The primary mechanism of action by which GiveDirectly benefits Kenya is by raising the price of shillings in the foreign exchange market and making more hard currency available to sellers of shillings. &nbsp;So far as I can tell, this argument ought to generalize: &nbsp;<strong>Any argument that the Kenyan government could not accomplish most of the same good by printing shillings will mean that the primary mechanism of GiveWell's effectiveness must be the U.S. dollars being exchanged for the shillings on the foreign currency market. &nbsp;</strong>This in turn means that GiveDirectly could accomplish most of its good by buying the same shillings on the foreign currency market and burning them.</p>\n<p style=\"padding-left: 30px;\">(Or to sharpen the total point of this article: &nbsp;<strong>The sum of the good accomplished by GiveDirectly should equal:</strong></p>\n<ul style=\"padding-left: 30px;\">\n<li>The good accomplished by the Kenyan government<strong> printing shillings and distributing them to the same recipients;</strong></li>\n<li><strong>plus </strong>the good accomplished by GiveDirectly then<strong> purchasing shillings on the foreign exchange market using US dollars, and burning them.</strong></li>\n</ul>\n<p style=\"padding-left: 30px;\">Indeed, since these mechanisms of action seem mostly independent, we ought to be able to state a percentage of good accomplished which is allegedly attributed to each, summing to 1. &nbsp;E.g. maybe 80% of the good would be achieved by printing shillings and distributing them to the same recipients, and 20% would be achieved by purchasing shillings on the foreign exchange market and burning them. &nbsp;But then we have mostly the same questions as before about how to generate wealth by printing shillings.)</p>\n<p><em>(2) </em>&nbsp;Inequality in Kenya is such that redistributing the supply of shillings toward the very poor increases utility in Kenya. Thus the Kenyan government could accomplish as much good as GiveDirectly by printing an equivalent number of shillings and giving them to the same recipients. &nbsp;This would create inflation that is a loss to other Kenyans, some of them also very poor, but so much of the shilling supply is held by the rich that the net results are favorable. &nbsp;Printing shillings can create happiness because it shifts resources from making speedboats for the rich to making corrugated iron roofs for the poor.</p>\n<p style=\"padding-left: 30px;\">(It would be nice if the Kenyan government just printed shillings for GiveDirectly to use, but this the Kenyan government will <em>not </em>realistically do. &nbsp;Effective altruists must live in the real world, and in the real world GiveDirectly will only accomplish its goals with the aid of effective altruists. &nbsp;One cannot live in the should-universe where Kenya's government is taking up the burden. &nbsp;Effective altruists should reason as if the Kenya government consists of plastic dolls who <em>cannot </em>be the locus of responsibility instead of them - that's heroic epistemology 101. &nbsp;Maybe there will eventually be returns on lobbying for Minimum Guaranteed Income in Kenya if the programs work, but that's for tomorrow, not right now.)</p>\n<p><em>(3) </em>&nbsp;Like the European Union, Kenya is not printing enough shillings under standard economic theory. &nbsp;(I have no idea if this is plausibly true for Kenya in particular.) &nbsp;If the government printed shillings and gave them to the same recipients, this would create real wealth in Kenya because the economy was operating below capacity and velocity of trade would pick up. &nbsp;The shillings purchased by GiveDirectly would otherwise have stayed in bank accounts rather than going to other Kenyans. &nbsp;Note that this contradicts the argument step in (1) where we said that the purchased shillings would otherwise have been spent elsewhere, so you should have questioned one argument step or the other.</p>\n<p><em>(4)</em> &nbsp;Village moneylenders and bosses can successfully extract most surplus generated within their villages by raising rents or demanding bribes. &nbsp;The only way that individuals can escape the grasp of moneylenders and rentiers is with a one-time gift that was not expected and which the moneylenders and bosses could not arrange to capture. &nbsp;The government could accomplish as much good as GiveDirectly by printing the same number of shillings and giving them to the same people in an unpredictable pattern. &nbsp;This would create some inflation but village moneylenders or bosses would ease off on people from whom they couldn't extract as much value, whereas the one-time gift recipients can purchase capital goods that will make them permanently better off in ways that don't allow the new value to be extracted by moneylenders or bosses.</p>\n<p>If I recall correctly, GiveDirectly uses the example of a family using some of the gift money to purchase a corrugated iron roof. &nbsp;From my perspective the obvious objection is that they could just be purchasing a corrugated iron roof that would've gone to someone else and raising the prices of roofs. &nbsp;(1) says that Kenya has more foreign exchange on hands and can import, not one more corrugated iron roof, but a variety of other foreign goods;&nbsp;(2) says that the resources used in the corrugated iron roof would otherwise have been used to make a speedboat;&nbsp;(3) says that a new trade takes place in which somebody makes a corrugated iron roof that wouldn't have been manufactured otherwise; and (4) says that the village moneylenders usually adjust their interest rates so as to prevent anyone from saving up enough money to buy a corrugated iron roof.</p>\n<p>The trouble is that all of these mechanisms of action seem much harder to measure and be sure of, than the measurable outcomes for gift recipients vs. non-recipients.</p>\n<p>To reiterate, the sum of the good accomplished by GiveDirectly should equal the good accomplished by the Kenyan government printing shillings and distributing them to the same recipients,&nbsp;<em>plus&nbsp;</em>the good accomplished by GiveDirectly purchasing shillings on the foreign exchange market using US dollars and then burning them. &nbsp;<strong>It seems to me to be difficult to arrive at a state of strong evidence about either of the two terms in this sum,</strong>&nbsp;with respect to any mechanism of action I've thought of so far.</p>\n<p>With respect to the second term in this sum: &nbsp;GiveDirectly buying shillings on the foreign exchange market and burning them might create wealth, but it's hard to see how you would measure this over the relevant amounts, and no such evidence was cited in the recommendation of GiveDirectly as the #2 charity.</p>\n<p>With respect to the first term in this sum: &nbsp;Under the Bayesian definition of evidence, strong evidence is evidence we are unlikely to see when the theory is false. &nbsp;Even in the absence of any mechanism whereby printing nominal shillings creates happiness or wealth, we would still expect to find that the wealth and happiness of gift recipients exceeded the wealth of non-recipients. &nbsp;So measuring that the gift recipients are wealthier and happier is not strong or even medium evidence that printing nominal shillings creates wealth, unless I'm missing something here. &nbsp;Our posterior that printing shillings and giving them to certain people would create net wealth in any given quantity, should roughly equal our prior, after updating on the stated experimental evidence.</p>\n<hr />\n<p>When I posed a shortened form of this question to Holden Karnofsky, he replied (roughly, I am trying to rephrase from memory):</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">It seems to me that this is a perverse decomposition of the benefit accomplished. &nbsp;There's no inflation in the shilling because you're buying them, and since this is true, decomposing the benefit into an operation that does inflationary damage as a side effect, and then another operation that makes up for the inflation, is perverse. &nbsp;It's like criticizing the Against Malaria Foundation based on a hypothetical which involves the mosquito nets being made from the flesh of babies and then adding another effect which saves the lives of other babies. &nbsp;Since this is a perverse sum involving a strange extra side effect, it's okay that we can't get good estimates involving either of the terms in it.</p>\n</blockquote>\n<p><em>Please</em>&nbsp;keep in mind that this is&nbsp;Holden's off-the-cuff, non-written in-person response as rephrased by Eliezer Yudkowsky from imperfect memory.</p>\n<p>With that said, I've thought about (what I think was) Holden's answer and I feel like I'm still missing something. &nbsp;I agree that if U.S. dollars were being sent directly to Kenyan recipients and used only to purchase foreign goods, so that foreign goods were being directly sent from the U.S. to Kenyan recipients, then improvement in measured outcome for recipients compared to non-recipients would be an appropriate metric, and that the decomposition would be perverse. &nbsp;But if the received money, in the form of Kenyan shillings, is being used primarily to purchase Kenyan goods, and causing those goods to be shipped to one villager rather than another while also possibly increasing velocity of trade, remedying inequality, and enabling completely different actors to buy some amount of foreign goods, then I honestly don't understand why this scenario should have the same causal mechanisms as the scenario where foreign goods are being shipped in from outside the country. &nbsp;And then I honestly don't understand why measured improvements for one Kenyan over another should be a good proxy for aggregate welfare change to the country.</p>\n<p>I may be missing something that an economist would find obvious or I may have misunderstood Holden's reply. &nbsp;But to me, my sum seems like an obvious <em>causal</em>&nbsp;decomposition of the effects in Kenya, neither of whose terms can be estimated well. &nbsp;I don't understand why I should expect the uncertainty in these two estimates to cancel out when they are added; I don't understand what background causal model yields this conclusion.</p>\n<hr />\n<ul>\n</ul>\n<p>To be clear, I personally would <em>guess </em>that the U.S. would be net better off, if the Federal Reserve directly sent everyone in the U.S. with income under $20K/year a one-time $6,000 check with the money phasing out at a 10% rate up to $80K/year. &nbsp;This is because, in order of importance:</p>\n<ul>\n<li>I buy the analogous market monetarist argument (3) that the U.S. is printing too little money.</li>\n<li>I buy the analogous argument (2) about inequality.</li>\n<li>(However, I also somewhat suspect that some analogous form of (4) is going on with poor people somehow systematically having all but a certain amount of value extracted from them, which is in general how a modern country can have only 2% instead of 95% of the population being farmers, and yet there are still people living hand-to-mouth. &nbsp;I would worry that a predictable, universal one-time gift of $6K would not defeat this phenomenon, and that the gift money will just be extracted again somehow. &nbsp;In the case of Minimum Guaranteed Income, I would worry that the labor share of income will drop proportionally to small amounts of MGI as wages are just bid down by people who can live on less. &nbsp;Or something. &nbsp;This would be a much longer discussion and the ideas are much less simple than the above two notions, probably also less important. &nbsp;I'm just mentioning it again because of my long-term puzzlement with the question \"Why are there still poor people after agricultural productivity rose by a factor of 100?\")</li>\n</ul>\n<p>What I <em>wouldn't</em>&nbsp;say is that my belief in the above is as strong as my belief in, say, <a href=\"http://intelligence.org/files/IEM.pdf\">the intelligence explosion</a>. &nbsp;I'd <em>guess </em>that the printing operation would do more good than harm, but it's not what I would call a strong evidence-based conclusion. &nbsp;If we're going to be okay with that standard of argument generally, then the top charity under that standard of reasoning, generally and evenhandedly applied, ought to work out to some charity that does science and technology research. &nbsp;(X-risk minimization might seem substantially 'weirder' than that, but the best science-funding charities should be only equally weird.) &nbsp;And I wouldn't measure the excess of happiness of gift-recipients compared to non-recipients in a pilot program, and call this a good estimate of the net good if a Minimum Guaranteed Income were universally adopted.</p>\n<p>So to reiterate, my question to Givewell is not \"Why do you think GiveDirectly might maybe end up doing some good anyway?\" but \"Does GiveDirectly rise to the standards required for your #2 evidence-based charity?\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "PDJ6KqJBRzvKPfuS3": 1, "xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eYCj6GnnSSmDKDZgG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 31, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "23161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-07-31T20:02:30.245Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T00:21:20.906Z", "modifiedAt": null, "url": null, "title": "Common Task Time Estimation Repository", "slug": "common-task-time-estimation-repository", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S8NSPxWbH6Z33BJBr/common-task-time-estimation-repository", "pageUrlRelative": "/posts/S8NSPxWbH6Z33BJBr/common-task-time-estimation-repository", "linkUrl": "https://www.lesswrong.com/posts/S8NSPxWbH6Z33BJBr/common-task-time-estimation-repository", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Common%20Task%20Time%20Estimation%20Repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommon%20Task%20Time%20Estimation%20Repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8NSPxWbH6Z33BJBr%2Fcommon-task-time-estimation-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Common%20Task%20Time%20Estimation%20Repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8NSPxWbH6Z33BJBr%2Fcommon-task-time-estimation-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8NSPxWbH6Z33BJBr%2Fcommon-task-time-estimation-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p><strong>Related:</strong>&nbsp;<a href=\"/r/discussion/lw/i64/repository_repository/\">Repository Repository</a></p>\n<p>It would be cool if we had a repository for the Outside/calibrated view for how long commonplace tasks take. This way we can help one another get around the planning fallacy to a certain degree. Nothing beats direct measurement, but we can possibly save energy if enough evidence comes in to make us suspect that the time a certain task takes doesn't vary.</p>\n<p>I could update this thread as more contribute. Depending on the variety of tasks, categories could be created and an index can be compiled. Observe.</p>\n<p><span style=\"text-decoration: underline;\">Chores</span></p>\n<p><strong>Washing Dishes: </strong>30 mins</p>\n<p><strong>Doing Laundry: </strong>1 hour including drying</p>\n<p><span style=\"text-decoration: underline;\">Rationality</span></p>\n<p><strong>Average LessWrong Post: </strong>3 minutes</p>\n<p><strong>\"Mysterious Answers to Mysterious Questions\": </strong>4 hours</p>\n<p><span style=\"text-decoration: underline;\">Misc.</span></p>\n<p><strong>...: </strong>...</p>\n<p>... and so on.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S8NSPxWbH6Z33BJBr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 1.28612732245816e-06, "legacy": true, "legacyId": "23587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sEaDmtwrmTC7kTqcf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T06:04:57.281Z", "modifiedAt": null, "url": null, "title": "RapGenius + Sequences = ?", "slug": "rapgenius-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:32.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XWdnafnyKHHiyP7f8/rapgenius-sequences", "pageUrlRelative": "/posts/XWdnafnyKHHiyP7f8/rapgenius-sequences", "linkUrl": "https://www.lesswrong.com/posts/XWdnafnyKHHiyP7f8/rapgenius-sequences", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20RapGenius%20%2B%20Sequences%20%3D%20%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARapGenius%20%2B%20Sequences%20%3D%20%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXWdnafnyKHHiyP7f8%2Frapgenius-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=RapGenius%20%2B%20Sequences%20%3D%20%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXWdnafnyKHHiyP7f8%2Frapgenius-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXWdnafnyKHHiyP7f8%2Frapgenius-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>I recently saw <a href=\"http://poetry.rapgenius.com/Steve-jobs-academy-of-achievement-speech-1982-lyrics\">Ashton Kutcher's annotation of a speech by Steve Jobs</a>&nbsp;on <a href=\"http://rapgenius.com/\">RapGenius</a>. For those that haven't heard of it, RapGenius is a content annotation platform, where the \"Rap\" part is purely incidental.</p>\n<p>The format seems quite interesting, so I wondered what the LessWrong community could do if allowed to annotate popular articles (or other texts like MIRI publications) in the same way.</p>\n<p>To experiment, I created a <a href=\"http://poetry.rapgenius.com/Eliezer-yudkowsky-tsuyoku-naritai-i-want-to-become-stronger-lyrics\">RapGenius page for Tsuyoku Naritai</a>&nbsp;and started with an annotation.</p>\n<p>Feel free to add other annotations etc. and let's see if we can do something interesting with the medium.</p>\n<p>Note: If Eliezer/LW/MIRI have an issue with the wholesale copying of the text, let me know and I will remove as much of it as I can (RG doesn't allow removal of text that has been annotated if the annotation is not removed as well)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XWdnafnyKHHiyP7f8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 20, "extendedScore": null, "score": 1.2864090964721356e-06, "legacy": true, "legacyId": "23600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T09:43:06.142Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Open Discussion", "slug": "meetup-vancouver-open-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:29.260Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X7jpExmGFGWtkeWeP/meetup-vancouver-open-discussion", "pageUrlRelative": "/posts/X7jpExmGFGWtkeWeP/meetup-vancouver-open-discussion", "linkUrl": "https://www.lesswrong.com/posts/X7jpExmGFGWtkeWeP/meetup-vancouver-open-discussion", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Open%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Open%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX7jpExmGFGWtkeWeP%2Fmeetup-vancouver-open-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Open%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX7jpExmGFGWtkeWeP%2Fmeetup-vancouver-open-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX7jpExmGFGWtkeWeP%2Fmeetup-vancouver-open-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pf'>Vancouver Open Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 August 2013 02:34:17AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 W Broadway, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having our usual meetup on Saturday. We're looking to expand the breadth of our meetup, so we'll be fielding suggestions and discussion for new activities to try. Otherwise, we don't have any plans, so feel free to drop in and start a discussion about whatever you feel like!</p>\n\n<p>If you're there first, please put up a paper sign that says 'Less Wrong/Rationality Meetup', as last week we had trouble with newcomers who couldn't find us.</p>\n\n<p>Whether you're a regular, you haven't been in six months, or this is your first time, we'd love to see you there. The meetup is at 1530 at Benny's Bagels on Broadway.</p>\n\n<p>And if you haven't already done so, join the <a href=\"https://groups.google.com/forum/#!forum/vancouver-rationalists\" rel=\"nofollow\">mailing list</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pf'>Vancouver Open Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X7jpExmGFGWtkeWeP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2865880450446956e-06, "legacy": true, "legacyId": "23604", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Open_Discussion\">Discussion article for the meetup : <a href=\"/meetups/pf\">Vancouver Open Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 August 2013 02:34:17AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 W Broadway, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having our usual meetup on Saturday. We're looking to expand the breadth of our meetup, so we'll be fielding suggestions and discussion for new activities to try. Otherwise, we don't have any plans, so feel free to drop in and start a discussion about whatever you feel like!</p>\n\n<p>If you're there first, please put up a paper sign that says 'Less Wrong/Rationality Meetup', as last week we had trouble with newcomers who couldn't find us.</p>\n\n<p>Whether you're a regular, you haven't been in six months, or this is your first time, we'd love to see you there. The meetup is at 1530 at Benny's Bagels on Broadway.</p>\n\n<p>And if you haven't already done so, join the <a href=\"https://groups.google.com/forum/#!forum/vancouver-rationalists\" rel=\"nofollow\">mailing list</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Open_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/pf\">Vancouver Open Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Open Discussion", "anchor": "Discussion_article_for_the_meetup___Vancouver_Open_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Open Discussion", "anchor": "Discussion_article_for_the_meetup___Vancouver_Open_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T11:42:24.140Z", "modifiedAt": null, "url": null, "title": "How does MIRI Know it Has a Medium Probability of Success?", "slug": "how-does-miri-know-it-has-a-medium-probability-of-success", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:07.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iELiEEZERCXJS9DT4/how-does-miri-know-it-has-a-medium-probability-of-success", "pageUrlRelative": "/posts/iELiEEZERCXJS9DT4/how-does-miri-know-it-has-a-medium-probability-of-success", "linkUrl": "https://www.lesswrong.com/posts/iELiEEZERCXJS9DT4/how-does-miri-know-it-has-a-medium-probability-of-success", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20does%20MIRI%20Know%20it%20Has%20a%20Medium%20Probability%20of%20Success%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20does%20MIRI%20Know%20it%20Has%20a%20Medium%20Probability%20of%20Success%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiELiEEZERCXJS9DT4%2Fhow-does-miri-know-it-has-a-medium-probability-of-success%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20does%20MIRI%20Know%20it%20Has%20a%20Medium%20Probability%20of%20Success%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiELiEEZERCXJS9DT4%2Fhow-does-miri-know-it-has-a-medium-probability-of-success", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiELiEEZERCXJS9DT4%2Fhow-does-miri-know-it-has-a-medium-probability-of-success", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>In the past, people like Eliezer Yudkowsky (see <a href=\"/lw/hjn/earning_to_give_vs_altruistic_career_choice/92gu\">1</a>, <a href=\"/lw/gzq/bayesian_adjustment_does_not_defeat_existential/8ntt\">2</a>, <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4o5s\">3</a>, <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">4</a>, and <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">5</a>) have argued that MIRI has a <em>medium</em> probability of success. &nbsp;What is this probability estimate based on and how is success defined?</p>\n<p>I've read standard MIRI literature (like <a href=\"http://intelligence.org/files/IE-EI.pdf\">\"Evidence and Import\"</a> and <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">\"Five Theses\"</a>), but I may have missed something.</p>\n<p>-</p>\n<p><em>(<strong>Meta:</strong> I don't think this deserves a discussion thread, but <a href=\"/lw/i6l/open_thread_july_29august_4_2013/9ho4\">I posted this on the open thread</a> and no-one responded, and I think it's important enough to merit a response.)</em></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iELiEEZERCXJS9DT4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 27, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "23605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 146, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T12:18:29.663Z", "modifiedAt": null, "url": null, "title": "Meetup : Picnic au Parc de la T\u00eate d'Or", "slug": "meetup-picnic-au-parc-de-la-tete-d-or", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:59.269Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oriane", "createdAt": "2013-06-15T06:22:47.851Z", "isAdmin": false, "displayName": "Oriane"}, "userId": "aKb3Xf7wYLxw7mQjC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tdHmSJQvMi6YduNed/meetup-picnic-au-parc-de-la-tete-d-or", "pageUrlRelative": "/posts/tdHmSJQvMi6YduNed/meetup-picnic-au-parc-de-la-tete-d-or", "linkUrl": "https://www.lesswrong.com/posts/tdHmSJQvMi6YduNed/meetup-picnic-au-parc-de-la-tete-d-or", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Picnic%20au%20Parc%20de%20la%20T%C3%AAte%20d'Or&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Picnic%20au%20Parc%20de%20la%20T%C3%AAte%20d'Or%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtdHmSJQvMi6YduNed%2Fmeetup-picnic-au-parc-de-la-tete-d-or%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Picnic%20au%20Parc%20de%20la%20T%C3%AAte%20d'Or%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtdHmSJQvMi6YduNed%2Fmeetup-picnic-au-parc-de-la-tete-d-or", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtdHmSJQvMi6YduNed%2Fmeetup-picnic-au-parc-de-la-tete-d-or", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pg'>Picnic au Parc de la T\u00eate d'Or</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 August 2013 12:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Parc de la T\u00eate d'Or, Lyon</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be having our second meetup in Lyon.</p>\n\n<p>If the weather is nice enough, we will be having a picnic in the park. \nPlease check this page again if it is not the case to find out where our meetup will be located.</p>\n\n<p>We will meet at 12:30 at La Porte des Enfants du Rh\u00f4ne. In case you arrive late, please send me a text and I will let you know where to find us. My number is 06 02 35 34 42.</p>\n\n<p>Feel free to bring all sorts of food and beverages but please remember that some of our members are vegetarians.</p>\n\n<p>We will be focusing our discussions on different methods and games of self-improvement and better decision-making techniques. I will be sharing my experience of the CFAR workshop and present some of their material.</p>\n\n<p>I look forward to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pg'>Picnic au Parc de la T\u00eate d'Or</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tdHmSJQvMi6YduNed", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2867155414432166e-06, "legacy": true, "legacyId": "23606", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Picnic_au_Parc_de_la_T_te_d_Or\">Discussion article for the meetup : <a href=\"/meetups/pg\">Picnic au Parc de la T\u00eate d'Or</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 August 2013 12:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Parc de la T\u00eate d'Or, Lyon</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be having our second meetup in Lyon.</p>\n\n<p>If the weather is nice enough, we will be having a picnic in the park. \nPlease check this page again if it is not the case to find out where our meetup will be located.</p>\n\n<p>We will meet at 12:30 at La Porte des Enfants du Rh\u00f4ne. In case you arrive late, please send me a text and I will let you know where to find us. My number is 06 02 35 34 42.</p>\n\n<p>Feel free to bring all sorts of food and beverages but please remember that some of our members are vegetarians.</p>\n\n<p>We will be focusing our discussions on different methods and games of self-improvement and better decision-making techniques. I will be sharing my experience of the CFAR workshop and present some of their material.</p>\n\n<p>I look forward to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Picnic_au_Parc_de_la_T_te_d_Or1\">Discussion article for the meetup : <a href=\"/meetups/pg\">Picnic au Parc de la T\u00eate d'Or</a></h2>", "sections": [{"title": "Discussion article for the meetup : Picnic au Parc de la T\u00eate d'Or", "anchor": "Discussion_article_for_the_meetup___Picnic_au_Parc_de_la_T_te_d_Or", "level": 1}, {"title": "Discussion article for the meetup : Picnic au Parc de la T\u00eate d'Or", "anchor": "Discussion_article_for_the_meetup___Picnic_au_Parc_de_la_T_te_d_Or1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T18:46:53.630Z", "modifiedAt": null, "url": null, "title": "The Fermi paradox as evidence against the likelyhood of unfriendly AI", "slug": "the-fermi-paradox-as-evidence-against-the-likelyhood-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:37.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qDzRkPuomtA5wfaxd/the-fermi-paradox-as-evidence-against-the-likelyhood-of", "pageUrlRelative": "/posts/qDzRkPuomtA5wfaxd/the-fermi-paradox-as-evidence-against-the-likelyhood-of", "linkUrl": "https://www.lesswrong.com/posts/qDzRkPuomtA5wfaxd/the-fermi-paradox-as-evidence-against-the-likelyhood-of", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fermi%20paradox%20as%20evidence%20against%20the%20likelyhood%20of%20unfriendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fermi%20paradox%20as%20evidence%20against%20the%20likelyhood%20of%20unfriendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDzRkPuomtA5wfaxd%2Fthe-fermi-paradox-as-evidence-against-the-likelyhood-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fermi%20paradox%20as%20evidence%20against%20the%20likelyhood%20of%20unfriendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDzRkPuomtA5wfaxd%2Fthe-fermi-paradox-as-evidence-against-the-likelyhood-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDzRkPuomtA5wfaxd%2Fthe-fermi-paradox-as-evidence-against-the-likelyhood-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 610, "htmlBody": "<p><strong>Edit after two weeks</strong>: Thanks to everyone involved in this very interesting discussion! I now accept that any possible differences in how UFAI and FAI might spread over the universe pale before the Fermi paradox's evidence against the pre-existence of any of them. I enjoyed thinking about this a lot, so thanks again for considering my original argument, which follows below...</p>\n<p><a id=\"more\"></a></p>\n<p>The assumptions that intelligence is substrate-independent, as well as that intelligent systems will always attempt to become more intelligent lead to the conclusion that, in the words of Paul Davies, \"<a href=\"http://www.dailygalaxy.com/my_weblog/2013/07/if-we-ever-encounter-extraterrestrial-intelligence-it-is-overwhelmingly-likely-to-be-post-biological.html\">if we ever encounter extraterrestrial intelligence, it is overwhelmingly likely to be post-biological</a>\".</p>\n<p>At Less Wrong, we have this notion of <a href=\"http://wiki.lesswrong.com/wiki/UFAI\">unfriendly artificial intelligence</a> - AIs that use their superior intelligence to grow themselves and maximize their own utility at the expense of humans much like we maximize our own utility at the expense of mosquitoes. <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>, on the other hand, should have a positive effect on humanity. Details are beyond my comprehension or indeed rather vague, but presumably such an AI would prioritize particular elements of its home biosphere over its own interests as an agent that - aware of its own intelligence and the fact that is what helps it maximize its utility - should want to grow smarter. The distinction should make as much sense on any alien planet as it does on our own.</p>\n<p>We know that self-replicating probes, travelling at, say, 1% of the speed of light, could colonize the entire galaxy in <a href=\"http://www.scientificamerican.com/article.cfm?id=where-are-they\">millions, not billions</a>, of years. Obviously, an intelligence looking only to grow itself (and <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">maximize paperclips</a> or whatever) can do this much more easily than one restrained by its biological-or-similar parents. Between two alien superintelligences, one strictly self-maximizing should out-compete one that cares about things like the habitability of planets (especially its home planet) by the standards of its parents. It follows that if we ever encounter post-biological extraterrestrial intelligence, it should be expected (at least by the Less Wrong community) to be hostile.</p>\n<p>But we havent. What does that tell us?</p>\n<p>Our astronomical observations increasingly allow us to rule out some possible pictures of life in the rest of the galaxy. This means we can also rule out some possible explanations for the <a href=\"https://en.wikipedia.org/wiki/Fermi_paradox\">Fermi paradox</a>. For example, until a few years ago, we didn't know how common it was for stars to have solar systems. This created the possibility that Earth was rare because it was inside a rare solar system. Or that, as imagined in the Charles Stross novel <a href=\"http://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando-intro.html\">Accelerando</a>, a lot of planetary systems are already <a href=\"http://en.wikipedia.org/wiki/Matrioshka_brain\">Matrioshka brains</a> (which we're speculating are the optimal substrate for a self-replicating intelligent system capable of advanced nanotechnology and interstellar travel). Now we know planetary systems, and planets, are apparently quite common. So we can rule out that Matrioshka brains are the norm.</p>\n<p>Therefore, it very much seems like no self-replicating unfriendly artificial intelligence has arisen anywhere in the galaxy in the - very roughly - 10 billion years since intelligent life could have arisen somewhere in the galaxy. If there had, our own solar system would have been converted into its hardware already. There still could be intelligences out there ethical enough to not bother solar systems with life in them - but then they wouldn't be unfriendly, right?</p>\n<p>I see two possible conclusions from this. Either intelligence is incredibly rare and we're indeed the only ones in the galaxy where unfriendly artificial intelligence is a real threat. Or intelligence is not so rare, has arisen elsewhere, but never, not even in one case, has evolved into the paperclip-maximizing behemoth that we're trying to defend ourselves from. Both possibilities reinforce the need for AI (and astronomical) research.</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qDzRkPuomtA5wfaxd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 8, "extendedScore": null, "score": 1.2870343141233016e-06, "legacy": true, "legacyId": "23608", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-01T20:38:28.460Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, August 1-15", "slug": "group-rationality-diary-august-1-15-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:36.436Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BqgA8tZYsDrBMQjbn/group-rationality-diary-august-1-15-0", "pageUrlRelative": "/posts/BqgA8tZYsDrBMQjbn/group-rationality-diary-august-1-15-0", "linkUrl": "https://www.lesswrong.com/posts/BqgA8tZYsDrBMQjbn/group-rationality-diary-august-1-15-0", "postedAtFormatted": "Thursday, August 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20August%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20August%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBqgA8tZYsDrBMQjbn%2Fgroup-rationality-diary-august-1-15-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20August%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBqgA8tZYsDrBMQjbn%2Fgroup-rationality-diary-august-1-15-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBqgA8tZYsDrBMQjbn%2Fgroup-rationality-diary-august-1-15-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<div id=\"entry_t3_i05\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hvy\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hqf\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>T<span style=\"color: #333333;\">his is the public group instrumental rationality diary for August 1-15. <br /></span></p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/lw/i05/group_rationality_diary_july_1631/\">Immediate past diary</a>:&nbsp; July 16-31<a href=\"/lw/hvy/group_rationality_diary_july_115/\"></a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/r/discussion/lw/ic6/group_rationality_diary_august_1631/\">Next diary</a>:&nbsp; August 16-31</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality Diaries archive</a></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BqgA8tZYsDrBMQjbn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23610", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zuuHNLdwm2Xyy3hjd", "QXYsonygGQRc6fjJy", "TDZbs6Ah34PTd9Txw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-02T15:23:06.901Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-40", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/75nn7krhxd8WKZa4E/weekly-lw-meetups-40", "pageUrlRelative": "/posts/75nn7krhxd8WKZa4E/weekly-lw-meetups-40", "linkUrl": "https://www.lesswrong.com/posts/75nn7krhxd8WKZa4E/weekly-lw-meetups-40", "postedAtFormatted": "Friday, August 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F75nn7krhxd8WKZa4E%2Fweekly-lw-meetups-40%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F75nn7krhxd8WKZa4E%2Fweekly-lw-meetups-40", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F75nn7krhxd8WKZa4E%2Fweekly-lw-meetups-40", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 548, "htmlBody": "<p><strong>This summary was posted to LW main on July 26th. The following week's summary is <a href=\"/lw/i89/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/p5\">Atlanta LessWrong: August Meetup :&nbsp;<span class=\"date\">04 August 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/p7\">Brussels meetup:&nbsp;<span class=\"date\">17 August 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/p3\">Chicago: Discuss Thinking, Fast and Slow:&nbsp;<span class=\"date\">03 August 2013 09:58PM</span></a></li>\n<li><a href=\"/meetups/or\">Cincinnati: Financial optimisation:&nbsp;<span class=\"date\">28 July 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/p0\">Cleveland Ohio Meetup:&nbsp;<span class=\"date\">28 July 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/p4\">Moscow: Miscellaneous Rationality:&nbsp;<span class=\"date\">04 August 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/p2\">Philadelphia informal meetup:&nbsp;<span class=\"date\">28 July 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ow\">Zagreb meetup - July 27th:&nbsp;<span class=\"date\">27 July 2013 07:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">27 July 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/oz\">[Boston] Becoming Stronger:&nbsp;<span class=\"date\">28 July 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ot\">Durham NC/Triangle Area: Meetup!:&nbsp;<span class=\"date\">01 August 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/p1\">London Meetup - Achieving Better Goals:&nbsp;<span class=\"date\">04 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/oy\">Melbourne, practical rationality:&nbsp;<span class=\"date\">02 August 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/p6\">West LA Meetup&mdash;Introduction to Anthropics:&nbsp;<span class=\"date\">31 July 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "75nn7krhxd8WKZa4E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.288049853589959e-06, "legacy": true, "legacyId": "23523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zfgsPyNi8s4HQ2JgY", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-02T17:47:53.909Z", "modifiedAt": null, "url": null, "title": "Meetup : Vienna Meetup", "slug": "meetup-vienna-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6P7RjC6xNA48xDHmR/meetup-vienna-meetup-0", "pageUrlRelative": "/posts/6P7RjC6xNA48xDHmR/meetup-vienna-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/6P7RjC6xNA48xDHmR/meetup-vienna-meetup-0", "postedAtFormatted": "Friday, August 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vienna%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vienna%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6P7RjC6xNA48xDHmR%2Fmeetup-vienna-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vienna%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6P7RjC6xNA48xDHmR%2Fmeetup-vienna-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6P7RjC6xNA48xDHmR%2Fmeetup-vienna-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ph'>Vienna Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 August 2013 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Universit\u00e4tsstra\u00dfe 7, 1010 Wien</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Meetup will be held at the NIG, room 2G on the second floor, Universit\u00e4tsstra\u00dfe 7, 1010 Wien.</p>\n\n<p>This time, the discussion will be led by Andreas, focusing on the book \"Thinking, Fast and Slow\", from Kahneman.</p>\n\n<p>facebook link: https://www.facebook.com/events/438894002884502/?ref=3</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ph'>Vienna Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6P7RjC6xNA48xDHmR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2881688840537737e-06, "legacy": true, "legacyId": "23626", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vienna_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ph\">Vienna Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 August 2013 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Universit\u00e4tsstra\u00dfe 7, 1010 Wien</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Meetup will be held at the NIG, room 2G on the second floor, Universit\u00e4tsstra\u00dfe 7, 1010 Wien.</p>\n\n<p>This time, the discussion will be led by Andreas, focusing on the book \"Thinking, Fast and Slow\", from Kahneman.</p>\n\n<p>facebook link: https://www.facebook.com/events/438894002884502/?ref=3</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vienna_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ph\">Vienna Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vienna Meetup", "anchor": "Discussion_article_for_the_meetup___Vienna_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vienna Meetup", "anchor": "Discussion_article_for_the_meetup___Vienna_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-02T18:41:08.410Z", "modifiedAt": null, "url": null, "title": "[Link] AI advances: computers can be almost as funny as people", "slug": "link-ai-advances-computers-can-be-almost-as-funny-as-people", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:31.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y7vmkDkN9b33Hx82o/link-ai-advances-computers-can-be-almost-as-funny-as-people", "pageUrlRelative": "/posts/Y7vmkDkN9b33Hx82o/link-ai-advances-computers-can-be-almost-as-funny-as-people", "linkUrl": "https://www.lesswrong.com/posts/Y7vmkDkN9b33Hx82o/link-ai-advances-computers-can-be-almost-as-funny-as-people", "postedAtFormatted": "Friday, August 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20AI%20advances%3A%20computers%20can%20be%20almost%20as%20funny%20as%20people&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20AI%20advances%3A%20computers%20can%20be%20almost%20as%20funny%20as%20people%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7vmkDkN9b33Hx82o%2Flink-ai-advances-computers-can-be-almost-as-funny-as-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20AI%20advances%3A%20computers%20can%20be%20almost%20as%20funny%20as%20people%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7vmkDkN9b33Hx82o%2Flink-ai-advances-computers-can-be-almost-as-funny-as-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7vmkDkN9b33Hx82o%2Flink-ai-advances-computers-can-be-almost-as-funny-as-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">\"Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.\"</span></p>\n<p>From <a href=\"http://www.acl2013.org/site/short/2197.html\">this paper</a>:</p>\n<h2 style=\"font-size: 12pt; font-family: Arial, Helvetica, sans-serif;\">Unsupervised joke generation from big data</h2>\n<h3 style=\"font-size: 11pt; font-family: Arial, Helvetica, sans-serif;\">Sasa Petrovic and David Matthews</h3>\n<p><strong style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\"><a style=\"text-decoration: none; color: #4f82cb; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit;\" href=\"http://acl2013.org/\">The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers (ACL Short Papers 2013)</a></strong><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">&nbsp;</span><br style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\" /><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">Sofia, Bulgaria, August 4-9, 2013</span></p>\n<p style=\"font-size: 13px; font-family: Arial, Helvetica, sans-serif;\">&nbsp;</p>\n<hr style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\" />\n<p style=\"font-size: 13px; font-family: Arial, Helvetica, sans-serif;\">&nbsp;</p>\n<h2 style=\"font-size: 12pt; font-family: Arial, Helvetica, sans-serif;\">Abstract</h2>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">Humor generation is a very hard problem. It is difficult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y, Z jokes, where X, Y, and Z are variables to be filled in. This is, to the best of our knowledge, the first fully unsupervised humor generation system. Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.</span></p>\n<p style=\"font-size: 13px; font-family: Arial, Helvetica, sans-serif;\">&nbsp;</p>\n<p>From <a href=\"http://www.theregister.co.uk/2013/08/02/heard_the_one_about/\">The Register</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">It uses 2,000,000 noun-adjective pairs of words to draw up jokes \"with an element of surprise\", something the creators claim is key to good comedy.</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">...</span></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; padding-left: 30px;\">&nbsp;jokes calculated by the software include:</p>\n<ul style=\"padding: 0px 0px 0px 31px; margin: 1em 0px; font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">\n<li style=\"margin: 0.2em 0px 0px; padding: 0px;\">I like my relationships like I like my source code... open</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y7vmkDkN9b33Hx82o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "23627", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">\"Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.\"</span></p>\n<p>From <a href=\"http://www.acl2013.org/site/short/2197.html\">this paper</a>:</p>\n<h2 style=\"font-size: 12pt; font-family: Arial, Helvetica, sans-serif;\" id=\"Unsupervised_joke_generation_from_big_data\">Unsupervised joke generation from big data</h2>\n<h3 style=\"font-size: 11pt; font-family: Arial, Helvetica, sans-serif;\" id=\"Sasa_Petrovic_and_David_Matthews\">Sasa Petrovic and David Matthews</h3>\n<p><strong style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\"><a style=\"text-decoration: none; color: #4f82cb; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit;\" href=\"http://acl2013.org/\">The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers (ACL Short Papers 2013)</a></strong><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">&nbsp;</span><br style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">Sofia, Bulgaria, August 4-9, 2013</span></p>\n<p style=\"font-size: 13px; font-family: Arial, Helvetica, sans-serif;\">&nbsp;</p>\n<hr style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">\n<p style=\"font-size: 13px; font-family: Arial, Helvetica, sans-serif;\">&nbsp;</p>\n<h2 style=\"font-size: 12pt; font-family: Arial, Helvetica, sans-serif;\" id=\"Abstract\">Abstract</h2>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">Humor generation is a very hard problem. It is difficult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y, Z jokes, where X, Y, and Z are variables to be filled in. This is, to the best of our knowledge, the first fully unsupervised humor generation system. Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.</span></p>\n<p style=\"font-size: 13px; font-family: Arial, Helvetica, sans-serif;\">&nbsp;</p>\n<p>From <a href=\"http://www.theregister.co.uk/2013/08/02/heard_the_one_about/\">The Register</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">It uses 2,000,000 noun-adjective pairs of words to draw up jokes \"with an element of surprise\", something the creators claim is key to good comedy.</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">...</span></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; padding-left: 30px;\">&nbsp;jokes calculated by the software include:</p>\n<ul style=\"padding: 0px 0px 0px 31px; margin: 1em 0px; font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px;\">\n<li style=\"margin: 0.2em 0px 0px; padding: 0px;\">I like my relationships like I like my source code... open</li>\n</ul>", "sections": [{"title": "Unsupervised joke generation from big data", "anchor": "Unsupervised_joke_generation_from_big_data", "level": 1}, {"title": "Sasa Petrovic and David Matthews", "anchor": "Sasa_Petrovic_and_David_Matthews", "level": 2}, {"title": "Abstract", "anchor": "Abstract", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-02T20:59:04.223Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes August 2013", "slug": "rationality-quotes-august-2013", "viewCount": null, "lastCommentedAt": "2015-03-03T23:20:52.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6CjfpnjJ7pPozSbb8/rationality-quotes-august-2013", "pageUrlRelative": "/posts/6CjfpnjJ7pPozSbb8/rationality-quotes-august-2013", "linkUrl": "https://www.lesswrong.com/posts/6CjfpnjJ7pPozSbb8/rationality-quotes-august-2013", "postedAtFormatted": "Friday, August 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20August%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20August%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CjfpnjJ7pPozSbb8%2Frationality-quotes-august-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20August%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CjfpnjJ7pPozSbb8%2Frationality-quotes-august-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CjfpnjJ7pPozSbb8%2Frationality-quotes-august-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<div id=\"entry_t3_hlk\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so <a href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6CjfpnjJ7pPozSbb8", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "23609", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 736, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-08-02T20:59:04.223Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-02T21:49:13.274Z", "modifiedAt": null, "url": null, "title": "August 2013 Media Thread", "slug": "august-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.565Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2PejHXfkuHmidgSrp/august-2013-media-thread", "pageUrlRelative": "/posts/2PejHXfkuHmidgSrp/august-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/2PejHXfkuHmidgSrp/august-2013-media-thread", "postedAtFormatted": "Friday, August 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20August%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAugust%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PejHXfkuHmidgSrp%2Faugust-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=August%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PejHXfkuHmidgSrp%2Faugust-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PejHXfkuHmidgSrp%2Faugust-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2PejHXfkuHmidgSrp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 1.2883673253889702e-06, "legacy": true, "legacyId": "23628", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 122, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-03T02:29:21.928Z", "modifiedAt": null, "url": null, "title": "Algorithmic Progress in Six Domains", "slug": "algorithmic-progress-in-six-domains", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:32.720Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ueBMpvDsDEZgKiESt/algorithmic-progress-in-six-domains", "pageUrlRelative": "/posts/ueBMpvDsDEZgKiESt/algorithmic-progress-in-six-domains", "linkUrl": "https://www.lesswrong.com/posts/ueBMpvDsDEZgKiESt/algorithmic-progress-in-six-domains", "postedAtFormatted": "Saturday, August 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Algorithmic%20Progress%20in%20Six%20Domains&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlgorithmic%20Progress%20in%20Six%20Domains%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FueBMpvDsDEZgKiESt%2Falgorithmic-progress-in-six-domains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Algorithmic%20Progress%20in%20Six%20Domains%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FueBMpvDsDEZgKiESt%2Falgorithmic-progress-in-six-domains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FueBMpvDsDEZgKiESt%2Falgorithmic-progress-in-six-domains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 416, "htmlBody": "<p>Today MIRI released a new technical report by visiting researcher <a href=\"http://meteuphoric.wordpress.com/\">Katja Grace</a>&nbsp;called \"<strong><a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\">Algorithmic Progress in Six Domains</a></strong>.\" The report summarizes data on algorithmic progress &ndash; that is, better performance per fixed amount of computing hardware &ndash; in six domains:</p>\n<ul>\n<li><span style=\"line-height: 13px;\">SAT solvers,</span></li>\n<li><span style=\"line-height: 13px;\">Chess and Go programs,</span></li>\n<li><span style=\"line-height: 13px;\">Physics simulations,</span></li>\n<li><span style=\"line-height: 13px;\">Factoring,</span></li>\n<li><span style=\"line-height: 13px;\">Mixed integer programming, and</span></li>\n<li><span style=\"line-height: 13px;\">Some forms of machine learning. </span></li>\n</ul>\n<p>MIRI's purpose for collecting these data was to shed light on the question of <a href=\"/lw/hbd/new_report_intelligence_explosion_microeconomics/\">intelligence explosion microeconomics</a>, though we suspect the report will be of broad interest within the software industry and computer science academia.</p>\n<p>One finding from the report was previously discussed by Robin Hanson <a href=\"http://www.overcomingbias.com/2013/06/why-does-hardware-grow-like-algorithms.html\">here</a>. (Robin saw an early draft on the intelligence explosion microeconomics <a href=\"https://docs.google.com/forms/d/1KElE2Zt_XQRqj8vWrc_rG89nrO4JtHWxIFldJ3IY_FQ/viewform\">mailing list</a>.)</p>\n<p>This is the preferred page for discussing the report in general.</p>\n<p>Summary:</p>\n<blockquote>In recent <em>boolean satisfiability</em> (SAT) competitions, SAT solver performance has increased 5&ndash;15% per year, depending on the type of problem. However, these gains have been driven by widely varying improvements on particular problems. Retrospective surveys of SAT performance (on problems chosen after the fact) display significantly faster progress.</blockquote>\n<blockquote><em>Chess programs</em> have improved by around 50 Elo points per year over the last four decades. Estimates for the significance of hardware improvements are very noisy, but are consistent with hardware improvements being responsible for approximately half of progress. Progress has been smooth on the scale of years since the 1960s, except for the past five. <em>Go programs</em> have improved about one stone per year for the last three decades. Hardware doublings produce diminishing Elo gains, on a scale consistent with accounting for around half of progress.</blockquote>\n<blockquote>Improvements in a variety of <em>physics simulations</em> (selected after the fact to exhibit performance increases due to software) appear to be roughly half due to hardware progress.</blockquote>\n<blockquote>The <em>largest number factored</em> to date has grown by about 5.5 digits per year for the last two decades; computing power increased 10,000-fold over this period, and it is unclear how much of the increase is due to hardware progress.</blockquote>\n<blockquote>Some <em>mixed integer programming</em> (MIP) algorithms, run on modern MIP instances with modern hardware, have roughly doubled in speed each year. MIP is an important optimization problem, but one which has been called to attention after the fact due to performance improvements. Other optimization problems have had more inconsistent (and harder to determine) improvements.</blockquote>\n<blockquote>Various forms of <em>machine learning</em> have had steeply diminishing progress in percentage accuracy over recent decades. Some vision tasks have recently seen faster progress.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GY5kPPpCoyt9fnTMn": 2, "sYm3HiWcfZvrGu3ui": 1, "HFou6RHqFagkyrKkW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ueBMpvDsDEZgKiESt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 38, "extendedScore": null, "score": 1.2885977576743543e-06, "legacy": true, "legacyId": "23634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CZQuFoqgPXQawH9aL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-04T03:20:18.859Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-13", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wAz32rJBuyBadCrTG/meetup-washington-dc-fun-and-games-meetup-13", "pageUrlRelative": "/posts/wAz32rJBuyBadCrTG/meetup-washington-dc-fun-and-games-meetup-13", "linkUrl": "https://www.lesswrong.com/posts/wAz32rJBuyBadCrTG/meetup-washington-dc-fun-and-games-meetup-13", "postedAtFormatted": "Sunday, August 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAz32rJBuyBadCrTG%2Fmeetup-washington-dc-fun-and-games-meetup-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAz32rJBuyBadCrTG%2Fmeetup-washington-dc-fun-and-games-meetup-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwAz32rJBuyBadCrTG%2Fmeetup-washington-dc-fun-and-games-meetup-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pi'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pi'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wAz32rJBuyBadCrTG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2898253638528016e-06, "legacy": true, "legacyId": "23641", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/pi\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/pi\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-04T10:58:12.742Z", "modifiedAt": null, "url": null, "title": "Seeking a reader for LessWrong sequences", "slug": "seeking-a-reader-for-lesswrong-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:59.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rick_from_Castify", "createdAt": "2012-12-03T09:33:28.512Z", "isAdmin": false, "displayName": "Rick_from_Castify"}, "userId": "XyTqQupkZ9SW7nGCB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EKpypTgt9GXesPz9C/seeking-a-reader-for-lesswrong-sequences", "pageUrlRelative": "/posts/EKpypTgt9GXesPz9C/seeking-a-reader-for-lesswrong-sequences", "linkUrl": "https://www.lesswrong.com/posts/EKpypTgt9GXesPz9C/seeking-a-reader-for-lesswrong-sequences", "postedAtFormatted": "Sunday, August 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20a%20reader%20for%20LessWrong%20sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20a%20reader%20for%20LessWrong%20sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKpypTgt9GXesPz9C%2Fseeking-a-reader-for-lesswrong-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20a%20reader%20for%20LessWrong%20sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKpypTgt9GXesPz9C%2Fseeking-a-reader-for-lesswrong-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKpypTgt9GXesPz9C%2Fseeking-a-reader-for-lesswrong-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p>As some of you are aware we make audio versions of LessWrong sequences.&nbsp; We are looking to hire someone from the LessWrong community who has a good voice and is keen to record the remaining sequences.&nbsp; There is quite a bit of material as I'm sure you are all aware.&nbsp;</p>\n<p>Anybody who is interested please record yourself reading <a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a> and send it to support@castify.co.&nbsp; Please say what sort of recording device you used and keep the recording size rational.</p>\n<p>Thanks to all of you who have been listening.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EKpypTgt9GXesPz9C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.2902027998755708e-06, "legacy": true, "legacyId": "23643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-05T00:31:56.319Z", "modifiedAt": null, "url": null, "title": "Crossing the experiments: a baby", "slug": "crossing-the-experiments-a-baby", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.169Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BkfB3gxFgrNcQsPAK/crossing-the-experiments-a-baby", "pageUrlRelative": "/posts/BkfB3gxFgrNcQsPAK/crossing-the-experiments-a-baby", "linkUrl": "https://www.lesswrong.com/posts/BkfB3gxFgrNcQsPAK/crossing-the-experiments-a-baby", "postedAtFormatted": "Monday, August 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Crossing%20the%20experiments%3A%20a%20baby&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACrossing%20the%20experiments%3A%20a%20baby%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkfB3gxFgrNcQsPAK%2Fcrossing-the-experiments-a-baby%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Crossing%20the%20experiments%3A%20a%20baby%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkfB3gxFgrNcQsPAK%2Fcrossing-the-experiments-a-baby", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkfB3gxFgrNcQsPAK%2Fcrossing-the-experiments-a-baby", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>I've always been more of a theoretician, but it's important to try one's hand at practical problems from time to time. In that vein, I've decided to try three simultaneous experiments on major Less Wrong themes. I will aim to acquire <a href=\"/lw/nb/something_to_protect/\">something to protect</a>, I will practice training a <a href=\"/lw/we/recursive_selfimprovement/\">seed intelligence</a>, and I will become more familiar with <a href=\"/lw/l3/thou_art_godshatter/\">many consequences</a> of evolutionary psychology.</p>\n<p>In the spirit of <a href=\"/lw/vb/efficient_crossdomain_optimization/\">efficiency</a> I'll combine all these experiments into one:</p>\n<p><img src=\"http://images.lesswrong.com/t3_i1y_0.png?v=587126465f9d2fda242bf0d625e3efd7\" alt=\"\" width=\"768\" height=\"720\" /></p>\n<p>She's never seen Star Wars or Doctor Who.</p>\n<p>She's never seen David Attenborough or read J. L. Borges.</p>\n<p>She's never had a philosophical debate.</p>\n<p>She's never been skiing.</p>\n<p>Never had sex, never been hugged or even been licked by a dog!</p>\n<p>She has so much to look forwards to...</p>\n<p>(Though she'll be very boring for several months yet!)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BkfB3gxFgrNcQsPAK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 71, "baseScore": 5, "extendedScore": null, "score": 1.2908740202704339e-06, "legacy": true, "legacyId": "23398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB", "JBadX7rwdcRFzGuju", "cSXZpvqpa9vbGGLtG", "yLeEPFnnB9wE7KLx2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-05T02:20:42.382Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC books meetup", "slug": "meetup-washington-dc-books-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L8EpTpSPG5iGh7drf/meetup-washington-dc-books-meetup-0", "pageUrlRelative": "/posts/L8EpTpSPG5iGh7drf/meetup-washington-dc-books-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/L8EpTpSPG5iGh7drf/meetup-washington-dc-books-meetup-0", "postedAtFormatted": "Monday, August 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20books%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20books%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8EpTpSPG5iGh7drf%2Fmeetup-washington-dc-books-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20books%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8EpTpSPG5iGh7drf%2Fmeetup-washington-dc-books-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8EpTpSPG5iGh7drf%2Fmeetup-washington-dc-books-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pj'>Washington DC books meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about and exchange books.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pj'>Washington DC books meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L8EpTpSPG5iGh7drf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2909637865482875e-06, "legacy": true, "legacyId": "23645", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_books_meetup\">Discussion article for the meetup : <a href=\"/meetups/pj\">Washington DC books meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about and exchange books.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_books_meetup1\">Discussion article for the meetup : <a href=\"/meetups/pj\">Washington DC books meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC books meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_books_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC books meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_books_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-05T05:33:23.425Z", "modifiedAt": null, "url": null, "title": "Meetup : Saskatoon's First Meetup!", "slug": "meetup-saskatoon-s-first-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:29.349Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nicholas_Rutherford", "createdAt": "2013-08-01T02:29:11.736Z", "isAdmin": false, "displayName": "Nicholas_Rutherford"}, "userId": "nucgkHPJBwJuK8sY7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KhoZzt5KnLghwgdou/meetup-saskatoon-s-first-meetup", "pageUrlRelative": "/posts/KhoZzt5KnLghwgdou/meetup-saskatoon-s-first-meetup", "linkUrl": "https://www.lesswrong.com/posts/KhoZzt5KnLghwgdou/meetup-saskatoon-s-first-meetup", "postedAtFormatted": "Monday, August 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saskatoon's%20First%20Meetup!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saskatoon's%20First%20Meetup!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhoZzt5KnLghwgdou%2Fmeetup-saskatoon-s-first-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saskatoon's%20First%20Meetup!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhoZzt5KnLghwgdou%2Fmeetup-saskatoon-s-first-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhoZzt5KnLghwgdou%2Fmeetup-saskatoon-s-first-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pk'>Saskatoon's First Meetup!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 August 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's time for central Canada's first meetup!</p>\n\n<p>It will be at the Broadway Roaster on 8th street (not on broadway!) at the pleasant time of 1:00 in the afternoon.</p>\n\n<p>Feel free to bring along anyone who you think might be interested, even if they are not LW people.</p>\n\n<p>I look forward to meeting you all! :D</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pk'>Saskatoon's First Meetup!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KhoZzt5KnLghwgdou", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.2911228361775682e-06, "legacy": true, "legacyId": "23653", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saskatoon_s_First_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/pk\">Saskatoon's First Meetup!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 August 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's time for central Canada's first meetup!</p>\n\n<p>It will be at the Broadway Roaster on 8th street (not on broadway!) at the pleasant time of 1:00 in the afternoon.</p>\n\n<p>Feel free to bring along anyone who you think might be interested, even if they are not LW people.</p>\n\n<p>I look forward to meeting you all! :D</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saskatoon_s_First_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/pk\">Saskatoon's First Meetup!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saskatoon's First Meetup!", "anchor": "Discussion_article_for_the_meetup___Saskatoon_s_First_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : Saskatoon's First Meetup!", "anchor": "Discussion_article_for_the_meetup___Saskatoon_s_First_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-05T06:50:46.816Z", "modifiedAt": null, "url": null, "title": " Open thread, August 5-11, 2013 ", "slug": "open-thread-august-5-11-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8S2WGny7A4mNhvT6k/open-thread-august-5-11-2013", "pageUrlRelative": "/posts/8S2WGny7A4mNhvT6k/open-thread-august-5-11-2013", "linkUrl": "https://www.lesswrong.com/posts/8S2WGny7A4mNhvT6k/open-thread-august-5-11-2013", "postedAtFormatted": "Monday, August 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Open%20thread%2C%20August%205-11%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Open%20thread%2C%20August%205-11%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8S2WGny7A4mNhvT6k%2Fopen-thread-august-5-11-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Open%20thread%2C%20August%205-11%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8S2WGny7A4mNhvT6k%2Fopen-thread-august-5-11-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8S2WGny7A4mNhvT6k%2Fopen-thread-august-5-11-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8S2WGny7A4mNhvT6k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 310, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-06T11:58:51.153Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Munich Meetup in August", "slug": "meetup-lw-munich-meetup-in-august", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Teobaldo", "createdAt": "2012-10-28T17:25:26.852Z", "isAdmin": false, "displayName": "Teobaldo"}, "userId": "BhijBsy7WLfpnsZGJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jysdvGAgDD8spyMEP/meetup-lw-munich-meetup-in-august", "pageUrlRelative": "/posts/jysdvGAgDD8spyMEP/meetup-lw-munich-meetup-in-august", "linkUrl": "https://www.lesswrong.com/posts/jysdvGAgDD8spyMEP/meetup-lw-munich-meetup-in-august", "postedAtFormatted": "Tuesday, August 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Munich%20Meetup%20in%20August&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Munich%20Meetup%20in%20August%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjysdvGAgDD8spyMEP%2Fmeetup-lw-munich-meetup-in-august%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Munich%20Meetup%20in%20August%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjysdvGAgDD8spyMEP%2Fmeetup-lw-munich-meetup-in-august", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjysdvGAgDD8spyMEP%2Fmeetup-lw-munich-meetup-in-august", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pl'>LW Munich Meetup in August</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 August 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Hirschgarten, 80639 Munich, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Munich meetup will take place on August 10th. You are highly welcome to come and say hi, no matter how long you\u2019ve been reading LessWrong. If you plan to attend, please (optionally) post a comment saying what topics you\u2019d like to discuss. Also if you on Facebook, think about joining the group (see comments to the previous meetup).</p>\n\n<p>Location: Since the weather is supposed to become bettter on Saturday, I'd suggest we meet in Hirschgarten Biergarten. GPS: 48.149063, 11.512099 near the tables (I'll bring a blanket). Also we would probably go to the tables later so leave a comment if you are going to join later.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pl'>LW Munich Meetup in August</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jysdvGAgDD8spyMEP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.292631373209391e-06, "legacy": true, "legacyId": "23666", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August\">Discussion article for the meetup : <a href=\"/meetups/pl\">LW Munich Meetup in August</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 August 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Hirschgarten, 80639 Munich, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Munich meetup will take place on August 10th. You are highly welcome to come and say hi, no matter how long you\u2019ve been reading LessWrong. If you plan to attend, please (optionally) post a comment saying what topics you\u2019d like to discuss. Also if you on Facebook, think about joining the group (see comments to the previous meetup).</p>\n\n<p>Location: Since the weather is supposed to become bettter on Saturday, I'd suggest we meet in Hirschgarten Biergarten. GPS: 48.149063, 11.512099 near the tables (I'll bring a blanket). Also we would probably go to the tables later so leave a comment if you are going to join later.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August1\">Discussion article for the meetup : <a href=\"/meetups/pl\">LW Munich Meetup in August</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Munich Meetup in August", "anchor": "Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August", "level": 1}, {"title": "Discussion article for the meetup : LW Munich Meetup in August", "anchor": "Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-06T11:58:52.256Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Munich Meetup in August", "slug": "meetup-lw-munich-meetup-in-august-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Teobaldo", "createdAt": "2012-10-28T17:25:26.852Z", "isAdmin": false, "displayName": "Teobaldo"}, "userId": "BhijBsy7WLfpnsZGJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v37HnuAQ429BfALhw/meetup-lw-munich-meetup-in-august-0", "pageUrlRelative": "/posts/v37HnuAQ429BfALhw/meetup-lw-munich-meetup-in-august-0", "linkUrl": "https://www.lesswrong.com/posts/v37HnuAQ429BfALhw/meetup-lw-munich-meetup-in-august-0", "postedAtFormatted": "Tuesday, August 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Munich%20Meetup%20in%20August&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Munich%20Meetup%20in%20August%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv37HnuAQ429BfALhw%2Fmeetup-lw-munich-meetup-in-august-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Munich%20Meetup%20in%20August%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv37HnuAQ429BfALhw%2Fmeetup-lw-munich-meetup-in-august-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv37HnuAQ429BfALhw%2Fmeetup-lw-munich-meetup-in-august-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pm'>LW Munich Meetup in August</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 August 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Hirschgarten, 80639 Munich, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Munich meetup will take place on August 10th. You are highly welcome to come and say hi, no matter how long you\u2019ve been reading LessWrong. If you plan to attend, please (optionally) post a comment saying what topics you\u2019d like to discuss. Also if you on Facebook, think about joining the group (see comments to the previous meetup).</p>\n\n<p>Location: Since the weather is supposed to become bettter on Saturday, I'd suggest we meet in Hirschgarten Biergarten. GPS: 48.149063, 11.512099 near the tables (I'll bring a blanket). Also we would probably go to the tables later so leave a comment if you are going to join later.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pm'>LW Munich Meetup in August</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v37HnuAQ429BfALhw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2926313883892598e-06, "legacy": true, "legacyId": "23667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August\">Discussion article for the meetup : <a href=\"/meetups/pm\">LW Munich Meetup in August</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 August 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Hirschgarten, 80639 Munich, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Munich meetup will take place on August 10th. You are highly welcome to come and say hi, no matter how long you\u2019ve been reading LessWrong. If you plan to attend, please (optionally) post a comment saying what topics you\u2019d like to discuss. Also if you on Facebook, think about joining the group (see comments to the previous meetup).</p>\n\n<p>Location: Since the weather is supposed to become bettter on Saturday, I'd suggest we meet in Hirschgarten Biergarten. GPS: 48.149063, 11.512099 near the tables (I'll bring a blanket). Also we would probably go to the tables later so leave a comment if you are going to join later.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August1\">Discussion article for the meetup : <a href=\"/meetups/pm\">LW Munich Meetup in August</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Munich Meetup in August", "anchor": "Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August", "level": 1}, {"title": "Discussion article for the meetup : LW Munich Meetup in August", "anchor": "Discussion_article_for_the_meetup___LW_Munich_Meetup_in_August1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-06T12:11:12.797Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Israel September meetup", "slug": "meetup-lesswrong-israel-september-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:01.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MXFvZ54WvaQjmjL7L/meetup-lesswrong-israel-september-meetup", "pageUrlRelative": "/posts/MXFvZ54WvaQjmjL7L/meetup-lesswrong-israel-september-meetup", "linkUrl": "https://www.lesswrong.com/posts/MXFvZ54WvaQjmjL7L/meetup-lesswrong-israel-september-meetup", "postedAtFormatted": "Tuesday, August 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Israel%20September%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Israel%20September%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXFvZ54WvaQjmjL7L%2Fmeetup-lesswrong-israel-september-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Israel%20September%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXFvZ54WvaQjmjL7L%2Fmeetup-lesswrong-israel-september-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXFvZ54WvaQjmjL7L%2Fmeetup-lesswrong-israel-september-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pn'>LessWrong Israel September meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 September 2013 08:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Adventure Park, Park hayarqon tel aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong><em>*</em></strong> NOTE: THE LOCATION HAS CHANGED TO PARK HAYARQON!! <strong><em>*</em></strong></p>\n\n<p>Call 0545330678 (Gal) for details.</p>\n\n<p>We're going to have a meetup on Thursday, September 12 at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.\nOur program is:\n* 20:00-20:15: Assembly\n* 20:15-21:00: Main Talk\n* 21:00-22:00: Dinner &amp; Discussion\n* 22:00-23:00: Rump Session (minitalks)\n* 23:00-: End of official programming\nMain Talk: Mirrors and Sunlight: Dealing with Emotional Vampires / Guy Banay\nWe will learn how to recognize the Dramatic Personality Disorders (antisocial, borderline, narcissistic and histrionic) and some strategies to minimize the damage they can do.\nBackup Talk: Solomonoff Induction / Benjamin Fox\nSolomonoff Induction is basically the most generalized form of intelligence, intelligence treated on the most basic mathematical level.If we wish to find facts given provided information, Solomonoff Induction is the most fundamental algorithm to do so.\nRump Session: each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes.\n(Posted for Guy Banay, the organizer, who doesn't have enough LW karma yet.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pn'>LessWrong Israel September meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MXFvZ54WvaQjmjL7L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.2926415985424285e-06, "legacy": true, "legacyId": "23668", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Israel_September_meetup\">Discussion article for the meetup : <a href=\"/meetups/pn\">LessWrong Israel September meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 September 2013 08:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Adventure Park, Park hayarqon tel aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong><em>*</em></strong> NOTE: THE LOCATION HAS CHANGED TO PARK HAYARQON!! <strong><em>*</em></strong></p>\n\n<p>Call 0545330678 (Gal) for details.</p>\n\n<p>We're going to have a meetup on Thursday, September 12 at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.\nOur program is:\n* 20:00-20:15: Assembly\n* 20:15-21:00: Main Talk\n* 21:00-22:00: Dinner &amp; Discussion\n* 22:00-23:00: Rump Session (minitalks)\n* 23:00-: End of official programming\nMain Talk: Mirrors and Sunlight: Dealing with Emotional Vampires / Guy Banay\nWe will learn how to recognize the Dramatic Personality Disorders (antisocial, borderline, narcissistic and histrionic) and some strategies to minimize the damage they can do.\nBackup Talk: Solomonoff Induction / Benjamin Fox\nSolomonoff Induction is basically the most generalized form of intelligence, intelligence treated on the most basic mathematical level.If we wish to find facts given provided information, Solomonoff Induction is the most fundamental algorithm to do so.\nRump Session: each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes.\n(Posted for Guy Banay, the organizer, who doesn't have enough LW karma yet.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Israel_September_meetup1\">Discussion article for the meetup : <a href=\"/meetups/pn\">LessWrong Israel September meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Israel September meetup", "anchor": "Discussion_article_for_the_meetup___LessWrong_Israel_September_meetup", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Israel September meetup", "anchor": "Discussion_article_for_the_meetup___LessWrong_Israel_September_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-07T04:05:58.366Z", "modifiedAt": null, "url": null, "title": "How to Measure Anything", "slug": "how-to-measure-anything", "viewCount": null, "lastCommentedAt": "2021-01-21T04:38:05.909Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything", "pageUrlRelative": "/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything", "linkUrl": "https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything", "postedAtFormatted": "Wednesday, August 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Measure%20Anything&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Measure%20Anything%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FybYBCK9D7MZCcdArB%2Fhow-to-measure-anything%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Measure%20Anything%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FybYBCK9D7MZCcdArB%2Fhow-to-measure-anything", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FybYBCK9D7MZCcdArB%2Fhow-to-measure-anything", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6555, "htmlBody": "<p><a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/\"><img style=\"padding: 30px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/how-to-measure-anything.jpeg\" alt=\"\" align=\"right\" /></a>Douglas Hubbard&rsquo;s <em><a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/\">How to Measure Anything</a></em> is one of my favorite how-to books. I hope this summary inspires you to buy the book; it&rsquo;s worth it.</p>\n<p>The book opens:</p>\n<blockquote>\n<p>Anything can be measured. If a thing can be observed in any way at all, it lends itself to some type of measurement method. No matter how &ldquo;fuzzy&rdquo; the measurement is, it&rsquo;s still a measurement if it tells you more than you knew before. And those very things most likely to be seen as immeasurable are, virtually always, solved by relatively simple measurement methods.</p>\n</blockquote>\n<p>The sciences have many established measurement methods, so Hubbard&rsquo;s book focuses on the measurement of &ldquo;business intangibles&rdquo; that are important for decision-making but tricky to measure: things like management effectiveness, the &ldquo;flexibility&rdquo; to create new products, the risk of bankruptcy, and public image.</p>\n<p>&nbsp;</p>\n<h3 id=\"basicideas\">Basic Ideas</h3>\n<p>A <em>measurement</em> is an observation that quantitatively reduces uncertainty. Measurements might not yield precise, certain judgments, but they <em>do</em> reduce your uncertainty.</p>\n<p>To be measured, the <em>object of measurement</em> must be described clearly, in terms of observables. A good way to clarify a vague object of measurement like &ldquo;IT security&rdquo; is to ask &ldquo;What is IT security, and why do you care?&rdquo; Such probing can reveal that &ldquo;IT security&rdquo; means things like a reduction in unauthorized intrusions and malware attacks, which the IT department cares about because these things result in lost productivity, fraud losses, and legal liabilities.</p>\n<p><em>Uncertainty</em> is the lack of certainty: the true outcome/state/value is not known.</p>\n<p><em>Risk</em> is a state of uncertainty in which some of the possibilities involve a loss.</p>\n<p>Much pessimism about measurement comes from a lack of experience making measurements. Hubbard, who is <em>far</em> more experienced with measurement than his readers, says:</p>\n<ol>\n<li>Your problem is not as unique as you think.</li>\n<li>You have more data than you think.</li>\n<li>You need less data than you think.</li>\n<li>An adequate amount of new data is more accessible than you think.</li>\n</ol>\n<h3 id=\"appliedinformationeconomics\"><br /></h3>\n<h3>Applied Information Economics</h3>\n<p>Hubbard calls his method &ldquo;Applied Information Economics&rdquo; (AIE). It consists of 5 steps:</p>\n<ol>\n<li>Define a decision problem and the relevant variables. (Start with the decision you need to make, then figure out which variables would make your decision easier if you had better estimates of their values.)</li>\n<li>Determine what you know. (Quantify your uncertainty about those variables in terms of ranges and probabilities.)</li>\n<li>Pick a variable, and compute the value of additional information for that variable. (Repeat until you find a variable with reasonably high information value. If no remaining variables have enough information value to justify the cost of measuring them, skip to step 5.)</li>\n<li>Apply the relevant measurement instrument(s) to the high-information-value variable. (Then go back to step 3.)</li>\n<li>Make a decision and act on it. (When you&rsquo;ve done as much uncertainty reduction as is economically justified, it&rsquo;s time to act!)</li>\n</ol>\n<p>These steps are elaborated below.</p>\n<h3 id=\"step1:defineadecisionproblemandtherelevantvariables\"><a id=\"more\"></a><br /></h3>\n<h3>Step 1: Define a decision problem and the relevant variables</h3>\n<p>Hubbard illustrates this step by telling the story of how he helped the Department of Veterans Affairs (VA) with a measurement problem.</p>\n<p>The VA was considering seven proposed IT security projects. They wanted to know &ldquo;which&hellip; of the proposed investments were justified and, after they were implemented, whether improvements in security justified further investment&hellip;&rdquo; Hubbard asked his standard questions: &ldquo;What do you mean by &lsquo;IT security&rsquo;? Why does it matter to you? What are you observing when you observe improved IT security?&rdquo;</p>\n<p>It became clear that <em>nobody</em> at the VA had thought about the details of what &ldquo;IT security&rdquo; meant to them. But after Hubbard&rsquo;s probing, it became clear that by &ldquo;IT security&rdquo; they meant a reduction in the frequency and severity of some undesirable events: agency-wide virus attacks, unauthorized system access (external or internal),unauthorized physical access, and disasters affecting the IT infrastructure (fire, flood, etc.) And each undesirable event was on the list because of specific costs associated with it: productivity losses from virus attacks, legal liability from unauthorized system access, etc.</p>\n<p>Now that the VA knew what they meant by &ldquo;IT security,&rdquo; they could measure specific variables, such as the number of virus attacks per year.</p>\n<h3 id=\"step2:determinewhatyouknow\"><br /></h3>\n<h3>Step 2: Determine what you know</h3>\n<h4 id=\"uncertaintyandcalibration\">Uncertainty and calibration</h4>\n<p>The next step is to determine your level of uncertainty about the variables you want to measure. To do this, you can express a &ldquo;confidence interval&rdquo; (CI). A 90% CI is a range of values that is 90% likely to contain the correct value. For example, the security experts at the VA were 90% confident that each agency-wide virus attack would affect between 25,000 and 65,000 people.</p>\n<p>Unfortunately, few people are well-calibrated estimators. For example in some studies, the true value lay in subjects&rsquo; 90% CIs only 50% of the time! These subjects were overconfident. For a well-calibrated estimator, the true value will lie in her 90% CI roughly 90% of the time.</p>\n<p>Luckily, &ldquo;assessing uncertainty is a general skill that can be taught with a measurable improvement.&rdquo;</p>\n<p>Hubbard uses several methods to calibrate each client&rsquo;s value estimators, for example the security experts at the VA who needed to estimate the frequency of security breaches and their likely costs.</p>\n<p>His first technique is the <em>equivalent bet test</em>. Suppose you&rsquo;re asked to give a 90% CI for the year in which Newton published the universal laws of gravitation, and you can win $1,000 in one of two ways:</p>\n<ol>\n<li>You win $1,000 if the true year of publication falls within your 90% CI. Otherwise, you win nothing.</li>\n<li>You spin a dial divided into two &ldquo;pie slices,&rdquo; one covering 10% of the dial, and the other covering 90%. If the dial lands on the small slice, you win nothing. If it lands on the big slice, you win $1,000.</li>\n</ol>\n<p>If you find yourself preferring option #2, then you must think spinning the dial has a higher chance of winning you $1,000 than option #1. That suggest your stated 90% CI isn&rsquo;t really your 90% CI. Maybe it&rsquo;s your 65% CI or your 80% CI instead. By preferring option #2, your brain is trying to tell you that your originally stated 90% CI is overconfident.</p>\n<p>If instead you find yourself preferring option #1, then you must think there is <em>more</em> than a 90% chance your stated 90% CI contains the true value. By preferring option #1, your brain is trying to tell you that your original 90% CI is under confident.</p>\n<p>To make a better estimate, adjust your 90% CI until option #1 and option #2 seem equally good to you. Research suggests that even <em>pretending</em> to bet money in this way will improve your calibration.</p>\n<p>Hubbard&rsquo;s second method for improving calibration is simply <em>repetition and feedback</em>. Make lots of estimates and then see how well you did. For this, play CFAR&rsquo;s <a href=\"http://acritch.com/credence-game/\">Calibration Game</a>.</p>\n<p>Hubbard also asks people to identify reasons why a particular estimate might be right, and why it might be wrong.</p>\n<p>He also asks people to look more closely at each bound (upper and lower) on their estimated range. A 90% CI &ldquo;means there is a 5% chance the true value could be greater than the upper bound, and a 5% chance it could be less than the lower bound. This means the estimators must be 95% sure that the true value is less than the upper bound. If they are not that certain, they should increase the upper bound&hellip; A similar test is applied to the lower bound.&rdquo;</p>\n<h4 id=\"simulations\"><br /></h4>\n<h4>Simulations</h4>\n<p>Once you determine what you know about the uncertainties involved, how can you use that information to determine what you know about the <em>risks</em> involved? Hubbard summarizes:</p>\n<blockquote>\n<p>&hellip;all risk in any project&hellip; can be expressed by one method: the ranges of uncertainty on the costs and benefits, and probabilities on events that might affect them.</p>\n</blockquote>\n<p>The simplest tool for measuring such risks accurately is the Monte Carlo (MC) simulation, which can be run by Excel and many other programs. To illustrate this tool, suppose you are wondering whether to lease a new machine for one step in your manufacturing process.</p>\n<blockquote>\n<p>The one-year lease [for the machine] is $400,000 with no option for early cancellation. So if you aren&rsquo;t breaking even, you are still stuck with it for the rest of the year. You are considering signing the contract because you think the more advanced device will save some labor and raw materials and because you think the maintenance cost will be lower than the existing process.</p>\n</blockquote>\n<p>Your pre-calibrated estimators give their 90% CIs for the following variables:</p>\n<ul>\n<li>Maintenance savings (MS): $10 to $20 per unit</li>\n<li>Labor savings (LS): -$2 to $8 per unit</li>\n<li>Raw materials savings (RMS): $3 to $9 per unit</li>\n<li>Production level (PL): 15,000 to 35,000 units per year</li>\n</ul>\n<p>Thus, your annual savings will equal (MS + LS + RMS) &times; PL.</p>\n<p>When measuring risk, we don&rsquo;t just want to know the &ldquo;average&rdquo; risk or benefit. We want to know the probability of a huge loss, the probability of a small loss, the probability of a huge savings, and so on. That&rsquo;s what Monte Carlo can tell us.</p>\n<p>An MC simulation uses a computer to randomly generate thousands of possible values for each variable, based on the ranges we&rsquo;ve estimated. The computer then calculates the outcome (in this case, the annual savings) for each generated combination of values, and we&rsquo;re able to see how often different kinds of outcomes occur.</p>\n<p>To run an MC simulation we need not just the 90% CI for each variable but also the <em>shape</em> of each distribution. In many cases, the <a href=\"http://en.wikipedia.org/wiki/Normal_distribution\">normal distribution</a> will work just fine, and we&rsquo;ll use it for all the variables in this simplified illustration. (Hubbard&rsquo;s book shows you how to work with other distributions).</p>\n<p>To make an MC simulation of a normally distributed variable in Excel, we use this formula:</p>\n<blockquote>\n<p>=norminv(rand(), mean, standard deviation)</p>\n</blockquote>\n<p>So the formula for the maintenance savings variable should be:</p>\n<blockquote>\n<p>=norminv(rand(), 15, (20&ndash;10)/3.29)</p>\n</blockquote>\n<p>Suppose you enter this formula on cell A1 in Excel. To generate (say) 10,000 values for the maintenance savings value, just (1) copy the contents of cell A1, (2) enter &ldquo;A1:A10000&rdquo; in the cell range field to select cells A1 through A10000, and (3) paste the formula into all those cells.</p>\n<p>Now we can follow this process in other columns for the other variables, including a column for the &ldquo;total savings&rdquo; formula. To see how many rows made a total savings of $400,000 or more (break-even), use Excel&rsquo;s <a href=\"http://www.techonthenet.com/excel/formulas/countif.php\">countif</a> function. In this case, you should find that about 14% of the scenarios resulted in a savings of less than $400,000 &ndash; a loss.</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/histogram-of-MC-sim.png\" alt=\"\" align=\"right\" />We can also make a histogram (see right) to show how many of the 10,000 scenarios landed in each $100,000 increment (of total savings). This is even more informative, and tells us a great deal about the distribution of risk and benefits we might incur from investing in the new machine. (Download the full spreadsheet for this example <a href=\"http://www.hubbardresearch.com/htma-downloads/\">here</a>.)</p>\n<p>The simulation concept can (and in high-value cases <em>should</em>) be carried beyond this simple MC simulation. The first step is to learn how to use a greater variety of distributions in MC simulations. The second step is to deal with correlated (rather than independent) variables by generating correlated random numbers or by modeling what the variables have in common.</p>\n<p>A more complicated step is to use a <a href=\"http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov simulation</a>, in which the simulated scenario is divided into many time intervals. This is often used to model stock prices, the weather, and complex manufacturing or construction projects. Another more complicated step is to use an <a href=\"http://en.wikipedia.org/wiki/Agent-based_model\">agent-based model</a>, in which independently-acting agents are simulated. This method is often used for traffic simulations, in which each vehicle is modeled as an agent.</p>\n<h3 id=\"step3:pickavariableandcomputethevalueofadditionalinformationforthatvariable\"><br /></h3>\n<h3>Step 3: Pick a variable, and compute the value of additional information for that variable</h3>\n<p>Information can have three kinds of value:</p>\n<ol>\n<li>Information can affect people&rsquo;s behavior (e.g. common knowledge of germs affects sanitation behavior).</li>\n<li>Information can have its own market value (e.g. you can sell a book with useful information).</li>\n<li>Information can reduce uncertainty about important decisions. (This is what we&rsquo;re focusing on here.)</li>\n</ol>\n<p>When you&rsquo;re uncertain about a decision, this means there&rsquo;s a chance you&rsquo;ll make a non-optimal choice. The cost of a &ldquo;wrong&rdquo; decision is the difference between the wrong choice and the choice you would have made with perfect information. But it&rsquo;s too costly to acquire perfect information, so instead we&rsquo;d like to know which decision-relevant variables are the <em>most</em> valuable to measure more precisely, so we can decide which measurements to make.</p>\n<p>Here&rsquo;s a simple example:</p>\n<blockquote>\n<p>Suppose you could make $40 million profit if [an advertisement] works and lose $5 million (the cost of the campaign) if it fails. Then suppose your calibrated experts say they would put a 40% chance of failure on the campaign.</p>\n</blockquote>\n<p>The expected opportunity loss (EOL) for a choice is the probability of the choice being &ldquo;wrong&rdquo; times the cost of it being wrong. So for example the EOL if the campaign is approved is $5M &times; 40% = $2M, and the EOL if the campaign is rejected is $40M &times; 60% = $24M.</p>\n<p>The difference between EOL before and after a measurement is called the &ldquo;expected value of information&rdquo; (EVI).</p>\n<p>In most cases, we want to compute the VoI for a range of values rather than a binary succeed/fail. So let&rsquo;s tweak the advertising campaign example and say that a calibrated marketing expert&rsquo;s 90% CI for sales resulting from the campaign was from 100,000 units to 1 million units. The risk is that we don&rsquo;t sell enough units from this campaign to break even.</p>\n<p>Suppose we profit by $25 per unit sold, so we&rsquo;d have to sell at least 200,000 units from the campaign to break even (on a $5M campaign). To begin, let&rsquo;s calculate the expected value of <em>perfect</em> information (EVPI), which will provide an upper bound on how much we should spend to reduce our uncertainty about how many units will be sold as a result of the campaign. Here&rsquo;s how we compute it:</p>\n<ol>\n<li>Slice the distribution of our variable into thousands of small segments.</li>\n<li>Compute the EOL for each segment. EOL = segment midpoint times segment probability.</li>\n<li>Sum the products from step 2 for all segments.</li>\n</ol>\n<p>Of course, we&rsquo;ll do this with a computer. For the details, see Hubbard&rsquo;s book and the Value of Information spreadsheet from <a href=\"http://www.hubbardresearch.com/htma-downloads/\">his website</a>.</p>\n<p>In this case, the EVPI turns out to be about $337,000. This means that we shouldn&rsquo;t spend more than $337,000 to reduce our uncertainty about how many units will be sold as a result of the campaign.</p>\n<p>And in fact, we should probably spend much less than $337,000, because no measurement we make will give us <em>perfect</em> information. For more details on how to measure the value of <em>imperfect</em> information, see Hubbard&rsquo;s book and these three LessWrong posts: (1) <a href=\"/lw/cih/value_of_information_8_examples/\">VoI: 8 Examples</a>, (2) <a href=\"/lw/85x/value_of_information_four_examples/\">VoI: Four Examples</a>, and (3) <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">5-second level case study: VoI</a>.</p>\n<p>I do, however, want to quote Hubbard&rsquo;s comments about the &ldquo;measurement inversion&rdquo;:</p>\n<blockquote>\n<p>By 1999, I had completed the&hellip; Applied Information Economics analysis on about 20 major [IT] investments&hellip; Each of these business cases had 40 to 80 variables, such as initial development costs, adoption rate, productivity improvement, revenue growth, and so on. For each of these business cases, I ran a macro in Excel that computed the information value for each variable&hellip; [and] I began to see this pattern: * The vast majority of variables had an information value of zero&hellip; * The variables that had high information values were routinely those that the client had never measured&hellip; * The variables that clients [spent] the most time measuring were usually those with a very low (even zero) information value&hellip; &hellip;since then, I&rsquo;ve applied this same test to another 40 projects, and&hellip; [I&rsquo;ve] noticed the same phenomena arise in projects relating to research and development, military logistics, the environment, venture capital, and facilities expansion.</p>\n</blockquote>\n<p>Hubbard calls this the &ldquo;Measurement Inversion&rdquo;:</p>\n<blockquote>\n<p>In a business case, the economic value of measuring a variable is usually inversely proportional to how much measurement attention it usually gets.</p>\n</blockquote>\n<p>Here is one example:</p>\n<blockquote>\n<p>A stark illustration of the Measurement Inversion for IT projects can be seen in a large UK-based insurance client of mine that was an avid user of a software complexity measurement method called &ldquo;function points.&rdquo; This method was popular in the 1980s and 1990s as a basis of estimating the effort for large software development efforts. This organization had done a very good job of tracking initial estimates, function point estimates, and actual effort expended for over 300 IT projects. The estimation required three or four full-time persons as &ldquo;certified&rdquo; function point counters&hellip;</p>\n</blockquote>\n<blockquote>\n<p>But a very interesting pattern arose when I compared the function point estimates to the initial estimates provided by project managers&hellip; The costly, time-intensive function point counting did change the initial estimate but, on average, it was no closer to the actual project effort than the initial effort&hellip; Not only was this the single largest measurement effort in the IT organization, it literally added <em>no</em> value since it didn&rsquo;t reduce uncertainty at all. Certainly, more emphasis on measuring the benefits of the proposed projects &ndash; or almost anything else &ndash; would have been better money spent.</p>\n</blockquote>\n<p>Hence the importance of calculating EVI.</p>\n<h3 id=\"step4:applytherelevantmeasurementinstrumentstothehigh-information-valuevariable\"><br /></h3>\n<h3>Step 4: Apply the relevant measurement instrument(s) to the high-information-value variable</h3>\n<p>If you followed the first three steps, then you&rsquo;ve defined a variable you want to measure in terms of the decision it affects and how you observe it, you&rsquo;ve quantified your uncertainty about it, and you&rsquo;ve calculated the value of gaining additional information about it. Now it&rsquo;s time to reduce your uncertainty about the variable &ndash; that is, to measure it.</p>\n<p>Each scientific discipline has its own specialized measurement methods. Hubbard&rsquo;s book describes measurement methods that are often useful for reducing our uncertainty about the &ldquo;softer&rdquo; topics often encountered by decision-makers in business.</p>\n<h4 id=\"selectingameasurementmethod\"><br /></h4>\n<h4>Selecting a measurement method</h4>\n<p>To figure out which category of measurement methods are appropriate for a particular case, we must ask several questions:</p>\n<ol>\n<li>Decomposition: Which parts of the thing are we uncertain about?</li>\n<li>Secondary research: How has the thing (or its parts) been measured by others?</li>\n<li>Observation: How do the identified observables lend themselves to measurement?</li>\n<li>Measure just enough: How much do we need to measure it?</li>\n<li>Consider the error: How might our observations be misleading?</li>\n</ol>\n<h5 id=\"decomposition\"><br /></h5>\n<h5>Decomposition</h5>\n<p>Sometimes you&rsquo;ll want to start by decomposing an uncertain variable into several parts to identify which observables you can most easily measure. For example, rather than directly estimating the cost of a large construction project, you could break it into parts and estimate the cost of each part of the project.</p>\n<p>In Hubbard&rsquo;s experience, it&rsquo;s often the case that decomposition itself &ndash; even without making any new measurements &ndash; often reduces one&rsquo;s uncertainty about the variable of interest.</p>\n<h5 id=\"secondaryresearch\"><br /></h5>\n<h5>Secondary research</h5>\n<p>Don&rsquo;t reinvent the world. In almost all cases, someone has already invented the measurement tool you need, and you just need to find it. Here are Hubbard&rsquo;s tips on secondary research:</p>\n<ol>\n<li>If you&rsquo;re new to a topic, start with Wikipedia rather than Google. Wikipedia will give you a more organized perspective on the topic at hand.</li>\n<li>Use search terms often associated with quantitative data. E.g. don&rsquo;t just search for &ldquo;software quality&rdquo; or &ldquo;customer perception&rdquo; &ndash; add terms like &ldquo;table,&rdquo; &ldquo;survey,&rdquo; &ldquo;control group,&rdquo; and &ldquo;standard deviation.&rdquo;</li>\n<li>Think of internet research in two levels: general search engines and topic-specific repositories (e.g. the CIA World Fact Book).</li>\n<li>Try multiple search engines.</li>\n<li>If you find marginally related research that doesn&rsquo;t directly address your topic of interest, check the bibliography more relevant reading material.</li>\n</ol>\n<p>I&rsquo;d also recommend my post <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">Scholarship: How to Do It Efficiently</a>.</p>\n<h5 id=\"observation\"><br /></h5>\n<h5>Observation</h5>\n<p>If you&rsquo;re not sure how to measure your target variable&rsquo;s observables, ask these questions:</p>\n<ol>\n<li>Does it leave a trail? Example: longer waits on customer support lines cause customers to hang up and not call back. Maybe you can also find a correlation between customers who hang up after long waits and reduced sales to those customers.</li>\n<li>Can you observe it directly? Maybe you haven&rsquo;t been tracking how many of the customers in your parking lot show an out-of-state license, but you could start. Or at least, you can observe a sample of these data.</li>\n<li>Can you create a way to observe it indirectly? Amazon.com added a gift-wrapping feature in part so they could better track how many books were being purchased as gifts. Another example is when consumers are given coupons so that retailers can see which newspapers their customers read.</li>\n<li>Can the thing be forced to occur under new conditions which allow you to observe it more easily? E.g. you could implement a proposed returned-items policy in some stores but not others and compare the outcomes.</li>\n</ol>\n<h5 id=\"measurejustenough\"><br /></h5>\n<h5>Measure just enough</h5>\n<p>Because initial measurements often tell you quite a lot, and also change the value of continued measurement, Hubbard often aims for spending 10% of the EVPI on a measurement, and sometimes as little as 2% (especially for very large projects).</p>\n<h5 id=\"considertheerror\"><br /></h5>\n<h5>Consider the error</h5>\n<p>It&rsquo;s important to be conscious of some common ways in which measurements can mislead.</p>\n<p>Scientists distinguish two types of measurement error: systemic and random. Random errors are random variations from one observation to the next. They can&rsquo;t be individually predicted, but they fall into patterns that can be accounted for with the laws of probability. Systemic errors, in contrast, are consistent. For example, the sales staff may routinely overestimate the next quarter&rsquo;s revenue by 50% (on average).</p>\n<p>We must also distinguish precision and accuracy. A &ldquo;precise&rdquo; measurement tool has low random error. E.g. if a bathroom scale gives the exact same displayed weight every time we set a particular book on it, then the scale has high precision. An &ldquo;accurate&rdquo; measurement tool has low systemic error. The bathroom scale, while precise, might be inaccurate if the weight displayed is systemically biased in one direction &ndash; say, eight pounds too heavy. A measurement tool can also have low precision but good accuracy, if it gives inconsistent measurements but they average to the true value.</p>\n<p>Random error tends to be easier to handle. Consider this example:</p>\n<blockquote>\n<p>For example, to determine how much time sales reps spend in meetings with clients versus other administrative tasks, they might choose a complete review of all time sheets&hellip; [But] if a complete review of 5,000 time sheets&hellip; tells us that sales reps spend 34% of their time in direct communication with customers, we still don&rsquo;t know how far from the truth it might be. Still, this &ldquo;exact&rdquo; number seems reassuring to many managers. Now, suppose a sample of direct observations of randomly chosen sales reps at random points in time finds that sales reps were in client meetings or on client phone calls only 13 out of 100 of those instances. (We can compute this without interrupting a meeting by asking as soon as the rep is available.) As we will see [later], in the latter case, we can statistically compute a 90% CI to be 7.5% to 18.5%. Even though this random sampling approach gives us only a range, we should prefer its findings to the census audit of time sheets. The census&hellip; gives us an exact number, but we have no way to know by how much and in which direction the time sheets err.</p>\n</blockquote>\n<p>Systemic error is also called a &ldquo;bias.&rdquo; Based on his experience, Hubbard suspects the three most important to avoid are:</p>\n<ol>\n<li>Confirmation bias: people see what they want to see.</li>\n<li>Selection bias: your sample might not be representative of the group you&rsquo;re trying to measure.</li>\n<li>Observer bias: the very act of observation can affect what you observe. E.g. in one study, researchers found that worker productivity improved no matter <em>what</em> they changed about the workplace. The workers seem to have been responding merely to the <em>fact</em> that they were being observed in <em>some</em> way.</li>\n</ol>\n<h5 id=\"chooseanddesignthemeasurementinstrument\"><br /></h5>\n<h5>Choose and design the measurement instrument</h5>\n<p>After following the above steps, Hubbard writes, &ldquo;the measurement instrument should be almost completely formed in your mind.&rdquo; But if you still can&rsquo;t come up with a way to measure the target variable, here are some additional tips:</p>\n<ol>\n<li><em>Work through the consequences</em>. If the value is surprisingly high, or surprisingly low, what would you expect to see?</li>\n<li><em>Be iterative</em>. Start with just a few observations, and then recalculate the information value.</li>\n<li><em>Consider multiple approaches</em>. Your first measurement tool may not work well. Try others.</li>\n<li><em>What&rsquo;s the really simple question that makes the rest of the measurement moot?</em> First see if you can detect <em>any</em> change in research quality before trying to measure it more comprehensively.</li>\n</ol>\n<h4 id=\"samplingreality\"><br /></h4>\n<h4>Sampling reality</h4>\n<p>In most cases, we&rsquo;ll estimate the values in a population by measuring the values in a small sample from that population. And for reasons discussed in chapter 7, a very small sample can often offer large reductions in uncertainty.</p>\n<p>There are a variety of tools we can use to build our estimates from small samples, and which one we should use often depends on how outliers are distributed in the population. In some cases, outliers are very close to the mean, and thus our estimate of the mean can converge quickly on the true mean as we look at new samples. In other cases, outliers can be several orders of magnitude away from the mean, and our estimate converges very slowly or not at all. Here are some examples:</p>\n<ul>\n<li>Very quick convergence, only 1&ndash;2 samples needed: cholesterol level of your blood, purity of public water supply, weight of jelly beans.</li>\n<li>Usually quickly convergence, 5&ndash;30 samples needed: Percentage of customers who like the new product, failure loads of bricks, age of your customers, how many movies people see in a year.</li>\n<li>Potentially slow convergence: Software project cost overruns, factory downtime due to an accident.</li>\n<li>Maybe non-convergent: Market value of corporations, individual levels of income, casualties of wars, size of volcanic eruptions.</li>\n</ul>\n<p>Below, I survey just a few of the many sampling methods Hubbard covers in his book.</p>\n<h5 id=\"mathlessestimation\"><br /></h5>\n<h5>Mathless estimation</h5>\n<p>When working with a quickly converging phenomenon and a symmetric distribution (uniform, normal, camel-back, or bow-tie) for the population, you can use the <a href=\"http://en.wikipedia.org/wiki/T-statistic\">t-statistic</a> to develop a 90% CI even when working with very small samples. (See the book for instructions.)</p>\n<p>Or, even easier, make use of the <em>Rule of FIve</em>: &ldquo;There is a 93.75% chance that the median of a population is between the smallest and largest values in any random sample of five from that population.&rdquo;</p>\n<p>The Rule of Five has another advantage over the t-statistic: it works for any distribution of values in the population, including ones with slow convergence or no convergence at all! It can do this because it gives us a confidence interval for the <em>median</em> rather than the <em>mean</em>, and it&rsquo;s the mean that is far more affected by outliers.</p>\n<p>Hubbard calls this a &ldquo;mathless&rdquo; estimation technique because it doesn&rsquo;t require us to take square roots or calculate standard deviation or anything like that. Moreover, this mathless technique extends beyond the Rule of Five: If we sample 8 items, there is a 99.2% chance that the median of the population falls within the largest and smallest values. If we take the <em>2nd</em> largest and smallest values (out of 8 total values), we get something close to a 90% CI for the median. Hubbard generalizes the tool with this handy reference table:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/mathless-90-percent-CI-for-median.jpg\" alt=\"\" align=\"center\" /></p>\n<p>And if the distribution is symmetrical, then the mathless table gives us a 90% CI for the mean as well as for the median.</p>\n<h5 id=\"catch-recatch\"><br /></h5>\n<h5>Catch-recatch</h5>\n<p>How does a biologist measure the number of fish in a lake? SHe catches and tags a sample of fish &ndash; say, 1000 of them &ndash; and then releases them. After the fish have had time to spread amongst the rest of the population, she&rsquo;ll catch another sample of fish. Suppose she caught 1000 fish again, and 50 of them were tagged. This would mean 5% of the fish were tagged, and thus that were about 20,000 fish in the entire lake. (See Hubbard&rsquo;s book for the details on how to calculate the 90% CI.)</p>\n<h5 id=\"spotsampling\"><br /></h5>\n<h5>Spot sampling</h5>\n<p>The fish example was a special case of a common problem: population proportion sampling. Often, we want to know what proportion of a population has a particular trait. How many registered voters in California are Democrats? What percentage of your customers prefer a new product design over the old one?</p>\n<p>Hubbard&rsquo;s book discusses how to solve the general problem, but for now let&rsquo;s just consider another special case: spot sampling.</p>\n<p>In spot sampling, you take random snapshots of things rather than tracking them constantly. What proportion of their work hours do employees spend on Facebook? To answer this, you &ldquo;randomly sample people through the day to see what they were doing <em>at that moment</em>. If you find that in 12 instances out of 100 random samples&rdquo; employees were on Facebook, you can guess they spend about 12% of their time on Facebook (the 90% CI is 8% to 18%).</p>\n<h5 id=\"clusteredsampling\"><br /></h5>\n<h5>Clustered sampling</h5>\n<p>Hubbard writes:</p>\n<blockquote>\n<p>&ldquo;Clustered sampling&rdquo; is defined as taking a random sample of groups, then conducting a census or a more concentrated sampling within the group. For example, if you want to see what share of households has satellite dishes&hellip; it might be cost effective to randomly choose several city blocks, then conduct a complete census of everything in a block. (Zigzagging across town to individually selected households would be time consuming.) In such cases, we can&rsquo;t really consider the number of [households] in the groups&hellip; to be the number of random samples. Within a block, households may be very similar&hellip; [and therefore] it might be necessary to treat the effective number of random samples as the number of blocks&hellip;</p>\n</blockquote>\n<h5 id=\"measuretothethreshold\"><br /></h5>\n<h5>Measure to the threshold</h5>\n<p>For many decisions, one decision is required if a value is above some threshold, and another decision is required if that value is below the threshold. For such decisions, you don&rsquo;t care as much about a measurement that reduces uncertainty in general as you do about a measurement that tells you which decision to make based on the threshold. Hubbard gives an example:</p>\n<blockquote>\n<p>Suppose you needed to measure the average amount of time spent by employees in meetings that could be conducted remotely&hellip; If a meeting is among staff members who communicate regularly and for a relatively routine topic, but someone has to travel to make the meeting, you probably can conduct it remotely. You start out with your calibrated estimate that the median employee spends between 3% to 15% traveling to meetings that could be conducted remotely. You determine that if this percentage is actually over 7%, you should make a significant investment in tele meetings. The [EVPI] calculation shows that it is worth no more than $15,000 to study this. According to our rule of thumb for measurement costs, we might try to spend about $1,500&hellip;</p>\n</blockquote>\n<blockquote>\n<p>Let&rsquo;s say you sampled 10 employees and&hellip; you find that only 1 spends less time in these activities than the 7% threshold. Given this information, what is the chance that the median time spent in such activities is actually below 7%, in which case the investment would not be justified? One &ldquo;common sense&rdquo; answer is 1/10, or 10%. Actually&hellip; the real chance is much smaller.</p>\n</blockquote>\n<p>Hubbard shows how to derive the real chance in his book. The key point is that &ldquo;the uncertainty about the threshold can fall much faster than the uncertainty about the quantity in general.&rdquo;</p>\n<h5 id=\"regressionmodeling\"><br /></h5>\n<h5>Regression modeling</h5>\n<p>What if you want to figure out the cause of something that has many possible causes? One method is to perform a <em>controlled experiment</em>, and compare the outcomes of a test group to a control group. Hubbard discusses this in his book (and yes, he&rsquo;s a Bayesian, and a skeptic of p-value hypothesis testing). For this summary, I&rsquo;ll instead mention another method for isolating causes: regression modeling. Hubbard explains:</p>\n<blockquote>\n<p>If we use regression modeling with historical data, we may not need to conduct a controlled experiment. Perhaps, for example, it is difficult to tie an IT project to an increase in sales, but we might have lots of data about how something <em>else</em> affects sales, such as faster time to market of new products. If we know that faster time to market is possible by automating certain tasks, that this IT investment eliminates certain tasks, and those tasks are on the critical path in the time-to-market, we can make the connection.</p>\n</blockquote>\n<p>Hubbard&rsquo;s book explains the basics of linear regressions, and of course gives the caveat that correlation does not imply causation. But, he writes, &ldquo;you should conclude that one thing causes another only if you have some <em>other</em> good reason besides the correlation itself to suspect a cause-and-effect relationship.&rdquo;</p>\n<h4 id=\"bayes\"><br /></h4>\n<h4>Bayes</h4>\n<p>Hubbard&rsquo;s 10th chapter opens with a tutorial on Bayes&rsquo; Theorem. For an online tutorial, see <a href=\"http://yudkowsky.net/rational/bayes\">here</a>.</p>\n<p>Hubbard then zooms out to a big-picture view of measurement, and recommends the &ldquo;instinctive Bayesian approach&rdquo;:</p>\n<ol>\n<li>Start with your calibrated estimate.</li>\n<li>Gather additional information (polling, reading other studies, etc.)</li>\n<li>Update your calibrated estimate subjectively, without doing any additional math.</li>\n</ol>\n<p>Hubbard says a few things in support of this approach. First, he points to some studies (e.g. <a href=\"http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476620\">El-Gamal &amp; Grether (1995)</a>) showing that people often reason in roughly-Bayesian ways. Next, he says that in his experience, people become better intuitive Bayesians when they (1) are made aware of the <a href=\"http://en.wikipedia.org/wiki/Base_rate_fallacy\">base rate fallacy</a>, and when they (2) are better calibrated.</p>\n<p>Hubbard says that once these conditions are met,</p>\n<blockquote>\n<p>[then] humans seem to be mostly logical when incorporating new information into their estimates along with the old information. This fact is extremely useful because a human can consider qualitative information that does not fit in standard statistics. For example, if you were giving a forecast for how a new policy might change &ldquo;public image&rdquo; &ndash; measured in part by a reduction in customer complaints, increased revenue, and the like &ndash; a calibrated expert should be able to update current knowledge with &ldquo;qualitative&rdquo; information about how the policy worked for other companies, feedback from focus groups, and similar details. Even with sampling information, the calibrated estimator &ndash; who has a Bayesian instinct &ndash; can consider qualitative information on samples that most textbooks don&rsquo;t cover.</p>\n</blockquote>\n<p>He also offers a chart showing how a pure Bayesian estimator compares to other estimators:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/confidence-versus-information-emphasis.jpg\" alt=\"\" align=\"center\" /></p>\n<p>Also, Bayes&rsquo; Theorem allows us to perform a &ldquo;Bayesian inversion&rdquo;:</p>\n<blockquote>\n<p>Given a particular observation, it may seem more obvious to frame a measurement by asking the question &ldquo;What can I conclude from this observation?&rdquo; or, in probabilistic terms, &ldquo;What is the probability X is true, given my observation?&rdquo; But Bayes showed us that we could, instead, start with the question, &ldquo;What is the probability of this observation if X were true?&rdquo;</p>\n</blockquote>\n<blockquote>\n<p>The second form of the question is useful because the answer is often more straightforward and it leads to the answer to the other question. It also forces us to think about the likelihood of different observations given a particular hypothesis and what that means for interpreting an observation.</p>\n</blockquote>\n<blockquote>\n<p>[For example] if, hypothetically, we know that only 20% of the population will continue to shop at our store, then we can determine the chance [that] exactly 15 out of 20 would say so&hellip; [The details are explained in the book.] Then we can invert the problem with Bayes&rsquo; theorem to compute the chance that only 20% of the population will continue to shop there given [that] 15 out of 20 said so in a random sample. We would find that chance to be very nearly zero&hellip;</p>\n</blockquote>\n<h4 id=\"othermethods\"><br /></h4>\n<h4>Other methods</h4>\n<p>Other chapters discuss other measurement methods, for example prediction markets, \u0010Rasch models,\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010 methods for measuring preferences and happiness, methods for improving the subjective judgments of experts, and many others. \u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010</p>\n<h3 id=\"step5:makeadecisionandactonit\"><br /></h3>\n<h3>Step 5: Make a decision and act on it</h3>\n<p>The last step will make more sense if we first &ldquo;bring the pieces together.&rdquo; Hubbard now organizes his consulting work with a firm into 3 phases, so let&rsquo;s review what we&rsquo;ve learned in the context of his 3 phases.</p>\n<h4 id=\"phase0:projectpreparation\"><br /></h4>\n<h4>Phase 0: Project Preparation</h4>\n<ul>\n<li><em>Initial research</em>: Interviews and secondary research to get familiar on the nature of the decision problem.</li>\n<li><em>Expert identification</em>: Usually 4&ndash;5 experts who provide estimates.</li>\n</ul>\n<h4 id=\"phase1:decisionmodeling\"><br /></h4>\n<h4>Phase 1: Decision Modeling</h4>\n<ul>\n<li><em>Decision problem definition</em>: Experts define the problem they&rsquo;re trying to analyze.</li>\n<li><em>Decision model detail</em>: Using an Excel spreadsheet, the AIE analyst elicits from the experts all the factors that matter for the decision being analyzed: costs and benefits, ROI, etc.</li>\n<li><em>Initial calibrated estimates</em>: First, the experts undergo calibration training. Then, they fill in the values (as 90% CIs or other probability distributions) for the variables in the decision model.</li>\n</ul>\n<h4 id=\"phase2:optimalmeasurements\"><br /></h4>\n<h4>Phase 2: Optimal measurements</h4>\n<ul>\n<li><em>Value of information analysis</em>: Using Excel macros, the AIE analyst runs a value of information analysis on every variable in the model.</li>\n<li><em>Preliminary measurement method designs</em>: Focusing on the few variables with highest information value, the AIE analyst chooses measurement methods that should reduce uncertainty.</li>\n<li><em>Measurement methods</em>: Decomposition, random sampling, Bayesian inversion, controlled experiments, and other methods are used (as appropriate) to reduce the uncertainty of the high-VoI variables.</li>\n<li><em>Updated decision model</em>: The AIE analyst updates the decision model based on the results of the measurements.</li>\n<li><em>Final value of information analysis</em>: The AIE analyst runs a VoI analysis on each variable again. As long as this analysis shows information value much greater than the cost of measurement for some variables, measurement and VoI analysis continues in multiple iterations. Usually, though, only one or two iterations are needed before the VoI analysis shows that no further measurements are justified.</li>\n</ul>\n<h4 id=\"phase3:decisionoptimizationandthefinalrecommendation\"><br /></h4>\n<h4>Phase 3: Decision optimization and the final recommendation</h4>\n<ul>\n<li><em>Completed risk/return analysis</em>: A final MC simulation shows the likelihood of possible outcomes.</li>\n<li><em>Identified metrics procedures</em>: Procedures are put in place to measure some variables (e.g. about project progress or external factors) continually.</li>\n<li><em>Decision optimization</em>: The final business decision recommendation is made (this is rarely a simple &ldquo;yes/no&rdquo; answer).</li>\n</ul>\n<h4 id=\"finalthoughts\"><br /></h4>\n<h4>Final thoughts</h4>\n<p>Hubbard&rsquo;s book includes two case studies in which Hubbard describes how he led two fairly different clients (the EPA and U.S. Marine Corps) through each phase of the AIE process. Then, he closes the book with the following summary:</p>\n<ul>\n<li>If it&rsquo;s really that important, it&rsquo;s something you can define. If it&rsquo;s something you think exists at all, it&rsquo;s something you&rsquo;ve already observed somehow.</li>\n<li>If it&rsquo;s something important and something uncertain, you have a cost of being wrong and a chance of being wrong.</li>\n<li>You can quantify your current uncertainty with calibrated estimates.</li>\n<li>You can compute the value of additional information by knowing the &ldquo;threshold&rdquo; of the measurement where it begins to make a difference compared to your existing uncertainty.</li>\n<li>Once you know what it&rsquo;s worth to measure something, you can put the measurement effort in context and decide on the effort it should take.</li>\n<li>Knowing just a few methods for random sampling, controlled experiments, or even merely improving on the judgments of experts can lead to a significant reduction in uncertainty.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 7, "4Kcm4etxAJjmeDkHP": 2, "dPPATLhRmhdJtJM2t": 2, "KoXbd2HmbdRfqLngk": 10, "SJFsFfFhE6m2ThAYJ": 2, "DbMQGrxbhLxtNkmca": 2, "LhX3F2SvGDarZCuh6": 2, "fkABsGCJZ6y9qConW": 2, "5f5c37ee1b5cdee568cfb294": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ybYBCK9D7MZCcdArB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 100, "extendedScore": null, "score": 0.00024, "legacy": true, "legacyId": "23639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 101, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/\"><img style=\"padding: 30px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/how-to-measure-anything.jpeg\" alt=\"\" align=\"right\"></a>Douglas Hubbard\u2019s <em><a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/\">How to Measure Anything</a></em> is one of my favorite how-to books. I hope this summary inspires you to buy the book; it\u2019s worth it.</p>\n<p>The book opens:</p>\n<blockquote>\n<p>Anything can be measured. If a thing can be observed in any way at all, it lends itself to some type of measurement method. No matter how \u201cfuzzy\u201d the measurement is, it\u2019s still a measurement if it tells you more than you knew before. And those very things most likely to be seen as immeasurable are, virtually always, solved by relatively simple measurement methods.</p>\n</blockquote>\n<p>The sciences have many established measurement methods, so Hubbard\u2019s book focuses on the measurement of \u201cbusiness intangibles\u201d that are important for decision-making but tricky to measure: things like management effectiveness, the \u201cflexibility\u201d to create new products, the risk of bankruptcy, and public image.</p>\n<p>&nbsp;</p>\n<h3 id=\"Basic_Ideas\">Basic Ideas</h3>\n<p>A <em>measurement</em> is an observation that quantitatively reduces uncertainty. Measurements might not yield precise, certain judgments, but they <em>do</em> reduce your uncertainty.</p>\n<p>To be measured, the <em>object of measurement</em> must be described clearly, in terms of observables. A good way to clarify a vague object of measurement like \u201cIT security\u201d is to ask \u201cWhat is IT security, and why do you care?\u201d Such probing can reveal that \u201cIT security\u201d means things like a reduction in unauthorized intrusions and malware attacks, which the IT department cares about because these things result in lost productivity, fraud losses, and legal liabilities.</p>\n<p><em>Uncertainty</em> is the lack of certainty: the true outcome/state/value is not known.</p>\n<p><em>Risk</em> is a state of uncertainty in which some of the possibilities involve a loss.</p>\n<p>Much pessimism about measurement comes from a lack of experience making measurements. Hubbard, who is <em>far</em> more experienced with measurement than his readers, says:</p>\n<ol>\n<li>Your problem is not as unique as you think.</li>\n<li>You have more data than you think.</li>\n<li>You need less data than you think.</li>\n<li>An adequate amount of new data is more accessible than you think.</li>\n</ol>\n<h3 id=\"appliedinformationeconomics\"><br></h3>\n<h3 id=\"Applied_Information_Economics\">Applied Information Economics</h3>\n<p>Hubbard calls his method \u201cApplied Information Economics\u201d (AIE). It consists of 5 steps:</p>\n<ol>\n<li>Define a decision problem and the relevant variables. (Start with the decision you need to make, then figure out which variables would make your decision easier if you had better estimates of their values.)</li>\n<li>Determine what you know. (Quantify your uncertainty about those variables in terms of ranges and probabilities.)</li>\n<li>Pick a variable, and compute the value of additional information for that variable. (Repeat until you find a variable with reasonably high information value. If no remaining variables have enough information value to justify the cost of measuring them, skip to step 5.)</li>\n<li>Apply the relevant measurement instrument(s) to the high-information-value variable. (Then go back to step 3.)</li>\n<li>Make a decision and act on it. (When you\u2019ve done as much uncertainty reduction as is economically justified, it\u2019s time to act!)</li>\n</ol>\n<p>These steps are elaborated below.</p>\n<h3 id=\"step1:defineadecisionproblemandtherelevantvariables\"><a id=\"more\"></a><br></h3>\n<h3 id=\"Step_1__Define_a_decision_problem_and_the_relevant_variables\">Step 1: Define a decision problem and the relevant variables</h3>\n<p>Hubbard illustrates this step by telling the story of how he helped the Department of Veterans Affairs (VA) with a measurement problem.</p>\n<p>The VA was considering seven proposed IT security projects. They wanted to know \u201cwhich\u2026 of the proposed investments were justified and, after they were implemented, whether improvements in security justified further investment\u2026\u201d Hubbard asked his standard questions: \u201cWhat do you mean by \u2018IT security\u2019? Why does it matter to you? What are you observing when you observe improved IT security?\u201d</p>\n<p>It became clear that <em>nobody</em> at the VA had thought about the details of what \u201cIT security\u201d meant to them. But after Hubbard\u2019s probing, it became clear that by \u201cIT security\u201d they meant a reduction in the frequency and severity of some undesirable events: agency-wide virus attacks, unauthorized system access (external or internal),unauthorized physical access, and disasters affecting the IT infrastructure (fire, flood, etc.) And each undesirable event was on the list because of specific costs associated with it: productivity losses from virus attacks, legal liability from unauthorized system access, etc.</p>\n<p>Now that the VA knew what they meant by \u201cIT security,\u201d they could measure specific variables, such as the number of virus attacks per year.</p>\n<h3 id=\"step2:determinewhatyouknow\"><br></h3>\n<h3 id=\"Step_2__Determine_what_you_know\">Step 2: Determine what you know</h3>\n<h4 id=\"Uncertainty_and_calibration\">Uncertainty and calibration</h4>\n<p>The next step is to determine your level of uncertainty about the variables you want to measure. To do this, you can express a \u201cconfidence interval\u201d (CI). A 90% CI is a range of values that is 90% likely to contain the correct value. For example, the security experts at the VA were 90% confident that each agency-wide virus attack would affect between 25,000 and 65,000 people.</p>\n<p>Unfortunately, few people are well-calibrated estimators. For example in some studies, the true value lay in subjects\u2019 90% CIs only 50% of the time! These subjects were overconfident. For a well-calibrated estimator, the true value will lie in her 90% CI roughly 90% of the time.</p>\n<p>Luckily, \u201cassessing uncertainty is a general skill that can be taught with a measurable improvement.\u201d</p>\n<p>Hubbard uses several methods to calibrate each client\u2019s value estimators, for example the security experts at the VA who needed to estimate the frequency of security breaches and their likely costs.</p>\n<p>His first technique is the <em>equivalent bet test</em>. Suppose you\u2019re asked to give a 90% CI for the year in which Newton published the universal laws of gravitation, and you can win $1,000 in one of two ways:</p>\n<ol>\n<li>You win $1,000 if the true year of publication falls within your 90% CI. Otherwise, you win nothing.</li>\n<li>You spin a dial divided into two \u201cpie slices,\u201d one covering 10% of the dial, and the other covering 90%. If the dial lands on the small slice, you win nothing. If it lands on the big slice, you win $1,000.</li>\n</ol>\n<p>If you find yourself preferring option #2, then you must think spinning the dial has a higher chance of winning you $1,000 than option #1. That suggest your stated 90% CI isn\u2019t really your 90% CI. Maybe it\u2019s your 65% CI or your 80% CI instead. By preferring option #2, your brain is trying to tell you that your originally stated 90% CI is overconfident.</p>\n<p>If instead you find yourself preferring option #1, then you must think there is <em>more</em> than a 90% chance your stated 90% CI contains the true value. By preferring option #1, your brain is trying to tell you that your original 90% CI is under confident.</p>\n<p>To make a better estimate, adjust your 90% CI until option #1 and option #2 seem equally good to you. Research suggests that even <em>pretending</em> to bet money in this way will improve your calibration.</p>\n<p>Hubbard\u2019s second method for improving calibration is simply <em>repetition and feedback</em>. Make lots of estimates and then see how well you did. For this, play CFAR\u2019s <a href=\"http://acritch.com/credence-game/\">Calibration Game</a>.</p>\n<p>Hubbard also asks people to identify reasons why a particular estimate might be right, and why it might be wrong.</p>\n<p>He also asks people to look more closely at each bound (upper and lower) on their estimated range. A 90% CI \u201cmeans there is a 5% chance the true value could be greater than the upper bound, and a 5% chance it could be less than the lower bound. This means the estimators must be 95% sure that the true value is less than the upper bound. If they are not that certain, they should increase the upper bound\u2026 A similar test is applied to the lower bound.\u201d</p>\n<h4 id=\"simulations\"><br></h4>\n<h4 id=\"Simulations\">Simulations</h4>\n<p>Once you determine what you know about the uncertainties involved, how can you use that information to determine what you know about the <em>risks</em> involved? Hubbard summarizes:</p>\n<blockquote>\n<p>\u2026all risk in any project\u2026 can be expressed by one method: the ranges of uncertainty on the costs and benefits, and probabilities on events that might affect them.</p>\n</blockquote>\n<p>The simplest tool for measuring such risks accurately is the Monte Carlo (MC) simulation, which can be run by Excel and many other programs. To illustrate this tool, suppose you are wondering whether to lease a new machine for one step in your manufacturing process.</p>\n<blockquote>\n<p>The one-year lease [for the machine] is $400,000 with no option for early cancellation. So if you aren\u2019t breaking even, you are still stuck with it for the rest of the year. You are considering signing the contract because you think the more advanced device will save some labor and raw materials and because you think the maintenance cost will be lower than the existing process.</p>\n</blockquote>\n<p>Your pre-calibrated estimators give their 90% CIs for the following variables:</p>\n<ul>\n<li>Maintenance savings (MS): $10 to $20 per unit</li>\n<li>Labor savings (LS): -$2 to $8 per unit</li>\n<li>Raw materials savings (RMS): $3 to $9 per unit</li>\n<li>Production level (PL): 15,000 to 35,000 units per year</li>\n</ul>\n<p>Thus, your annual savings will equal (MS + LS + RMS) \u00d7 PL.</p>\n<p>When measuring risk, we don\u2019t just want to know the \u201caverage\u201d risk or benefit. We want to know the probability of a huge loss, the probability of a small loss, the probability of a huge savings, and so on. That\u2019s what Monte Carlo can tell us.</p>\n<p>An MC simulation uses a computer to randomly generate thousands of possible values for each variable, based on the ranges we\u2019ve estimated. The computer then calculates the outcome (in this case, the annual savings) for each generated combination of values, and we\u2019re able to see how often different kinds of outcomes occur.</p>\n<p>To run an MC simulation we need not just the 90% CI for each variable but also the <em>shape</em> of each distribution. In many cases, the <a href=\"http://en.wikipedia.org/wiki/Normal_distribution\">normal distribution</a> will work just fine, and we\u2019ll use it for all the variables in this simplified illustration. (Hubbard\u2019s book shows you how to work with other distributions).</p>\n<p>To make an MC simulation of a normally distributed variable in Excel, we use this formula:</p>\n<blockquote>\n<p>=norminv(rand(), mean, standard deviation)</p>\n</blockquote>\n<p>So the formula for the maintenance savings variable should be:</p>\n<blockquote>\n<p>=norminv(rand(), 15, (20\u201310)/3.29)</p>\n</blockquote>\n<p>Suppose you enter this formula on cell A1 in Excel. To generate (say) 10,000 values for the maintenance savings value, just (1) copy the contents of cell A1, (2) enter \u201cA1:A10000\u201d in the cell range field to select cells A1 through A10000, and (3) paste the formula into all those cells.</p>\n<p>Now we can follow this process in other columns for the other variables, including a column for the \u201ctotal savings\u201d formula. To see how many rows made a total savings of $400,000 or more (break-even), use Excel\u2019s <a href=\"http://www.techonthenet.com/excel/formulas/countif.php\">countif</a> function. In this case, you should find that about 14% of the scenarios resulted in a savings of less than $400,000 \u2013 a loss.</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/histogram-of-MC-sim.png\" alt=\"\" align=\"right\">We can also make a histogram (see right) to show how many of the 10,000 scenarios landed in each $100,000 increment (of total savings). This is even more informative, and tells us a great deal about the distribution of risk and benefits we might incur from investing in the new machine. (Download the full spreadsheet for this example <a href=\"http://www.hubbardresearch.com/htma-downloads/\">here</a>.)</p>\n<p>The simulation concept can (and in high-value cases <em>should</em>) be carried beyond this simple MC simulation. The first step is to learn how to use a greater variety of distributions in MC simulations. The second step is to deal with correlated (rather than independent) variables by generating correlated random numbers or by modeling what the variables have in common.</p>\n<p>A more complicated step is to use a <a href=\"http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov simulation</a>, in which the simulated scenario is divided into many time intervals. This is often used to model stock prices, the weather, and complex manufacturing or construction projects. Another more complicated step is to use an <a href=\"http://en.wikipedia.org/wiki/Agent-based_model\">agent-based model</a>, in which independently-acting agents are simulated. This method is often used for traffic simulations, in which each vehicle is modeled as an agent.</p>\n<h3 id=\"step3:pickavariableandcomputethevalueofadditionalinformationforthatvariable\"><br></h3>\n<h3 id=\"Step_3__Pick_a_variable__and_compute_the_value_of_additional_information_for_that_variable\">Step 3: Pick a variable, and compute the value of additional information for that variable</h3>\n<p>Information can have three kinds of value:</p>\n<ol>\n<li>Information can affect people\u2019s behavior (e.g. common knowledge of germs affects sanitation behavior).</li>\n<li>Information can have its own market value (e.g. you can sell a book with useful information).</li>\n<li>Information can reduce uncertainty about important decisions. (This is what we\u2019re focusing on here.)</li>\n</ol>\n<p>When you\u2019re uncertain about a decision, this means there\u2019s a chance you\u2019ll make a non-optimal choice. The cost of a \u201cwrong\u201d decision is the difference between the wrong choice and the choice you would have made with perfect information. But it\u2019s too costly to acquire perfect information, so instead we\u2019d like to know which decision-relevant variables are the <em>most</em> valuable to measure more precisely, so we can decide which measurements to make.</p>\n<p>Here\u2019s a simple example:</p>\n<blockquote>\n<p>Suppose you could make $40 million profit if [an advertisement] works and lose $5 million (the cost of the campaign) if it fails. Then suppose your calibrated experts say they would put a 40% chance of failure on the campaign.</p>\n</blockquote>\n<p>The expected opportunity loss (EOL) for a choice is the probability of the choice being \u201cwrong\u201d times the cost of it being wrong. So for example the EOL if the campaign is approved is $5M \u00d7 40% = $2M, and the EOL if the campaign is rejected is $40M \u00d7 60% = $24M.</p>\n<p>The difference between EOL before and after a measurement is called the \u201cexpected value of information\u201d (EVI).</p>\n<p>In most cases, we want to compute the VoI for a range of values rather than a binary succeed/fail. So let\u2019s tweak the advertising campaign example and say that a calibrated marketing expert\u2019s 90% CI for sales resulting from the campaign was from 100,000 units to 1 million units. The risk is that we don\u2019t sell enough units from this campaign to break even.</p>\n<p>Suppose we profit by $25 per unit sold, so we\u2019d have to sell at least 200,000 units from the campaign to break even (on a $5M campaign). To begin, let\u2019s calculate the expected value of <em>perfect</em> information (EVPI), which will provide an upper bound on how much we should spend to reduce our uncertainty about how many units will be sold as a result of the campaign. Here\u2019s how we compute it:</p>\n<ol>\n<li>Slice the distribution of our variable into thousands of small segments.</li>\n<li>Compute the EOL for each segment. EOL = segment midpoint times segment probability.</li>\n<li>Sum the products from step 2 for all segments.</li>\n</ol>\n<p>Of course, we\u2019ll do this with a computer. For the details, see Hubbard\u2019s book and the Value of Information spreadsheet from <a href=\"http://www.hubbardresearch.com/htma-downloads/\">his website</a>.</p>\n<p>In this case, the EVPI turns out to be about $337,000. This means that we shouldn\u2019t spend more than $337,000 to reduce our uncertainty about how many units will be sold as a result of the campaign.</p>\n<p>And in fact, we should probably spend much less than $337,000, because no measurement we make will give us <em>perfect</em> information. For more details on how to measure the value of <em>imperfect</em> information, see Hubbard\u2019s book and these three LessWrong posts: (1) <a href=\"/lw/cih/value_of_information_8_examples/\">VoI: 8 Examples</a>, (2) <a href=\"/lw/85x/value_of_information_four_examples/\">VoI: Four Examples</a>, and (3) <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">5-second level case study: VoI</a>.</p>\n<p>I do, however, want to quote Hubbard\u2019s comments about the \u201cmeasurement inversion\u201d:</p>\n<blockquote>\n<p>By 1999, I had completed the\u2026 Applied Information Economics analysis on about 20 major [IT] investments\u2026 Each of these business cases had 40 to 80 variables, such as initial development costs, adoption rate, productivity improvement, revenue growth, and so on. For each of these business cases, I ran a macro in Excel that computed the information value for each variable\u2026 [and] I began to see this pattern: * The vast majority of variables had an information value of zero\u2026 * The variables that had high information values were routinely those that the client had never measured\u2026 * The variables that clients [spent] the most time measuring were usually those with a very low (even zero) information value\u2026 \u2026since then, I\u2019ve applied this same test to another 40 projects, and\u2026 [I\u2019ve] noticed the same phenomena arise in projects relating to research and development, military logistics, the environment, venture capital, and facilities expansion.</p>\n</blockquote>\n<p>Hubbard calls this the \u201cMeasurement Inversion\u201d:</p>\n<blockquote>\n<p>In a business case, the economic value of measuring a variable is usually inversely proportional to how much measurement attention it usually gets.</p>\n</blockquote>\n<p>Here is one example:</p>\n<blockquote>\n<p>A stark illustration of the Measurement Inversion for IT projects can be seen in a large UK-based insurance client of mine that was an avid user of a software complexity measurement method called \u201cfunction points.\u201d This method was popular in the 1980s and 1990s as a basis of estimating the effort for large software development efforts. This organization had done a very good job of tracking initial estimates, function point estimates, and actual effort expended for over 300 IT projects. The estimation required three or four full-time persons as \u201ccertified\u201d function point counters\u2026</p>\n</blockquote>\n<blockquote>\n<p>But a very interesting pattern arose when I compared the function point estimates to the initial estimates provided by project managers\u2026 The costly, time-intensive function point counting did change the initial estimate but, on average, it was no closer to the actual project effort than the initial effort\u2026 Not only was this the single largest measurement effort in the IT organization, it literally added <em>no</em> value since it didn\u2019t reduce uncertainty at all. Certainly, more emphasis on measuring the benefits of the proposed projects \u2013 or almost anything else \u2013 would have been better money spent.</p>\n</blockquote>\n<p>Hence the importance of calculating EVI.</p>\n<h3 id=\"step4:applytherelevantmeasurementinstrumentstothehigh-information-valuevariable\"><br></h3>\n<h3 id=\"Step_4__Apply_the_relevant_measurement_instrument_s__to_the_high_information_value_variable\">Step 4: Apply the relevant measurement instrument(s) to the high-information-value variable</h3>\n<p>If you followed the first three steps, then you\u2019ve defined a variable you want to measure in terms of the decision it affects and how you observe it, you\u2019ve quantified your uncertainty about it, and you\u2019ve calculated the value of gaining additional information about it. Now it\u2019s time to reduce your uncertainty about the variable \u2013 that is, to measure it.</p>\n<p>Each scientific discipline has its own specialized measurement methods. Hubbard\u2019s book describes measurement methods that are often useful for reducing our uncertainty about the \u201csofter\u201d topics often encountered by decision-makers in business.</p>\n<h4 id=\"selectingameasurementmethod\"><br></h4>\n<h4 id=\"Selecting_a_measurement_method\">Selecting a measurement method</h4>\n<p>To figure out which category of measurement methods are appropriate for a particular case, we must ask several questions:</p>\n<ol>\n<li>Decomposition: Which parts of the thing are we uncertain about?</li>\n<li>Secondary research: How has the thing (or its parts) been measured by others?</li>\n<li>Observation: How do the identified observables lend themselves to measurement?</li>\n<li>Measure just enough: How much do we need to measure it?</li>\n<li>Consider the error: How might our observations be misleading?</li>\n</ol>\n<h5 id=\"decomposition\"><br></h5>\n<h5>Decomposition</h5>\n<p>Sometimes you\u2019ll want to start by decomposing an uncertain variable into several parts to identify which observables you can most easily measure. For example, rather than directly estimating the cost of a large construction project, you could break it into parts and estimate the cost of each part of the project.</p>\n<p>In Hubbard\u2019s experience, it\u2019s often the case that decomposition itself \u2013 even without making any new measurements \u2013 often reduces one\u2019s uncertainty about the variable of interest.</p>\n<h5 id=\"secondaryresearch\"><br></h5>\n<h5>Secondary research</h5>\n<p>Don\u2019t reinvent the world. In almost all cases, someone has already invented the measurement tool you need, and you just need to find it. Here are Hubbard\u2019s tips on secondary research:</p>\n<ol>\n<li>If you\u2019re new to a topic, start with Wikipedia rather than Google. Wikipedia will give you a more organized perspective on the topic at hand.</li>\n<li>Use search terms often associated with quantitative data. E.g. don\u2019t just search for \u201csoftware quality\u201d or \u201ccustomer perception\u201d \u2013 add terms like \u201ctable,\u201d \u201csurvey,\u201d \u201ccontrol group,\u201d and \u201cstandard deviation.\u201d</li>\n<li>Think of internet research in two levels: general search engines and topic-specific repositories (e.g. the CIA World Fact Book).</li>\n<li>Try multiple search engines.</li>\n<li>If you find marginally related research that doesn\u2019t directly address your topic of interest, check the bibliography more relevant reading material.</li>\n</ol>\n<p>I\u2019d also recommend my post <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">Scholarship: How to Do It Efficiently</a>.</p>\n<h5 id=\"observation\"><br></h5>\n<h5>Observation</h5>\n<p>If you\u2019re not sure how to measure your target variable\u2019s observables, ask these questions:</p>\n<ol>\n<li>Does it leave a trail? Example: longer waits on customer support lines cause customers to hang up and not call back. Maybe you can also find a correlation between customers who hang up after long waits and reduced sales to those customers.</li>\n<li>Can you observe it directly? Maybe you haven\u2019t been tracking how many of the customers in your parking lot show an out-of-state license, but you could start. Or at least, you can observe a sample of these data.</li>\n<li>Can you create a way to observe it indirectly? Amazon.com added a gift-wrapping feature in part so they could better track how many books were being purchased as gifts. Another example is when consumers are given coupons so that retailers can see which newspapers their customers read.</li>\n<li>Can the thing be forced to occur under new conditions which allow you to observe it more easily? E.g. you could implement a proposed returned-items policy in some stores but not others and compare the outcomes.</li>\n</ol>\n<h5 id=\"measurejustenough\"><br></h5>\n<h5>Measure just enough</h5>\n<p>Because initial measurements often tell you quite a lot, and also change the value of continued measurement, Hubbard often aims for spending 10% of the EVPI on a measurement, and sometimes as little as 2% (especially for very large projects).</p>\n<h5 id=\"considertheerror\"><br></h5>\n<h5>Consider the error</h5>\n<p>It\u2019s important to be conscious of some common ways in which measurements can mislead.</p>\n<p>Scientists distinguish two types of measurement error: systemic and random. Random errors are random variations from one observation to the next. They can\u2019t be individually predicted, but they fall into patterns that can be accounted for with the laws of probability. Systemic errors, in contrast, are consistent. For example, the sales staff may routinely overestimate the next quarter\u2019s revenue by 50% (on average).</p>\n<p>We must also distinguish precision and accuracy. A \u201cprecise\u201d measurement tool has low random error. E.g. if a bathroom scale gives the exact same displayed weight every time we set a particular book on it, then the scale has high precision. An \u201caccurate\u201d measurement tool has low systemic error. The bathroom scale, while precise, might be inaccurate if the weight displayed is systemically biased in one direction \u2013 say, eight pounds too heavy. A measurement tool can also have low precision but good accuracy, if it gives inconsistent measurements but they average to the true value.</p>\n<p>Random error tends to be easier to handle. Consider this example:</p>\n<blockquote>\n<p>For example, to determine how much time sales reps spend in meetings with clients versus other administrative tasks, they might choose a complete review of all time sheets\u2026 [But] if a complete review of 5,000 time sheets\u2026 tells us that sales reps spend 34% of their time in direct communication with customers, we still don\u2019t know how far from the truth it might be. Still, this \u201cexact\u201d number seems reassuring to many managers. Now, suppose a sample of direct observations of randomly chosen sales reps at random points in time finds that sales reps were in client meetings or on client phone calls only 13 out of 100 of those instances. (We can compute this without interrupting a meeting by asking as soon as the rep is available.) As we will see [later], in the latter case, we can statistically compute a 90% CI to be 7.5% to 18.5%. Even though this random sampling approach gives us only a range, we should prefer its findings to the census audit of time sheets. The census\u2026 gives us an exact number, but we have no way to know by how much and in which direction the time sheets err.</p>\n</blockquote>\n<p>Systemic error is also called a \u201cbias.\u201d Based on his experience, Hubbard suspects the three most important to avoid are:</p>\n<ol>\n<li>Confirmation bias: people see what they want to see.</li>\n<li>Selection bias: your sample might not be representative of the group you\u2019re trying to measure.</li>\n<li>Observer bias: the very act of observation can affect what you observe. E.g. in one study, researchers found that worker productivity improved no matter <em>what</em> they changed about the workplace. The workers seem to have been responding merely to the <em>fact</em> that they were being observed in <em>some</em> way.</li>\n</ol>\n<h5 id=\"chooseanddesignthemeasurementinstrument\"><br></h5>\n<h5>Choose and design the measurement instrument</h5>\n<p>After following the above steps, Hubbard writes, \u201cthe measurement instrument should be almost completely formed in your mind.\u201d But if you still can\u2019t come up with a way to measure the target variable, here are some additional tips:</p>\n<ol>\n<li><em>Work through the consequences</em>. If the value is surprisingly high, or surprisingly low, what would you expect to see?</li>\n<li><em>Be iterative</em>. Start with just a few observations, and then recalculate the information value.</li>\n<li><em>Consider multiple approaches</em>. Your first measurement tool may not work well. Try others.</li>\n<li><em>What\u2019s the really simple question that makes the rest of the measurement moot?</em> First see if you can detect <em>any</em> change in research quality before trying to measure it more comprehensively.</li>\n</ol>\n<h4 id=\"samplingreality\"><br></h4>\n<h4 id=\"Sampling_reality\">Sampling reality</h4>\n<p>In most cases, we\u2019ll estimate the values in a population by measuring the values in a small sample from that population. And for reasons discussed in chapter 7, a very small sample can often offer large reductions in uncertainty.</p>\n<p>There are a variety of tools we can use to build our estimates from small samples, and which one we should use often depends on how outliers are distributed in the population. In some cases, outliers are very close to the mean, and thus our estimate of the mean can converge quickly on the true mean as we look at new samples. In other cases, outliers can be several orders of magnitude away from the mean, and our estimate converges very slowly or not at all. Here are some examples:</p>\n<ul>\n<li>Very quick convergence, only 1\u20132 samples needed: cholesterol level of your blood, purity of public water supply, weight of jelly beans.</li>\n<li>Usually quickly convergence, 5\u201330 samples needed: Percentage of customers who like the new product, failure loads of bricks, age of your customers, how many movies people see in a year.</li>\n<li>Potentially slow convergence: Software project cost overruns, factory downtime due to an accident.</li>\n<li>Maybe non-convergent: Market value of corporations, individual levels of income, casualties of wars, size of volcanic eruptions.</li>\n</ul>\n<p>Below, I survey just a few of the many sampling methods Hubbard covers in his book.</p>\n<h5 id=\"mathlessestimation\"><br></h5>\n<h5>Mathless estimation</h5>\n<p>When working with a quickly converging phenomenon and a symmetric distribution (uniform, normal, camel-back, or bow-tie) for the population, you can use the <a href=\"http://en.wikipedia.org/wiki/T-statistic\">t-statistic</a> to develop a 90% CI even when working with very small samples. (See the book for instructions.)</p>\n<p>Or, even easier, make use of the <em>Rule of FIve</em>: \u201cThere is a 93.75% chance that the median of a population is between the smallest and largest values in any random sample of five from that population.\u201d</p>\n<p>The Rule of Five has another advantage over the t-statistic: it works for any distribution of values in the population, including ones with slow convergence or no convergence at all! It can do this because it gives us a confidence interval for the <em>median</em> rather than the <em>mean</em>, and it\u2019s the mean that is far more affected by outliers.</p>\n<p>Hubbard calls this a \u201cmathless\u201d estimation technique because it doesn\u2019t require us to take square roots or calculate standard deviation or anything like that. Moreover, this mathless technique extends beyond the Rule of Five: If we sample 8 items, there is a 99.2% chance that the median of the population falls within the largest and smallest values. If we take the <em>2nd</em> largest and smallest values (out of 8 total values), we get something close to a 90% CI for the median. Hubbard generalizes the tool with this handy reference table:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/mathless-90-percent-CI-for-median.jpg\" alt=\"\" align=\"center\"></p>\n<p>And if the distribution is symmetrical, then the mathless table gives us a 90% CI for the mean as well as for the median.</p>\n<h5 id=\"catch-recatch\"><br></h5>\n<h5>Catch-recatch</h5>\n<p>How does a biologist measure the number of fish in a lake? SHe catches and tags a sample of fish \u2013 say, 1000 of them \u2013 and then releases them. After the fish have had time to spread amongst the rest of the population, she\u2019ll catch another sample of fish. Suppose she caught 1000 fish again, and 50 of them were tagged. This would mean 5% of the fish were tagged, and thus that were about 20,000 fish in the entire lake. (See Hubbard\u2019s book for the details on how to calculate the 90% CI.)</p>\n<h5 id=\"spotsampling\"><br></h5>\n<h5>Spot sampling</h5>\n<p>The fish example was a special case of a common problem: population proportion sampling. Often, we want to know what proportion of a population has a particular trait. How many registered voters in California are Democrats? What percentage of your customers prefer a new product design over the old one?</p>\n<p>Hubbard\u2019s book discusses how to solve the general problem, but for now let\u2019s just consider another special case: spot sampling.</p>\n<p>In spot sampling, you take random snapshots of things rather than tracking them constantly. What proportion of their work hours do employees spend on Facebook? To answer this, you \u201crandomly sample people through the day to see what they were doing <em>at that moment</em>. If you find that in 12 instances out of 100 random samples\u201d employees were on Facebook, you can guess they spend about 12% of their time on Facebook (the 90% CI is 8% to 18%).</p>\n<h5 id=\"clusteredsampling\"><br></h5>\n<h5>Clustered sampling</h5>\n<p>Hubbard writes:</p>\n<blockquote>\n<p>\u201cClustered sampling\u201d is defined as taking a random sample of groups, then conducting a census or a more concentrated sampling within the group. For example, if you want to see what share of households has satellite dishes\u2026 it might be cost effective to randomly choose several city blocks, then conduct a complete census of everything in a block. (Zigzagging across town to individually selected households would be time consuming.) In such cases, we can\u2019t really consider the number of [households] in the groups\u2026 to be the number of random samples. Within a block, households may be very similar\u2026 [and therefore] it might be necessary to treat the effective number of random samples as the number of blocks\u2026</p>\n</blockquote>\n<h5 id=\"measuretothethreshold\"><br></h5>\n<h5>Measure to the threshold</h5>\n<p>For many decisions, one decision is required if a value is above some threshold, and another decision is required if that value is below the threshold. For such decisions, you don\u2019t care as much about a measurement that reduces uncertainty in general as you do about a measurement that tells you which decision to make based on the threshold. Hubbard gives an example:</p>\n<blockquote>\n<p>Suppose you needed to measure the average amount of time spent by employees in meetings that could be conducted remotely\u2026 If a meeting is among staff members who communicate regularly and for a relatively routine topic, but someone has to travel to make the meeting, you probably can conduct it remotely. You start out with your calibrated estimate that the median employee spends between 3% to 15% traveling to meetings that could be conducted remotely. You determine that if this percentage is actually over 7%, you should make a significant investment in tele meetings. The [EVPI] calculation shows that it is worth no more than $15,000 to study this. According to our rule of thumb for measurement costs, we might try to spend about $1,500\u2026</p>\n</blockquote>\n<blockquote>\n<p>Let\u2019s say you sampled 10 employees and\u2026 you find that only 1 spends less time in these activities than the 7% threshold. Given this information, what is the chance that the median time spent in such activities is actually below 7%, in which case the investment would not be justified? One \u201ccommon sense\u201d answer is 1/10, or 10%. Actually\u2026 the real chance is much smaller.</p>\n</blockquote>\n<p>Hubbard shows how to derive the real chance in his book. The key point is that \u201cthe uncertainty about the threshold can fall much faster than the uncertainty about the quantity in general.\u201d</p>\n<h5 id=\"regressionmodeling\"><br></h5>\n<h5>Regression modeling</h5>\n<p>What if you want to figure out the cause of something that has many possible causes? One method is to perform a <em>controlled experiment</em>, and compare the outcomes of a test group to a control group. Hubbard discusses this in his book (and yes, he\u2019s a Bayesian, and a skeptic of p-value hypothesis testing). For this summary, I\u2019ll instead mention another method for isolating causes: regression modeling. Hubbard explains:</p>\n<blockquote>\n<p>If we use regression modeling with historical data, we may not need to conduct a controlled experiment. Perhaps, for example, it is difficult to tie an IT project to an increase in sales, but we might have lots of data about how something <em>else</em> affects sales, such as faster time to market of new products. If we know that faster time to market is possible by automating certain tasks, that this IT investment eliminates certain tasks, and those tasks are on the critical path in the time-to-market, we can make the connection.</p>\n</blockquote>\n<p>Hubbard\u2019s book explains the basics of linear regressions, and of course gives the caveat that correlation does not imply causation. But, he writes, \u201cyou should conclude that one thing causes another only if you have some <em>other</em> good reason besides the correlation itself to suspect a cause-and-effect relationship.\u201d</p>\n<h4 id=\"bayes\"><br></h4>\n<h4 id=\"Bayes\">Bayes</h4>\n<p>Hubbard\u2019s 10th chapter opens with a tutorial on Bayes\u2019 Theorem. For an online tutorial, see <a href=\"http://yudkowsky.net/rational/bayes\">here</a>.</p>\n<p>Hubbard then zooms out to a big-picture view of measurement, and recommends the \u201cinstinctive Bayesian approach\u201d:</p>\n<ol>\n<li>Start with your calibrated estimate.</li>\n<li>Gather additional information (polling, reading other studies, etc.)</li>\n<li>Update your calibrated estimate subjectively, without doing any additional math.</li>\n</ol>\n<p>Hubbard says a few things in support of this approach. First, he points to some studies (e.g. <a href=\"http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476620\">El-Gamal &amp; Grether (1995)</a>) showing that people often reason in roughly-Bayesian ways. Next, he says that in his experience, people become better intuitive Bayesians when they (1) are made aware of the <a href=\"http://en.wikipedia.org/wiki/Base_rate_fallacy\">base rate fallacy</a>, and when they (2) are better calibrated.</p>\n<p>Hubbard says that once these conditions are met,</p>\n<blockquote>\n<p>[then] humans seem to be mostly logical when incorporating new information into their estimates along with the old information. This fact is extremely useful because a human can consider qualitative information that does not fit in standard statistics. For example, if you were giving a forecast for how a new policy might change \u201cpublic image\u201d \u2013 measured in part by a reduction in customer complaints, increased revenue, and the like \u2013 a calibrated expert should be able to update current knowledge with \u201cqualitative\u201d information about how the policy worked for other companies, feedback from focus groups, and similar details. Even with sampling information, the calibrated estimator \u2013 who has a Bayesian instinct \u2013 can consider qualitative information on samples that most textbooks don\u2019t cover.</p>\n</blockquote>\n<p>He also offers a chart showing how a pure Bayesian estimator compares to other estimators:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/confidence-versus-information-emphasis.jpg\" alt=\"\" align=\"center\"></p>\n<p>Also, Bayes\u2019 Theorem allows us to perform a \u201cBayesian inversion\u201d:</p>\n<blockquote>\n<p>Given a particular observation, it may seem more obvious to frame a measurement by asking the question \u201cWhat can I conclude from this observation?\u201d or, in probabilistic terms, \u201cWhat is the probability X is true, given my observation?\u201d But Bayes showed us that we could, instead, start with the question, \u201cWhat is the probability of this observation if X were true?\u201d</p>\n</blockquote>\n<blockquote>\n<p>The second form of the question is useful because the answer is often more straightforward and it leads to the answer to the other question. It also forces us to think about the likelihood of different observations given a particular hypothesis and what that means for interpreting an observation.</p>\n</blockquote>\n<blockquote>\n<p>[For example] if, hypothetically, we know that only 20% of the population will continue to shop at our store, then we can determine the chance [that] exactly 15 out of 20 would say so\u2026 [The details are explained in the book.] Then we can invert the problem with Bayes\u2019 theorem to compute the chance that only 20% of the population will continue to shop there given [that] 15 out of 20 said so in a random sample. We would find that chance to be very nearly zero\u2026</p>\n</blockquote>\n<h4 id=\"othermethods\"><br></h4>\n<h4 id=\"Other_methods\">Other methods</h4>\n<p>Other chapters discuss other measurement methods, for example prediction markets, \u0010Rasch models,\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010 methods for measuring preferences and happiness, methods for improving the subjective judgments of experts, and many others. \u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010\u0010</p>\n<h3 id=\"step5:makeadecisionandactonit\"><br></h3>\n<h3 id=\"Step_5__Make_a_decision_and_act_on_it\">Step 5: Make a decision and act on it</h3>\n<p>The last step will make more sense if we first \u201cbring the pieces together.\u201d Hubbard now organizes his consulting work with a firm into 3 phases, so let\u2019s review what we\u2019ve learned in the context of his 3 phases.</p>\n<h4 id=\"phase0:projectpreparation\"><br></h4>\n<h4 id=\"Phase_0__Project_Preparation\">Phase 0: Project Preparation</h4>\n<ul>\n<li><em>Initial research</em>: Interviews and secondary research to get familiar on the nature of the decision problem.</li>\n<li><em>Expert identification</em>: Usually 4\u20135 experts who provide estimates.</li>\n</ul>\n<h4 id=\"phase1:decisionmodeling\"><br></h4>\n<h4 id=\"Phase_1__Decision_Modeling\">Phase 1: Decision Modeling</h4>\n<ul>\n<li><em>Decision problem definition</em>: Experts define the problem they\u2019re trying to analyze.</li>\n<li><em>Decision model detail</em>: Using an Excel spreadsheet, the AIE analyst elicits from the experts all the factors that matter for the decision being analyzed: costs and benefits, ROI, etc.</li>\n<li><em>Initial calibrated estimates</em>: First, the experts undergo calibration training. Then, they fill in the values (as 90% CIs or other probability distributions) for the variables in the decision model.</li>\n</ul>\n<h4 id=\"phase2:optimalmeasurements\"><br></h4>\n<h4 id=\"Phase_2__Optimal_measurements\">Phase 2: Optimal measurements</h4>\n<ul>\n<li><em>Value of information analysis</em>: Using Excel macros, the AIE analyst runs a value of information analysis on every variable in the model.</li>\n<li><em>Preliminary measurement method designs</em>: Focusing on the few variables with highest information value, the AIE analyst chooses measurement methods that should reduce uncertainty.</li>\n<li><em>Measurement methods</em>: Decomposition, random sampling, Bayesian inversion, controlled experiments, and other methods are used (as appropriate) to reduce the uncertainty of the high-VoI variables.</li>\n<li><em>Updated decision model</em>: The AIE analyst updates the decision model based on the results of the measurements.</li>\n<li><em>Final value of information analysis</em>: The AIE analyst runs a VoI analysis on each variable again. As long as this analysis shows information value much greater than the cost of measurement for some variables, measurement and VoI analysis continues in multiple iterations. Usually, though, only one or two iterations are needed before the VoI analysis shows that no further measurements are justified.</li>\n</ul>\n<h4 id=\"phase3:decisionoptimizationandthefinalrecommendation\"><br></h4>\n<h4 id=\"Phase_3__Decision_optimization_and_the_final_recommendation\">Phase 3: Decision optimization and the final recommendation</h4>\n<ul>\n<li><em>Completed risk/return analysis</em>: A final MC simulation shows the likelihood of possible outcomes.</li>\n<li><em>Identified metrics procedures</em>: Procedures are put in place to measure some variables (e.g. about project progress or external factors) continually.</li>\n<li><em>Decision optimization</em>: The final business decision recommendation is made (this is rarely a simple \u201cyes/no\u201d answer).</li>\n</ul>\n<h4 id=\"finalthoughts\"><br></h4>\n<h4 id=\"Final_thoughts\">Final thoughts</h4>\n<p>Hubbard\u2019s book includes two case studies in which Hubbard describes how he led two fairly different clients (the EPA and U.S. Marine Corps) through each phase of the AIE process. Then, he closes the book with the following summary:</p>\n<ul>\n<li>If it\u2019s really that important, it\u2019s something you can define. If it\u2019s something you think exists at all, it\u2019s something you\u2019ve already observed somehow.</li>\n<li>If it\u2019s something important and something uncertain, you have a cost of being wrong and a chance of being wrong.</li>\n<li>You can quantify your current uncertainty with calibrated estimates.</li>\n<li>You can compute the value of additional information by knowing the \u201cthreshold\u201d of the measurement where it begins to make a difference compared to your existing uncertainty.</li>\n<li>Once you know what it\u2019s worth to measure something, you can put the measurement effort in context and decide on the effort it should take.</li>\n<li>Knowing just a few methods for random sampling, controlled experiments, or even merely improving on the judgments of experts can lead to a significant reduction in uncertainty.</li>\n</ul>", "sections": [{"title": "Basic Ideas", "anchor": "Basic_Ideas", "level": 1}, {"title": "Applied Information Economics", "anchor": "Applied_Information_Economics", "level": 1}, {"title": "Step 1: Define a decision problem and the relevant variables", "anchor": "Step_1__Define_a_decision_problem_and_the_relevant_variables", "level": 1}, {"title": "Step 2: Determine what you know", "anchor": "Step_2__Determine_what_you_know", "level": 1}, {"title": "Uncertainty and calibration", "anchor": "Uncertainty_and_calibration", "level": 2}, {"title": "Simulations", "anchor": "Simulations", "level": 2}, {"title": "Step 3: Pick a variable, and compute the value of additional information for that variable", "anchor": "Step_3__Pick_a_variable__and_compute_the_value_of_additional_information_for_that_variable", "level": 1}, {"title": "Step 4: Apply the relevant measurement instrument(s) to the high-information-value variable", "anchor": "Step_4__Apply_the_relevant_measurement_instrument_s__to_the_high_information_value_variable", "level": 1}, {"title": "Selecting a measurement method", "anchor": "Selecting_a_measurement_method", "level": 2}, {"title": "Sampling reality", "anchor": "Sampling_reality", "level": 2}, {"title": "Bayes", "anchor": "Bayes", "level": 2}, {"title": "Other methods", "anchor": "Other_methods", "level": 2}, {"title": "Step 5: Make a decision and act on it", "anchor": "Step_5__Make_a_decision_and_act_on_it", "level": 1}, {"title": "Phase 0: Project Preparation", "anchor": "Phase_0__Project_Preparation", "level": 2}, {"title": "Phase 1: Decision Modeling", "anchor": "Phase_1__Decision_Modeling", "level": 2}, {"title": "Phase 2: Optimal measurements", "anchor": "Phase_2__Optimal_measurements", "level": 2}, {"title": "Phase 3: Decision optimization and the final recommendation", "anchor": "Phase_3__Decision_optimization_and_the_final_recommendation", "level": 2}, {"title": "Final thoughts", "anchor": "Final_thoughts", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "52 comments"}], "headingsCount": 20}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xiojTDJP6FWdb2Fmb", "vADtvr9iDeYsCDfxd", "xDiqYyqeqPo92PojS", "37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-07T04:32:42.499Z", "modifiedAt": null, "url": null, "title": "Meetup : [Boston] Using Causal Graphs to Understand Bias in the Medical Literature", "slug": "meetup-boston-using-causal-graphs-to-understand-bias-in-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:31.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ben_LandauTaylor", "createdAt": "2013-07-25T17:40:48.283Z", "isAdmin": false, "displayName": "Ben_LandauTaylor"}, "userId": "ZvoQwr4zZjPuC2oNJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7yMDRQHb96RD784fR/meetup-boston-using-causal-graphs-to-understand-bias-in-the", "pageUrlRelative": "/posts/7yMDRQHb96RD784fR/meetup-boston-using-causal-graphs-to-understand-bias-in-the", "linkUrl": "https://www.lesswrong.com/posts/7yMDRQHb96RD784fR/meetup-boston-using-causal-graphs-to-understand-bias-in-the", "postedAtFormatted": "Wednesday, August 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BBoston%5D%20Using%20Causal%20Graphs%20to%20Understand%20Bias%20in%20the%20Medical%20Literature&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BBoston%5D%20Using%20Causal%20Graphs%20to%20Understand%20Bias%20in%20the%20Medical%20Literature%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7yMDRQHb96RD784fR%2Fmeetup-boston-using-causal-graphs-to-understand-bias-in-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BBoston%5D%20Using%20Causal%20Graphs%20to%20Understand%20Bias%20in%20the%20Medical%20Literature%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7yMDRQHb96RD784fR%2Fmeetup-boston-using-causal-graphs-to-understand-bias-in-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7yMDRQHb96RD784fR%2Fmeetup-boston-using-causal-graphs-to-understand-bias-in-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<h2><strong style=\"font-size: small;\">WHEN:</strong><span style=\"font-size: small;\"> </span><span class=\"date\" style=\"font-size: small;\">11 August 2013 02:00:00PM (-0400)</span></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHERE:</strong> <span class=\"address\">25 Ames St, Cambridge, MA</span></p>\n<p><span class=\"address\"><a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/130598002/\">Meetup event page</a></span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>This talk is a very gentle introduction to modern causality theory, as developed by Jamie Robins and Judea Pearl. We will use as little mathematics as possible to introduce some of the central ideas in the field, with examples from medicine and epidemiology. In the one hour talk, we will cover:</p>\n<p>(1) Causal Directed Acyclic Graphs and the rules of D-Separation</p>\n<p>(2) Definitions of confounding and selection bias</p>\n<p>(3) Methods to adjust for confounding</p>\n<p>(4) Examples of situations where standard methods are always biased.</p>\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n<p>Our default schedule is as follows:</p>\n<p>&mdash;Phase 1: Arrival, greetings, unstructured conversation.</p>\n<p>&mdash;Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes.</p>\n<p>&mdash;Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n<p>&mdash;Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7yMDRQHb96RD784fR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.2934539938126458e-06, "legacy": true, "legacyId": "23676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-07T06:29:56.266Z", "modifiedAt": null, "url": null, "title": "Improving Enjoyment and Retention Reading Technical Literature", "slug": "improving-enjoyment-and-retention-reading-technical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:49.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sentientplatypus", "createdAt": "2013-05-01T02:43:34.377Z", "isAdmin": false, "displayName": "sentientplatypus"}, "userId": "vNepEav2ET9qeYM9y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JZTBonFhSxGbXpS5w/improving-enjoyment-and-retention-reading-technical", "pageUrlRelative": "/posts/JZTBonFhSxGbXpS5w/improving-enjoyment-and-retention-reading-technical", "linkUrl": "https://www.lesswrong.com/posts/JZTBonFhSxGbXpS5w/improving-enjoyment-and-retention-reading-technical", "postedAtFormatted": "Wednesday, August 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Improving%20Enjoyment%20and%20Retention%20Reading%20Technical%20Literature&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImproving%20Enjoyment%20and%20Retention%20Reading%20Technical%20Literature%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZTBonFhSxGbXpS5w%2Fimproving-enjoyment-and-retention-reading-technical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Improving%20Enjoyment%20and%20Retention%20Reading%20Technical%20Literature%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZTBonFhSxGbXpS5w%2Fimproving-enjoyment-and-retention-reading-technical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZTBonFhSxGbXpS5w%2Fimproving-enjoyment-and-retention-reading-technical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 890, "htmlBody": "<p>A little background on myself first &ndash; I am currently studying to become involved with aging rejuvenation therapies like SENS.</p>\n<p>This requires learning quite a lot about molecular biology. Which is fine, because I find cell biology quite interesting. The problem is naturally that textbooks and technical literature on the subject often make very little effort to be interesting.</p>\n<p>Many of the books I&rsquo;ve been reading lately are largely lacking in energy. And I was finding my mind was often drifting away from what I was reading and generally just not enjoying the process. Which was bad, because I need to spend a lot of time doing it.</p>\n<p>I asked myself, do I hate learning about molecular biology and engineering? Should I shift my goals to something I&rsquo;m more interested in? But, I didn&rsquo;t seem to actually be disinterested in the subject. I loved talking about what I&rsquo;d learned. And I frequently thought about it with interest. I was passionate about the goal of defeating aging. So the problem then was probably the books themselves.</p>\n<p>So then the question was: how do I make boringly written biology books fun to read? Find better books? Well unfortunately, based on my research, the only biology books written to be interesting tend to be focusing on on other sects of the science; most good molecular biology books are boring. If anyone knows of any books on the subject that are unusually well written, please let me know. But I couldn't find any.</p>\n<p>So I looked for the bright spots: where reading <em>was</em> fun. What makes reading a novel fun? I asked. Interesting story, character interactions, suspense, humor, <em>dramatic scenes</em>.</p>\n<p>None of these are incorporated in molecular biology books and publications that I can find. But the answer was still there: visualize what I read. But not just visualize like the little diagrams of cellular interactions books usually give you &ndash; like stupid, over-the-top, Hollywood-status visualization. I had to make it <em>dramatic</em>. I had to mentally reconstruct the biology of a cell in massive, fast, and explosive terms.</p>\n<p>Suddenly, I was reading about genetic engineering with a grin on my face; because I was visualizing a cackling mad scientist taking a jackhammer to a gene sequence.</p>\n<p>Which, yes, is totally not what is happening in any way, but I remember what I read better because the unusual things are what stick in human memories; just reading a passage <em>normally </em>makes it easy to forget what I&rsquo;ve read. And the weirdness seems to make the parts around it more memorable, so I&rsquo;m remembering what I read a lot better, I find.</p>\n<p>Most of the time I try not to make it that absurd. But if I imagine spliceosomes blasting introns out of RNA molecules or cell lysis as an overstated explosion of a cell I simply remember the concepts better. It isn&rsquo;t the most accurate view of reality, but I'm aware of that when I think back on it, and it&rsquo;s better than <em>not</em> remembering it.</p>\n<p>But this strategy eventually gets a little tiring to maintain alone, I find, so I had to add in a second technique. Every time my mind wants to start wandering I stop, close my eyes, and refocus on what I'm reading, I recite &lsquo;<em><a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai</a></em>&rsquo;, and why I want to become stronger, what I have to protect. And then I continue. I find this little technique to make a massive difference. It reorients me, so that I continue to concentrate and it briefly reminds of what I'm pursuing and why. And if that doesn't give you the motivation to continue you should probably find a different project.</p>\n<p>A third useful strategy has been planning how long I will read instead of how much and then break up the time spent reading over the course of a day. First, it encourages reading to understand fully rather than reading to finish fifty pages. Also, I find it tends to get me to read more pages, despite defeating the motivation to go fast. Time goals just take the pressure of failing to complete work off a bit, I find. As an example, I read about a 160 pages of a molecular biology textbook today using an input-based time goal. I used to plan for fifty pages of a similar type of material on a regular day and sometimes not finish even that. To be fair, I'm spending more time reading now, but I think using input based goals instead of output goals had a part in that that.</p>\n<p>The other results I've gotten from these strategies have been pretty good as well. I&rsquo;ve been trying to quantify my happiness lately, on a scale where every full number corresponds to doubled enjoyment, and now that I&rsquo;m doing these three things my average happiness while reading technical passages has gone up by nearly a full point. My enjoyment of technical literature has gone from somewhere around 'yeah, it&rsquo;s ok, I guess' to 'happy' while reading. And because it&rsquo;s just more fun to do, it helps me to spend more time reading about molecular biology, more time working towards an unaging future.</p>\n<p>Anyway, I thought I&rsquo;d post the ideas in case they helped anyone else out (although the first might not work as well for things that are harder to visualize). I&rsquo;m also interested if anyone does anything similar (or different) to increase their enjoyment of similar texts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 2, "fkABsGCJZ6y9qConW": 2, "fF9GEdWXKJ3z73TmB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JZTBonFhSxGbXpS5w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 45, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "23677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-07T19:56:56.207Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver!", "slug": "meetup-vancouver-6", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ENka8jWXLLPBrgfhG/meetup-vancouver-6", "pageUrlRelative": "/posts/ENka8jWXLLPBrgfhG/meetup-vancouver-6", "linkUrl": "https://www.lesswrong.com/posts/ENka8jWXLLPBrgfhG/meetup-vancouver-6", "postedAtFormatted": "Wednesday, August 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FENka8jWXLLPBrgfhG%2Fmeetup-vancouver-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FENka8jWXLLPBrgfhG%2Fmeetup-vancouver-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FENka8jWXLLPBrgfhG%2Fmeetup-vancouver-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pp'>Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 August 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 west broadway, vancouver, bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual, come chat and hang out with cool smart people who are into LW.</p>\n\n<p>15:30 on Sunday at Benny's</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pp'>Vancouver!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ENka8jWXLLPBrgfhG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.294219815070086e-06, "legacy": true, "legacyId": "23681", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_\">Discussion article for the meetup : <a href=\"/meetups/pp\">Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 August 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 west broadway, vancouver, bc</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual, come chat and hang out with cool smart people who are into LW.</p>\n\n<p>15:30 on Sunday at Benny's</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_1\">Discussion article for the meetup : <a href=\"/meetups/pp\">Vancouver!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_", "level": 1}, {"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-07T20:59:40.037Z", "modifiedAt": null, "url": null, "title": "What Would it Take to \"Prove\" a Speculative Cause?", "slug": "what-would-it-take-to-prove-a-speculative-cause", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:38.091Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JvvGJxrfCgCiRLraq/what-would-it-take-to-prove-a-speculative-cause", "pageUrlRelative": "/posts/JvvGJxrfCgCiRLraq/what-would-it-take-to-prove-a-speculative-cause", "linkUrl": "https://www.lesswrong.com/posts/JvvGJxrfCgCiRLraq/what-would-it-take-to-prove-a-speculative-cause", "postedAtFormatted": "Wednesday, August 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Would%20it%20Take%20to%20%22Prove%22%20a%20Speculative%20Cause%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Would%20it%20Take%20to%20%22Prove%22%20a%20Speculative%20Cause%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJvvGJxrfCgCiRLraq%2Fwhat-would-it-take-to-prove-a-speculative-cause%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Would%20it%20Take%20to%20%22Prove%22%20a%20Speculative%20Cause%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJvvGJxrfCgCiRLraq%2Fwhat-would-it-take-to-prove-a-speculative-cause", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJvvGJxrfCgCiRLraq%2Fwhat-would-it-take-to-prove-a-speculative-cause", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1289, "htmlBody": "<p><em>Follow up to: <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">Why I'm Skeptical About Unproven Causes (And You Should Be Too)</a></em></p>\n<p>-</p>\n<p>My previous essay <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">Why I'm Skeptical About Unproven Causes (And You Should Be Too)</a>&nbsp;generated a lot of discussion here and on <a href=\"http://effective-altruism.com/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">the Effective Altruist blog</a>. &nbsp;Some related questions that came up a lot was: what does it take to prove a cause? &nbsp;What separates \"proven\" from \"speculative\" causes? &nbsp;And how do you get a \"speculative\" cause to move into the \"proven\" column? &nbsp;I've decided that this discussion is important enough that it merits a bit of elaboration at length, so I'm going to do that in this essay.</p>\n<p>&nbsp;</p>\n<h2><strong>Proven Cause vs. Speculative Cause</strong></h2>\n<p>My prime example of proven causes are <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's top charities</a>. &nbsp;These organizations -- <a href=\"http://www.againstmalaria.com/Default.aspx\">The Against Malaria Foundation</a>&nbsp;(AMF), <a href=\"http://www.givedirectly.org\">GiveDirectly</a>, and <a href=\"http://www3.imperial.ac.uk/schisto\">Schistosomiasis Control Initiative</a>&nbsp;(SCI) -- are rolling out programs that have been the target of significant scientific scrutiny. &nbsp;For example, delivering long-lasting insecticide-treated anti-malaria nets (what AMF does) has been studied by <a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets\">23 different randomized, controlled trials</a>&nbsp;(RCTs). &nbsp;GiveWell has also published thorough reviews of all three organizations (see reviews for <a href=\"http://www.givewell.org/international/top-charities/amf\">AMF</a>, <a href=\"http://www.givewell.org/international/top-charities/give-directly\">GiveDirectly</a>, and <a href=\"http://www.givewell.org/international/top-charities/schistosomiasis-control-initiative\">SCI</a>).</p>\n<p>On the other hand, a speculative cause is a cause where the case is made entirely by intuition and speculation, with zero scientific study. &nbsp;For some of these causes, scientific study may even be impossible.</p>\n<p>&nbsp;</p>\n<p>Now, I think 23 RCTs is a very high burden to meet. &nbsp;Instead, we should recognize that being \"proven\" is not a binary yes or no, but rather a sliding scale. &nbsp;Even AMF isn't proven -- there still are some areas of concern or potential weaknesses in the case for AMF. Likewise, other organizations working in the area, like <a href=\"http://www.nothingbutnets.net/\">Nothing But Nets</a>, also are nearly as proven, but don't have key elements of transparency and track record to make myself confident enough. &nbsp;And AMF is a lot more proven that GiveDirectly, which is potentially more proven than SCI given <a href=\"http://blog.givewell.org/2012/07/13/new-cochrane-review-of-the-effectiveness-of-deworming/\">recent developments in deworming research</a>.</p>\n<p>Ideally, we'd take a <a href=\"http://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/\">Bayesian approach</a>, where we have a certain prior estimate about how cost-effective the organization is, and then update our cost-effectiveness estimate based on additional evidence as it comes in. &nbsp;For reasons I argued earlier and GiveWell has argued in <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">\"Why We Can't Take Expected Value Estimates Literally (Even When They're Unbiased)\"</a>, <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">\"Maximizing Cost-Effectiveness Estimates via Critical Inquiry\"</a>&lt;/a&gt;, <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">\"Some Considerations Against More Investment in Cost-Effectiveness Estimates\"</a>, I think our prior estimate should be quite skeptical (i.e. expect cost-effectiveness to be not as good as AMF / much closer to average than na&iuml;vely estimated) until proven otherwise.</p>\n<p>&nbsp;</p>\n<p>Right now, I consider AMF, GiveDirectly, and SCI to be the only sufficiently proven interventions, but I'm open to other organizations also entering this area. &nbsp;Of course, this doesn't mean that all other organizations must be speculative -- instead there is a middle ground of organizations that are neither speculative or \"sufficiently proven\".</p>\n<p>&nbsp;</p>\n<h2>From Speculative to Proven</h2>\n<p>So how does a cause become proven? &nbsp;Through more measurement. &nbsp;I think this is best described through examples:</p>\n<p><a href=\"http://www.veganoutreach.org\">Vegan Outreach</a>&nbsp;and <a href=\"http://www.thehumaneleague.com/\">The Humane League</a>&nbsp;work to advertise people reducing the amount of meat in their diets in order to avoid <a href=\"http://www.everydayutilitarian.com/essays/why-eat-less-meat/\">cruelty in factory farms</a>. &nbsp;They do this through leafleting and Facebook ads. &nbsp;Na&iuml;ve cost-effectiveness estimates would guess that, even under rather pessimistic assumptions, this kind of advocacy <a href=\"http://www.everydayutilitarian.com/vegan-outreach-cost-effectiveness-calculator/\">is very cost-effective</a>, perhaps around <a href=\"http://www.everydayutilitarian.com/essays/how-much-does-it-cost-to-buy-a-vegetarian/\">$0.02 to $65.92 to reduce one year of suffering on a factory farm</a>.</p>\n<p>But we can't be sure enough about this and I don't think this estimate is reliable. &nbsp;But we can make it better with additional study. &nbsp;I think that if we ran three or so more studies that were relatively independent (taking place in different areas and run by different researchers), addressed current problems with the studies (like lack of a control group), had longer time-frames and larger sample sizes, and still pointed toward a conversion rate of 1% or more, than I would start donating to this kind of outreach instead, believing it to be \"sufficiently proven\".</p>\n<p>Another example could be <a href=\"http://www.80000hours.org\">80,000 Hours</a>, an organization that runs careers advice and encourages people to shoot for higher impact careers using their free careers advising and resources. &nbsp;One could select a group of people that seem like good candidates for careers advice, give them all an initial survey asking them specific things about their current thoughts on careers, and then randomly accept or deny them to get careers advice. &nbsp;Then follow up with everyone a year or two later and see what initial careers they ended up in, how they got the jobs, and for the group that got advising, how valuable in retrospect the advising was. &nbsp;With continued follow up, one could measure the difference in expected impact between the two groups and figure out how good 80K is at careers advice.</p>\n<p>Perhaps even <a href=\"http://www.intelligence.org/\">The Machine Intelligence Research Institute</a>&nbsp;(MIRI) could benefit from more measurement. &nbsp;The trouble is that it's working on a problem (making sure that advanced artificial intelligence goes well for humanity) that's so distant, it's difficult to get feedback. &nbsp;But they still potentially could assess the success or failures of their attempt to influence the AI community and they still could try to solicit more external reviews of their work from independent AI experts. &nbsp;I'm not close enough to MIRI to know whether these would be good or bad ideas, but it seems plausible at first glance that even MIRI could be better measured.</p>\n<p>&nbsp;</p>\n<p>And it wouldn't be too difficult to expand this to other areas. &nbsp;For example, I think GiveWell's tracking of money moved is reliable enough and <a href=\"http://www.givewell.org/about/self-evaluation\">their commitment to self-evaluation</a>&nbsp;(and external review) strong enough that I would strongly consider funding them before any of their top charities, if they ever had any room for more funding (which they currently do not and <a href=\"http://blog.givewell.org/2012/11/03/giving-to-givewells-recommended-charities-helps-givewell/\">urge you to donate to their top charities instead</a>). &nbsp;<a href=\"http://www.effectiveanimalactivism.org\">Effective Animal Activism</a> could do the same and I think have even higher success, because I think it's moderately likely that if someone starts donating to animal charities after joining EAA, there are few other things that could have influenced them.</p>\n<p>Of course, these forms of measurement have their problems, and no measurement -- even two dozen RCTs -- will be perfect. &nbsp;But some level of feedback and measurement is incredibly necessary to avoid our own biases and failures in na&iuml;ve estimation.</p>\n<p>&nbsp;</p>\n<h2>The Proven and The Promising: My Current Donation Strategy</h2>\n<p>My current donation strategy is to separate organizations into three categories: <strong>proven</strong>, <strong>promising</strong>, and <strong>not promising</strong>.</p>\n<p>Proven organizations are the ones that I talked about earlier -- AMF, GiveDirectly, and SCI.</p>\n<p>Promising organizations are organizations I think have a hope of becoming proven, someday. &nbsp;They're organizations practicing interventions that intuitively seem like they would have high upside (like 80K Hours in getting people into better careers and The Humane League in persuading a bunch of people to become vegetarian), have a good commitment to transparency and self-measurement (The Humane League shines here), and have opportunities for additional money to be converted into additional information on their impact.</p>\n<p>My goal in donating would be to first ensure the survival of all promising organizations (make sure they have enough funding to stay around) and then try to buy information from promising organizations as much as I can. &nbsp;For example, I'd be interested in funding more <a href=\"http://humaneleaguelabs.wordpress.com\">studies about vegetarian outreach</a>&nbsp;or making sure 80K has the money they need to <a href=\"http://www.80000hours.org/recruitment\">hire a new careers advisor</a>.</p>\n<p>Once these needs are met, I'll save a fair amount of my donation to meet future needs down the road. &nbsp;But then, I'll spend some on proven organizations to (a) achieve impact, (b) continue the incentive for organizations to want to be proven, and (c) show public support for those organizations and donating in general.</p>\n<p>...Now I just need to actually get some more money.</p>\n<p>-</p>\n<p><em>(This was also <a href=\"http://www.everydayutilitarian.com/essays/what-would-it-take-to-prove-a-speculative-cause/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>\n<p><em>I'd like to thank Jonas Vollmer for having the critical conversation with me that inspired this piece.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JvvGJxrfCgCiRLraq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.2942718230083551e-06, "legacy": true, "legacyId": "23682", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Follow up to: <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">Why I'm Skeptical About Unproven Causes (And You Should Be Too)</a></em></p>\n<p>-</p>\n<p>My previous essay <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">Why I'm Skeptical About Unproven Causes (And You Should Be Too)</a>&nbsp;generated a lot of discussion here and on <a href=\"http://effective-altruism.com/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">the Effective Altruist blog</a>. &nbsp;Some related questions that came up a lot was: what does it take to prove a cause? &nbsp;What separates \"proven\" from \"speculative\" causes? &nbsp;And how do you get a \"speculative\" cause to move into the \"proven\" column? &nbsp;I've decided that this discussion is important enough that it merits a bit of elaboration at length, so I'm going to do that in this essay.</p>\n<p>&nbsp;</p>\n<h2 id=\"Proven_Cause_vs__Speculative_Cause\"><strong>Proven Cause vs. Speculative Cause</strong></h2>\n<p>My prime example of proven causes are <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's top charities</a>. &nbsp;These organizations -- <a href=\"http://www.againstmalaria.com/Default.aspx\">The Against Malaria Foundation</a>&nbsp;(AMF), <a href=\"http://www.givedirectly.org\">GiveDirectly</a>, and <a href=\"http://www3.imperial.ac.uk/schisto\">Schistosomiasis Control Initiative</a>&nbsp;(SCI) -- are rolling out programs that have been the target of significant scientific scrutiny. &nbsp;For example, delivering long-lasting insecticide-treated anti-malaria nets (what AMF does) has been studied by <a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets\">23 different randomized, controlled trials</a>&nbsp;(RCTs). &nbsp;GiveWell has also published thorough reviews of all three organizations (see reviews for <a href=\"http://www.givewell.org/international/top-charities/amf\">AMF</a>, <a href=\"http://www.givewell.org/international/top-charities/give-directly\">GiveDirectly</a>, and <a href=\"http://www.givewell.org/international/top-charities/schistosomiasis-control-initiative\">SCI</a>).</p>\n<p>On the other hand, a speculative cause is a cause where the case is made entirely by intuition and speculation, with zero scientific study. &nbsp;For some of these causes, scientific study may even be impossible.</p>\n<p>&nbsp;</p>\n<p>Now, I think 23 RCTs is a very high burden to meet. &nbsp;Instead, we should recognize that being \"proven\" is not a binary yes or no, but rather a sliding scale. &nbsp;Even AMF isn't proven -- there still are some areas of concern or potential weaknesses in the case for AMF. Likewise, other organizations working in the area, like <a href=\"http://www.nothingbutnets.net/\">Nothing But Nets</a>, also are nearly as proven, but don't have key elements of transparency and track record to make myself confident enough. &nbsp;And AMF is a lot more proven that GiveDirectly, which is potentially more proven than SCI given <a href=\"http://blog.givewell.org/2012/07/13/new-cochrane-review-of-the-effectiveness-of-deworming/\">recent developments in deworming research</a>.</p>\n<p>Ideally, we'd take a <a href=\"http://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/\">Bayesian approach</a>, where we have a certain prior estimate about how cost-effective the organization is, and then update our cost-effectiveness estimate based on additional evidence as it comes in. &nbsp;For reasons I argued earlier and GiveWell has argued in <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">\"Why We Can't Take Expected Value Estimates Literally (Even When They're Unbiased)\"</a>, <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">\"Maximizing Cost-Effectiveness Estimates via Critical Inquiry\"</a>&lt;/a&gt;, <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">\"Some Considerations Against More Investment in Cost-Effectiveness Estimates\"</a>, I think our prior estimate should be quite skeptical (i.e. expect cost-effectiveness to be not as good as AMF / much closer to average than na\u00efvely estimated) until proven otherwise.</p>\n<p>&nbsp;</p>\n<p>Right now, I consider AMF, GiveDirectly, and SCI to be the only sufficiently proven interventions, but I'm open to other organizations also entering this area. &nbsp;Of course, this doesn't mean that all other organizations must be speculative -- instead there is a middle ground of organizations that are neither speculative or \"sufficiently proven\".</p>\n<p>&nbsp;</p>\n<h2 id=\"From_Speculative_to_Proven\">From Speculative to Proven</h2>\n<p>So how does a cause become proven? &nbsp;Through more measurement. &nbsp;I think this is best described through examples:</p>\n<p><a href=\"http://www.veganoutreach.org\">Vegan Outreach</a>&nbsp;and <a href=\"http://www.thehumaneleague.com/\">The Humane League</a>&nbsp;work to advertise people reducing the amount of meat in their diets in order to avoid <a href=\"http://www.everydayutilitarian.com/essays/why-eat-less-meat/\">cruelty in factory farms</a>. &nbsp;They do this through leafleting and Facebook ads. &nbsp;Na\u00efve cost-effectiveness estimates would guess that, even under rather pessimistic assumptions, this kind of advocacy <a href=\"http://www.everydayutilitarian.com/vegan-outreach-cost-effectiveness-calculator/\">is very cost-effective</a>, perhaps around <a href=\"http://www.everydayutilitarian.com/essays/how-much-does-it-cost-to-buy-a-vegetarian/\">$0.02 to $65.92 to reduce one year of suffering on a factory farm</a>.</p>\n<p>But we can't be sure enough about this and I don't think this estimate is reliable. &nbsp;But we can make it better with additional study. &nbsp;I think that if we ran three or so more studies that were relatively independent (taking place in different areas and run by different researchers), addressed current problems with the studies (like lack of a control group), had longer time-frames and larger sample sizes, and still pointed toward a conversion rate of 1% or more, than I would start donating to this kind of outreach instead, believing it to be \"sufficiently proven\".</p>\n<p>Another example could be <a href=\"http://www.80000hours.org\">80,000 Hours</a>, an organization that runs careers advice and encourages people to shoot for higher impact careers using their free careers advising and resources. &nbsp;One could select a group of people that seem like good candidates for careers advice, give them all an initial survey asking them specific things about their current thoughts on careers, and then randomly accept or deny them to get careers advice. &nbsp;Then follow up with everyone a year or two later and see what initial careers they ended up in, how they got the jobs, and for the group that got advising, how valuable in retrospect the advising was. &nbsp;With continued follow up, one could measure the difference in expected impact between the two groups and figure out how good 80K is at careers advice.</p>\n<p>Perhaps even <a href=\"http://www.intelligence.org/\">The Machine Intelligence Research Institute</a>&nbsp;(MIRI) could benefit from more measurement. &nbsp;The trouble is that it's working on a problem (making sure that advanced artificial intelligence goes well for humanity) that's so distant, it's difficult to get feedback. &nbsp;But they still potentially could assess the success or failures of their attempt to influence the AI community and they still could try to solicit more external reviews of their work from independent AI experts. &nbsp;I'm not close enough to MIRI to know whether these would be good or bad ideas, but it seems plausible at first glance that even MIRI could be better measured.</p>\n<p>&nbsp;</p>\n<p>And it wouldn't be too difficult to expand this to other areas. &nbsp;For example, I think GiveWell's tracking of money moved is reliable enough and <a href=\"http://www.givewell.org/about/self-evaluation\">their commitment to self-evaluation</a>&nbsp;(and external review) strong enough that I would strongly consider funding them before any of their top charities, if they ever had any room for more funding (which they currently do not and <a href=\"http://blog.givewell.org/2012/11/03/giving-to-givewells-recommended-charities-helps-givewell/\">urge you to donate to their top charities instead</a>). &nbsp;<a href=\"http://www.effectiveanimalactivism.org\">Effective Animal Activism</a> could do the same and I think have even higher success, because I think it's moderately likely that if someone starts donating to animal charities after joining EAA, there are few other things that could have influenced them.</p>\n<p>Of course, these forms of measurement have their problems, and no measurement -- even two dozen RCTs -- will be perfect. &nbsp;But some level of feedback and measurement is incredibly necessary to avoid our own biases and failures in na\u00efve estimation.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Proven_and_The_Promising__My_Current_Donation_Strategy\">The Proven and The Promising: My Current Donation Strategy</h2>\n<p>My current donation strategy is to separate organizations into three categories: <strong>proven</strong>, <strong>promising</strong>, and <strong>not promising</strong>.</p>\n<p>Proven organizations are the ones that I talked about earlier -- AMF, GiveDirectly, and SCI.</p>\n<p>Promising organizations are organizations I think have a hope of becoming proven, someday. &nbsp;They're organizations practicing interventions that intuitively seem like they would have high upside (like 80K Hours in getting people into better careers and The Humane League in persuading a bunch of people to become vegetarian), have a good commitment to transparency and self-measurement (The Humane League shines here), and have opportunities for additional money to be converted into additional information on their impact.</p>\n<p>My goal in donating would be to first ensure the survival of all promising organizations (make sure they have enough funding to stay around) and then try to buy information from promising organizations as much as I can. &nbsp;For example, I'd be interested in funding more <a href=\"http://humaneleaguelabs.wordpress.com\">studies about vegetarian outreach</a>&nbsp;or making sure 80K has the money they need to <a href=\"http://www.80000hours.org/recruitment\">hire a new careers advisor</a>.</p>\n<p>Once these needs are met, I'll save a fair amount of my donation to meet future needs down the road. &nbsp;But then, I'll spend some on proven organizations to (a) achieve impact, (b) continue the incentive for organizations to want to be proven, and (c) show public support for those organizations and donating in general.</p>\n<p>...Now I just need to actually get some more money.</p>\n<p>-</p>\n<p><em>(This was also <a href=\"http://www.everydayutilitarian.com/essays/what-would-it-take-to-prove-a-speculative-cause/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>\n<p><em>I'd like to thank Jonas Vollmer for having the critical conversation with me that inspired this piece.</em></p>", "sections": [{"title": "Proven Cause vs. Speculative Cause", "anchor": "Proven_Cause_vs__Speculative_Cause", "level": 1}, {"title": "From Speculative to Proven", "anchor": "From_Speculative_to_Proven", "level": 1}, {"title": "The Proven and The Promising: My Current Donation Strategy", "anchor": "The_Proven_and_The_Promising__My_Current_Donation_Strategy", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XiN948y5QDgNbuTXP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-08T00:27:43.472Z", "modifiedAt": null, "url": null, "title": "Motivation and Merciless Commitment Contracts", "slug": "motivation-and-merciless-commitment-contracts", "viewCount": null, "lastCommentedAt": "2013-08-12T04:28:32.733Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peirce", "createdAt": "2013-06-06T03:59:10.790Z", "isAdmin": false, "displayName": "peirce"}, "userId": "wgxiAsQ8bfmJhgYbw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/awZBXAZJW6mdED4Pj/motivation-and-merciless-commitment-contracts", "pageUrlRelative": "/posts/awZBXAZJW6mdED4Pj/motivation-and-merciless-commitment-contracts", "linkUrl": "https://www.lesswrong.com/posts/awZBXAZJW6mdED4Pj/motivation-and-merciless-commitment-contracts", "postedAtFormatted": "Thursday, August 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Motivation%20and%20Merciless%20Commitment%20Contracts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMotivation%20and%20Merciless%20Commitment%20Contracts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FawZBXAZJW6mdED4Pj%2Fmotivation-and-merciless-commitment-contracts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Motivation%20and%20Merciless%20Commitment%20Contracts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FawZBXAZJW6mdED4Pj%2Fmotivation-and-merciless-commitment-contracts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FawZBXAZJW6mdED4Pj%2Fmotivation-and-merciless-commitment-contracts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1326, "htmlBody": "<h4>Commitment contracts</h4>\n<p>I have been using commitment contracts (eg. Via Stickk and Beeminder) for a while now with quite a high degree of success. The basic idea is that you precommit to reward or punish yourself for anything that you know you should do. Example: You want to lose weight. You define a certain amount of weight that you want to lose over a certain time period (like a pound a week). If you fail to do this, you lose a certain amount of money - pay it to a charity, pay it to a commitment contract company etc.. If you do lose the weight, you gain a predefined reward - eg. You buy yourself a nice hat or something. Fairly simple.</p>\n<h4>Howfar should/ can commitment contracts be taken?</h4>\n<p>It seems that for everything that anyone wants to do, but lacks the motivation, there is always something that would motivate you to do it. Everything has a price right? And by making use of commitment contracts you can force yourself to choose between paying a huge price (financial or otherwise) or doing whatever you know you should do but don't really want to do, you can ultimately make yourself do that thing that you don't want to do. Whatever it is. Maybe I'm getting ahead of myself, but it seems like from that perspective, akrasia is a pretty solved problem?</p>\n<h4>Personal Example</h4>\n<p>My situation is this. I could do with a little bit more social confidence. I don't think I'm underconfident really, but more confidence would be good, which I think is probably the same for most people. So I figured, it would probably be a lot better to solve this problem soon. The sooner the better.</p>\n<p>I also figured there is a process I could go through to make this happen. Lets say I make a list of all the things that cause me the most social anxiety, and also that wouldn't be too damaging for my social life afterwards (for example, starting fights with random strangers or walking round my local city naked would be pretty high on the list, but I don't want to be arrested or be known as \"that crazy streaker\" for the rest of my life. Of the top of my head, some ideas would be: going to a city far away from my home and walking up to people and pretending to be crazy (knocking on people's doors and asking \"have you seen my pet fish?\" until I get the door shut in my face), going to clubs and sitting in the middle of the dance floor, or anything else which would be very socially painful to do.</p>\n<h4>Contract</h4>\n<p>So I could set up a commitment contract stating I must do each of these activities until my anxiety has decreased to half of its initial level by the end of a certain date. If I don't do this then I pay x pound to y person. I'm pretty confident that after doing stuff like that for say, a whole week, I would have enough social confidence for almost all normal purposes, and social confidence would no longer be a problem in my life.</p>\n<p>Of course, these things make me feel a little bit nervous just when I think of myself doing them, so I'd need a hell of a lot of motivation to do them. I'd say a commitment contract worth a couple of thousand pounds would do the trick. But of course, I don't want to lose the contract. If I do, it would be a disaster, I would end up with a huge financial loss, and no increase in social confidence. It seems to me then, that to increase the expectancy of success, I should just increase the amount of money that I place on the bet. Lets say &pound;10,000. I'd say for that amount of money, I'd almost certainly go through with the project. Still if I complete it, it would be an almost unbearable loss, but because of this, I reckon that my chances of success are high enough to mean that if I do the expected utility calculations of probability of failure vs. success and value of gains vs. losses, it is probably a good bet to make.</p>\n<p>Also, to make sure I don't have the option of backing out and cancelling the contract, I could just set up some sort of legal contract, and have someone else be the referee for whether I have succeeded with the project.</p>\n<h4>Problem</h4>\n<p>When thinking about this, I got quite anxious just by thinking about making myself do this. I realised, that this state of anxiety would not be fun, and that having the threat of a huge loss like will probably make you pretty miserable in the long-term. This is why I don't think this would be a great idea for something like losing weight. It is a long term goal, and during that time you'd probably be constantly scared shitless of losing all your money (you might end up losing the weight from stress). So overall, it seems that this form of merciless commitment contract would be best for the short term projects - like a week long - which would minimise the amount of stress/ anxiety of being faced with two extremely painful options in the short term (losing a shit load of money or doing something incredibly painful). As I was experiencing a bit of anxiety by thinking about all of this, I also figured that the best option would be to spend as little time thinking about making the contract as possible, and just make the contract, because dithering over it also causes stress/ anxiety.</p>\n<p>At this point I got really stressed and anxious because I realised that what seemed to me to be the most rational option was to make a huge commitment contract right then in the moment to do activities that would cause me a great deal of social anxiety over the next week. At this point I got too stressed, and realised that I couldn't motivate myself to make myself make the contract and decided not to think about any of this stuff for a while because I'd managed to immerse myself into a state of sweaty paralysis at the thought of making commitment contracts. I wish I could say that I didn't do that, and that I actually made these contracts, and came out after a very stressful week feeling socially invincible. But I didn't.</p>\n<h4>Fictional Example</h4>\n<p>Then I realised that if I wish that I did do that, then I still think I have made the wrong choice. In the film Fight Club there is a scene where Tyler Durden goes to an off licence late at night, pulls the shopkeeper out into the car park, and puts a gun to his head. He then asks the poor guy what did you used to want to be when you grew up. The guy says a vet and he didn't do it because it was too hard. Tyler takes his wallet, with information about his address etc. and says that if the guy isn't on the way to becoming a vet in 6 weeks, he will kill him. (I think this is what happened, I haven't seen the film in a year or two). So in a way, I'm kind of envious of that shopkeeper.</p>\n<p>I'm not actually too sure about what Existentialism is, but it seems like this is a bit of an existential crisis.</p>\n<h4>Note</h4>\n<p>You may think that a) doing these things wouldn't actually improve social confidence enough b) that as the loss is too high, even a small risk wouldn't be worth it c) that the stress you put yourself under wouldn't make it worth it d) some other objection. You may be right&hellip; My point is, that for most people, if they think about it, there is some sort of commitment contract like this which would be worth them making.</p>\n<p>&nbsp;</p>\n<p>So&hellip; erm&hellip; Any thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "awZBXAZJW6mdED4Pj", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.2944443433989395e-06, "legacy": true, "legacyId": "23683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h4 id=\"Commitment_contracts\">Commitment contracts</h4>\n<p>I have been using commitment contracts (eg. Via Stickk and Beeminder) for a while now with quite a high degree of success. The basic idea is that you precommit to reward or punish yourself for anything that you know you should do. Example: You want to lose weight. You define a certain amount of weight that you want to lose over a certain time period (like a pound a week). If you fail to do this, you lose a certain amount of money - pay it to a charity, pay it to a commitment contract company etc.. If you do lose the weight, you gain a predefined reward - eg. You buy yourself a nice hat or something. Fairly simple.</p>\n<h4 id=\"Howfar_should__can_commitment_contracts_be_taken_\">Howfar should/ can commitment contracts be taken?</h4>\n<p>It seems that for everything that anyone wants to do, but lacks the motivation, there is always something that would motivate you to do it. Everything has a price right? And by making use of commitment contracts you can force yourself to choose between paying a huge price (financial or otherwise) or doing whatever you know you should do but don't really want to do, you can ultimately make yourself do that thing that you don't want to do. Whatever it is. Maybe I'm getting ahead of myself, but it seems like from that perspective, akrasia is a pretty solved problem?</p>\n<h4 id=\"Personal_Example\">Personal Example</h4>\n<p>My situation is this. I could do with a little bit more social confidence. I don't think I'm underconfident really, but more confidence would be good, which I think is probably the same for most people. So I figured, it would probably be a lot better to solve this problem soon. The sooner the better.</p>\n<p>I also figured there is a process I could go through to make this happen. Lets say I make a list of all the things that cause me the most social anxiety, and also that wouldn't be too damaging for my social life afterwards (for example, starting fights with random strangers or walking round my local city naked would be pretty high on the list, but I don't want to be arrested or be known as \"that crazy streaker\" for the rest of my life. Of the top of my head, some ideas would be: going to a city far away from my home and walking up to people and pretending to be crazy (knocking on people's doors and asking \"have you seen my pet fish?\" until I get the door shut in my face), going to clubs and sitting in the middle of the dance floor, or anything else which would be very socially painful to do.</p>\n<h4 id=\"Contract\">Contract</h4>\n<p>So I could set up a commitment contract stating I must do each of these activities until my anxiety has decreased to half of its initial level by the end of a certain date. If I don't do this then I pay x pound to y person. I'm pretty confident that after doing stuff like that for say, a whole week, I would have enough social confidence for almost all normal purposes, and social confidence would no longer be a problem in my life.</p>\n<p>Of course, these things make me feel a little bit nervous just when I think of myself doing them, so I'd need a hell of a lot of motivation to do them. I'd say a commitment contract worth a couple of thousand pounds would do the trick. But of course, I don't want to lose the contract. If I do, it would be a disaster, I would end up with a huge financial loss, and no increase in social confidence. It seems to me then, that to increase the expectancy of success, I should just increase the amount of money that I place on the bet. Lets say \u00a310,000. I'd say for that amount of money, I'd almost certainly go through with the project. Still if I complete it, it would be an almost unbearable loss, but because of this, I reckon that my chances of success are high enough to mean that if I do the expected utility calculations of probability of failure vs. success and value of gains vs. losses, it is probably a good bet to make.</p>\n<p>Also, to make sure I don't have the option of backing out and cancelling the contract, I could just set up some sort of legal contract, and have someone else be the referee for whether I have succeeded with the project.</p>\n<h4 id=\"Problem\">Problem</h4>\n<p>When thinking about this, I got quite anxious just by thinking about making myself do this. I realised, that this state of anxiety would not be fun, and that having the threat of a huge loss like will probably make you pretty miserable in the long-term. This is why I don't think this would be a great idea for something like losing weight. It is a long term goal, and during that time you'd probably be constantly scared shitless of losing all your money (you might end up losing the weight from stress). So overall, it seems that this form of merciless commitment contract would be best for the short term projects - like a week long - which would minimise the amount of stress/ anxiety of being faced with two extremely painful options in the short term (losing a shit load of money or doing something incredibly painful). As I was experiencing a bit of anxiety by thinking about all of this, I also figured that the best option would be to spend as little time thinking about making the contract as possible, and just make the contract, because dithering over it also causes stress/ anxiety.</p>\n<p>At this point I got really stressed and anxious because I realised that what seemed to me to be the most rational option was to make a huge commitment contract right then in the moment to do activities that would cause me a great deal of social anxiety over the next week. At this point I got too stressed, and realised that I couldn't motivate myself to make myself make the contract and decided not to think about any of this stuff for a while because I'd managed to immerse myself into a state of sweaty paralysis at the thought of making commitment contracts. I wish I could say that I didn't do that, and that I actually made these contracts, and came out after a very stressful week feeling socially invincible. But I didn't.</p>\n<h4 id=\"Fictional_Example\">Fictional Example</h4>\n<p>Then I realised that if I wish that I did do that, then I still think I have made the wrong choice. In the film Fight Club there is a scene where Tyler Durden goes to an off licence late at night, pulls the shopkeeper out into the car park, and puts a gun to his head. He then asks the poor guy what did you used to want to be when you grew up. The guy says a vet and he didn't do it because it was too hard. Tyler takes his wallet, with information about his address etc. and says that if the guy isn't on the way to becoming a vet in 6 weeks, he will kill him. (I think this is what happened, I haven't seen the film in a year or two). So in a way, I'm kind of envious of that shopkeeper.</p>\n<p>I'm not actually too sure about what Existentialism is, but it seems like this is a bit of an existential crisis.</p>\n<h4 id=\"Note\">Note</h4>\n<p>You may think that a) doing these things wouldn't actually improve social confidence enough b) that as the loss is too high, even a small risk wouldn't be worth it c) that the stress you put yourself under wouldn't make it worth it d) some other objection. You may be right\u2026 My point is, that for most people, if they think about it, there is some sort of commitment contract like this which would be worth them making.</p>\n<p>&nbsp;</p>\n<p>So\u2026 erm\u2026 Any thoughts?</p>", "sections": [{"title": "Commitment contracts", "anchor": "Commitment_contracts", "level": 1}, {"title": "Howfar should/ can commitment contracts be taken?", "anchor": "Howfar_should__can_commitment_contracts_be_taken_", "level": 1}, {"title": "Personal Example", "anchor": "Personal_Example", "level": 1}, {"title": "Contract", "anchor": "Contract", "level": 1}, {"title": "Problem", "anchor": "Problem", "level": 1}, {"title": "Fictional Example", "anchor": "Fictional_Example", "level": 1}, {"title": "Note", "anchor": "Note", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "70 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-08-08T00:27:43.472Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-08T14:04:27.531Z", "modifiedAt": null, "url": null, "title": "Should you work at 80,000 Hours?", "slug": "should-you-work-at-80-000-hours", "viewCount": null, "lastCommentedAt": "2013-08-11T21:44:40.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jess_Whittlestone", "createdAt": "2012-10-05T09:10:15.519Z", "isAdmin": false, "displayName": "Jess_Whittlestone"}, "userId": "hkNDdzLsLjfMeWEcW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B4vxxgSqnGTWR43C3/should-you-work-at-80-000-hours", "pageUrlRelative": "/posts/B4vxxgSqnGTWR43C3/should-you-work-at-80-000-hours", "linkUrl": "https://www.lesswrong.com/posts/B4vxxgSqnGTWR43C3/should-you-work-at-80-000-hours", "postedAtFormatted": "Thursday, August 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20you%20work%20at%2080%2C000%20Hours%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20you%20work%20at%2080%2C000%20Hours%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4vxxgSqnGTWR43C3%2Fshould-you-work-at-80-000-hours%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20you%20work%20at%2080%2C000%20Hours%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4vxxgSqnGTWR43C3%2Fshould-you-work-at-80-000-hours", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4vxxgSqnGTWR43C3%2Fshould-you-work-at-80-000-hours", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2658, "htmlBody": "<p class=\"p1\"><strong>The purpose of this post is to discuss some considerations relevant to whether it is high impact for you as an individual to work for 80,000 Hours.</strong></p>\n<p class=\"p1\"><strong><br /></strong></p>\n<address>Disclaimer: I am an employee at 80,000 Hours (from here on 80k). We are currently recruiting, and want to attract people to work for us who are likely to add the most value to 80k so that we can increase our impact. It seems likely that some such people might be found on LessWrong, so we want to encourage critical discussion on here about who should work at 80k.&nbsp;</address><address><br /></address>\n<p>This post will be in the format of an interview with Ben Todd, co-founder and executive director of 80,000 Hours, and thus best placed to begin a discussion about who should work for 80k. In what follows, we&rsquo;ll cover:</p>\n<ul>\n<li>If you want to support 80k, whether you should work for 80k or fund 80k. This involves some discussion of whether to earn to give or to work directly, adding to the <a href=\"/lw/hjn/earning_to_give_vs_altruistic_career_choice/\">discussion elsewhere on LW</a>.</li>\n<li>What skills and characteristics are most valuable to 80k</li>\n<li>Whether working at 80k is likely to help your future career prospects</li>\n<li>How working at 80k might compare to some alternatives: other EA organisations and professional jobs</li>\n<li>Who probably <em>shouldn&rsquo;t</em> work for 80k</li>\n</ul>\n<p class=\"p2\">What this post won&rsquo;t cover:</p>\n<ul>\n<li>Whether 80k itself is high impact. This has already been discussed at length on LW <a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">here</a> and <a href=\"/lw/gqq/cea_does_not_seem_to_be_credibly_high_impact/\">here</a>.</li>\n<li>Whether you&rsquo;d enjoy working at 80k. We&rsquo;ve already posted on our blog about <a href=\"http://80000hours.org/blog/236-80-000-hours-is-hiring\">why we think it&rsquo;s good to work at 80k</a>.</li>\n</ul>\n<div><br /></div>\n<p class=\"p2\"><a id=\"more\"></a></p>\n<h2>Summary&nbsp;</h2>\n<p>&nbsp;</p>\n<p class=\"p2\">If you fit the profile we&rsquo;re looking for in terms of skills and experience (see below or our job descriptions linked to on the blog), <strong>think that meta-charity is potentially very high impact and want to innovate in this area</strong>, then you should consider applying to work for 80k.&nbsp;</p>\n<p class=\"p2\">Even if you&rsquo;re not entirely convinced that 80k is currently high impact but have ideas of how we could improve so that we <em>are</em>, we&rsquo;d be very interested to hear them.</p>\n<p class=\"p2\">If you don&rsquo;t believe that meta-charity in general or 80k specifically is potentially high impact, believe strongly that a certain narrow cause area is most important, or have a strong desire to work in a specific professional career area, then you may be better working elsewhere.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\"><span style=\"font-style: italic;\">Thanks to Ben Todd and Roman Duda for their help with conducting the discussion that follows.</span></p>\n<p class=\"p2\"><a id=\"more\"></a></p>\n<h4><br /></h4>\n<h4>A big question for effective altruists is whether it&rsquo;s higher impact to work for an organisation directly or to fund them to employ multiple people in your place. How can someone tell whether it would be better to work for 80k or to fund 80k?</h4>\n<p>&nbsp;</p>\n<p><strong>Ben</strong>: <strong>I think a good rule of thumb for this is where you rank in the recruitment process: if you rank pretty highly, then you should probably work for us; if you&rsquo;re closer to average or below average then you&rsquo;d probably have more impact funding us.</strong> So the short answer is that if you&rsquo;re interested then the best thing to do is probably to apply and find out!</p>\n<p class=\"p2\">To expand on this, if you rank highly compared to other applicants then you&rsquo;re likely to be able to quickly take on high level responsibilities like managing several other team members, fundraising or producing valuable research. You&rsquo;re also likely to be able to act relatively autonomously and help the rest of the team to perform at a higher level. We find that normally it&rsquo;s difficult to replace people capable of this even with significant numbers of staff at the margin. This means it&rsquo;s difficult for people in this position contribute equally with donations, unless you have the option of a very high paying job (able to donate in excess of $100,000).</p>\n<p class=\"p2\">This is supported by research, which suggests that the within similar roles <a href=\"http://80000hours.org/blog/69-how-good-are-the-best\">the best people have many times the output of the average person</a>. The difference is largest for complex roles, and we think most of our roles are highly complex. Moreover, in practice the difference is even larger, because there is variance in the *type of role* people can take as well as within the role. Finally, replacing one person with multiple people adds significantly to the communication costs. This all adds up to our top staff probably having the output of 2-10 staff at the margin.&nbsp;</p>\n<p class=\"p2\">CEA and 80k feel pretty heavily talent constrained at these levels. There&rsquo;s a lot of high value stuff we could do (e.g. apply to more fundraising from foundations, place more stories in the media, do more research that looks really worthwhile, set up new projects) that we&rsquo;re not able to do due to a lack of people.</p>\n<p class=\"p2\">Putting this all together, plus the fact that our ranking process is not perfect, if you&rsquo;re highly ranked I roughly guess you would need to be looking to donate $30,000 - $150,000 per year to break even. I think this drops down fairly quickly as you move down the ranking.&nbsp;</p>\n<p class=\"p2\">Note that a crucial consideration is how these figures will evolve over time. <strong>My very speculative guess is that we&rsquo;ll be talent constrained among people who are autonomous and high ability or can act as leader figures for the long-term; whereas we may end up pretty much not funding constrained</strong> (that&rsquo;s the position that GiveWell seems to be in). That speaks in favour of not earning to give.</p>\n<p class=\"p2\">We&rsquo;re happy to talk this through with individual people who are wondering whether to work for us or earn to give.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4>What are some general guidelines for the kind of person who is likely to be highly irreplaceable at 80k?&nbsp;</h4>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\"><strong>Ben</strong>: You need to have <strong>strong analytical skills</strong>: most people who work for us have a degree in a tough analytical or scientific subject (e.g. maths, physics, economics, philosophy), but if you have work experience to demonstrate this that&rsquo;s good too. You should be able to <strong>manage lots of projects and keep control of lots of tasks at once</strong>, and it&rsquo;s really important that you&rsquo;re able to motivate yourself easily and <strong>work with a high degree of autonomy</strong>. Ideally, we&rsquo;re able to just hand people a portion of what needs doing in one meeting per week and trust them to get things done using appropriate priorities. Because we&rsquo;re working on some pretty ambitious projects, we look for people who can <strong>persist at a tough goal over a long period</strong> even in the face of setbacks.</p>\n<p class=\"p2\">We want people who can not only fit into our team easily, but <strong>take on a leadership role and be persuasive and engaging</strong> in their communications with others. We&rsquo;ll assess this in an interview and also look for past management and leadership experience. Of course, <strong>it&rsquo;s also crucial that you really care about making the world a better place, and doing so in a rigorous, evidence-based way</strong>. We put high weight on <strong>being able to look at things in a rational way and be open to and responsive to new evidence</strong>. Ideally you&rsquo;d also have knowledge of our previous content and that of related organisations like GiveWell. Previous involvement with effective altruist organisations or activities is a plus here, but not necessary.</p>\n<p class=\"p2\">In terms of the skills and experience we&rsquo;re looking for more specifically: leadership experience, research skills, knowledge of economics, organisational/personality psychology, and social sciences more generally are all highly valued. Experience in coaching or something relevant is an advantage, and we&rsquo;re also interested in people with communications, branding, marketing, design or fundraising backgrounds. Generally we don&rsquo;t expect anyone to have years and years of experience, but our perfect hire would be someone with at least a couple of years work experience, rather than someone just graduated.</p>\n<p class=\"p2\">If you want to be really certain of how valuable you&rsquo;d be at 80k, it&rsquo;s best to do an internship with us and find out. And of course the best indicator of where you&rsquo;d rank in our recruitment process is just to apply.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4>How good is working at 80k for personal development and future job prospects, especially compared to more professional jobs?</h4>\n<p>&nbsp;</p>\n<p class=\"p2\"><strong>Ben</strong>: Because 80k is a small and growing organisation, <strong>you&rsquo;ll be able to develop high level skills such as management and gain responsibility more quickly than in almost any job</strong>. We see this as a real advantage of working for 80k, as even in high status professional jobs like consulting and finance, it&rsquo;s hard to get this kind of experience for quite a few years. There&rsquo;s also a high level of autonomy and flexibility working at 80k, making it relatively easy to fit other learning and skill development around the job: we set aside around 10% of our employees&rsquo; time for generally building highly transferable skills. There&rsquo;s a strong culture of personal development. A lot of people here are interested in quantified self and personal productivity.&nbsp;</p>\n<p class=\"p2\"><strong>The networking opportunities are pretty good: for example,&nbsp;</strong><strong>we currently share our offices with the&nbsp;<a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>&nbsp;and the&nbsp;<a href=\"http://www.practicalethics.ox.ac.uk/\">Uehiro Centre for Practical Ethics</a>&nbsp;in central Oxford. More generally,&nbsp;</strong><strong>working for 80k is obviously a great way to interact with a number of people in the effective altruism community</strong>.&nbsp;The people we interact with through coaching tend to have impressive backgrounds in a variety of industries (including entrepreneurship, finance, tech, research etc.) And since you&rsquo;re able to get lots of responsibility quickly here you get to interact with high level people early on because you&rsquo;ll be externally representing 80k: going to conferences on our behalf, for example.</p>\n<p class=\"p2\">In terms of general credentials (&ldquo;CV points&rdquo;), working for 80k might be a bit weaker than some immediately available alternatives - it&rsquo;s acknowledged within the effective altruism community but it doesn&rsquo;t look as good on your CV as McKinsey or Goldman Sachs or Google, obviously. But again, because we&rsquo;re growing fast and you&rsquo;ll get large amounts of responsibility this means there&rsquo;s an opportunity to get some impressive achievements attributed to you if you seek them out. Examples might be running major promotional campaigns or managing a whole team of researchers.</p>\n<p class=\"p2\">In addition, 80k is part of CEA, which is also growing quickly, and is moving towards becoming an incubator of effective altruist projects. This means there will be plenty of other opportunities within CEA that you could be promoted into, even if you don&rsquo;t stick within 80k itself.</p>\n<p class=\"p2\"><strong>Compared to more professional jobs like consulting or finance, working for 80k is probably higher risk in terms of future opportunities, because it depends how good your achievements end up being</strong>. It&rsquo;s easier to impress a wider range of people with McKinsey than with 80k. Consulting may well be a more reliable and widely applicable to get what we call &ldquo;career capital&rdquo; for many people. Especially if you&rsquo;ve got a strong desire to get into a specific area (you want to go into finance say, or programming) then it might be better to develop more targeted skills and experience by working in these industries. We&rsquo;re happy to talk this through with people with specific alternative opportunities. We&rsquo;ve done this in the past with people and sometimes do recommend that they&rsquo;re better working elsewhere for personal development, other times working with us does seem to provide a lot of opportunity.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4>What about more generally working for 80k compared to working for other organisations - effective altruist or otherwise? Who shouldn&rsquo;t work for 80k and would probably be better off elsewhere?</h4>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\"><strong>Ben</strong>: A useful way of thinking about this in general is to start by estimating how you rank relative to other applicants at 80k, and how you rank relative to others at the alternative organisation. Roughly speaking you should then go for where your ranking is highest, adjusted by how high impact you expect the organisations to be.&nbsp;</p>\n<p class=\"p2\">Some more specific considerations:</p>\n<p class=\"p2\"><strong>The main reason you&rsquo;d probably be better off working for somewhere other than 80k is if you think what we&rsquo;re doing has very little chance of paying off, even with a lot of iterating</strong>. If you&rsquo;re generally sceptical about meta-charity, or just think there&rsquo;s another project that&rsquo;s much better, then you&rsquo;ll probably want to work elsewhere! One quick thing I&rsquo;d say in favour of 80k here is that we have the advantage and flexibility of being cause-neutral. This means that if you&rsquo;re uncertain about which cause is the most important, working at 80k might be advantageous as it means you don&rsquo;t have to specify, and when we get more information about what the best cause is, we&rsquo;ve got a multiplier on this by helping direct more people in that direction.&nbsp;</p>\n<p class=\"p2\">The other side of this, of course, is that i<strong>f you already think one cause is much more important than anything else, you&rsquo;re probably better off working for an organisation specifically focused on that cause than for 80k</strong>. For instance, if you&rsquo;re more certain that AI xrisk is the top cause, then you might want to work for <a href=\"http://intelligence.org/\">MIRI </a>or <a href=\"http://www.fhi.ox.ac.uk/\">FHI</a>, or if you think it's global poverty, then working at our sister organisation, <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a>, may be better. You might also argue that whilst 80k provides flexibility, earning to give offers even more: if you get new information which suggests you&rsquo;re actually more replaceable than you thought you can&rsquo;t just switch jobs, whereas you can always switch your donations when you learn more. But there&rsquo;s a lot of uncertainty around the question of whether talent gaps are in general more pressing than funding gaps, and how this will change over time.</p>\n<p class=\"p2\"><strong>Even if you&rsquo;re convinced that meta-charity is high impact, working for <a href=\"http://www.givewell.org/\">GiveWell </a>might plausibly be a safer bet, especially if you want to work somewhere that has more of a track record</strong>. GiveWell is highly respected, thought of as very well run, is doubling its money moved each year and has partnered with Good Ventures (which has $3bn behind it). You have the chance to receive a rigorous training in a useful skill-set. &nbsp;I think one advantage 80k has over GiveWell is having more innovation value. Right now, we&rsquo;re doing something completely new: working to identify the biggest talent gaps in the world and help people take them. We&rsquo;re also smaller and less established, so on that basis I&rsquo;d expect there to be more opportunity to take a leadership role and shape the development of the organisation (I haven&rsquo;t asked GiveWell about this though). Finally, GiveWell seems to be emphasising recruiting research orientated folks, whereas we&rsquo;re also recruiting for people to work on communications, coaching, fundraising and more.</p>\n<p class=\"p2\">As I mentioned before, <strong>if you&rsquo;re after a specific career area long term (e.g. development policy, politics, academia), then it&rsquo;s probably best to go straight into it and start building up your network in that area</strong>. We also can&rsquo;t offer as much formal training as an established company, though we offer lots of other personal development opportunities. More likely, you&rsquo;re uncertain about where you want to go long-term, in which case it might be best to try us (and other opportunities) out. Most people can spare 6-12 months without closing down their options in other fields.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4>What roles might you take?</h4>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\"><strong>Ben</strong>: Our priorities for the next 6 months include (i) <strong>doing high quality <a href=\"http://80000hours.org/blog/232-we-re-changing-our-career-coaching\">case studies</a></strong> (ii) <strong>writing up our core findings as blog posts</strong> (iii) <strong>rebranding the website</strong> (iv) <strong>fundraising</strong> (v) <strong>evaluating our impact</strong> (vi) <strong>seeking media coverage</strong>. After that, it&rsquo;s difficult to say, but it&rsquo;ll be some mixture of doing research, operations and promoting that research.</p>\n<p class=\"p2\">We aim to keep roles flexible, and adapt them to your skill set and interests. Some people end up as researchers, others focus on promotion, and so on. A typical week involves some meetings with the team, the people you manage and your manager, reading articles and writing up research reports, having meetings with our coachees, talking to external organisations, email, and preparing strategy. You can find out more <a href=\"http://80000hours.org/blog/236-80-000-hours-is-hiring\">here</a>.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\">You might also want to check out these other effective altruist organisations currently recruiting: <a href=\"http://givingwhatwecan.org/blog/2013-07-22/were-hiring\">Giving What We Can</a> and <a href=\"http://effectiveanimalactivism.org/about/get-involved\">Effective Animal Activism</a>. Our umbrella organisation, the Centre for Effective Altruism, is also recruiting for a <a href=\"http://effective-altruism.com/cea-hiring\">Research Fellow in Global Prioritisation</a>.</p>\n<address><span id=\"docs-internal-guid-1c1a1251-5e1f-e432-3c37-a08599c945af\"><br /><br /><br /><br /><br /></span></address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"se3XDuQ4xbeWvu4eF": 3, "4kQXps8dYsKJgaayN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B4vxxgSqnGTWR43C3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 26, "extendedScore": null, "score": 1.2951219703218096e-06, "legacy": true, "legacyId": "23688", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"p1\"><strong id=\"The_purpose_of_this_post_is_to_discuss_some_considerations_relevant_to_whether_it_is_high_impact_for_you_as_an_individual_to_work_for_80_000_Hours_\">The purpose of this post is to discuss some considerations relevant to whether it is high impact for you as an individual to work for 80,000 Hours.</strong></p>\n<p class=\"p1\"><strong><br></strong></p>\n<address>Disclaimer: I am an employee at 80,000 Hours (from here on 80k). We are currently recruiting, and want to attract people to work for us who are likely to add the most value to 80k so that we can increase our impact. It seems likely that some such people might be found on LessWrong, so we want to encourage critical discussion on here about who should work at 80k.&nbsp;</address><address><br></address>\n<p>This post will be in the format of an interview with Ben Todd, co-founder and executive director of 80,000 Hours, and thus best placed to begin a discussion about who should work for 80k. In what follows, we\u2019ll cover:</p>\n<ul>\n<li>If you want to support 80k, whether you should work for 80k or fund 80k. This involves some discussion of whether to earn to give or to work directly, adding to the <a href=\"/lw/hjn/earning_to_give_vs_altruistic_career_choice/\">discussion elsewhere on LW</a>.</li>\n<li>What skills and characteristics are most valuable to 80k</li>\n<li>Whether working at 80k is likely to help your future career prospects</li>\n<li>How working at 80k might compare to some alternatives: other EA organisations and professional jobs</li>\n<li>Who probably <em>shouldn\u2019t</em> work for 80k</li>\n</ul>\n<p class=\"p2\">What this post won\u2019t cover:</p>\n<ul>\n<li>Whether 80k itself is high impact. This has already been discussed at length on LW <a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">here</a> and <a href=\"/lw/gqq/cea_does_not_seem_to_be_credibly_high_impact/\">here</a>.</li>\n<li>Whether you\u2019d enjoy working at 80k. We\u2019ve already posted on our blog about <a href=\"http://80000hours.org/blog/236-80-000-hours-is-hiring\">why we think it\u2019s good to work at 80k</a>.</li>\n</ul>\n<div><br></div>\n<p class=\"p2\"><a id=\"more\"></a></p>\n<h2 id=\"Summary_\">Summary&nbsp;</h2>\n<p>&nbsp;</p>\n<p class=\"p2\">If you fit the profile we\u2019re looking for in terms of skills and experience (see below or our job descriptions linked to on the blog), <strong>think that meta-charity is potentially very high impact and want to innovate in this area</strong>, then you should consider applying to work for 80k.&nbsp;</p>\n<p class=\"p2\">Even if you\u2019re not entirely convinced that 80k is currently high impact but have ideas of how we could improve so that we <em>are</em>, we\u2019d be very interested to hear them.</p>\n<p class=\"p2\">If you don\u2019t believe that meta-charity in general or 80k specifically is potentially high impact, believe strongly that a certain narrow cause area is most important, or have a strong desire to work in a specific professional career area, then you may be better working elsewhere.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\"><span style=\"font-style: italic;\">Thanks to Ben Todd and Roman Duda for their help with conducting the discussion that follows.</span></p>\n<p class=\"p2\"><a id=\"more\"></a></p>\n<h4><br></h4>\n<h4 id=\"A_big_question_for_effective_altruists_is_whether_it_s_higher_impact_to_work_for_an_organisation_directly_or_to_fund_them_to_employ_multiple_people_in_your_place__How_can_someone_tell_whether_it_would_be_better_to_work_for_80k_or_to_fund_80k_\">A big question for effective altruists is whether it\u2019s higher impact to work for an organisation directly or to fund them to employ multiple people in your place. How can someone tell whether it would be better to work for 80k or to fund 80k?</h4>\n<p>&nbsp;</p>\n<p><strong>Ben</strong>: <strong>I think a good rule of thumb for this is where you rank in the recruitment process: if you rank pretty highly, then you should probably work for us; if you\u2019re closer to average or below average then you\u2019d probably have more impact funding us.</strong> So the short answer is that if you\u2019re interested then the best thing to do is probably to apply and find out!</p>\n<p class=\"p2\">To expand on this, if you rank highly compared to other applicants then you\u2019re likely to be able to quickly take on high level responsibilities like managing several other team members, fundraising or producing valuable research. You\u2019re also likely to be able to act relatively autonomously and help the rest of the team to perform at a higher level. We find that normally it\u2019s difficult to replace people capable of this even with significant numbers of staff at the margin. This means it\u2019s difficult for people in this position contribute equally with donations, unless you have the option of a very high paying job (able to donate in excess of $100,000).</p>\n<p class=\"p2\">This is supported by research, which suggests that the within similar roles <a href=\"http://80000hours.org/blog/69-how-good-are-the-best\">the best people have many times the output of the average person</a>. The difference is largest for complex roles, and we think most of our roles are highly complex. Moreover, in practice the difference is even larger, because there is variance in the *type of role* people can take as well as within the role. Finally, replacing one person with multiple people adds significantly to the communication costs. This all adds up to our top staff probably having the output of 2-10 staff at the margin.&nbsp;</p>\n<p class=\"p2\">CEA and 80k feel pretty heavily talent constrained at these levels. There\u2019s a lot of high value stuff we could do (e.g. apply to more fundraising from foundations, place more stories in the media, do more research that looks really worthwhile, set up new projects) that we\u2019re not able to do due to a lack of people.</p>\n<p class=\"p2\">Putting this all together, plus the fact that our ranking process is not perfect, if you\u2019re highly ranked I roughly guess you would need to be looking to donate $30,000 - $150,000 per year to break even. I think this drops down fairly quickly as you move down the ranking.&nbsp;</p>\n<p class=\"p2\">Note that a crucial consideration is how these figures will evolve over time. <strong>My very speculative guess is that we\u2019ll be talent constrained among people who are autonomous and high ability or can act as leader figures for the long-term; whereas we may end up pretty much not funding constrained</strong> (that\u2019s the position that GiveWell seems to be in). That speaks in favour of not earning to give.</p>\n<p class=\"p2\">We\u2019re happy to talk this through with individual people who are wondering whether to work for us or earn to give.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4 id=\"What_are_some_general_guidelines_for_the_kind_of_person_who_is_likely_to_be_highly_irreplaceable_at_80k__\">What are some general guidelines for the kind of person who is likely to be highly irreplaceable at 80k?&nbsp;</h4>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\"><strong>Ben</strong>: You need to have <strong>strong analytical skills</strong>: most people who work for us have a degree in a tough analytical or scientific subject (e.g. maths, physics, economics, philosophy), but if you have work experience to demonstrate this that\u2019s good too. You should be able to <strong>manage lots of projects and keep control of lots of tasks at once</strong>, and it\u2019s really important that you\u2019re able to motivate yourself easily and <strong>work with a high degree of autonomy</strong>. Ideally, we\u2019re able to just hand people a portion of what needs doing in one meeting per week and trust them to get things done using appropriate priorities. Because we\u2019re working on some pretty ambitious projects, we look for people who can <strong>persist at a tough goal over a long period</strong> even in the face of setbacks.</p>\n<p class=\"p2\">We want people who can not only fit into our team easily, but <strong>take on a leadership role and be persuasive and engaging</strong> in their communications with others. We\u2019ll assess this in an interview and also look for past management and leadership experience. Of course, <strong>it\u2019s also crucial that you really care about making the world a better place, and doing so in a rigorous, evidence-based way</strong>. We put high weight on <strong>being able to look at things in a rational way and be open to and responsive to new evidence</strong>. Ideally you\u2019d also have knowledge of our previous content and that of related organisations like GiveWell. Previous involvement with effective altruist organisations or activities is a plus here, but not necessary.</p>\n<p class=\"p2\">In terms of the skills and experience we\u2019re looking for more specifically: leadership experience, research skills, knowledge of economics, organisational/personality psychology, and social sciences more generally are all highly valued. Experience in coaching or something relevant is an advantage, and we\u2019re also interested in people with communications, branding, marketing, design or fundraising backgrounds. Generally we don\u2019t expect anyone to have years and years of experience, but our perfect hire would be someone with at least a couple of years work experience, rather than someone just graduated.</p>\n<p class=\"p2\">If you want to be really certain of how valuable you\u2019d be at 80k, it\u2019s best to do an internship with us and find out. And of course the best indicator of where you\u2019d rank in our recruitment process is just to apply.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4 id=\"How_good_is_working_at_80k_for_personal_development_and_future_job_prospects__especially_compared_to_more_professional_jobs_\">How good is working at 80k for personal development and future job prospects, especially compared to more professional jobs?</h4>\n<p>&nbsp;</p>\n<p class=\"p2\"><strong>Ben</strong>: Because 80k is a small and growing organisation, <strong>you\u2019ll be able to develop high level skills such as management and gain responsibility more quickly than in almost any job</strong>. We see this as a real advantage of working for 80k, as even in high status professional jobs like consulting and finance, it\u2019s hard to get this kind of experience for quite a few years. There\u2019s also a high level of autonomy and flexibility working at 80k, making it relatively easy to fit other learning and skill development around the job: we set aside around 10% of our employees\u2019 time for generally building highly transferable skills. There\u2019s a strong culture of personal development. A lot of people here are interested in quantified self and personal productivity.&nbsp;</p>\n<p class=\"p2\"><strong>The networking opportunities are pretty good: for example,&nbsp;</strong><strong>we currently share our offices with the&nbsp;<a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>&nbsp;and the&nbsp;<a href=\"http://www.practicalethics.ox.ac.uk/\">Uehiro Centre for Practical Ethics</a>&nbsp;in central Oxford. More generally,&nbsp;</strong><strong>working for 80k is obviously a great way to interact with a number of people in the effective altruism community</strong>.&nbsp;The people we interact with through coaching tend to have impressive backgrounds in a variety of industries (including entrepreneurship, finance, tech, research etc.) And since you\u2019re able to get lots of responsibility quickly here you get to interact with high level people early on because you\u2019ll be externally representing 80k: going to conferences on our behalf, for example.</p>\n<p class=\"p2\">In terms of general credentials (\u201cCV points\u201d), working for 80k might be a bit weaker than some immediately available alternatives - it\u2019s acknowledged within the effective altruism community but it doesn\u2019t look as good on your CV as McKinsey or Goldman Sachs or Google, obviously. But again, because we\u2019re growing fast and you\u2019ll get large amounts of responsibility this means there\u2019s an opportunity to get some impressive achievements attributed to you if you seek them out. Examples might be running major promotional campaigns or managing a whole team of researchers.</p>\n<p class=\"p2\">In addition, 80k is part of CEA, which is also growing quickly, and is moving towards becoming an incubator of effective altruist projects. This means there will be plenty of other opportunities within CEA that you could be promoted into, even if you don\u2019t stick within 80k itself.</p>\n<p class=\"p2\"><strong>Compared to more professional jobs like consulting or finance, working for 80k is probably higher risk in terms of future opportunities, because it depends how good your achievements end up being</strong>. It\u2019s easier to impress a wider range of people with McKinsey than with 80k. Consulting may well be a more reliable and widely applicable to get what we call \u201ccareer capital\u201d for many people. Especially if you\u2019ve got a strong desire to get into a specific area (you want to go into finance say, or programming) then it might be better to develop more targeted skills and experience by working in these industries. We\u2019re happy to talk this through with people with specific alternative opportunities. We\u2019ve done this in the past with people and sometimes do recommend that they\u2019re better working elsewhere for personal development, other times working with us does seem to provide a lot of opportunity.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4 id=\"What_about_more_generally_working_for_80k_compared_to_working_for_other_organisations___effective_altruist_or_otherwise__Who_shouldn_t_work_for_80k_and_would_probably_be_better_off_elsewhere_\">What about more generally working for 80k compared to working for other organisations - effective altruist or otherwise? Who shouldn\u2019t work for 80k and would probably be better off elsewhere?</h4>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\"><strong>Ben</strong>: A useful way of thinking about this in general is to start by estimating how you rank relative to other applicants at 80k, and how you rank relative to others at the alternative organisation. Roughly speaking you should then go for where your ranking is highest, adjusted by how high impact you expect the organisations to be.&nbsp;</p>\n<p class=\"p2\">Some more specific considerations:</p>\n<p class=\"p2\"><strong>The main reason you\u2019d probably be better off working for somewhere other than 80k is if you think what we\u2019re doing has very little chance of paying off, even with a lot of iterating</strong>. If you\u2019re generally sceptical about meta-charity, or just think there\u2019s another project that\u2019s much better, then you\u2019ll probably want to work elsewhere! One quick thing I\u2019d say in favour of 80k here is that we have the advantage and flexibility of being cause-neutral. This means that if you\u2019re uncertain about which cause is the most important, working at 80k might be advantageous as it means you don\u2019t have to specify, and when we get more information about what the best cause is, we\u2019ve got a multiplier on this by helping direct more people in that direction.&nbsp;</p>\n<p class=\"p2\">The other side of this, of course, is that i<strong>f you already think one cause is much more important than anything else, you\u2019re probably better off working for an organisation specifically focused on that cause than for 80k</strong>. For instance, if you\u2019re more certain that AI xrisk is the top cause, then you might want to work for <a href=\"http://intelligence.org/\">MIRI </a>or <a href=\"http://www.fhi.ox.ac.uk/\">FHI</a>, or if you think it's global poverty, then working at our sister organisation, <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a>, may be better. You might also argue that whilst 80k provides flexibility, earning to give offers even more: if you get new information which suggests you\u2019re actually more replaceable than you thought you can\u2019t just switch jobs, whereas you can always switch your donations when you learn more. But there\u2019s a lot of uncertainty around the question of whether talent gaps are in general more pressing than funding gaps, and how this will change over time.</p>\n<p class=\"p2\"><strong>Even if you\u2019re convinced that meta-charity is high impact, working for <a href=\"http://www.givewell.org/\">GiveWell </a>might plausibly be a safer bet, especially if you want to work somewhere that has more of a track record</strong>. GiveWell is highly respected, thought of as very well run, is doubling its money moved each year and has partnered with Good Ventures (which has $3bn behind it). You have the chance to receive a rigorous training in a useful skill-set. &nbsp;I think one advantage 80k has over GiveWell is having more innovation value. Right now, we\u2019re doing something completely new: working to identify the biggest talent gaps in the world and help people take them. We\u2019re also smaller and less established, so on that basis I\u2019d expect there to be more opportunity to take a leadership role and shape the development of the organisation (I haven\u2019t asked GiveWell about this though). Finally, GiveWell seems to be emphasising recruiting research orientated folks, whereas we\u2019re also recruiting for people to work on communications, coaching, fundraising and more.</p>\n<p class=\"p2\">As I mentioned before, <strong>if you\u2019re after a specific career area long term (e.g. development policy, politics, academia), then it\u2019s probably best to go straight into it and start building up your network in that area</strong>. We also can\u2019t offer as much formal training as an established company, though we offer lots of other personal development opportunities. More likely, you\u2019re uncertain about where you want to go long-term, in which case it might be best to try us (and other opportunities) out. Most people can spare 6-12 months without closing down their options in other fields.</p>\n<p class=\"p2\">&nbsp;</p>\n<h4 id=\"What_roles_might_you_take_\">What roles might you take?</h4>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\"><strong>Ben</strong>: Our priorities for the next 6 months include (i) <strong>doing high quality <a href=\"http://80000hours.org/blog/232-we-re-changing-our-career-coaching\">case studies</a></strong> (ii) <strong>writing up our core findings as blog posts</strong> (iii) <strong>rebranding the website</strong> (iv) <strong>fundraising</strong> (v) <strong>evaluating our impact</strong> (vi) <strong>seeking media coverage</strong>. After that, it\u2019s difficult to say, but it\u2019ll be some mixture of doing research, operations and promoting that research.</p>\n<p class=\"p2\">We aim to keep roles flexible, and adapt them to your skill set and interests. Some people end up as researchers, others focus on promotion, and so on. A typical week involves some meetings with the team, the people you manage and your manager, reading articles and writing up research reports, having meetings with our coachees, talking to external organisations, email, and preparing strategy. You can find out more <a href=\"http://80000hours.org/blog/236-80-000-hours-is-hiring\">here</a>.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\">You might also want to check out these other effective altruist organisations currently recruiting: <a href=\"http://givingwhatwecan.org/blog/2013-07-22/were-hiring\">Giving What We Can</a> and <a href=\"http://effectiveanimalactivism.org/about/get-involved\">Effective Animal Activism</a>. Our umbrella organisation, the Centre for Effective Altruism, is also recruiting for a <a href=\"http://effective-altruism.com/cea-hiring\">Research Fellow in Global Prioritisation</a>.</p>\n<address><span id=\"docs-internal-guid-1c1a1251-5e1f-e432-3c37-a08599c945af\"><br><br><br><br><br></span></address>", "sections": [{"title": "The purpose of this post is to discuss some considerations relevant to whether it is high impact for you as an individual to work for 80,000 Hours.", "anchor": "The_purpose_of_this_post_is_to_discuss_some_considerations_relevant_to_whether_it_is_high_impact_for_you_as_an_individual_to_work_for_80_000_Hours_", "level": 3}, {"title": "Summary\u00a0", "anchor": "Summary_", "level": 1}, {"title": "A big question for effective altruists is whether it\u2019s higher impact to work for an organisation directly or to fund them to employ multiple people in your place. How can someone tell whether it would be better to work for 80k or to fund 80k?", "anchor": "A_big_question_for_effective_altruists_is_whether_it_s_higher_impact_to_work_for_an_organisation_directly_or_to_fund_them_to_employ_multiple_people_in_your_place__How_can_someone_tell_whether_it_would_be_better_to_work_for_80k_or_to_fund_80k_", "level": 2}, {"title": "What are some general guidelines for the kind of person who is likely to be highly irreplaceable at 80k?\u00a0", "anchor": "What_are_some_general_guidelines_for_the_kind_of_person_who_is_likely_to_be_highly_irreplaceable_at_80k__", "level": 2}, {"title": "How good is working at 80k for personal development and future job prospects, especially compared to more professional jobs?", "anchor": "How_good_is_working_at_80k_for_personal_development_and_future_job_prospects__especially_compared_to_more_professional_jobs_", "level": 2}, {"title": "What about more generally working for 80k compared to working for other organisations - effective altruist or otherwise? Who shouldn\u2019t work for 80k and would probably be better off elsewhere?", "anchor": "What_about_more_generally_working_for_80k_compared_to_working_for_other_organisations___effective_altruist_or_otherwise__Who_shouldn_t_work_for_80k_and_would_probably_be_better_off_elsewhere_", "level": 2}, {"title": "What roles might you take?", "anchor": "What_roles_might_you_take_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3Ss29ihXsBb8tuoxK", "FCiMtrsM8mcmBtfTR", "KBDiWMqhaYe7uPHvN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-08T14:08:17.043Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight", "slug": "meetup-london-social-the-unwelcome-but-probable-decline-and", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3fHmtFKNMpHbedQH8/meetup-london-social-the-unwelcome-but-probable-decline-and", "pageUrlRelative": "/posts/3fHmtFKNMpHbedQH8/meetup-london-social-the-unwelcome-but-probable-decline-and", "linkUrl": "https://www.lesswrong.com/posts/3fHmtFKNMpHbedQH8/meetup-london-social-the-unwelcome-but-probable-decline-and", "postedAtFormatted": "Thursday, August 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20-%20The%20Unwelcome%20but%20Probable%20Decline%20and%20Fall%20of%20Direct%20Sunlight&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20-%20The%20Unwelcome%20but%20Probable%20Decline%20and%20Fall%20of%20Direct%20Sunlight%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3fHmtFKNMpHbedQH8%2Fmeetup-london-social-the-unwelcome-but-probable-decline-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20-%20The%20Unwelcome%20but%20Probable%20Decline%20and%20Fall%20of%20Direct%20Sunlight%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3fHmtFKNMpHbedQH8%2Fmeetup-london-social-the-unwelcome-but-probable-decline-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3fHmtFKNMpHbedQH8%2Fmeetup-london-social-the-unwelcome-but-probable-decline-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pq'>London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 August 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">St James Park, London, SW1H</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our next social meetup will be on Sunday 18th August. Its measure will be split mostly between: a day of nice weather spent in St James' Park, approximately <a href=\"https://maps.google.co.uk/maps?q=51.501705,+-0.134730\" rel=\"nofollow\">here</a>; and a wet or cold day spent in <a href=\"https://www.google.co.uk/maps?q=51.499888,-0.133722\" rel=\"nofollow\">The Old Star</a>.</p>\n\n<p>I intend to update by 1pm on the day, to report on which branch of the wavefunction we find ourselves in, but if you have difficulty finding us, my number is 0779 200 9646.</p>\n\n<p>Please join us, and feel free to bring friends, enemies, and long-lost relations.</p>\n\n<p><strong>Update</strong>: Looks like we have warm and reasonably-sunny, so we'll head to the park.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pq'>London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3fHmtFKNMpHbedQH8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "23689", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social___The_Unwelcome_but_Probable_Decline_and_Fall_of_Direct_Sunlight\">Discussion article for the meetup : <a href=\"/meetups/pq\">London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 August 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">St James Park, London, SW1H</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our next social meetup will be on Sunday 18th August. Its measure will be split mostly between: a day of nice weather spent in St James' Park, approximately <a href=\"https://maps.google.co.uk/maps?q=51.501705,+-0.134730\" rel=\"nofollow\">here</a>; and a wet or cold day spent in <a href=\"https://www.google.co.uk/maps?q=51.499888,-0.133722\" rel=\"nofollow\">The Old Star</a>.</p>\n\n<p>I intend to update by 1pm on the day, to report on which branch of the wavefunction we find ourselves in, but if you have difficulty finding us, my number is 0779 200 9646.</p>\n\n<p>Please join us, and feel free to bring friends, enemies, and long-lost relations.</p>\n\n<p><strong>Update</strong>: Looks like we have warm and reasonably-sunny, so we'll head to the park.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social___The_Unwelcome_but_Probable_Decline_and_Fall_of_Direct_Sunlight1\">Discussion article for the meetup : <a href=\"/meetups/pq\">London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight", "anchor": "Discussion_article_for_the_meetup___London_Social___The_Unwelcome_but_Probable_Decline_and_Fall_of_Direct_Sunlight", "level": 1}, {"title": "Discussion article for the meetup : London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight", "anchor": "Discussion_article_for_the_meetup___London_Social___The_Unwelcome_but_Probable_Decline_and_Fall_of_Direct_Sunlight1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-08T19:28:16.758Z", "modifiedAt": null, "url": null, "title": "New article on in vitro iterated embryo selection", "slug": "new-article-on-in-vitro-iterated-embryo-selection", "viewCount": null, "lastCommentedAt": "2019-01-18T23:12:10.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9DwbvEr2thuueG2yK/new-article-on-in-vitro-iterated-embryo-selection", "pageUrlRelative": "/posts/9DwbvEr2thuueG2yK/new-article-on-in-vitro-iterated-embryo-selection", "linkUrl": "https://www.lesswrong.com/posts/9DwbvEr2thuueG2yK/new-article-on-in-vitro-iterated-embryo-selection", "postedAtFormatted": "Thursday, August 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20article%20on%20in%20vitro%20iterated%20embryo%20selection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20article%20on%20in%20vitro%20iterated%20embryo%20selection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DwbvEr2thuueG2yK%2Fnew-article-on-in-vitro-iterated-embryo-selection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20article%20on%20in%20vitro%20iterated%20embryo%20selection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DwbvEr2thuueG2yK%2Fnew-article-on-in-vitro-iterated-embryo-selection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DwbvEr2thuueG2yK%2Fnew-article-on-in-vitro-iterated-embryo-selection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>The <a href=\"http://jme.bmj.com/content/early/2013/02/13/medethics-2012-101200.full?sid=e04fe105-6117-4c50-8902-0bbc6891dc30\">article</a>&nbsp;by Robert Sparrow:</p>\n<blockquote>\n<p id=\"p-1\" style=\"margin: 2px 0px 15px; padding: 0px; border: 0px; outline: 0px; font-size: 1.2em; font-family: arial, sans-serif; line-height: 1.4em; vertical-align: baseline; background-color: transparent; color: #333333;\">A series of recent scientific results suggest that, in the not-too-distant future, it will be possible to create viable human gametes from human stem cells. This paper discusses the potential of this technology to make possible what I call &lsquo;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 1em; font-family: inherit; line-height: inherit; text-align: inherit; vertical-align: baseline; background-color: transparent;\">in vitro</em>&nbsp;eugenics&rsquo;: the deliberate breeding of human beings&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 1em; font-family: inherit; line-height: inherit; text-align: inherit; vertical-align: baseline; background-color: transparent;\">in vitro</em>&nbsp;by fusing sperm and egg derived from different stem-cell lines to create an embryo and then deriving new gametes from stem cells derived from that embryo. Repeated iterations of this process would allow scientists to proceed through multiple human generations in the laboratory.&nbsp;<em style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; font-size: 1em; font-family: inherit; line-height: inherit; text-align: inherit; vertical-align: baseline; background-color: transparent;\">In vitro</em>&nbsp;eugenics might be used to study the heredity of genetic disorders and to produce cell lines of a desired character for medical applications. More controversially, it might also function as a powerful technology of &lsquo;human enhancement&rsquo; by allowing researchers to use all the techniques of selective breeding to produce individuals with a desired genotype.</p>\n</blockquote>\n<p style=\"margin: 2px 0px 15px; padding: 0px; border: 0px; outline: 0px; font-size: 1.2em; font-family: arial, sans-serif; line-height: 1.4em; vertical-align: baseline; background-color: transparent; color: #333333;\">Quote:</p>\n<blockquote>\n<p style=\"margin: 2px 0px 15px; padding: 0px; border: 0px; outline: 0px; font-size: 1.2em; font-family: arial, sans-serif; line-height: 1.4em; vertical-align: baseline; background-color: transparent; color: #333333;\"><span style=\"font-size: 12px; line-height: 16.796875px;\">However, to my knowledge, the current paper is the first to explicitly discuss the possibility of the iterative use of this technology for reproductive purposes</span></p>\n</blockquote>\n<p style=\"margin: 2px 0px 15px; padding: 0px; border: 0px; outline: 0px; font-size: 1.2em; font-family: arial, sans-serif; line-height: 1.4em; vertical-align: baseline; background-color: transparent; color: #333333;\"><span style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">The possibility was <a href=\"http://theuncertainfuture.com/faq.html#7\">discussed</a></span><span style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">&nbsp;in MIRI's \"Uncertain Future\" toy forecasting model back in 2009, and the analysis formulated a few years before that.<br /><br />ETA: And further discussed in James Miller's recent book, \"Singularity Rising.\"</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9DwbvEr2thuueG2yK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 1.2953908109793122e-06, "legacy": true, "legacyId": "23691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-08T19:33:07.461Z", "modifiedAt": null, "url": null, "title": "[LINK] Bitcoins, \"Investment of Money,\" and Law", "slug": "link-bitcoins-investment-of-money-and-law", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:31.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimS", "createdAt": "2011-10-11T12:16:35.235Z", "isAdmin": false, "displayName": "TimS"}, "userId": "pewD8vNSS3LGCvE4t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vP7uNqhETwFn5Xjh2/link-bitcoins-investment-of-money-and-law", "pageUrlRelative": "/posts/vP7uNqhETwFn5Xjh2/link-bitcoins-investment-of-money-and-law", "linkUrl": "https://www.lesswrong.com/posts/vP7uNqhETwFn5Xjh2/link-bitcoins-investment-of-money-and-law", "postedAtFormatted": "Thursday, August 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Bitcoins%2C%20%22Investment%20of%20Money%2C%22%20and%20Law&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Bitcoins%2C%20%22Investment%20of%20Money%2C%22%20and%20Law%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvP7uNqhETwFn5Xjh2%2Flink-bitcoins-investment-of-money-and-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Bitcoins%2C%20%22Investment%20of%20Money%2C%22%20and%20Law%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvP7uNqhETwFn5Xjh2%2Flink-bitcoins-investment-of-money-and-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvP7uNqhETwFn5Xjh2%2Flink-bitcoins-investment-of-money-and-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>A federal magistrate judge in Texas issue a <a href=\"http://ia800904.us.archive.org/35/items/gov.uscourts.txed.146063/gov.uscourts.txed.146063.23.0.pdf\">decision</a> on Tuesday that \"Bitcoin is a currency or form of money, and investors wishing to invest in BTCST provided an investment of money.\" More discussion on the legal reasoning is <a href=\"http://www.volokh.com/2013/08/08/bitcoin-is-a-currency-or-form-of-money-for-purposes-of-american-securities-laws/#disqus_thread\">here</a>.&nbsp;</p>\n<p>Some context from reading the decision: A promoter offered a profit if investors gave him Bitcoins. The Securities and Exchange Commission asserts that the promoter made false or misleading statements as part of this scheme (which is illegal).&nbsp;</p>\n<p>The promoter asserted the scheme was not within the authority of the SEC because it was not a \"security.\" (Everyone agrees that the SEC can only regulate securities). To the surprise of (hopefully) no one, the judge ruled that the promoter's conduct was covered by the SEC. Specifically, the court found that the scheme was an \"investment contract.\" One of the elements of an investment contract is that there is an \"investment of money.\"&nbsp;</p>\n<p>Thus, the case is potentially interesting to BitCoiners because of the judge's reasoning about why Bitcoins can be characterized as an investment of money.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vP7uNqhETwFn5Xjh2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "23692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-08T21:48:24.987Z", "modifiedAt": null, "url": null, "title": "Random variables and Evidential Decision Theory", "slug": "random-variables-and-evidential-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:04.237Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "twanvl", "createdAt": "2009-07-23T14:08:02.633Z", "isAdmin": false, "displayName": "twanvl"}, "userId": "tSciHLMSqaRcuX4ym", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PPBC6mKBJdd68Mfv9/random-variables-and-evidential-decision-theory", "pageUrlRelative": "/posts/PPBC6mKBJdd68Mfv9/random-variables-and-evidential-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/PPBC6mKBJdd68Mfv9/random-variables-and-evidential-decision-theory", "postedAtFormatted": "Thursday, August 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Random%20variables%20and%20Evidential%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARandom%20variables%20and%20Evidential%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPBC6mKBJdd68Mfv9%2Frandom-variables-and-evidential-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Random%20variables%20and%20Evidential%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPBC6mKBJdd68Mfv9%2Frandom-variables-and-evidential-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPBC6mKBJdd68Mfv9%2Frandom-variables-and-evidential-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 701, "htmlBody": "<p>This post is inspired by the <a href=\"/lw/hwq/evidential_decision_theory_selection_bias_and/9c8u\">recent discussion</a> I had with <a href=\"/user/IlyaShpitser/overview\">IlyaShpitser</a> and <a href=\"/user/Vaniver/overview\">Vaniver</a> on EDT.</p>\n<h2>A random variable only ever has one value</h2>\n<p>In probability theory, statistics and so on, we often use the notion of a random variable (RV). If you go look at <a href=\"http://en.wikipedia.org/wiki/Random_variable#Definition\">the definition</a>, you will see that a RV is a function of the sample space. What that means is that a RV assigns a value to each possible outcome of a system. In reality, where there are no closed systems, this means that a RV assigns a value to each possible <em>universe</em>.</p>\n<p>For example, a random variable <tt>X</tt> representing the outcome a die roll is a function of type \"Universe &rarr; {1..6}\". The value of <tt>X</tt> in a particular universe <tt>u</tt> is then <tt>X(u)</tt>. Uncertainty in <tt>X</tt> corresponds to uncertainty about the universe we are in. Since <tt>X</tt> is a pure mathematical function, its value is fixed for each input. That means that in a fixed universe, say our universe, such a random variable only ever takes on one value.</p>\n<p>So, before the die roll, the value of <tt>X</tt> is undefined<a name=\"unnote1\" href=\"#footnote1\"><sup>1</sup></a>, and after the roll <tt>X</tt> is forever fixed. <tt>X</tt> is the outcome of a certain particular roll. If I roll the same die again, that doesn't change the value of <tt>X</tt>. If you want to talk about multiple rolls, you have to use different variables. The usual solution is to use indices, <tt>X<sub>1</sub></tt>, <tt>X<sub>2</sub></tt>, etc.</p>\n<p>This also means that the nodes in a causal model, are <em>not</em> random variables. For example in the causal model \"Smoking &rarr; Cancer\", there is no single RV for smoking. Rather, the model is implicitly a generalized to mean \"Smoking<sub>i</sub> &rarr; Cancer<sub>i</sub>\" for all persons i.</p>\n<h2>What this means for EDT</h2>\n<p>It is sometimes claimed that <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">Evidential Decision Theory</a> (EDT) can not deal with causal structure. But I would disagree. To avoid confusion, I will refer to my interpretation as Estimated Evidential Decision Theory (EEDT).</p>\n<p>Decision theories such as (E)EDT rely on the following formula to make decisions:<br /> <img src=\"http://www.codecogs.com/png.latex?V(a) = \\sum\\limits_{j} P(O=o_j\\mid a) U(o_j),\" alt=\"V(a) = \\sum_j P(O=o_j | a) U(o_j),\" /><br /> where <tt>o<sub>j</sub></tt> are the possible outcomes, <tt>U(o<sub>j</sub>)</tt> is the utility of an outcome, <tt>O</tt> is a random variable that represents the actual outcome, and <tt>a</tt> is an action. The (E)EDT policy is to take the action that maximizes <tt>V(a)</tt>, the value of that action.</p>\n<p>How would you evaluate this formula in practice? To do that, you need to know <tt>P(O=o<sub>j</sub> | a)</tt>. I.e. the probability of a certain outcome given that you take a certain action. But keep in mind the previous section! There is only one random variable <tt>O</tt>, which is the outcome of <em>this</em> action. Without assuming some prior knowledge, <tt>O</tt> is unrelated to the outcome of other similar actions in similar situations.</p>\n<p>At the time an agent has to decide what action <tt>a</tt> to take, the action has not happened yet, and the outcome is not yet known to him. This means that the agent has no observations of <tt>O</tt>. The agent therefore has to estimate <tt>P(O=o<sub>j</sub>|a)</tt> by using only his prior knowledge. How this estimation is done exactly is <em>not</em> specified by EEDT. If the agent wants to use a causal model, he is perfectly free to do so!</p>\n<p>You might argue that by not specifying how the conditional probabilities <tt>P(O=o<sub>j</sub>|a)</tt> are calculated, I have taken out the interesting part of the decision theory. With the right choice of estimation procedure, EEDT can describe CDT, normal/naive EDT, and even UDT<a name=\"unnote2\" href=\"#footnote2\"><sup>2</sup></a>. But EEDT is not so general as to be completely useless. What it does give you is a way to reduce the problem of making decisions to that of estimating conditional probabilities.</p>\n<hr />\n<h3>Footnotes</h3>\n<p><a name=\"footnote1\" href=\"#unnote1\">1</a>. Technically, 'undefined' is not in the domain of <tt>X</tt>. What I mean is that <tt>X</tt> is a partial function of universes, or a function only of universes in which the die has been rolled.</p>\n<p><a name=\"footnote2\" href=\"#unnote2\">2</a>. To get CDT, assume there is a causal model for <tt>A -&gt; O</tt>, and use that to estimate <tt>P(O=o<sub>j</sub> | do A=a)</tt>. To get naive EDT, estimate the probabilities from data without taking causality or confounders into account. To get UDT, model <tt>A</tt> as being the choice of all sufficiently similar agents, not just yourself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PPBC6mKBJdd68Mfv9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -2, "extendedScore": null, "score": 1.2955071855522221e-06, "legacy": true, "legacyId": "23690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post is inspired by the <a href=\"/lw/hwq/evidential_decision_theory_selection_bias_and/9c8u\">recent discussion</a> I had with <a href=\"/user/IlyaShpitser/overview\">IlyaShpitser</a> and <a href=\"/user/Vaniver/overview\">Vaniver</a> on EDT.</p>\n<h2 id=\"A_random_variable_only_ever_has_one_value\">A random variable only ever has one value</h2>\n<p>In probability theory, statistics and so on, we often use the notion of a random variable (RV). If you go look at <a href=\"http://en.wikipedia.org/wiki/Random_variable#Definition\">the definition</a>, you will see that a RV is a function of the sample space. What that means is that a RV assigns a value to each possible outcome of a system. In reality, where there are no closed systems, this means that a RV assigns a value to each possible <em>universe</em>.</p>\n<p>For example, a random variable <tt>X</tt> representing the outcome a die roll is a function of type \"Universe \u2192 {1..6}\". The value of <tt>X</tt> in a particular universe <tt>u</tt> is then <tt>X(u)</tt>. Uncertainty in <tt>X</tt> corresponds to uncertainty about the universe we are in. Since <tt>X</tt> is a pure mathematical function, its value is fixed for each input. That means that in a fixed universe, say our universe, such a random variable only ever takes on one value.</p>\n<p>So, before the die roll, the value of <tt>X</tt> is undefined<a name=\"unnote1\" href=\"#footnote1\"><sup>1</sup></a>, and after the roll <tt>X</tt> is forever fixed. <tt>X</tt> is the outcome of a certain particular roll. If I roll the same die again, that doesn't change the value of <tt>X</tt>. If you want to talk about multiple rolls, you have to use different variables. The usual solution is to use indices, <tt>X<sub>1</sub></tt>, <tt>X<sub>2</sub></tt>, etc.</p>\n<p>This also means that the nodes in a causal model, are <em>not</em> random variables. For example in the causal model \"Smoking \u2192 Cancer\", there is no single RV for smoking. Rather, the model is implicitly a generalized to mean \"Smoking<sub>i</sub> \u2192 Cancer<sub>i</sub>\" for all persons i.</p>\n<h2 id=\"What_this_means_for_EDT\">What this means for EDT</h2>\n<p>It is sometimes claimed that <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">Evidential Decision Theory</a> (EDT) can not deal with causal structure. But I would disagree. To avoid confusion, I will refer to my interpretation as Estimated Evidential Decision Theory (EEDT).</p>\n<p>Decision theories such as (E)EDT rely on the following formula to make decisions:<br> <img src=\"http://www.codecogs.com/png.latex?V(a) = \\sum\\limits_{j} P(O=o_j\\mid a) U(o_j),\" alt=\"V(a) = \\sum_j P(O=o_j | a) U(o_j),\"><br> where <tt>o<sub>j</sub></tt> are the possible outcomes, <tt>U(o<sub>j</sub>)</tt> is the utility of an outcome, <tt>O</tt> is a random variable that represents the actual outcome, and <tt>a</tt> is an action. The (E)EDT policy is to take the action that maximizes <tt>V(a)</tt>, the value of that action.</p>\n<p>How would you evaluate this formula in practice? To do that, you need to know <tt>P(O=o<sub>j</sub> | a)</tt>. I.e. the probability of a certain outcome given that you take a certain action. But keep in mind the previous section! There is only one random variable <tt>O</tt>, which is the outcome of <em>this</em> action. Without assuming some prior knowledge, <tt>O</tt> is unrelated to the outcome of other similar actions in similar situations.</p>\n<p>At the time an agent has to decide what action <tt>a</tt> to take, the action has not happened yet, and the outcome is not yet known to him. This means that the agent has no observations of <tt>O</tt>. The agent therefore has to estimate <tt>P(O=o<sub>j</sub>|a)</tt> by using only his prior knowledge. How this estimation is done exactly is <em>not</em> specified by EEDT. If the agent wants to use a causal model, he is perfectly free to do so!</p>\n<p>You might argue that by not specifying how the conditional probabilities <tt>P(O=o<sub>j</sub>|a)</tt> are calculated, I have taken out the interesting part of the decision theory. With the right choice of estimation procedure, EEDT can describe CDT, normal/naive EDT, and even UDT<a name=\"unnote2\" href=\"#footnote2\"><sup>2</sup></a>. But EEDT is not so general as to be completely useless. What it does give you is a way to reduce the problem of making decisions to that of estimating conditional probabilities.</p>\n<hr>\n<h3 id=\"Footnotes\">Footnotes</h3>\n<p><a name=\"footnote1\" href=\"#unnote1\">1</a>. Technically, 'undefined' is not in the domain of <tt>X</tt>. What I mean is that <tt>X</tt> is a partial function of universes, or a function only of universes in which the die has been rolled.</p>\n<p><a name=\"footnote2\" href=\"#unnote2\">2</a>. To get CDT, assume there is a causal model for <tt>A -&gt; O</tt>, and use that to estimate <tt>P(O=o<sub>j</sub> | do A=a)</tt>. To get naive EDT, estimate the probabilities from data without taking causality or confounders into account. To get UDT, model <tt>A</tt> as being the choice of all sufficiently similar agents, not just yourself.</p>", "sections": [{"title": "A random variable only ever has one value", "anchor": "A_random_variable_only_ever_has_one_value", "level": 1}, {"title": "What this means for EDT", "anchor": "What_this_means_for_EDT", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "37 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-09T03:19:58.218Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta LessWrong: Games Night", "slug": "meetup-atlanta-lesswrong-games-night", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BuJMX3vo6iGXTwPBZ/meetup-atlanta-lesswrong-games-night", "pageUrlRelative": "/posts/BuJMX3vo6iGXTwPBZ/meetup-atlanta-lesswrong-games-night", "linkUrl": "https://www.lesswrong.com/posts/BuJMX3vo6iGXTwPBZ/meetup-atlanta-lesswrong-games-night", "postedAtFormatted": "Friday, August 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20LessWrong%3A%20Games%20Night&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20LessWrong%3A%20Games%20Night%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBuJMX3vo6iGXTwPBZ%2Fmeetup-atlanta-lesswrong-games-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20LessWrong%3A%20Games%20Night%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBuJMX3vo6iGXTwPBZ%2Fmeetup-atlanta-lesswrong-games-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBuJMX3vo6iGXTwPBZ%2Fmeetup-atlanta-lesswrong-games-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pr'>Atlanta LessWrong: Games Night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 August 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come be social and enjoy the fun and games with your fellow rationalists!</p>\n\n<p>If you've not yet attended, this is an excellent time to come to the ATLesswrong for low-stress introductions and high-fun getting to know everyone.</p>\n\n<p>We'll get to know each other, have snacks, play games (some may have to do with rationality, but only tangentially, we swear), and have weird discussions on everything from how exactly you do an EEG on a dolphin, to what your favorite type of pie is and why.</p>\n\n<p>Bring your favorite games! Bring your favorite snacks and drinks! Bring your swimsuit, in case part of the party moves to the pool! (But you are in no way required to swim!)</p>\n\n<p>Those of us who are up-to-date on HPMOR are certain to grab a corner and promote our pet theories while gesturing erratically at the ceiling. The rest of us who are not up-to-date are certain to frantically yell \"no spoilers!\" while gesturing erratically at that corner of the room.</p>\n\n<p>Come one, come all. Bring your friend who likes games but who has never heard of utilitarianism or epistemology. Bring kittens you need to get adopted and force them on unsuspecting guests. Come hang out!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pr'>Atlanta LessWrong: Games Night</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BuJMX3vo6iGXTwPBZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.2957825926700438e-06, "legacy": true, "legacyId": "23694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_LessWrong__Games_Night\">Discussion article for the meetup : <a href=\"/meetups/pr\">Atlanta LessWrong: Games Night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 August 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come be social and enjoy the fun and games with your fellow rationalists!</p>\n\n<p>If you've not yet attended, this is an excellent time to come to the ATLesswrong for low-stress introductions and high-fun getting to know everyone.</p>\n\n<p>We'll get to know each other, have snacks, play games (some may have to do with rationality, but only tangentially, we swear), and have weird discussions on everything from how exactly you do an EEG on a dolphin, to what your favorite type of pie is and why.</p>\n\n<p>Bring your favorite games! Bring your favorite snacks and drinks! Bring your swimsuit, in case part of the party moves to the pool! (But you are in no way required to swim!)</p>\n\n<p>Those of us who are up-to-date on HPMOR are certain to grab a corner and promote our pet theories while gesturing erratically at the ceiling. The rest of us who are not up-to-date are certain to frantically yell \"no spoilers!\" while gesturing erratically at that corner of the room.</p>\n\n<p>Come one, come all. Bring your friend who likes games but who has never heard of utilitarianism or epistemology. Bring kittens you need to get adopted and force them on unsuspecting guests. Come hang out!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_LessWrong__Games_Night1\">Discussion article for the meetup : <a href=\"/meetups/pr\">Atlanta LessWrong: Games Night</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta LessWrong: Games Night", "anchor": "Discussion_article_for_the_meetup___Atlanta_LessWrong__Games_Night", "level": 1}, {"title": "Discussion article for the meetup : Atlanta LessWrong: Games Night", "anchor": "Discussion_article_for_the_meetup___Atlanta_LessWrong__Games_Night1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-09T06:07:27.370Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup\u2014Confess Your Unpopular Opinion", "slug": "meetup-west-la-meetup-confess-your-unpopular-opinion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PLZ6xuwkZJi3sPuwj/meetup-west-la-meetup-confess-your-unpopular-opinion", "pageUrlRelative": "/posts/PLZ6xuwkZJi3sPuwj/meetup-west-la-meetup-confess-your-unpopular-opinion", "linkUrl": "https://www.lesswrong.com/posts/PLZ6xuwkZJi3sPuwj/meetup-west-la-meetup-confess-your-unpopular-opinion", "postedAtFormatted": "Friday, August 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%E2%80%94Confess%20Your%20Unpopular%20Opinion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%E2%80%94Confess%20Your%20Unpopular%20Opinion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPLZ6xuwkZJi3sPuwj%2Fmeetup-west-la-meetup-confess-your-unpopular-opinion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%E2%80%94Confess%20Your%20Unpopular%20Opinion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPLZ6xuwkZJi3sPuwj%2Fmeetup-west-la-meetup-confess-your-unpopular-opinion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPLZ6xuwkZJi3sPuwj%2Fmeetup-west-la-meetup-confess-your-unpopular-opinion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 437, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ps'>West LA Meetup\u2014Confess Your Unpopular Opinion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 August 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you apply some chicanery.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>Lonely dissent doesn't feel like going to school dressed in black. It feels like going to school wearing a clown suit.\u2014<a href=\"http://lesswrong.com/lw/mb/lonely_dissent/\">Eliezer Yudkowsky</a></p>\n  \n  <p>Sometimes it feels more like going to clown school wearing a suit.\u2014<a href=\"https://twitter.com/aristosophy/status/288510905283670016\" rel=\"nofollow\">Catharine G. Evans</a></p>\n</blockquote>\n\n<p>On August 7, #confessyourunpopularopinion made the rounds as a (perhaps ironically, very popular) Twitter hashtag.</p>\n\n<p>Examples:</p>\n\n<ul>\n<li><a href=\"https://twitter.com/shlevy/status/365281959464796160\" rel=\"nofollow\">&quot;Someone who has not directly experienced a form of oppression can have useful knowledge about it &quot;</a></li>\n<li><a href=\"https://twitter.com/shlevy/status/365284380853272578\" rel=\"nofollow\">&quot;Abortion is often morally the best choice&quot;</a></li>\n<li><a href=\"https://twitter.com/TheViewFromHell/status/365340417027346432\" rel=\"nofollow\">&quot;objects are syllables in a message/one aspect of god&#39;s lonely edge&quot;</a></li>\n<li><a href=\"https://twitter.com/dgrey0/status/365290114609451011\" rel=\"nofollow\">&quot;Inheritance should be illegal. You deserve nothing from your parents&#39; deaths.&quot;</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman/status/365286555872530433\" rel=\"nofollow\">&quot;hypocrisy is what makes us human and sincere &quot;people&quot; are monsters who should walk to the bottom of a lake &quot;</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman/status/365287743141588995\" rel=\"nofollow\">&quot;What we call &quot;art&quot; is just proof-of-work, no more transcendent or special than hashing random bitstrings. &quot;</a></li>\n<li><a href=\"https://twitter.com/FranklinH3000/status/365343626911428608\" rel=\"nofollow\">&quot;If you vote, <em>you&#39;re</em> the one who doesn&#39;t have a right to complain.&quot;</a></li>\n<li><a href=\"https://twitter.com/Johnwbh/status/365449070376009729\" rel=\"nofollow\">&quot;Most of the time the faceless soulless bureaucracy is right&quot;</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman/status/365283148747128834\" rel=\"nofollow\">&quot;no matter how I look at it it&#39;s your fault my opinion is unpopular&quot;</a></li>\n</ul>\n\n<p>This is an inherently LessWrongish thing to do, so this is what we will be doing. I don't expect it to be a problem, but just in case, I will be enforcing (with my <em>fists</em>) a rule: <em>No one's opinions are to be ridiculed or mocked. The <strong>point</strong> is to profess <strong>unpopular</strong> opinions.</em></p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/mb/lonely_dissent/\">Lonely Dissent</a> and its prerequisites</li>\n<li><a href=\"http://www.paulgraham.com/say.html\" rel=\"nofollow\">What You Can&#39;t Say</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/9kf/ive_had_it_with_those_dark_rumours_about_our/\">I&#39;ve had it with those dark rumours about our culture rigorously suppressing opinions</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. Because a few people didn't get the joke last time, I have been instructed not to pretend that you have to have memorized the core sequences in order to attend without getting kicked out. It's still a good idea to <a href=\"http://www.quora.com/What-are-some-must-read-Less-Wrong-sequences/answer/George-Koleszarik/\" rel=\"nofollow\">read the sequences</a>, though.</p>\n\n<p>There may or may not be a highly visible whiteboard, which may or may not have Bayes's Theorem written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ps'>West LA Meetup\u2014Confess Your Unpopular Opinion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PLZ6xuwkZJi3sPuwj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.295921755133073e-06, "legacy": true, "legacyId": "23696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_Confess_Your_Unpopular_Opinion\">Discussion article for the meetup : <a href=\"/meetups/ps\">West LA Meetup\u2014Confess Your Unpopular Opinion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 August 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you apply some chicanery.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>Lonely dissent doesn't feel like going to school dressed in black. It feels like going to school wearing a clown suit.\u2014<a href=\"http://lesswrong.com/lw/mb/lonely_dissent/\">Eliezer Yudkowsky</a></p>\n  \n  <p>Sometimes it feels more like going to clown school wearing a suit.\u2014<a href=\"https://twitter.com/aristosophy/status/288510905283670016\" rel=\"nofollow\">Catharine G. Evans</a></p>\n</blockquote>\n\n<p>On August 7, #confessyourunpopularopinion made the rounds as a (perhaps ironically, very popular) Twitter hashtag.</p>\n\n<p>Examples:</p>\n\n<ul>\n<li><a href=\"https://twitter.com/shlevy/status/365281959464796160\" rel=\"nofollow\">\"Someone who has not directly experienced a form of oppression can have useful knowledge about it \"</a></li>\n<li><a href=\"https://twitter.com/shlevy/status/365284380853272578\" rel=\"nofollow\">\"Abortion is often morally the best choice\"</a></li>\n<li><a href=\"https://twitter.com/TheViewFromHell/status/365340417027346432\" rel=\"nofollow\">\"objects are syllables in a message/one aspect of god's lonely edge\"</a></li>\n<li><a href=\"https://twitter.com/dgrey0/status/365290114609451011\" rel=\"nofollow\">\"Inheritance should be illegal. You deserve nothing from your parents' deaths.\"</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman/status/365286555872530433\" rel=\"nofollow\">\"hypocrisy is what makes us human and sincere \"people\" are monsters who should walk to the bottom of a lake \"</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman/status/365287743141588995\" rel=\"nofollow\">\"What we call \"art\" is just proof-of-work, no more transcendent or special than hashing random bitstrings. \"</a></li>\n<li><a href=\"https://twitter.com/FranklinH3000/status/365343626911428608\" rel=\"nofollow\">\"If you vote, <em>you're</em> the one who doesn't have a right to complain.\"</a></li>\n<li><a href=\"https://twitter.com/Johnwbh/status/365449070376009729\" rel=\"nofollow\">\"Most of the time the faceless soulless bureaucracy is right\"</a></li>\n<li><a href=\"https://twitter.com/admittedlyhuman/status/365283148747128834\" rel=\"nofollow\">\"no matter how I look at it it's your fault my opinion is unpopular\"</a></li>\n</ul>\n\n<p>This is an inherently LessWrongish thing to do, so this is what we will be doing. I don't expect it to be a problem, but just in case, I will be enforcing (with my <em>fists</em>) a rule: <em>No one's opinions are to be ridiculed or mocked. The <strong>point</strong> is to profess <strong>unpopular</strong> opinions.</em></p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/mb/lonely_dissent/\">Lonely Dissent</a> and its prerequisites</li>\n<li><a href=\"http://www.paulgraham.com/say.html\" rel=\"nofollow\">What You Can't Say</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/9kf/ive_had_it_with_those_dark_rumours_about_our/\">I've had it with those dark rumours about our culture rigorously suppressing opinions</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. Because a few people didn't get the joke last time, I have been instructed not to pretend that you have to have memorized the core sequences in order to attend without getting kicked out. It's still a good idea to <a href=\"http://www.quora.com/What-are-some-must-read-Less-Wrong-sequences/answer/George-Koleszarik/\" rel=\"nofollow\">read the sequences</a>, though.</p>\n\n<p>There may or may not be a highly visible whiteboard, which may or may not have Bayes's Theorem written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_Confess_Your_Unpopular_Opinion1\">Discussion article for the meetup : <a href=\"/meetups/ps\">West LA Meetup\u2014Confess Your Unpopular Opinion</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup\u2014Confess Your Unpopular Opinion", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_Confess_Your_Unpopular_Opinion", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup\u2014Confess Your Unpopular Opinion", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_Confess_Your_Unpopular_Opinion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CEGnJBHmkcwPTysb7", "T8Huvskn2Ab5m8wkx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-09T12:50:44.199Z", "modifiedAt": null, "url": null, "title": "Greatest Philosopher in History", "slug": "greatest-philosopher-in-history", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kajQMk6AgpEFGvyCe/greatest-philosopher-in-history", "pageUrlRelative": "/posts/kajQMk6AgpEFGvyCe/greatest-philosopher-in-history", "linkUrl": "https://www.lesswrong.com/posts/kajQMk6AgpEFGvyCe/greatest-philosopher-in-history", "postedAtFormatted": "Friday, August 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Greatest%20Philosopher%20in%20History&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreatest%20Philosopher%20in%20History%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkajQMk6AgpEFGvyCe%2Fgreatest-philosopher-in-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Greatest%20Philosopher%20in%20History%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkajQMk6AgpEFGvyCe%2Fgreatest-philosopher-in-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkajQMk6AgpEFGvyCe%2Fgreatest-philosopher-in-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Since LessWrong is&nbsp;a major congregation point for certain philosophical ideas, and because people here tend to be more objective (in the sense of not being self-deluded) than elsewhere, I thought I'd ask people's views.</p>\r\n<p>To be clear, by \"Greatest Philosopher\" I am referring not to the most correct philosopher in human history but the one who deserves the most credit for advancing human philosophy towards being more true.</p>\r\n<p>Off the top of my head I would say that&nbsp;a prime candidate would be Hume- amongst other things&nbsp;he rejected the idea of a soul, realised to a much greater extent than his predecessors the limits of human knowledge, and opposed the idea that reason is somehow an objective force that can make priorities independent of emotions.</p>\r\n<p>Aristotle deserves considerable credit relative for his time but doesn't make the list because although it wasn't his fault his ideas were dogmatically accepted and held back both science and philosophy later on.</p>\r\n<p>Your thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kajQMk6AgpEFGvyCe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}], "voteCount": 25, "baseScore": 0, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "23697", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-09T15:46:24.673Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-100", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zfgsPyNi8s4HQ2JgY/weekly-lw-meetups-100", "pageUrlRelative": "/posts/zfgsPyNi8s4HQ2JgY/weekly-lw-meetups-100", "linkUrl": "https://www.lesswrong.com/posts/zfgsPyNi8s4HQ2JgY/weekly-lw-meetups-100", "postedAtFormatted": "Friday, August 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzfgsPyNi8s4HQ2JgY%2Fweekly-lw-meetups-100%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzfgsPyNi8s4HQ2JgY%2Fweekly-lw-meetups-100", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzfgsPyNi8s4HQ2JgY%2Fweekly-lw-meetups-100", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 520, "htmlBody": "<p><strong>This meetup summary was posted to LW Main on August 2nd. The following week's meetup is <a href=\"/lw/iab/new_lw_meetup_saskatoon/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/p5\">Atlanta LessWrong: August Meetup :&nbsp;<span class=\"date\">04 August 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/p7\">Brussels meetup:&nbsp;<span class=\"date\">17 August 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/p3\">Chicago: Discuss Thinking, Fast and Slow:&nbsp;<span class=\"date\">03 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pb\">Helsinki Practical Meetup:&nbsp;<span class=\"date\">04 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pg\">[Lyon] Picnic au Parc de la T&ecirc;te d'Or:&nbsp;<span class=\"date\">17 August 2013 12:30PM</span></a></li>\n<li><a href=\"/meetups/p4\">Moscow: Miscellaneous Rationality:&nbsp;<span class=\"date\">04 August 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/pf\">Vancouver Open Discussion:&nbsp;<span class=\"date\">03 August 2013 02:34AM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/pe\">[Boston] Goal Factoring:&nbsp;<span class=\"date\">04 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/pd\">Durham/RTLW HPMoR discussion, ch. 79-81:&nbsp;<span class=\"date\">10 August 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/p1\">London Meetup - Achieving Better Goals:&nbsp;<span class=\"date\">04 August 2013 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zfgsPyNi8s4HQ2JgY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.296403007407665e-06, "legacy": true, "legacyId": "23625", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JwsqG8n6jaL4yMHkf", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-09T17:18:08.287Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: \"Rationality is hard! Let's do math\" (Utility Functions/Prospect Theory)", "slug": "meetup-durham-nc-triangle-area-rationality-is-hard-let-s-do", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:34.084Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Yhb4ym4Pe7EmivdmQ/meetup-durham-nc-triangle-area-rationality-is-hard-let-s-do", "pageUrlRelative": "/posts/Yhb4ym4Pe7EmivdmQ/meetup-durham-nc-triangle-area-rationality-is-hard-let-s-do", "linkUrl": "https://www.lesswrong.com/posts/Yhb4ym4Pe7EmivdmQ/meetup-durham-nc-triangle-area-rationality-is-hard-let-s-do", "postedAtFormatted": "Friday, August 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20%22Rationality%20is%20hard!%20Let's%20do%20math%22%20(Utility%20Functions%2FProspect%20Theory)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20%22Rationality%20is%20hard!%20Let's%20do%20math%22%20(Utility%20Functions%2FProspect%20Theory)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhb4ym4Pe7EmivdmQ%2Fmeetup-durham-nc-triangle-area-rationality-is-hard-let-s-do%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20%22Rationality%20is%20hard!%20Let's%20do%20math%22%20(Utility%20Functions%2FProspect%20Theory)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhb4ym4Pe7EmivdmQ%2Fmeetup-durham-nc-triangle-area-rationality-is-hard-let-s-do", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYhb4ym4Pe7EmivdmQ%2Fmeetup-durham-nc-triangle-area-rationality-is-hard-let-s-do", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pt'>Durham NC/Triangle Area: &quot;Rationality is hard! Let's do math&quot; (Utility Functions/Prospect Theory)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 August 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Next Thursday's meetup topics will be utility functions &amp; prospect theory! Suggested reading and more detailed schedule forthcoming.</p>\n\n<p>Basic schedule: \n7:00 meet at Cocoa Cinnamon; chat &amp; obtain beverages <br />\n7:30 discuss! <br />\n9:30 ish adjourn to Fullsteam for further beverages</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pt'>Durham NC/Triangle Area: &quot;Rationality is hard! Let's do math&quot; (Utility Functions/Prospect Theory)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Yhb4ym4Pe7EmivdmQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area___Rationality_is_hard__Let_s_do_math___Utility_Functions_Prospect_Theory_\">Discussion article for the meetup : <a href=\"/meetups/pt\">Durham NC/Triangle Area: \"Rationality is hard! Let's do math\" (Utility Functions/Prospect Theory)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 August 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Next Thursday's meetup topics will be utility functions &amp; prospect theory! Suggested reading and more detailed schedule forthcoming.</p>\n\n<p>Basic schedule: \n7:00 meet at Cocoa Cinnamon; chat &amp; obtain beverages <br>\n7:30 discuss! <br>\n9:30 ish adjourn to Fullsteam for further beverages</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area___Rationality_is_hard__Let_s_do_math___Utility_Functions_Prospect_Theory_1\">Discussion article for the meetup : <a href=\"/meetups/pt\">Durham NC/Triangle Area: \"Rationality is hard! Let's do math\" (Utility Functions/Prospect Theory)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: \"Rationality is hard! Let's do math\" (Utility Functions/Prospect Theory)", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area___Rationality_is_hard__Let_s_do_math___Utility_Functions_Prospect_Theory_", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: \"Rationality is hard! Let's do math\" (Utility Functions/Prospect Theory)", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area___Rationality_is_hard__Let_s_do_math___Utility_Functions_Prospect_Theory_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-09T17:26:03.598Z", "modifiedAt": null, "url": null, "title": "Interesting new Pew Research study on American opinions about radical life extension", "slug": "interesting-new-pew-research-study-on-american-opinions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:11.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "x2yCfa9q99c7Ptyxm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ctdTvy3htZgQFb2kN/interesting-new-pew-research-study-on-american-opinions", "pageUrlRelative": "/posts/ctdTvy3htZgQFb2kN/interesting-new-pew-research-study-on-american-opinions", "linkUrl": "https://www.lesswrong.com/posts/ctdTvy3htZgQFb2kN/interesting-new-pew-research-study-on-american-opinions", "postedAtFormatted": "Friday, August 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interesting%20new%20Pew%20Research%20study%20on%20American%20opinions%20about%20radical%20life%20extension&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteresting%20new%20Pew%20Research%20study%20on%20American%20opinions%20about%20radical%20life%20extension%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctdTvy3htZgQFb2kN%2Finteresting-new-pew-research-study-on-american-opinions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interesting%20new%20Pew%20Research%20study%20on%20American%20opinions%20about%20radical%20life%20extension%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctdTvy3htZgQFb2kN%2Finteresting-new-pew-research-study-on-american-opinions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctdTvy3htZgQFb2kN%2Finteresting-new-pew-research-study-on-american-opinions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 465, "htmlBody": "<p><a href=\"http://www.pewforum.org/2013/08/06/living-to-120-and-beyond-americans-views-on-aging-medical-advances-and-radical-life-extension/\">This new study</a>&nbsp;by Pew Research on American opinions about radical life extension turned up some interesting results:</p>\n<blockquote>\n<p>Asked whether they, personally, would choose to undergo medical treatments to slow the aging process and live to be 120 or more, a majority of U.S. adults (56%) say no. But roughly two-thirds (68%) think that most <em>other </em>people would.</p>\n<p>Asked about the consequences for society if new medical treatments could slow the aging process and allow the average person to live decades longer, to at least 120 years old, about half of U.S. adults (51%) say the treatments would be a bad thing for society, while 41% say they would be a good thing.</p>\n<p>An overwhelming majority believes that everyone should be able to get these treatments if they want them (79%). But two-thirds think that in practice, only wealthy people would have access to the treatments... About two-thirds agree that longer life expectancies would strain our natural resources and that medical scientists would offer the treatment before they fully understood how it affects people's health. And about six-in-ten (58%) say these treatments would be fundamentally unnatural.</p>\n<p>About two-thirds of adults (63%) say medical advances that prolong life are generally good because they allow people to live longer, while about three-in-ten (32%) say medical advances are bad because they interfere with the natural cycle of life.</p>\n<p>The survey contains a number of null findings that may be surprising. It turns out, for example, that many standard measures of religious beliefs and practices, including belief in God and frequency of attendance at religious services, are related to views on radical life extension only weakly, if at all. Nor is there a strong relationship in the survey between the gender, education or political party identification of respondents and what they say about longer human life spans... At least one question that deals directly with death, however, is correlated with views on radical life extension. People who oppose the death penalty are more inclined to say that longer life spans would be good for society.</p>\n</blockquote>\n<p>I also find the demographic splits on page 3 to be surprising. On the question of whether treatments to extend life by decades would be a good thing for society, whites are significantly less likely to agree: 36% of whites agree whereas 48% of Hispanics and 56% of blacks do. There is a negative correlation with age (48% of adults 18-29, 46% of adults 30-49, 37% of adults 50-64, 31% of adults 65 and older) and with income (47% of those earning 30k and less, 42% of those earning from 30k-75k, and 39% of those earning 75k+). The income result in particular surprises me, as my intuition was that people with a higher quality of life would be significantly more pro-life extension.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ctdTvy3htZgQFb2kN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 1.2964858715823672e-06, "legacy": true, "legacyId": "23701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-10T01:58:14.667Z", "modifiedAt": null, "url": null, "title": "Effective Rationality Training Online", "slug": "effective-rationality-training-online", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:34.539Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Brendon_Wong", "createdAt": "2013-07-18T17:10:38.559Z", "isAdmin": false, "displayName": "Brendon_Wong"}, "userId": "TPaHQ6APqYu44ENNx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NFHa9ec9FmdvceQFy/effective-rationality-training-online", "pageUrlRelative": "/posts/NFHa9ec9FmdvceQFy/effective-rationality-training-online", "linkUrl": "https://www.lesswrong.com/posts/NFHa9ec9FmdvceQFy/effective-rationality-training-online", "postedAtFormatted": "Saturday, August 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Effective%20Rationality%20Training%20Online&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEffective%20Rationality%20Training%20Online%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNFHa9ec9FmdvceQFy%2Feffective-rationality-training-online%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Effective%20Rationality%20Training%20Online%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNFHa9ec9FmdvceQFy%2Feffective-rationality-training-online", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNFHa9ec9FmdvceQFy%2Feffective-rationality-training-online", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 529, "htmlBody": "<p>Article Prerequisite:&nbsp;<a title=\"Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality\" href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\" target=\"_blank\">Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality</a></p>\n<p><strong>Introduction</strong></p>\n<p>The goal of this post is to explore the idea of rationality training, feedback and ideas are greatly appreciated.</p>\n<p>Less Wrong&rsquo;s stated mission is to help people become more rational, and it has made progress toward that goal. Members read and discuss useful ideas on the internet, get instant feedback because of the voting system, and schedule meetups with other members. Less Wrong also helps attract more people to rationality.</p>\n<p>Less Wrong helps with sharing ideas, but it fails to help people put elements of epistemic and instrumental rationality into practice. This is a serious problem, but it would be hard to fix without altering the core functionality of Less Wrong.</p>\n<p>Having separate websites for reading and discussing ideas and then actually using those ideas would improve the real world performance of the Less Wrong community while maintaining the idea discussion, &ldquo;marketing&rdquo;, and other benefits of the Less Wrong website.</p>\n<p><strong>How to create a useful website for self improvement</strong></p>\n<p>1. Knowledge Management</p>\n<p>When reading blogs, people only see recent posts and those posts are not significantly revised. A wiki would allow for the creation of a large body of organized knowledge that is frequently revised. Each wiki post would have a description, benefits of the topic described, resources to learn the topic, user submitted resources to learn the topic, and reviews of each resource. Posts would be organized hierarchically and voted on for usefulness to help readers effectively improve what they are looking for. Users could share self-improvement plans to help others improve effectiveness in general or in a specific topic as quickly as possible.</p>\n<p>2. Effective Learning</p>\n<p>Resources to learn topics should be arranged or written for effective skill acquisition, and there may be different resource categories like exercises for deliberate practice or active recall questions for spaced repetition.</p>\n<p>3. Quality Contributors</p>\n<p>Contributors would, at the very least, need to be familiar with how to write articles that supported the skill acquisition process agreed upon by the entire community. Required writing and research skills would produce higher quality work. I am not sure if being a rationalist would improve the quality of articles.</p>\n<p><strong>Problems</strong></p>\n<p>1. Difficult requirements</p>\n<p>The number of prerequisites necessary to contribute to and use the wiki would really lower the amount of people who will be able to benefit from the wiki. It's a trade off between effectiveness and popularity. What elements should be included to maximize the effectiveness of the website?</p>\n<p>2. Interest</p>\n<p>There has to be enough interest in the website, or else a different project should be started instead. How many people in the Less Wrong community, and the world at large, would be interested in self improvement and rationality?&nbsp;</p>\n<p>3. Increasing the effectiveness of non altruistic people</p>\n<p>How much of the target audience wants to improve the world? If most do not, then the wiki would essentially be a net negative on the world. What should the criteria be to view and contribute to the wiki? Perhaps only Less Wrong members should be able to view and edit the wiki, and contributors must read a quick start guide and pass a quick test before being allowed to post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NFHa9ec9FmdvceQFy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2969119331533225e-06, "legacy": true, "legacyId": "23703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Article Prerequisite:&nbsp;<a title=\"Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality\" href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\" target=\"_blank\">Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality</a></p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>The goal of this post is to explore the idea of rationality training, feedback and ideas are greatly appreciated.</p>\n<p>Less Wrong\u2019s stated mission is to help people become more rational, and it has made progress toward that goal. Members read and discuss useful ideas on the internet, get instant feedback because of the voting system, and schedule meetups with other members. Less Wrong also helps attract more people to rationality.</p>\n<p>Less Wrong helps with sharing ideas, but it fails to help people put elements of epistemic and instrumental rationality into practice. This is a serious problem, but it would be hard to fix without altering the core functionality of Less Wrong.</p>\n<p>Having separate websites for reading and discussing ideas and then actually using those ideas would improve the real world performance of the Less Wrong community while maintaining the idea discussion, \u201cmarketing\u201d, and other benefits of the Less Wrong website.</p>\n<p><strong id=\"How_to_create_a_useful_website_for_self_improvement\">How to create a useful website for self improvement</strong></p>\n<p>1. Knowledge Management</p>\n<p>When reading blogs, people only see recent posts and those posts are not significantly revised. A wiki would allow for the creation of a large body of organized knowledge that is frequently revised. Each wiki post would have a description, benefits of the topic described, resources to learn the topic, user submitted resources to learn the topic, and reviews of each resource. Posts would be organized hierarchically and voted on for usefulness to help readers effectively improve what they are looking for. Users could share self-improvement plans to help others improve effectiveness in general or in a specific topic as quickly as possible.</p>\n<p>2. Effective Learning</p>\n<p>Resources to learn topics should be arranged or written for effective skill acquisition, and there may be different resource categories like exercises for deliberate practice or active recall questions for spaced repetition.</p>\n<p>3. Quality Contributors</p>\n<p>Contributors would, at the very least, need to be familiar with how to write articles that supported the skill acquisition process agreed upon by the entire community. Required writing and research skills would produce higher quality work. I am not sure if being a rationalist would improve the quality of articles.</p>\n<p><strong id=\"Problems\">Problems</strong></p>\n<p>1. Difficult requirements</p>\n<p>The number of prerequisites necessary to contribute to and use the wiki would really lower the amount of people who will be able to benefit from the wiki. It's a trade off between effectiveness and popularity. What elements should be included to maximize the effectiveness of the website?</p>\n<p>2. Interest</p>\n<p>There has to be enough interest in the website, or else a different project should be started instead. How many people in the Less Wrong community, and the world at large, would be interested in self improvement and rationality?&nbsp;</p>\n<p>3. Increasing the effectiveness of non altruistic people</p>\n<p>How much of the target audience wants to improve the world? If most do not, then the wiki would essentially be a net negative on the world. What should the criteria be to view and contribute to the wiki? Perhaps only Less Wrong members should be able to view and edit the wiki, and contributors must read a quick start guide and pass a quick test before being allowed to post.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "How to create a useful website for self improvement", "anchor": "How_to_create_a_useful_website_for_self_improvement", "level": 1}, {"title": "Problems", "anchor": "Problems", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uFYQaGCRwt3wKtyZP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-10T09:04:14.531Z", "modifiedAt": null, "url": null, "title": "Who are some of the best writers in history?", "slug": "who-are-some-of-the-best-writers-in-history", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:56.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Crux", "createdAt": "2011-09-15T17:49:36.096Z", "isAdmin": false, "displayName": "Crux"}, "userId": "XfQRFDS5eFdeYe6uM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QvLAXEBdys4tTpvwt/who-are-some-of-the-best-writers-in-history", "pageUrlRelative": "/posts/QvLAXEBdys4tTpvwt/who-are-some-of-the-best-writers-in-history", "linkUrl": "https://www.lesswrong.com/posts/QvLAXEBdys4tTpvwt/who-are-some-of-the-best-writers-in-history", "postedAtFormatted": "Saturday, August 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Who%20are%20some%20of%20the%20best%20writers%20in%20history%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWho%20are%20some%20of%20the%20best%20writers%20in%20history%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvLAXEBdys4tTpvwt%2Fwho-are-some-of-the-best-writers-in-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Who%20are%20some%20of%20the%20best%20writers%20in%20history%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvLAXEBdys4tTpvwt%2Fwho-are-some-of-the-best-writers-in-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvLAXEBdys4tTpvwt%2Fwho-are-some-of-the-best-writers-in-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 702, "htmlBody": "<p>We currently have a thread in progress concerning <a href=\"/r/discussion/lw/ia9/greatest_philosopher_in_history/\">the greatest philosophers in history</a>. This reminded me of a question I've been considering for a while.</p>\n<p>Recently I realized that although I've spent a massive percentage of my time for the past few years reading, it's almost always been non-fiction, for example posts on Less Wrong, or posts on other forums or blogs, or old treatises on economics or philosophy. Although I feel as if I've stumbled onto some of the absolute best sources of insight the history of civilization has to offer, and some of the most brilliant thinkers to have ever walked the planet, it strikes me that the main optimization criteria that led to the popularity of most of these pieces of writing and most of these thinkers, and thus the reason they were visible enough for me to run into them, has been a matter not necessarily of the quality of writing style, but instead a matter of the level of insight perceived by whatever critical mass of people brought it to the forefront.</p>\n<p>In other words, I've mostly ran into articles and books that have garnered their level of fame not by the eloquence of their prose, but instead by the insight contained within. I've ran into plenty of famous, highly insightful thinkers who just aren't very good at writing. They became famous for another reason: the ideas they brought to the table, however incompetently. This is in utter contrast to another section of the history of writing: what we call \"fiction\", and in some cases \"poetry\". Although fiction writers or poets are generally expected to bring some sort of insight to the table, or even a lot of insight, this certainly isn't the overwhelming criterion determining their fame. Most of the scientists who have become famous for their work have been at least decent at writing, or nobody would have been able to get through their stuff (though there are exceptions). In the same way, the great fiction books of history certainly have insight; it's just often not what carries them to success.</p>\n<p>In non-fiction, the top contenders are usually there for their insight, though their writing is usually at least decent. And in fiction and poetry, the top contenders are usually there for their eloquence and writing style, though their insight is usually at least decent. There's at least one exception I can think of, where this person seems to be civilization class in both insight and writing style: David Hume. One of the greatest thinkers in history, and certainly also one of the greatest writers in history. Anybody who's read a decent amount of other famous writers and thinkers, and understands some of Hume's key arguments, would at least have some sort of sympathy for that characterization, despite the extreme level of praise I'm bestowing onto his work.</p>\n<p>So here's my point, and my question: I'm mostly interested in insight, but I'm also interested in communicating this insight in an extremely effective way, with the most solid prose possible, and the highest level of eloquence that can be attained. For this, I can't simply limit myself to reading the most insightful, revolutionary non-fiction work, as the market test that led to the widespread adoption of this work was not necessarily one of requiring a high level of eloquence or poetic ability. I'll need to read works that became famous for their writing style. So we come now to the question: Who are some of the best writers in the history of civilization? Who should I read for the purpose of getting better at writing?</p>\n<p><em>(TL;DR: I've spent a lot of time reading non-fiction books, but most of these books became famous for their level of insight, and not necessarily because of any sort of high level of competence in writing, or eloquence of style. I want to get better at writing, so although I've ignored fiction for a long time due to thinking my interest was scientific insight and not fiction stories, I've changed my mind, and realized I should probably be reading some fiction in order to learn from the masters of eloquence. So with that said, here's my question: Who are some of the best writers in history?)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QvLAXEBdys4tTpvwt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 1.2972664888327226e-06, "legacy": true, "legacyId": "23709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kajQMk6AgpEFGvyCe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-10T16:21:19.138Z", "modifiedAt": null, "url": null, "title": "Proposal: periodic repost of the Best Learning resources", "slug": "proposal-periodic-repost-of-the-best-learning-resources", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:32.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zhwpKgYf3nCX5ouxq/proposal-periodic-repost-of-the-best-learning-resources", "pageUrlRelative": "/posts/zhwpKgYf3nCX5ouxq/proposal-periodic-repost-of-the-best-learning-resources", "linkUrl": "https://www.lesswrong.com/posts/zhwpKgYf3nCX5ouxq/proposal-periodic-repost-of-the-best-learning-resources", "postedAtFormatted": "Saturday, August 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20periodic%20repost%20of%20the%20Best%20Learning%20resources&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20periodic%20repost%20of%20the%20Best%20Learning%20resources%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzhwpKgYf3nCX5ouxq%2Fproposal-periodic-repost-of-the-best-learning-resources%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20periodic%20repost%20of%20the%20Best%20Learning%20resources%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzhwpKgYf3nCX5ouxq%2Fproposal-periodic-repost-of-the-best-learning-resources", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzhwpKgYf3nCX5ouxq%2Fproposal-periodic-repost-of-the-best-learning-resources", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>One of the biggest benefits of LW for me, aside from specific discussions, has been finding high-quality learning resources. Since knowledge is pretty much the biggest power humans have and many of us spend a lot of time learning, learning more efficiently is extremely important - a good textbook vs. a bad one can cost a lot of time and quite probably make some of the area inaccessible.</p>\n<p>We've had a number of threads in that direction, e.g. this</p>\n<p><a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">http://lesswrong.com/lw/3gu/the_best_textbooks_on_every_subject/</a></p>\n<p><a href=\"/lw/g9l/course_recommendations_for_friendliness/\">http://lesswrong.com/lw/g9l/course_recommendations_for_friendliness/</a></p>\n<p>plus crumbs in the monthly media threads.</p>\n<p>The proposal is to have these discussions periodically, especially with the great influx of top-notch full online courses from the best schools via coursera, edx, udacity, etc. After that we can wikify some of the more stable recommendations and link the wiki back to the discussions.<br /><br />Please use this thread for meta-discussion, not specific recommendations. The big questions are should we have this periodically yes/no, what the period should be, at least initially and other helpful suggestions.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zhwpKgYf3nCX5ouxq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 1.2976304437988813e-06, "legacy": true, "legacyId": "23713", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "snzFQJsNYqzPZS2nK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-10T17:32:33.848Z", "modifiedAt": null, "url": null, "title": "Meetup: Philadelphia, August 11, games", "slug": "meetup-philadelphia-august-11-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:33.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EE4nJBfAWPkkhmHyf/meetup-philadelphia-august-11-games", "pageUrlRelative": "/posts/EE4nJBfAWPkkhmHyf/meetup-philadelphia-august-11-games", "linkUrl": "https://www.lesswrong.com/posts/EE4nJBfAWPkkhmHyf/meetup-philadelphia-august-11-games", "postedAtFormatted": "Saturday, August 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Philadelphia%2C%20August%2011%2C%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Philadelphia%2C%20August%2011%2C%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEE4nJBfAWPkkhmHyf%2Fmeetup-philadelphia-august-11-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Philadelphia%2C%20August%2011%2C%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEE4nJBfAWPkkhmHyf%2Fmeetup-philadelphia-august-11-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEE4nJBfAWPkkhmHyf%2Fmeetup-philadelphia-august-11-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>August 11, 1 PM, <a href=\"http://www.giovanipizza.com/\">Giovanni Pizza</a> at 1515 Chestnut. We'll be playing games, probably with an emphasis on games where making up rules is part of the play.</p>\n<p>We have a <a href=\"https://groups.google.com/forum/#!topic/lesswrong-philadelphia/Z1zxwphuOkU\">google group</a> and an <a href=\"http://doodle.com/nbpzvyknn479sa4v\"> interactive calendar</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EE4nJBfAWPkkhmHyf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.2976897869546382e-06, "legacy": true, "legacyId": "23715", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-10T22:20:27.659Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-9", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:57.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qNDcPMqSZ2xzwGheT/meetup-helsinki-meetup-9", "pageUrlRelative": "/posts/qNDcPMqSZ2xzwGheT/meetup-helsinki-meetup-9", "linkUrl": "https://www.lesswrong.com/posts/qNDcPMqSZ2xzwGheT/meetup-helsinki-meetup-9", "postedAtFormatted": "Saturday, August 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNDcPMqSZ2xzwGheT%2Fmeetup-helsinki-meetup-9%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNDcPMqSZ2xzwGheT%2Fmeetup-helsinki-meetup-9", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNDcPMqSZ2xzwGheT%2Fmeetup-helsinki-meetup-9", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pu'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 August 2013 02:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The theme of this meetup will be applied rationality. We\u2019ll go through some <a href=\"http://rationality.org/\" rel=\"nofollow\">CFAR</a> materials, and we\u2019ll try a few rationality exercises. The location is still uncertain, but will be updated as soon as possible.</p>\n\n<p>Updated! We\u2019ll meet in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pu'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qNDcPMqSZ2xzwGheT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2979296375308628e-06, "legacy": true, "legacyId": "23716", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/pu\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 August 2013 02:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The theme of this meetup will be applied rationality. We\u2019ll go through some <a href=\"http://rationality.org/\" rel=\"nofollow\">CFAR</a> materials, and we\u2019ll try a few rationality exercises. The location is still uncertain, but will be updated as soon as possible.</p>\n\n<p>Updated! We\u2019ll meet in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/pu\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-11T06:59:21.424Z", "modifiedAt": null, "url": null, "title": "The Rebuttal Repository", "slug": "the-rebuttal-repository", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:15.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5pGKbwzyLfTp4cfND/the-rebuttal-repository", "pageUrlRelative": "/posts/5pGKbwzyLfTp4cfND/the-rebuttal-repository", "linkUrl": "https://www.lesswrong.com/posts/5pGKbwzyLfTp4cfND/the-rebuttal-repository", "postedAtFormatted": "Sunday, August 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Rebuttal%20Repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Rebuttal%20Repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pGKbwzyLfTp4cfND%2Fthe-rebuttal-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Rebuttal%20Repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pGKbwzyLfTp4cfND%2Fthe-rebuttal-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pGKbwzyLfTp4cfND%2Fthe-rebuttal-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>Since repositories <a href=\"/r/discussion/lw/i64/repository_repository/\">are popular</a>&nbsp;and useful, I thought it would be good to have one where we pair common bad/incorrect/flawed/misleanding/incomplete ideas with high-quality articles that explain why those ideas are bad/incorrect/flawed/misleading/incomplete.</p>\n<p>Examples:</p>\n<p><em>Myers-Briggs as a theory of personality. </em>-&gt; Richard Batty's <a href=\"http://www.80000hours.org/blog/60-the-myers-briggs-type-indicator-a-popular-but-flawed-way-of-understanding-your-personality\">\"The Myers-Briggs type Indicator: A Popular But Flawed Way of Understanding Your Personality\"</a> from 80000 Hours.</p>\n<p><em>Microfinance</em>&nbsp;-&gt; Ben Todd's <a href=\"http://80000hours.org/blog/123-is-microcredit-mostly-hype\">\"Is Microfinance Mostly Hype?\"</a>&nbsp;and&nbsp;GiveWell's <a href=\"http://blog.givewell.org/2009/10/23/6-myths-about-microfinance-charity-that-donors-can-do-without/\">\"6 Myths About Microfinance Charity Donors Can Do Without\"</a>.</p>\n<p><em>Zizek's talk on charity <a href=\"http://www.thersa.org/events/rsaanimate/animate/rsa-animate-first-as-tragedy,-then-as-farce\">\"First as Tragedy, then as Farce\"</a> (or the idea that charity is bad because it undermines political change)</em>&nbsp;-&gt; Jeff Kaufman's <a href=\"http://www.jefftk.com/p/good-charity-as-neither-tragedy-nor-farce\">\"Good Charity as Neither Tragedy or Farce\"</a>.</p>\n<p><em>The idea that the AI will be benevolent/Friendly by default.</em> -&gt; Luke and Louie's <a href=\"http://intelligence.org/files/IE-ME.pdf\">\"Intelligence Explosion and Machine Ethics\"</a>.</p>\n<p>etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5pGKbwzyLfTp4cfND", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 19, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "23718", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sEaDmtwrmTC7kTqcf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-11T17:50:54.944Z", "modifiedAt": null, "url": null, "title": "New Monthly Thread: Bragging", "slug": "new-monthly-thread-bragging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:03.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Joshua_Blaine", "createdAt": "2013-07-15T18:37:17.985Z", "isAdmin": false, "displayName": "Joshua_Blaine"}, "userId": "jzvkfAuoy9X7dp9Ma", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/53bfN42CCTfC3n5WA/new-monthly-thread-bragging", "pageUrlRelative": "/posts/53bfN42CCTfC3n5WA/new-monthly-thread-bragging", "linkUrl": "https://www.lesswrong.com/posts/53bfN42CCTfC3n5WA/new-monthly-thread-bragging", "postedAtFormatted": "Sunday, August 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Monthly%20Thread%3A%20Bragging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Monthly%20Thread%3A%20Bragging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53bfN42CCTfC3n5WA%2Fnew-monthly-thread-bragging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Monthly%20Thread%3A%20Bragging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53bfN42CCTfC3n5WA%2Fnew-monthly-thread-bragging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53bfN42CCTfC3n5WA%2Fnew-monthly-thread-bragging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>In an attempt to encourage more people to <em>actually do awesome things </em>(a la instrumental rationality), I am proposing a new monthly thread (can be changed to bi-weekly, should that be demanded). Your job, should you choose to accept it, is to comment on this thread explaining <strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of you self as you feel. You may unabashedly consider yourself <em>the coolest freaking person ever</em>&nbsp;because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p>Remember, however, that this <strong>isn't</strong>&nbsp;any kind of progress thread. Nor is it any kind of proposal thread.<em>This thread is solely for people to talk about the awesomest thing they've done all month. not will do. not are working on</em>. <strong>have already done. </strong>This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p>So, what's the coolest thing you've done this month?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "53bfN42CCTfC3n5WA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 39, "extendedScore": null, "score": 1.2989055666917754e-06, "legacy": true, "legacyId": "23719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 147, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-11T18:18:11.494Z", "modifiedAt": null, "url": null, "title": "Common sense as a prior", "slug": "common-sense-as-a-prior", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Beckstead", "createdAt": "2011-08-19T23:58:47.870Z", "isAdmin": false, "displayName": "Nick_Beckstead"}, "userId": "Sjm96fPXwa2x4eHHv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wgdfxQJ2DQuju73zC/common-sense-as-a-prior", "pageUrlRelative": "/posts/wgdfxQJ2DQuju73zC/common-sense-as-a-prior", "linkUrl": "https://www.lesswrong.com/posts/wgdfxQJ2DQuju73zC/common-sense-as-a-prior", "postedAtFormatted": "Sunday, August 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Common%20sense%20as%20a%20prior&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommon%20sense%20as%20a%20prior%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwgdfxQJ2DQuju73zC%2Fcommon-sense-as-a-prior%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Common%20sense%20as%20a%20prior%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwgdfxQJ2DQuju73zC%2Fcommon-sense-as-a-prior", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwgdfxQJ2DQuju73zC%2Fcommon-sense-as-a-prior", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8067, "htmlBody": "<h1>Introduction</h1>\n<p class=\"MsoNormal\">[I have edited the introduction of this post for increased clarity.]</p>\n<p class=\"MsoNormal\">This post is my attempt to answer the question, \"How should we take account of the distribution of opinion and epistemic standards in the world?\"&nbsp;By &ldquo;epistemic standards,&rdquo; I roughly mean a person&rsquo;s way of processing evidence to arrive at conclusions. If people were good Bayesians, their epistemic standards would correspond to their fundamental prior probability distributions. At a first pass, my answer to this questions is:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong>Main Recommendation</strong>: Believe what you think a broad coalition of trustworthy people would believe if they were trying to<span style=\"white-space: pre;\">&nbsp;</span>have accurate views and they had access to your evidence.</p>\n<p class=\"MsoNormal\">The rest of the post can be seen as an attempt to spell this out more precisely and to explain, in practical terms, how to follow the recommendation. Note that there are therefore two broad ways to disagree with the post: you might disagree with the main recommendation, or the guidelines for following main recommendation.</p>\n<div>The rough idea is to try find a group of people whose are trustworthy by clear and generally accepted indicators, and then use an impartial combination of the reasoning standards that they use when they are trying to have accurate views.&nbsp;I call this impartial combination <em>elite common sense</em>. I recommend using elite common sense as a prior in two senses. First, if you have no unusual information about a question, you should start with the same opinions as the broad coalition of trustworthy people would have. But their opinions are not the last word, and as you get more evidence, it can be reasonable to disagree. Second, a complete prior probability distribution specifies, for any possible set of evidence, what posterior probabilities you should have. In this deeper sense, I am not just recommending that you start with the same opinions as elite common sense, but also you update in ways that elite common sense would agree are the right ways to update.&nbsp;In practice, we can&rsquo;t specify the prior probability distribution of elite common sense or calculate the updates, so the framework is most useful from a conceptual perspective. It might also be useful to consider the output of this framework as one model in a larger <a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination</a>.</div>\n<div><br /></div>\n<p class=\"MsoNormal\">I am aware of two relatively close intellectual relatives to my framework: what philosophers call &ldquo;<a href=\"http://philsci-archive.pitt.edu/2940/1/refdis.pdf\">equal weight</a>&rdquo; or &ldquo;conciliatory&rdquo; views about disagreement and what people on LessWrong may know as &ldquo;<a href=\"http://www.overcomingbias.com/2007/03/on_majoritarian.html\">philosophical majoritarianism</a>.&rdquo; Equal weight views roughly hold that when two people who are expected to be roughly equally competent at answering a certain question have different subjective probability distributions over answers to that question, those people should adopt some impartial combination of their subjective probability distributions. Unlike equal weight views in philosophy, my position is meant as a set of rough practical guidelines rather than a set of exceptionless and fundamental rules. I accordingly focus on practical issues for applying the framework effectively and am open to limiting the framework&rsquo;s scope of application. Philosophical majoritarianism is the idea that on most issues, the average opinion of humanity as a whole will be a better guide to the truth than one&rsquo;s own personal judgment. My perspective differs from both equal weight views and philosophical majoritarianism in that it emphasizes an elite subset of the population rather than humanity as a whole and that it emphasizes epistemic standards more than individual opinions. My perspective differs from what you might call \"elite majoritarianism\" in that, according to me, you can disagree with what very trustworthy people think on average if you think that those people would accept your views if they had access to your evidence and were trying to have accurate opinions.</p>\n<p class=\"MsoNormal\">I am very grateful to Holden Karnofsky and Jonah Sinick for thought-provoking conversations on this topic which led to this post. Many of the ideas ultimately derive from Holden&rsquo;s thinking, but I've developed them, made them somewhat more precise and systematic, discussed additional considerations for and against adopting them, and put everything in my own words. I am also grateful to Luke Muehlhauser and Pablo Stafforini for feedback on this post.</p>\n<p class=\"MsoNormal\">In the rest of this post I will:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">Outline the framework and offer guidelines for applying it effectively. I explain why I favor relying on the epistemic standards of people who are trustworthy by clear indicators that many people would accept, why I favor paying more attention to what people think than why they say they think it (on the margin), and why I favor stress-testing critical assumptions by attempting to convince a broad coalition of trustworthy people to accept them.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Offer some considerations in favor of using the framework.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Respond to the objection that common sense is often wrong, the objection that the most successful people are very unconventional, and objections of the form &ldquo;elite common sense is wrong about X and can&rsquo;t be talked out of it.&rdquo;</span></li>\n<li><span style=\"text-indent: -0.25in;\">Discuss some limitations of the framework and some areas where it might be further developed. I suspect it is weakest in cases where there is a large upside to disregarding elite common sense, there is little downside, and you&rsquo;ll find out whether your bet against conventional wisdom was right within a tolerable time limit, and cases where people are unwilling to carefully consider arguments with the goal of having accurate beliefs.</span></li>\n</ol>\n<div style=\"text-indent: -24px;\"><br /></div>\n<p><span style=\"text-indent: -0.25in;\"><a id=\"more\"></a></span></p>\n<ol> </ol>\n<h1>An outline of the framework and some guidelines for applying it effectively</h1>\n<p class=\"MsoNormal\">My suggestion is to use elite common sense as a prior rather than the standards of reasoning that come most naturally to you personally. The three main steps for doing this are:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">Try to find out what people who are trustworthy by clear indicators that many people would accept believe about the issue.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Identify the information and analysis you can bring to bear on the issue.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Try to find out what elite common sense would make of this information and analysis, and adopt a similar perspective.</span></li>\n</ol>\n<p class=\"MsoNormal\">On the first step, people often have an instinctive sense of what others think, though you should beware the <a href=\"http://en.wikipedia.org/wiki/False-consensus_effect\">false consensus effect</a>. If you don&rsquo;t know what other opinions are out there, you can ask some friends or search the internet. In my experience, regular people often have similar opinions to very smart people on many issues, but are much worse at articulating considerations for and against their views. This may be because many people copy the opinions of the most trustworthy people.</p>\n<p class=\"MsoNormal\"><strong>I favor giving more weight to the opinions of people who can be shown to be trustworthy by clear indicators that many people would accept, rather than people that seem trustworthy to you personally.</strong> This guideline is intended to help avoid parochialism and increase <a href=\"/lw/dyk/selfskepticism_the_first_principle_of_rationality/\">self-skepticism</a>. Individual people have a variety of biases and blind spots that are hard for them to recognize. Some of these biases and blind spots&mdash;like the <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">ones studied in cognitive science</a>&mdash;may affect almost everyone, but others are idiosyncratic&mdash;like biases and blind spots we inherit from our families, friends, business networks, schools, political groups, and religious communities. It is plausible that combining independent perspectives can help idiosyncratic errors wash out.</p>\n<p class=\"MsoNormal\">In order for the errors to wash out, it is important to rely on the standards of people who are trustworthy by clear indicators that many people would accept rather than the standards of people that seem trustworthy to you personally. &nbsp;Why? The people who seem most impressive to us personally are often people who have similar strengths and weaknesses to ourselves, and similar biases and blind spots. For example, I suspect that academics and people who specialize in using a lot of explicit reasoning have a different set of strengths and weaknesses from people who rely more on implicit reasoning, and people who rely primarily on <a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">many weak arguments</a> have a different set of strengths and weaknesses from people who rely more on one relatively strong line of argument.</p>\n<p class=\"MsoNormal\">Some good indicators of general trustworthiness might include: IQ, business success, academic success, generally respected scientific or other intellectual achievements, wide acceptance as an intellectual authority by certain groups of people, or success in any area where there is intense competition and success is a function of ability to make accurate predictions and good decisions. I am less committed to any particular list of indicators than the general idea.</p>\n<p class=\"MsoNormal\">Of course, trustworthiness can also be domain-specific. Very often, elite common sense would recommend deferring to the opinions of experts (e.g., listening to what physicists say about physics, what biologists say about biology, and what doctors say about medicine). In other cases, elite common sense may give partial weight to what putative experts say without accepting it all (e.g. economics and psychology). In other cases, they may give less weight to what putative experts say (e.g. sociology and philosophy). Or there may be no putative experts on a question. In cases where elite common sense gives less weight to the opinions of putative experts or there are no plausible candidates for expertise, it becomes more relevant to think about what elite common sense would say about a question.</p>\n<p class=\"MsoNormal\">How should we assign weight to different groups of people? <strong>Other things being equal, a larger number of people is better, more trustworthy people are better, people who are trustworthy by clearer indicators that more people would accept are better, and a set of criteria which allows you to have some grip on what the people in question think is better, but you have to make trade-offs.</strong> If I only included, say, the 20 smartest people I had ever met as judged by me personally, that would probably be too small a number of people, the people would probably have biases and blind spots very similar to mine, and I would miss out on some of the most trustworthy people, but it would be a pretty trustworthy collection of people and I&rsquo;d have some reasonable sense of what they would say about various issues. If I went with, say, the 10 most-cited people in 10 of the most intellectually credible academic disciplines, 100 of the most generally respected people in business, and the 100 heads of different states, I would have a pretty large number of people and a broad set of people who were very trustworthy by clear standards that many people would accept, but I would have a hard time knowing what they would think about various issues because I haven&rsquo;t interacted with them enough. How these factors can be traded-off against each other in a way that is practically most helpful probably varies substantially from person to person.</p>\n<p class=\"MsoNormal\">I can&rsquo;t give any very precise answer to the question about whose opinions should be given significant weight, even in my own case. Luckily, I think the output of this framework is usually not very sensitive to how we answer this question, partly because most people would typically defer to other, more trustworthy people. If you want a rough guideline that I think many people who read this post could apply, I would recommend focusing on, say, the opinions of the top 10% of people who got Ivy-League-equivalent educations (note that I didn&rsquo;t get such an education, at least as an undergrad, though I think you should give weight to my opinion; I&rsquo;m just giving a rough guideline that I think works reasonably well in practice). You might give some additional weight to more accomplished people in cases where you have a grip on how they think.</p>\n<p class=\"MsoNormal\"><strong>I don&rsquo;t have a settled opinion about how to aggregate the opinions of elite common sense.</strong> I suspect that taking straight averages gives too much weight to the opinions of cranks and crackpots, so that you may want to remove some outliers or give less weight to them. For the purpose of making decisions, I think that sophisticated voting methods (such as the <a href=\"http://en.wikipedia.org/wiki/Condorcet_method\">Condorcet method</a>) and analogues of the <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">parliamentary approaches</a> outlined by Nick Bostrom and Toby Ord seem fairly promising as rough guidelines in the short run. I don&rsquo;t do calculations with this framework&mdash;as I said, it&rsquo;s mostly conceptual&mdash;so uncertainty about an aggregation procedure hasn&rsquo;t been a major issue for me.</p>\n<p class=\"MsoNormal\"><strong>On the margin, I favor paying more attention to people&rsquo;s opinions than their explicitly stated reasons for their opinions.</strong> Why? One reason is that I believe people can have highly adaptive opinions and patterns of reasoning without being able to articulate good defenses of those opinions and/or patterns of reasoning. (Luke Muehlhauser has discussed some related points <a href=\"/lw/h6b/explicit_and_tacit_rationality/\">here</a>.) One reason is that people can adopt practices that are successful without knowing why they are successful, others who interact with them can adopt those practices, others who interact with them can adopt those practices, and so forth. I heard an extreme example of this from Spencer Greenberg, who had read it in <a href=\"http://www.amazon.com/Scientists-Greater-than-Einstein-ebook/dp/B002TKLC1A/ref=sr_1_1?ie=UTF8&amp;qid=1374885896&amp;sr=8-1&amp;keywords=scientists+greater+than+einstein\">Scientists Greater than Einstein</a>. The story involved a folk remedy for visual impairment:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">There were folk remedies worthy of study as well. One widely used in Java on children with either night blindness or Bitot&rsquo;s spots consisted of dropping the juices of lightly roasted lamb&rsquo;s liver into the eyes of affected children. Sommer relates, &ldquo;We were bemused at the appropriateness of this technique and wondered how it could possibly be effective. We, therefore, attended several treatment sessions, which were conducted exactly as the villagers had described, except for one small addition&mdash;rather than discarding the remaining organ, they fed it to the affected child. For some unknown reason this was never considered part of the therapy itself.&rdquo; Sommer and his associates were bemused, but now understood why the folk remedy had persisted through the centuries. Liver, being the organ where vitamin A is stored in a lamb or any other animal, is the best food to eat to obtain vitamin A. (p. 14)</p>\n<p class=\"MsoNormal\">Another striking example is bedtime prayer. In many Christian traditions I am aware of, it is common to pray before going to sleep. And in the tradition I was raised in, the main components of prayer were listing things you were grateful for, asking for forgiveness for all the mistakes you made that day and thinking about what you would do to avoid similar mistakes in the future, and asking God for things. Christians might say the point of this is that it is a duty to God, that repentance is a requirement for entry to heaven, or that asking God for things makes God more likely to intervene and create miracles. However, I think these activities are reasonable for different reasons: <a href=\"/lw/i0c/for_happiness_keep_a_gratitude_journal/\">gratitude journals</a> are great, reflecting on mistakes is a great way to learn and overcome weaknesses, and it is a good idea to get clear about what you really want out of life in the short-term and the long-term.</p>\n<p class=\"MsoNormal\">Another reason I have this view is that if someone has an effective but different intellectual style from you, it&rsquo;s possible that your biases and blind spots will prevent you from appreciating their points that have significant merit. If you partly give weight to opinions independently of how good the arguments seem to you personally, this can be less of an issue for you. Jonah Sinick described a striking reason this might happen in <a href=\"/lw/hne/many_weak_arguments_and_the_typical_mind/\">Many Weak Arguments and the Typical Mind</a>:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\"><strong>We should pay more attention to people&rsquo;s bottom line than to their stated reasons</strong>&nbsp;&mdash; If most high functioning people aren&rsquo;t relying heavily on any one of the arguments that they give, if a typical high functioning person responds to a query of the type &ldquo;Why do you think X?&rdquo; by saying &ldquo;I believe X because of argument Y&rdquo; we shouldn&rsquo;t conclude that the person believes argument Y with high probability. Rather, we should assume that argument Y is one of many arguments that they believe with low confidence, most of which they&rsquo;re not expressing, and we should focus on their belief in X instead of argument Y. [emphasis his]</p>\n<p class=\"MsoNormal\">This idea interacts in a complementary way to Luke Muehlhauser&rsquo;s claim that some people who are not skilled at explicit rationality may be skilled in <a href=\"/lw/h6b/explicit_and_tacit_rationality/\">tacit rationality</a>, allowing them to be successful at making many types of important decisions. If we are interacting with such people, we should give significant weight to their opinions independently of their stated reasons.</p>\n<p class=\"MsoNormal\">A counterpoint to my claim that, on the margin, we should give more weight to others&rsquo; conclusions and less to their reasoning is that some very impressive people disagree. For example, Ray Dalio is the founder of Bridgewater, which, at least as of 2011, was the <a href=\"http://en.wikipedia.org/wiki/List_of_hedge_funds#The_Largest_Hedge_Fund_Firms\">world&rsquo;s largest hedge fund</a>. He explicitly disagrees with my claim:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">&ldquo;I stress-tested my opinions by having the smartest people I could find challenge them so I could find out where I was wrong. I never cared much about others&rsquo; conclusions&mdash;only for the reasoning that led to these conclusions. That reasoning had to make sense to me. Through this process, I improved my chances of being right, and I learned a lot from a lot of great people.&rdquo; (p. 7 of <a href=\"https://www.principles.com/#Principles\">Principles</a> by Ray Dalio)</p>\n<p class=\"MsoNormal\">I suspect that getting the reasoning to make sense to him was important because it helped him to get better in touch with elite common sense, and also because reasoning is more important when dealing with very formidable people, as I suspect Dalio did and does. I also think that for the some of the highest functioning people who are most in touch with elite common sense, it may make more sense to give more weight to reasoning than conclusions.</p>\n<p class=\"MsoNormal\"><strong>The elite common sense framework favors testing unconventional views by seeing if you can convince a broad coalition of impressive people that your views are true. </strong>If you can do this, it is often good evidence that your views are supported by elite common sense standards. If you can&rsquo;t, it&rsquo;s often good evidence that your views can&rsquo;t be so supported. Obviously, these are rules of thumb and we should restrict our attention to cases where you are persuading people by rational means, in contrast with using rhetorical techniques that exploit human biases. There are also some interesting cases where, for one reason or another, people are unwilling to hear your case or think about your case rationally, and applying this guideline to these cases is tricky.</p>\n<p class=\"MsoNormal\">Importantly, I don&rsquo;t think cases where elite common sense is biased are typically an exception to this rule. In my experience, I have very little difficulty convincing people that some genuine bias, such as scope insensitivity, really is biasing their judgment. And if the bias really is critical to the disagreement, I think it will be a case where you can convince elite common sense of your position. Other cases, such as deeply entrenched religious and political views, may be more of an exception, and I will discuss the case of religious views more in a later section.</p>\n<p class=\"MsoNormal\"><strong>The distinction between convincing and &ldquo;beating in an argument&rdquo; is important for applying this principle.</strong> It is much easier to tell whether you convinced someone than it is to tell whether you beat them in an argument. Often, both parties think they won. In addition, sometimes it is rational not to update much in favor of a view if an advocate for that view beats you in an argument.</p>\n<p class=\"MsoNormal\">In support of this claim, consider what would happen if the world&rsquo;s smartest creationist debated some fairly ordinary evolution-believing high school student. The student would be destroyed in argument, but the student should not reject evolution, and I suspect he should hardly update at all. Why not? The student should know that there are people out there in the world who could destroy him on either side of this argument, and his personal ability to respond to arguments is not very relevant. What should be most relevant to this student is the distribution of opinion among people who are most trustworthy, not his personal response to small sample of the available evidence. Even if you genuinely are beating people in arguments, there is a risk that you will be like this creationist debater.</p>\n<p class=\"MsoNormal\">An additional consideration is that certain beliefs and practices may be reasonable and adopted for reasons that are not accessible to people who have adopted those beliefs and practices, as illustrated with the examples of the liver ritual and bedtime prayer. You might be able to &ldquo;beat&rdquo; some Christian in an argument about the merits of bedtime prayer, but praying may still be better than not praying. (I think it would be better still to introduce a different routine that serves similar functions&mdash;this is something I have done in my own life&mdash;but the Christian may be doing better than you on this issue if you don&rsquo;t have a replacement routine yourself.)</p>\n<p class=\"MsoNormal\"><strong>Under the elite common sense framework, the question is not &ldquo;how reliable is elite common sense?&rdquo; but &ldquo;how reliable is elite common sense compared to me?&rdquo; </strong>Suppose I learn that, actually, people are much worse at pricing derivatives than I previously believed. For the sake of argument suppose this was a lesson of the 2008 financial crisis (for the purposes of this argument, it doesn't matter whether this is actually a correct lesson of the crisis). This information does not favor relying more on my own judgment unless I have reason to think that the bias applies less to me than the rest of the derivatives market. By analogy, it is not acceptable to say, &ldquo;People are really bad at thinking about philosophy. So I am going to give less weight to their judgments about philosophy (psst&hellip;and more weight to my personal hunches and the hunches of people I personally find impressive).&rdquo; This is only OK if you have evidence that your personal hunches and the hunches of the people you personally find impressive are better than elite common sense, with respect to philosophy. In contrast, it might be acceptable to say, &ldquo;People are very bad at thinking about the consequences of agricultural subsidies in comparison with economists, and most trustworthy people would agree with this if they had my evidence. And I have an unusual amount of information about what economists think. So my opinion gets more weight than elite common sense in this case.&rdquo; Whether this ultimately is acceptable to say would depend on how good elites are at thinking about the consequences of agricultural subsidies&mdash;I suspect they are actually pretty good at it&mdash;but this is isn&rsquo;t relevant to the general point that I&rsquo;m making. The general point is that this is one potentially correct form of an argument that your opinion is better than the current stance of elite common sense.</p>\n<p class=\"MsoNormal\">This is partly a semantic issue, but I count the above example as a case where &ldquo;you are more reliable than elite common sense,&rdquo; even though, in some sense, you are relying on expert opinion rather than your own. But you have different beliefs about who is a relevant expert or what experts say than common sense does, and in this sense you are relying on your own opinion.</p>\n<p class=\"MsoNormal\"><strong>I favor giving more weight to common sense judgments in cases where people are trying to have accurate views.</strong> For example, I think people don&rsquo;t try very hard to have correct political, religious, and philosophical views, but they do try to have correct views about how to do their job properly, how to keep their families happy, and how to impress their friends. In general, I expect people to try to have more accurate views in cases where it is in their present interests to have more accurate views. (A quick reference for this point is <a href=\"http://en.wikipedia.org/wiki/Rational_irrationality\">here</a>.) This means that I expect them to strive more for accuracy in decision-relevant cases, cases where the cost of being wrong is high, and cases where striving for more accuracy can be expected to yield more accuracy, though not necessarily in cases where the risks and rewards are won&rsquo;t come for a very long time. I suspect this is part of what explains why people can be skilled in <a href=\"/lw/h6b/explicit_and_tacit_rationality/\">tacit rationality</a> but not explicit rationality.</p>\n<p class=\"MsoNormal\">As I said above, what&rsquo;s critical is not how reliable elite common sense is but how reliable you are <em>in comparison with elite common sense</em>. So it only makes sense to give more weight to your views when learning that others aren&rsquo;t trying to be correct if you have compelling evidence that you <em>are</em> trying to be correct. Ideally, this evidence would be compelling to a broad class of trustworthy people and not just compelling to you personally.</p>\n<h1>Some further reasons to think that the framework is likely to be helpful</h1>\n<p class=\"MsoNormal\">In explaining the framework and outlining guidelines for applying it, I have given some reasons to expect this framework to be helpful. Here are some more <a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">weak arguments</a> in favor of my view:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">Some studies I haven&rsquo;t personally reviewed closely claim that combinations of expert forecasts are hard to beat. For instance, a review by (Clemen 1989) found that: \"</span>Considerable literature has accumulated over the years regarding the combination of forecasts. The primary conclusion of this line of research is that forecast accuracy can be substantially improved through the combination of multiple individual forecasts.\" (abstract) And a <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Ungar-et-al-The-Good-Judgment-Project-a-large-scale-test-of-different-methods-of-combining-expert-predictions.pdf\">recent work</a> by the <a href=\"http://www.goodjudgmentproject.com/\">Good Judgment Project</a> found that taking an average individual forecasts and transforming it away from .5 credence gave the lowest errors of a variety of different methods of aggregating judgments of forecasters (p. 42).</li>\n<li><span style=\"text-indent: -0.25in;\">There are </span><a style=\"text-indent: -0.25in;\" href=\"http://link.springer.com/article/10.1007/s11238-006-9004-4\">plausible</a><span style=\"text-indent: -0.25in;\"> </span><a style=\"text-indent: -0.25in;\" href=\"http://philsci-archive.pitt.edu/2940/1/refdis.pdf\">philosophical</a><span style=\"text-indent: -0.25in;\"> </span><a style=\"text-indent: -0.25in;\" href=\"http://www.overcomingbias.com/2007/03/on_majoritarian.html\">considerations</a><span style=\"text-indent: -0.25in;\"> suggesting that, absent special evidence, there is no compelling reason to favor your own epistemic standards over the epistemic standards that others use.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In practice, we are extremely reliant on conventional wisdom for almost everything we believe that isn&rsquo;t very closely related to our personal experience, and single individuals working in isolation have extremely limited ability to manipulate their environment in comparison with individuals who can build on the insights of others. To see this point, consider that a small group of very intelligent humans detached from all cultures wouldn&rsquo;t have much of an advantage at all over other animal species in competition for resources, but humans are </span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Anthropocene\">increasingly dominating</a><span style=\"text-indent: -0.25in;\"> the biosphere. A great deal of this must be chalked up to cultural accumulation of highly adaptive concepts, ideas, and procedures that no individual could develop on their own. I see trying to rely on elite common sense as highly continuous with this successful endeavor.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Highly adaptive practices and assumptions are more likely to get copied and spread, and these practices and assumptions often work because they help you to be right. If you use elite common sense as a prior, you&rsquo;ll be more likely to be working with more adaptive practices and assumptions.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Some successful processes for finding valuable information, such as </span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/PageRank\">PageRank</a><span style=\"text-indent: -0.25in;\"> and Quora, seem analogous to the framework I have outlined. PageRank is one algorithm that Google uses to decide how high different pages should be in searches, which is implicitly a way of ranking high-quality information. I&rsquo;m speaking about something I don&rsquo;t know very well, but my rough understanding is that PageRank gives pages more votes when more pages link to them, and votes from a page get more weight if that page itself has a lot of votes. This seems analogous to relying on elite common sense because information sources are favored when they are regarded as high quality by a broad coalition of other information sources. Quora seems analogous because it favors answers to questions that many people regard as good.</span></li>\n<li><span style=\"text-indent: -0.25in;\">I&rsquo;m going to go look at the first three questions I can find on Quora. I predict that I would prefer the answers that elite common sense would give to these questions to what ordinary common sense would say, and also that I would prefer elite common sense&rsquo;s answers to these questions to my own except in cases where I have strong inside information/analysis.&nbsp;</span><span style=\"text-indent: -0.25in;\">Results: </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Human-and-Animal-Senses/Would-a-person-who-was-born-blind-and-can-suddenly-see-be-able-to-recognize-a-shape-by-just-looking-at-it-for-the-first-time-I-E-If-he-was-looking-at-a-ball-for-the-first-time-ever-will-he-be-able-to-say-that-it-is-round\">1<sup>st</sup> question</a><span style=\"text-indent: -0.25in;\">: weakly prefer elite common sense, don&rsquo;t have much special information. </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Social-Media/Do-we-answer-questions-on-Quora-because-upvotes-give-us-a-falsehood-sense-of-satisfaction-like-retweets-on-Twitter-or-likes-on-Facebook\">2<sup>nd</sup> question</a><span style=\"text-indent: -0.25in;\">: prefer elite common sense, don&rsquo;t have much special information. </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Does-the-conservation-of-momentum-and-conservation-of-energy-apply-to-electrons-that-have-light-shone-on-them-and-if-so-why-cant-we-tell-their-position-and-velocity-by-measuring-the-amount-of-light-it-absorbed-and-the-direction-of-the-reflected-light\">3<sup>rd</sup> question</a><span style=\"text-indent: -0.25in;\">: prefer elite common sense, don&rsquo;t have much special information. Note that I skipped </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Web-Services/What-kind-of-online-service-you-wish-had-already-been-there\">a question</a><span style=\"text-indent: -0.25in;\"> because it was a matter of taste. This went essentially the way I predicted it to go.</span></li>\n<li><span style=\"text-indent: -0.25in;\">The type of mathematical considerations underlying </span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Condorcet's_jury_theorem\">Condorcet&rsquo;s Jury Theorem</a><span style=\"text-indent: -0.25in;\"> give us some reason to think that combined opinions are often more reliable than individual opinions, even though the assumptions underlying this theorem are far from totally correct.</span></li>\n<li><span style=\"text-indent: -0.25in;\">There&rsquo;s a general cluster of social science findings that goes under the heading &ldquo;</span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">wisdom of crowds</a><span style=\"text-indent: -0.25in;\">&rdquo; and suggests that aggregating opinions across people outperforms individual opinions in many contexts.</span></li>\n<li><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;</span><span style=\"text-indent: -0.25in;\">Some rough &ldquo;</span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Marketplace_of_ideas\">marketplace of ideas</a><span style=\"text-indent: -0.25in;\">&rdquo; arguments suggest that the best ideas will often become part of elite common sense. When claims are decision-relevant, people pay if they have dumb beliefs and benefit if they have smart beliefs. When claims aren&rsquo;t decision-relevant, people sometimes pay a social cost for saying dumb things and get social benefits for saying things that are smarter, and the people with more information have more incentive to speak. For analogous reasons, when people use and promote epistemic standards that are dumb, they pay costs and when they use and promote epistemic standards that are smart. Obviously there are many other factors, including ones that point in different directions, but there is some kind of positive force here.</span></li>\n</ol>\n<h1>Cases where people often don&rsquo;t follow the framework but I think they should</h1>\n<p class=\"MsoNormal\">I have seen a variety of cases where I believe people don&rsquo;t follow the principles I advocate. There are certain types of errors that I think many ordinary people make and others that are more common for sophisticated people to make. Most of these boil down to giving too much weight to personal judgments, giving too much weight to people who are impressive to you personally but not impressive by clear and uncontroversial standards, or not putting enough weight on what elite common sense has to say.</p>\n<p class=\"MsoNormal\"><strong>Giving too much weight to the opinions of people like you: </strong>People tend to hold <a href=\"http://answers.google.com/answers/threadview?id=272042\">religious views</a> and <a href=\"http://www.gallup.com/poll/14515/teens-stay-true-parents-political-perspectives.aspx\">political views</a> that are similar to the views of their parents. Many of these people probably aren&rsquo;t trying to have accurate views. And the situation would be much better if people gave more weight to the aggregated opinion of a broader coalition of perspectives.</p>\n<p class=\"MsoNormal\">I think a different problem arises in the LessWrong and <a href=\"http://effective-altruism.com/\">effective altruism</a> communities. In this case, people are much more reflectively choosing which sets of people to get their beliefs from, and I believe they are getting beliefs from some pretty good people. However, taking an outside perspective, it seems overwhelmingly likely that these communities are subject to their own biases and blind spots, and the people who are most attracted to these communities are most likely to suffer from the same biases and blind spots. I suspect elite common sense would take these communities more seriously than it currently does if it had access to more information about the communities, but I don&rsquo;t think it would take us sufficiently seriously to justify having high confidence in many of our more unusual views.</p>\n<p class=\"MsoNormal\"><strong>Being overconfident on open questions where we don&rsquo;t have a lot of evidence to work with: </strong>In my experience, it is common to give little weight to common sense takes on questions about which there is no generally accepted answer, even when it is impossible to use commonsense reasoning to arrive at conclusions that get broad support. Some less sophisticated people seem to see this as a license to think whatever they want, as Paul Graham <a href=\"http://www.paulgraham.com/identity.html\">has commented</a> in the case of politics and religion. I meet many more sophisticated people with unusual views about big picture philosophical, political, and economic questions in areas where they have very limited inside information and very limited information about the distribution of expert opinion. For example, I have now met a reasonably large number of non-experts who have very confident, detailed, unusual opinions about meta-ethics, libertarianism, and optimal methods of taxation. When I challenge people about this, I usually get some version of &ldquo;people are not good at thinking about this question&rdquo; but rarely a detailed explanation of why this person in particular is an exception to this generalization (more on this problem below).</p>\n<p class=\"MsoNormal\">There&rsquo;s an inverse version of this problem where people try to &ldquo;suspend judgment&rdquo; on questions where they don&rsquo;t have high-quality evidence, but actually end up taking very unusual stances without adequate justification. For example, I sometimes talk with people who say that improving the very long-term future would be <a href=\"https://sites.google.com/site/nbeckstead/research\">overwhelmingly important</a> if we could do it, but are skeptical about whether we can. In response, I sometimes run arguments of the form:<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;</span></p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">In expectation, it is possible to improve broad feature X of the world (education, governance quality, effectiveness of the scientific community, economic prosperity).</span></li>\n<li><span style=\"text-indent: -0.25in;\">If we improve feature X, it will help future people deal with various big challenges and opportunities better in expectation.</span></li>\n<li><span style=\"text-indent: -0.25in;\">If people deal with these challenges and opportunities better in expectation, the future will be better in expectation.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Therefore, it is possible to make the future better in expectation.</span></li>\n</ol>\n<p class=\"MsoNormal\">I&rsquo;ve presented some preliminary thoughts on related issues <a href=\"http://intelligence.org/wp-content/uploads/2013/07/Beckstead-Evaluating-Options-Using-Far-Future-Standards.pdf\">here</a>. Some people try to resist this argument on grounds of general skepticism about attempts at improving the world that haven&rsquo;t been documented with high-quality evidence. Peter Hurford&rsquo;s post on &ldquo;<a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">speculative causes</a>&rdquo; is the closest example that I can point to online, though I&rsquo;m not sure whether he still disagrees with me on this point. I believe that there can be some adjustment in the direction of skepticism in light of arguments that GiveWell has articulated <a href=\"http://blog.givewell.org/2013/04/04/deep-value-judgments-and-worldview-characteristics/\">here</a> under &ldquo;we are relatively skeptical,&rdquo; but I consider rejecting the second premise on these grounds a significant departure from elite common sense. I would have a similar view about anyone who rejected any of the other premises&mdash;at least if they rejected them for all values of X&mdash;for such reasons. It&rsquo;s not that I think the presumption in favor of elite common sense can&rsquo;t be overcome&mdash;I strongly favor thinking about such questions more carefully and am open to changing my mind&mdash;it&rsquo;s just that I don&rsquo;t think it can be overcome by these types of skeptical considerations. Why not? These types of considerations seem like they could make the probability distribution over impact on the very long-term narrower, but I don&rsquo;t see how they could put it tightly around zero. And in any case, GiveWell articulates other considerations in <a href=\"http://blog.givewell.org/2013/04/04/deep-value-judgments-and-worldview-characteristics/\">that post</a> and <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">other posts</a> which point in favor of less skepticism about the second premise.</p>\n<p class=\"MsoNormal\">Part of the issue may be confusion about &ldquo;rejecting&rdquo; a premise and &ldquo;suspending judgment.&rdquo; In my view, the question is &ldquo;What are the expected long-term effects of improving factor X?&rdquo; You can try not to think about this question or say &ldquo;I don&rsquo;t know,&rdquo; but when you make decisions you are implicitly committed to certain ranges of expected values on these questions. To justifiably ignore very long-term considerations, I think you probably need your implicit range to be close to zero. I often see people who say they are &ldquo;suspending judgment&rdquo; about these issues or who say they &ldquo;don&rsquo;t know&rdquo; acting as if this ranger were very close to zero. I see this as a very strong, precise claim which is contrary to elite common sense, rather than an open-minded, &ldquo;we&rsquo;ll wait until the evidence comes in&rdquo; type of view to have. Another way to put it is that my claim that improving some broad factor X has good long-run consequences is much more of an <a href=\"/lw/wm/disjunctions_antipredictions_etc/\">anti-prediction</a> than the claim that its expected effects are close to zero. (Independent point: I think that a more compelling argument than the argument that we can&rsquo;t affect the far future is the argument that that lots of ordinary actions have <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a> with astronomical expected impacts if anything does, so that people aiming explicitly at reducing <a href=\"/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/\">astronomical waste</a> are less privileged than one might think at first glance. I hope to write more about this issue in the future.)</p>\n<p class=\"MsoNormal\"><strong>Putting too much weight on your own opinions because you have better arguments on topics that interest you than other people, or the people you typically talk to:</strong> As mentioned above, I believe that some smart people, especially smart people who rely a lot on explicit reasoning, can become very good at developing strong arguments for their opinions without being very good at finding true beliefs. I think that in such instances, these people will generally not be very successful at getting a broad coalition of impressive people to accept their views (except perhaps by relying on non-rational methods of persuasion). Stress-testing your views by trying to actually convince others of your opinions, rather than just out-arguing them, can help you avoid this trap.</p>\n<p class=\"MsoNormal\"><strong>Putting too much weight on the opinions of single individuals who seem trustworthy to you personally but not to people in general, and have very unusual views: </strong>I have seen some people update significantly in favor of very unusual philosophical, scientific, and sociological claims when they encounter very intelligent advocates of these views. These people are often familiar with <a href=\"http://en.wikipedia.org/wiki/Aumann's_agreement_theorem\">Aumann&rsquo;s agreement theorem</a> and arguments for <a href=\"http://philsci-archive.pitt.edu/2940/1/refdis.pdf\">splitting the difference</a> with epistemic peers, and they are rightly troubled by the fact that someone fairly similar to them disagrees with them on an issue, so they try to correct for their own potential failures of rationality by giving additional weight to the advocates of these very unusual views.</p>\n<p class=\"MsoNormal\">However, I believe that taking disagreement seriously favors giving these very unusual views less weight, not more. The problem partly arises because philosophical discussion of disagreement often focuses on the simple case of two people sharing their evidence and opinions with each other. But what&rsquo;s more relevant is the distribution of quality-weighted opinion around the world <em>in general</em>, not the distribution of quality-weighted opinion of the people that you have had discussions with, and not the distribution of quality-weighted opinion of the people that seem trustworthy to you personally. The epistemically modest move here is to try to stay closer to elite common sense, not to split the difference.</p>\n<h1>Objections to this approach</h1>\n<h2>Objection: elite common sense is often wrong</h2>\n<p class=\"MsoNormal\">One objection I often hear is that elite common sense is often wrong. I believe this is true, but not a problem for my framework. I make the comparative claim that elite common sense is more trustworthy than the idiosyncratic standards of the vast majority of individual people, not the claim that elite common sense is almost always right. A further consideration is that analogous objections to analogous views fail. For instance, &ldquo;markets are often wrong in their valuation of assets&rdquo; is not a good objection to the efficient markets hypothesis. As explained above, the argument that &ldquo;markets are often wrong&rdquo; needs to point to specific way in which one can do better than the market in order for it to make sense to place less weight on what the market says than on one&rsquo;s own judgments.</p>\n<h2>Objection: the best people are highly unconventional</h2>\n<p class=\"MsoNormal\">Another objection I sometimes hear is that the most successful people often pay the least attention to conventional wisdom. I think this is true, but not a problem for my framework. One reason I believe this is that, according to my framework, when you go against elite common sense, what matters is whether elite common sense reasoning standards would justify your opinion if someone following those standards knew about your background, information, and analysis. Though I can&rsquo;t prove it, I suspect that the most successful people are often depart from elite common sense in ways that elite common sense would endorse if it had access to more information. I also believe that the most successful people tend to pay attention to elite common sense in many areas, and specifically bet against elite common sense in areas where they are most likely to be right.</p>\n<p class=\"MsoNormal\">A second consideration is that going against elite common sense may be a high-risk strategy, so that it is unsurprising if we see the most successful people pursuing it. People who give less weight to elite common sense are more likely to spend their time on pointless activities, join cults, and become crackpots, though they are also more likely to have revolutionary positive impacts. Consider an analogy: it may be that the gamblers who earned the most used the riskiest strategies, but this is not good evidence that you should use a risky strategy when gambling because the people who lost the most also played risky strategies.</p>\n<p class=\"MsoNormal\">A third consideration is that while it may be unreasonable to be too much of an independent thinker in a particular case, being an independent thinker helps you develop good epistemic habits. I think this point has a lot of merit, and could help explain why independent thinking is more common among the most successful people. This might seem like a good reason not to pay much attention to elite common sense. However, it seems to me that you can get the best of both worlds by being an independent thinker and keeping separate track of your own impressions and what elite common sense would make of your evidence. Where conflicts come up, you can try to use elite common sense to guide your decisions.</p>\n<p class=\"MsoNormal\">I feel my view is weakest in cases where there is a strong upside to disregarding elite common sense, there is little downside, and you&rsquo;ll find out whether your bet against conventional wisdom was right within a tolerable time limit. Perhaps many crazy-sounding entrepreneurial ideas and scientific hypotheses fit this description. I believe it may make sense to pick a relatively small number of these to bet on, even in cases where you can&rsquo;t convince elite common sense that you are on the right track. But I also believe that in cases where you really do have a great but unconventional idea, it will be possible to convince a reasonable chunk of elite common sense that your idea is worth trying out.</p>\n<h2>Objection: elite common sense is wrong about X, and can&rsquo;t be talked out of it, so your framework should be rejected in general</h2>\n<p class=\"MsoNormal\">Another common objection takes the form: view X is true, but X is not a view which elite common sense would give much weight to. Eliezer makes a related argument <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">here</a>, though he is addressing a different kind of deference to common sense. He points to religious beliefs, beliefs about diet, and the rejection of cryonics as evidence that you shouldn&rsquo;t just follow what the majority believes. My position is closer to &ldquo;follow the majority&rsquo;s epistemic standards&rdquo; than &ldquo;believe what the majority beliefs,&rdquo; and closer still to &ldquo;follow the best people&rsquo;s epistemic standards without cherry picking &ldquo;best&rdquo; to suit your biases,&rdquo; but objections of this form could have some force against the framework I have defended.</p>\n<p class=\"MsoNormal\">A first response is that unless one thinks there are many values of X in different areas where my framework fails, providing a few counterexamples is not very strong evidence that the framework isn&rsquo;t helpful in many cases. This is a general issue in philosophy which I think is underappreciated, and I&rsquo;ve made related arguments in chapter 2 of <a href=\"https://sites.google.com/site/nbeckstead/research\">my dissertation</a>. I think the most likely outcome of a careful version of this attack on my framework is that we identify some areas where the framework doesn&rsquo;t apply or has to be qualified.</p>\n<p class=\"MsoNormal\">But let&rsquo;s delve into the question about religion in greater detail. Yes, having some religious beliefs is generally more popular than being an atheist, and it would be hard to convince intelligent religious people to become atheists. However, my impression is that my framework does not recommend believing in God for the following reasons. Here are a number of weak arguments for this claim:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">My impression is that the people who are most trustworthy by clear and generally accepted standards are significantly more likely to be atheists than the general population. One illustration of my perspective is that in a 1998 </span><a style=\"text-indent: -0.25in;\" href=\"http://www.stephenjaygould.org/ctrl/news/file002.html\">survey</a><span style=\"text-indent: -0.25in;\"> of the National Academy of Sciences, only 7% of respondents reported that they believed in God. However, there is a flame war and people have pushed many arguments on this issue, and scientists are probably unrepresentative of many trustworthy people in this respect.</span></li>\n<li><span style=\"text-indent: -0.25in;\">While the world at large has broad agreement that some kind of higher power exists, there is very substantial disagreement about what this means, to the point where it isn&rsquo;t clear that these people are talking about the same thing.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In my experience, people generally do not try very hard to have accurate beliefs about religious questions and have little patience for people who want to carefully discuss arguments about religious questions at length. This makes it hard to stress-test one&rsquo;s views about religion by trying to get a broad coalition of impressive people to accept atheism, and makes it possible to give more weight to one&rsquo;s personal take if one has thought unusually carefully about religious questions.</span></li>\n<li><span style=\"text-indent: -0.25in;\">People are generally raised in religious families, and there are substantial social incentives to remain religious. Social incentives for atheists to remain non-religious generally seem weaker, though they can also be substantial. For example, given my current social network, I believe I would pay a significant cost if I wanted to become religious.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Despite the above point, in my experience, it is much more common for religious people to become atheists than it is for atheists to become religious.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In my experience, among people who try very hard to have accurate beliefs about whether God exists, atheism is significantly more common than belief in God.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In my experience, the most impressive people who are religious tend not to behave much differently from atheists or have different takes on scientific questions/questions about the future.</span></li>\n</ol>\n<p class=\"MsoNormal\">These points rely a lot on my personal experience, could stand to be researched more carefully, and feel uncomfortably close to lousy <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">contrarian excuses</a>, but I think they are nevertheless suggestive. In light of these points, I think my framework recommends that the vast majority of people with religious beliefs should be substantially less confident in their views, recommends modesty for atheists who haven&rsquo;t tried very hard to be right, and I suspect it allows reasonably high confidence that God doesn&rsquo;t exist for people who have strong indicators that they have thought carefully about the issue. I think it would be better if I saw a clear and principled way for the framework to push more strongly in the direction of atheism, but the case has enough unusual features that I don&rsquo;t see this as a major argument against the general helpfulness of the framework.</p>\n<p class=\"MsoNormal\">As a more general point, the framework seems less helpful in the case of religion and politics because people are generally unwilling to carefully consider arguments with the goal of having accurate beliefs. By and large, when people are unwilling to carefully consider arguments with the goal of having accurate beliefs, this is evidence that it is not useful to try to think carefully about this area. This follows from the idea mentioned above that people tend to try to have accurate views when it is in their present interests to have accurate views. So if this is the main way the framework breaks down, then the framework is mostly breaking down in cases where good epistemology is relatively unimportant.</p>\n<h1>Conclusion</h1>\n<p class=\"MsoNormal\">I&rsquo;ve outlined a framework for taking account of the distribution of opinions and epistemic standards in the world and discussed some of its strengths and weaknesses. I think the largest strengths of the framework are that it can help you avoid falling prey to idiosyncratic personal biases, and that using it derives benefits from the &ldquo;wisdom of crowds&rdquo; effects. The framework is less helpful in:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">cases where there is a large upside to disregarding elite common sense, there is little downside, and you&rsquo;ll find out whether your bet against conventional wisdom was right within a tolerable time limit, and</span></li>\n<li><span style=\"text-indent: -0.25in;\">cases where people are unwilling to carefully consider arguments with the goal of having accurate beliefs.</span></li>\n</ol>\n<p class=\"MsoNormal\">Some questions for people who want to further develop the framework include:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">How sensitive is the framework to other reasonable choices of standards for selecting trustworthy people? Are there more helpful standards to use?</span></li>\n<li><span style=\"text-indent: -0.25in;\">How sensitive is the framework to reasonable choices of standards for aggregating opinions of trustworthy people?</span></li>\n<li><span style=\"text-indent: -0.25in;\">What are the best ways of getting a better grip on elite common sense?</span></li>\n<li><span style=\"text-indent: -0.25in;\">What other areas are there where the framework is particularly weak or particularly strong?</span></li>\n<li><span style=\"text-indent: -0.25in;\">Can the framework be developed in ways that make it more helpful in cases where it is weakest?</span></li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "5f5c37ee1b5cdee568cfb19d": 2, "qf3kDBak4BQDDw3f2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wgdfxQJ2DQuju73zC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 52, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "23712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h1 id=\"Introduction\">Introduction</h1>\n<p class=\"MsoNormal\">[I have edited the introduction of this post for increased clarity.]</p>\n<p class=\"MsoNormal\">This post is my attempt to answer the question, \"How should we take account of the distribution of opinion and epistemic standards in the world?\"&nbsp;By \u201cepistemic standards,\u201d I roughly mean a person\u2019s way of processing evidence to arrive at conclusions. If people were good Bayesians, their epistemic standards would correspond to their fundamental prior probability distributions. At a first pass, my answer to this questions is:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong>Main Recommendation</strong>: Believe what you think a broad coalition of trustworthy people would believe if they were trying to<span style=\"white-space: pre;\">&nbsp;</span>have accurate views and they had access to your evidence.</p>\n<p class=\"MsoNormal\">The rest of the post can be seen as an attempt to spell this out more precisely and to explain, in practical terms, how to follow the recommendation. Note that there are therefore two broad ways to disagree with the post: you might disagree with the main recommendation, or the guidelines for following main recommendation.</p>\n<div>The rough idea is to try find a group of people whose are trustworthy by clear and generally accepted indicators, and then use an impartial combination of the reasoning standards that they use when they are trying to have accurate views.&nbsp;I call this impartial combination <em>elite common sense</em>. I recommend using elite common sense as a prior in two senses. First, if you have no unusual information about a question, you should start with the same opinions as the broad coalition of trustworthy people would have. But their opinions are not the last word, and as you get more evidence, it can be reasonable to disagree. Second, a complete prior probability distribution specifies, for any possible set of evidence, what posterior probabilities you should have. In this deeper sense, I am not just recommending that you start with the same opinions as elite common sense, but also you update in ways that elite common sense would agree are the right ways to update.&nbsp;In practice, we can\u2019t specify the prior probability distribution of elite common sense or calculate the updates, so the framework is most useful from a conceptual perspective. It might also be useful to consider the output of this framework as one model in a larger <a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination</a>.</div>\n<div><br></div>\n<p class=\"MsoNormal\">I am aware of two relatively close intellectual relatives to my framework: what philosophers call \u201c<a href=\"http://philsci-archive.pitt.edu/2940/1/refdis.pdf\">equal weight</a>\u201d or \u201cconciliatory\u201d views about disagreement and what people on LessWrong may know as \u201c<a href=\"http://www.overcomingbias.com/2007/03/on_majoritarian.html\">philosophical majoritarianism</a>.\u201d Equal weight views roughly hold that when two people who are expected to be roughly equally competent at answering a certain question have different subjective probability distributions over answers to that question, those people should adopt some impartial combination of their subjective probability distributions. Unlike equal weight views in philosophy, my position is meant as a set of rough practical guidelines rather than a set of exceptionless and fundamental rules. I accordingly focus on practical issues for applying the framework effectively and am open to limiting the framework\u2019s scope of application. Philosophical majoritarianism is the idea that on most issues, the average opinion of humanity as a whole will be a better guide to the truth than one\u2019s own personal judgment. My perspective differs from both equal weight views and philosophical majoritarianism in that it emphasizes an elite subset of the population rather than humanity as a whole and that it emphasizes epistemic standards more than individual opinions. My perspective differs from what you might call \"elite majoritarianism\" in that, according to me, you can disagree with what very trustworthy people think on average if you think that those people would accept your views if they had access to your evidence and were trying to have accurate opinions.</p>\n<p class=\"MsoNormal\">I am very grateful to Holden Karnofsky and Jonah Sinick for thought-provoking conversations on this topic which led to this post. Many of the ideas ultimately derive from Holden\u2019s thinking, but I've developed them, made them somewhat more precise and systematic, discussed additional considerations for and against adopting them, and put everything in my own words. I am also grateful to Luke Muehlhauser and Pablo Stafforini for feedback on this post.</p>\n<p class=\"MsoNormal\">In the rest of this post I will:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">Outline the framework and offer guidelines for applying it effectively. I explain why I favor relying on the epistemic standards of people who are trustworthy by clear indicators that many people would accept, why I favor paying more attention to what people think than why they say they think it (on the margin), and why I favor stress-testing critical assumptions by attempting to convince a broad coalition of trustworthy people to accept them.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Offer some considerations in favor of using the framework.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Respond to the objection that common sense is often wrong, the objection that the most successful people are very unconventional, and objections of the form \u201celite common sense is wrong about X and can\u2019t be talked out of it.\u201d</span></li>\n<li><span style=\"text-indent: -0.25in;\">Discuss some limitations of the framework and some areas where it might be further developed. I suspect it is weakest in cases where there is a large upside to disregarding elite common sense, there is little downside, and you\u2019ll find out whether your bet against conventional wisdom was right within a tolerable time limit, and cases where people are unwilling to carefully consider arguments with the goal of having accurate beliefs.</span></li>\n</ol>\n<div style=\"text-indent: -24px;\"><br></div>\n<p><span style=\"text-indent: -0.25in;\"><a id=\"more\"></a></span></p>\n<ol> </ol>\n<h1 id=\"An_outline_of_the_framework_and_some_guidelines_for_applying_it_effectively\">An outline of the framework and some guidelines for applying it effectively</h1>\n<p class=\"MsoNormal\">My suggestion is to use elite common sense as a prior rather than the standards of reasoning that come most naturally to you personally. The three main steps for doing this are:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">Try to find out what people who are trustworthy by clear indicators that many people would accept believe about the issue.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Identify the information and analysis you can bring to bear on the issue.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Try to find out what elite common sense would make of this information and analysis, and adopt a similar perspective.</span></li>\n</ol>\n<p class=\"MsoNormal\">On the first step, people often have an instinctive sense of what others think, though you should beware the <a href=\"http://en.wikipedia.org/wiki/False-consensus_effect\">false consensus effect</a>. If you don\u2019t know what other opinions are out there, you can ask some friends or search the internet. In my experience, regular people often have similar opinions to very smart people on many issues, but are much worse at articulating considerations for and against their views. This may be because many people copy the opinions of the most trustworthy people.</p>\n<p class=\"MsoNormal\"><strong>I favor giving more weight to the opinions of people who can be shown to be trustworthy by clear indicators that many people would accept, rather than people that seem trustworthy to you personally.</strong> This guideline is intended to help avoid parochialism and increase <a href=\"/lw/dyk/selfskepticism_the_first_principle_of_rationality/\">self-skepticism</a>. Individual people have a variety of biases and blind spots that are hard for them to recognize. Some of these biases and blind spots\u2014like the <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">ones studied in cognitive science</a>\u2014may affect almost everyone, but others are idiosyncratic\u2014like biases and blind spots we inherit from our families, friends, business networks, schools, political groups, and religious communities. It is plausible that combining independent perspectives can help idiosyncratic errors wash out.</p>\n<p class=\"MsoNormal\">In order for the errors to wash out, it is important to rely on the standards of people who are trustworthy by clear indicators that many people would accept rather than the standards of people that seem trustworthy to you personally. &nbsp;Why? The people who seem most impressive to us personally are often people who have similar strengths and weaknesses to ourselves, and similar biases and blind spots. For example, I suspect that academics and people who specialize in using a lot of explicit reasoning have a different set of strengths and weaknesses from people who rely more on implicit reasoning, and people who rely primarily on <a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">many weak arguments</a> have a different set of strengths and weaknesses from people who rely more on one relatively strong line of argument.</p>\n<p class=\"MsoNormal\">Some good indicators of general trustworthiness might include: IQ, business success, academic success, generally respected scientific or other intellectual achievements, wide acceptance as an intellectual authority by certain groups of people, or success in any area where there is intense competition and success is a function of ability to make accurate predictions and good decisions. I am less committed to any particular list of indicators than the general idea.</p>\n<p class=\"MsoNormal\">Of course, trustworthiness can also be domain-specific. Very often, elite common sense would recommend deferring to the opinions of experts (e.g., listening to what physicists say about physics, what biologists say about biology, and what doctors say about medicine). In other cases, elite common sense may give partial weight to what putative experts say without accepting it all (e.g. economics and psychology). In other cases, they may give less weight to what putative experts say (e.g. sociology and philosophy). Or there may be no putative experts on a question. In cases where elite common sense gives less weight to the opinions of putative experts or there are no plausible candidates for expertise, it becomes more relevant to think about what elite common sense would say about a question.</p>\n<p class=\"MsoNormal\">How should we assign weight to different groups of people? <strong>Other things being equal, a larger number of people is better, more trustworthy people are better, people who are trustworthy by clearer indicators that more people would accept are better, and a set of criteria which allows you to have some grip on what the people in question think is better, but you have to make trade-offs.</strong> If I only included, say, the 20 smartest people I had ever met as judged by me personally, that would probably be too small a number of people, the people would probably have biases and blind spots very similar to mine, and I would miss out on some of the most trustworthy people, but it would be a pretty trustworthy collection of people and I\u2019d have some reasonable sense of what they would say about various issues. If I went with, say, the 10 most-cited people in 10 of the most intellectually credible academic disciplines, 100 of the most generally respected people in business, and the 100 heads of different states, I would have a pretty large number of people and a broad set of people who were very trustworthy by clear standards that many people would accept, but I would have a hard time knowing what they would think about various issues because I haven\u2019t interacted with them enough. How these factors can be traded-off against each other in a way that is practically most helpful probably varies substantially from person to person.</p>\n<p class=\"MsoNormal\">I can\u2019t give any very precise answer to the question about whose opinions should be given significant weight, even in my own case. Luckily, I think the output of this framework is usually not very sensitive to how we answer this question, partly because most people would typically defer to other, more trustworthy people. If you want a rough guideline that I think many people who read this post could apply, I would recommend focusing on, say, the opinions of the top 10% of people who got Ivy-League-equivalent educations (note that I didn\u2019t get such an education, at least as an undergrad, though I think you should give weight to my opinion; I\u2019m just giving a rough guideline that I think works reasonably well in practice). You might give some additional weight to more accomplished people in cases where you have a grip on how they think.</p>\n<p class=\"MsoNormal\"><strong>I don\u2019t have a settled opinion about how to aggregate the opinions of elite common sense.</strong> I suspect that taking straight averages gives too much weight to the opinions of cranks and crackpots, so that you may want to remove some outliers or give less weight to them. For the purpose of making decisions, I think that sophisticated voting methods (such as the <a href=\"http://en.wikipedia.org/wiki/Condorcet_method\">Condorcet method</a>) and analogues of the <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">parliamentary approaches</a> outlined by Nick Bostrom and Toby Ord seem fairly promising as rough guidelines in the short run. I don\u2019t do calculations with this framework\u2014as I said, it\u2019s mostly conceptual\u2014so uncertainty about an aggregation procedure hasn\u2019t been a major issue for me.</p>\n<p class=\"MsoNormal\"><strong>On the margin, I favor paying more attention to people\u2019s opinions than their explicitly stated reasons for their opinions.</strong> Why? One reason is that I believe people can have highly adaptive opinions and patterns of reasoning without being able to articulate good defenses of those opinions and/or patterns of reasoning. (Luke Muehlhauser has discussed some related points <a href=\"/lw/h6b/explicit_and_tacit_rationality/\">here</a>.) One reason is that people can adopt practices that are successful without knowing why they are successful, others who interact with them can adopt those practices, others who interact with them can adopt those practices, and so forth. I heard an extreme example of this from Spencer Greenberg, who had read it in <a href=\"http://www.amazon.com/Scientists-Greater-than-Einstein-ebook/dp/B002TKLC1A/ref=sr_1_1?ie=UTF8&amp;qid=1374885896&amp;sr=8-1&amp;keywords=scientists+greater+than+einstein\">Scientists Greater than Einstein</a>. The story involved a folk remedy for visual impairment:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">There were folk remedies worthy of study as well. One widely used in Java on children with either night blindness or Bitot\u2019s spots consisted of dropping the juices of lightly roasted lamb\u2019s liver into the eyes of affected children. Sommer relates, \u201cWe were bemused at the appropriateness of this technique and wondered how it could possibly be effective. We, therefore, attended several treatment sessions, which were conducted exactly as the villagers had described, except for one small addition\u2014rather than discarding the remaining organ, they fed it to the affected child. For some unknown reason this was never considered part of the therapy itself.\u201d Sommer and his associates were bemused, but now understood why the folk remedy had persisted through the centuries. Liver, being the organ where vitamin A is stored in a lamb or any other animal, is the best food to eat to obtain vitamin A. (p. 14)</p>\n<p class=\"MsoNormal\">Another striking example is bedtime prayer. In many Christian traditions I am aware of, it is common to pray before going to sleep. And in the tradition I was raised in, the main components of prayer were listing things you were grateful for, asking for forgiveness for all the mistakes you made that day and thinking about what you would do to avoid similar mistakes in the future, and asking God for things. Christians might say the point of this is that it is a duty to God, that repentance is a requirement for entry to heaven, or that asking God for things makes God more likely to intervene and create miracles. However, I think these activities are reasonable for different reasons: <a href=\"/lw/i0c/for_happiness_keep_a_gratitude_journal/\">gratitude journals</a> are great, reflecting on mistakes is a great way to learn and overcome weaknesses, and it is a good idea to get clear about what you really want out of life in the short-term and the long-term.</p>\n<p class=\"MsoNormal\">Another reason I have this view is that if someone has an effective but different intellectual style from you, it\u2019s possible that your biases and blind spots will prevent you from appreciating their points that have significant merit. If you partly give weight to opinions independently of how good the arguments seem to you personally, this can be less of an issue for you. Jonah Sinick described a striking reason this might happen in <a href=\"/lw/hne/many_weak_arguments_and_the_typical_mind/\">Many Weak Arguments and the Typical Mind</a>:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\"><strong>We should pay more attention to people\u2019s bottom line than to their stated reasons</strong>&nbsp;\u2014 If most high functioning people aren\u2019t relying heavily on any one of the arguments that they give, if a typical high functioning person responds to a query of the type \u201cWhy do you think X?\u201d by saying \u201cI believe X because of argument Y\u201d we shouldn\u2019t conclude that the person believes argument Y with high probability. Rather, we should assume that argument Y is one of many arguments that they believe with low confidence, most of which they\u2019re not expressing, and we should focus on their belief in X instead of argument Y. [emphasis his]</p>\n<p class=\"MsoNormal\">This idea interacts in a complementary way to Luke Muehlhauser\u2019s claim that some people who are not skilled at explicit rationality may be skilled in <a href=\"/lw/h6b/explicit_and_tacit_rationality/\">tacit rationality</a>, allowing them to be successful at making many types of important decisions. If we are interacting with such people, we should give significant weight to their opinions independently of their stated reasons.</p>\n<p class=\"MsoNormal\">A counterpoint to my claim that, on the margin, we should give more weight to others\u2019 conclusions and less to their reasoning is that some very impressive people disagree. For example, Ray Dalio is the founder of Bridgewater, which, at least as of 2011, was the <a href=\"http://en.wikipedia.org/wiki/List_of_hedge_funds#The_Largest_Hedge_Fund_Firms\">world\u2019s largest hedge fund</a>. He explicitly disagrees with my claim:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">\u201cI stress-tested my opinions by having the smartest people I could find challenge them so I could find out where I was wrong. I never cared much about others\u2019 conclusions\u2014only for the reasoning that led to these conclusions. That reasoning had to make sense to me. Through this process, I improved my chances of being right, and I learned a lot from a lot of great people.\u201d (p. 7 of <a href=\"https://www.principles.com/#Principles\">Principles</a> by Ray Dalio)</p>\n<p class=\"MsoNormal\">I suspect that getting the reasoning to make sense to him was important because it helped him to get better in touch with elite common sense, and also because reasoning is more important when dealing with very formidable people, as I suspect Dalio did and does. I also think that for the some of the highest functioning people who are most in touch with elite common sense, it may make more sense to give more weight to reasoning than conclusions.</p>\n<p class=\"MsoNormal\"><strong>The elite common sense framework favors testing unconventional views by seeing if you can convince a broad coalition of impressive people that your views are true. </strong>If you can do this, it is often good evidence that your views are supported by elite common sense standards. If you can\u2019t, it\u2019s often good evidence that your views can\u2019t be so supported. Obviously, these are rules of thumb and we should restrict our attention to cases where you are persuading people by rational means, in contrast with using rhetorical techniques that exploit human biases. There are also some interesting cases where, for one reason or another, people are unwilling to hear your case or think about your case rationally, and applying this guideline to these cases is tricky.</p>\n<p class=\"MsoNormal\">Importantly, I don\u2019t think cases where elite common sense is biased are typically an exception to this rule. In my experience, I have very little difficulty convincing people that some genuine bias, such as scope insensitivity, really is biasing their judgment. And if the bias really is critical to the disagreement, I think it will be a case where you can convince elite common sense of your position. Other cases, such as deeply entrenched religious and political views, may be more of an exception, and I will discuss the case of religious views more in a later section.</p>\n<p class=\"MsoNormal\"><strong>The distinction between convincing and \u201cbeating in an argument\u201d is important for applying this principle.</strong> It is much easier to tell whether you convinced someone than it is to tell whether you beat them in an argument. Often, both parties think they won. In addition, sometimes it is rational not to update much in favor of a view if an advocate for that view beats you in an argument.</p>\n<p class=\"MsoNormal\">In support of this claim, consider what would happen if the world\u2019s smartest creationist debated some fairly ordinary evolution-believing high school student. The student would be destroyed in argument, but the student should not reject evolution, and I suspect he should hardly update at all. Why not? The student should know that there are people out there in the world who could destroy him on either side of this argument, and his personal ability to respond to arguments is not very relevant. What should be most relevant to this student is the distribution of opinion among people who are most trustworthy, not his personal response to small sample of the available evidence. Even if you genuinely are beating people in arguments, there is a risk that you will be like this creationist debater.</p>\n<p class=\"MsoNormal\">An additional consideration is that certain beliefs and practices may be reasonable and adopted for reasons that are not accessible to people who have adopted those beliefs and practices, as illustrated with the examples of the liver ritual and bedtime prayer. You might be able to \u201cbeat\u201d some Christian in an argument about the merits of bedtime prayer, but praying may still be better than not praying. (I think it would be better still to introduce a different routine that serves similar functions\u2014this is something I have done in my own life\u2014but the Christian may be doing better than you on this issue if you don\u2019t have a replacement routine yourself.)</p>\n<p class=\"MsoNormal\"><strong>Under the elite common sense framework, the question is not \u201chow reliable is elite common sense?\u201d but \u201chow reliable is elite common sense compared to me?\u201d </strong>Suppose I learn that, actually, people are much worse at pricing derivatives than I previously believed. For the sake of argument suppose this was a lesson of the 2008 financial crisis (for the purposes of this argument, it doesn't matter whether this is actually a correct lesson of the crisis). This information does not favor relying more on my own judgment unless I have reason to think that the bias applies less to me than the rest of the derivatives market. By analogy, it is not acceptable to say, \u201cPeople are really bad at thinking about philosophy. So I am going to give less weight to their judgments about philosophy (psst\u2026and more weight to my personal hunches and the hunches of people I personally find impressive).\u201d This is only OK if you have evidence that your personal hunches and the hunches of the people you personally find impressive are better than elite common sense, with respect to philosophy. In contrast, it might be acceptable to say, \u201cPeople are very bad at thinking about the consequences of agricultural subsidies in comparison with economists, and most trustworthy people would agree with this if they had my evidence. And I have an unusual amount of information about what economists think. So my opinion gets more weight than elite common sense in this case.\u201d Whether this ultimately is acceptable to say would depend on how good elites are at thinking about the consequences of agricultural subsidies\u2014I suspect they are actually pretty good at it\u2014but this is isn\u2019t relevant to the general point that I\u2019m making. The general point is that this is one potentially correct form of an argument that your opinion is better than the current stance of elite common sense.</p>\n<p class=\"MsoNormal\">This is partly a semantic issue, but I count the above example as a case where \u201cyou are more reliable than elite common sense,\u201d even though, in some sense, you are relying on expert opinion rather than your own. But you have different beliefs about who is a relevant expert or what experts say than common sense does, and in this sense you are relying on your own opinion.</p>\n<p class=\"MsoNormal\"><strong>I favor giving more weight to common sense judgments in cases where people are trying to have accurate views.</strong> For example, I think people don\u2019t try very hard to have correct political, religious, and philosophical views, but they do try to have correct views about how to do their job properly, how to keep their families happy, and how to impress their friends. In general, I expect people to try to have more accurate views in cases where it is in their present interests to have more accurate views. (A quick reference for this point is <a href=\"http://en.wikipedia.org/wiki/Rational_irrationality\">here</a>.) This means that I expect them to strive more for accuracy in decision-relevant cases, cases where the cost of being wrong is high, and cases where striving for more accuracy can be expected to yield more accuracy, though not necessarily in cases where the risks and rewards are won\u2019t come for a very long time. I suspect this is part of what explains why people can be skilled in <a href=\"/lw/h6b/explicit_and_tacit_rationality/\">tacit rationality</a> but not explicit rationality.</p>\n<p class=\"MsoNormal\">As I said above, what\u2019s critical is not how reliable elite common sense is but how reliable you are <em>in comparison with elite common sense</em>. So it only makes sense to give more weight to your views when learning that others aren\u2019t trying to be correct if you have compelling evidence that you <em>are</em> trying to be correct. Ideally, this evidence would be compelling to a broad class of trustworthy people and not just compelling to you personally.</p>\n<h1 id=\"Some_further_reasons_to_think_that_the_framework_is_likely_to_be_helpful\">Some further reasons to think that the framework is likely to be helpful</h1>\n<p class=\"MsoNormal\">In explaining the framework and outlining guidelines for applying it, I have given some reasons to expect this framework to be helpful. Here are some more <a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">weak arguments</a> in favor of my view:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">Some studies I haven\u2019t personally reviewed closely claim that combinations of expert forecasts are hard to beat. For instance, a review by (Clemen 1989) found that: \"</span>Considerable literature has accumulated over the years regarding the combination of forecasts. The primary conclusion of this line of research is that forecast accuracy can be substantially improved through the combination of multiple individual forecasts.\" (abstract) And a <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Ungar-et-al-The-Good-Judgment-Project-a-large-scale-test-of-different-methods-of-combining-expert-predictions.pdf\">recent work</a> by the <a href=\"http://www.goodjudgmentproject.com/\">Good Judgment Project</a> found that taking an average individual forecasts and transforming it away from .5 credence gave the lowest errors of a variety of different methods of aggregating judgments of forecasters (p. 42).</li>\n<li><span style=\"text-indent: -0.25in;\">There are </span><a style=\"text-indent: -0.25in;\" href=\"http://link.springer.com/article/10.1007/s11238-006-9004-4\">plausible</a><span style=\"text-indent: -0.25in;\"> </span><a style=\"text-indent: -0.25in;\" href=\"http://philsci-archive.pitt.edu/2940/1/refdis.pdf\">philosophical</a><span style=\"text-indent: -0.25in;\"> </span><a style=\"text-indent: -0.25in;\" href=\"http://www.overcomingbias.com/2007/03/on_majoritarian.html\">considerations</a><span style=\"text-indent: -0.25in;\"> suggesting that, absent special evidence, there is no compelling reason to favor your own epistemic standards over the epistemic standards that others use.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In practice, we are extremely reliant on conventional wisdom for almost everything we believe that isn\u2019t very closely related to our personal experience, and single individuals working in isolation have extremely limited ability to manipulate their environment in comparison with individuals who can build on the insights of others. To see this point, consider that a small group of very intelligent humans detached from all cultures wouldn\u2019t have much of an advantage at all over other animal species in competition for resources, but humans are </span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Anthropocene\">increasingly dominating</a><span style=\"text-indent: -0.25in;\"> the biosphere. A great deal of this must be chalked up to cultural accumulation of highly adaptive concepts, ideas, and procedures that no individual could develop on their own. I see trying to rely on elite common sense as highly continuous with this successful endeavor.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Highly adaptive practices and assumptions are more likely to get copied and spread, and these practices and assumptions often work because they help you to be right. If you use elite common sense as a prior, you\u2019ll be more likely to be working with more adaptive practices and assumptions.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Some successful processes for finding valuable information, such as </span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/PageRank\">PageRank</a><span style=\"text-indent: -0.25in;\"> and Quora, seem analogous to the framework I have outlined. PageRank is one algorithm that Google uses to decide how high different pages should be in searches, which is implicitly a way of ranking high-quality information. I\u2019m speaking about something I don\u2019t know very well, but my rough understanding is that PageRank gives pages more votes when more pages link to them, and votes from a page get more weight if that page itself has a lot of votes. This seems analogous to relying on elite common sense because information sources are favored when they are regarded as high quality by a broad coalition of other information sources. Quora seems analogous because it favors answers to questions that many people regard as good.</span></li>\n<li><span style=\"text-indent: -0.25in;\">I\u2019m going to go look at the first three questions I can find on Quora. I predict that I would prefer the answers that elite common sense would give to these questions to what ordinary common sense would say, and also that I would prefer elite common sense\u2019s answers to these questions to my own except in cases where I have strong inside information/analysis.&nbsp;</span><span style=\"text-indent: -0.25in;\">Results: </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Human-and-Animal-Senses/Would-a-person-who-was-born-blind-and-can-suddenly-see-be-able-to-recognize-a-shape-by-just-looking-at-it-for-the-first-time-I-E-If-he-was-looking-at-a-ball-for-the-first-time-ever-will-he-be-able-to-say-that-it-is-round\">1<sup>st</sup> question</a><span style=\"text-indent: -0.25in;\">: weakly prefer elite common sense, don\u2019t have much special information. </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Social-Media/Do-we-answer-questions-on-Quora-because-upvotes-give-us-a-falsehood-sense-of-satisfaction-like-retweets-on-Twitter-or-likes-on-Facebook\">2<sup>nd</sup> question</a><span style=\"text-indent: -0.25in;\">: prefer elite common sense, don\u2019t have much special information. </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Does-the-conservation-of-momentum-and-conservation-of-energy-apply-to-electrons-that-have-light-shone-on-them-and-if-so-why-cant-we-tell-their-position-and-velocity-by-measuring-the-amount-of-light-it-absorbed-and-the-direction-of-the-reflected-light\">3<sup>rd</sup> question</a><span style=\"text-indent: -0.25in;\">: prefer elite common sense, don\u2019t have much special information. Note that I skipped </span><a style=\"text-indent: -0.25in;\" href=\"http://www.quora.com/Web-Services/What-kind-of-online-service-you-wish-had-already-been-there\">a question</a><span style=\"text-indent: -0.25in;\"> because it was a matter of taste. This went essentially the way I predicted it to go.</span></li>\n<li><span style=\"text-indent: -0.25in;\">The type of mathematical considerations underlying </span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Condorcet's_jury_theorem\">Condorcet\u2019s Jury Theorem</a><span style=\"text-indent: -0.25in;\"> give us some reason to think that combined opinions are often more reliable than individual opinions, even though the assumptions underlying this theorem are far from totally correct.</span></li>\n<li><span style=\"text-indent: -0.25in;\">There\u2019s a general cluster of social science findings that goes under the heading \u201c</span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">wisdom of crowds</a><span style=\"text-indent: -0.25in;\">\u201d and suggests that aggregating opinions across people outperforms individual opinions in many contexts.</span></li>\n<li><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;</span><span style=\"text-indent: -0.25in;\">Some rough \u201c</span><a style=\"text-indent: -0.25in;\" href=\"http://en.wikipedia.org/wiki/Marketplace_of_ideas\">marketplace of ideas</a><span style=\"text-indent: -0.25in;\">\u201d arguments suggest that the best ideas will often become part of elite common sense. When claims are decision-relevant, people pay if they have dumb beliefs and benefit if they have smart beliefs. When claims aren\u2019t decision-relevant, people sometimes pay a social cost for saying dumb things and get social benefits for saying things that are smarter, and the people with more information have more incentive to speak. For analogous reasons, when people use and promote epistemic standards that are dumb, they pay costs and when they use and promote epistemic standards that are smart. Obviously there are many other factors, including ones that point in different directions, but there is some kind of positive force here.</span></li>\n</ol>\n<h1 id=\"Cases_where_people_often_don_t_follow_the_framework_but_I_think_they_should\">Cases where people often don\u2019t follow the framework but I think they should</h1>\n<p class=\"MsoNormal\">I have seen a variety of cases where I believe people don\u2019t follow the principles I advocate. There are certain types of errors that I think many ordinary people make and others that are more common for sophisticated people to make. Most of these boil down to giving too much weight to personal judgments, giving too much weight to people who are impressive to you personally but not impressive by clear and uncontroversial standards, or not putting enough weight on what elite common sense has to say.</p>\n<p class=\"MsoNormal\"><strong>Giving too much weight to the opinions of people like you: </strong>People tend to hold <a href=\"http://answers.google.com/answers/threadview?id=272042\">religious views</a> and <a href=\"http://www.gallup.com/poll/14515/teens-stay-true-parents-political-perspectives.aspx\">political views</a> that are similar to the views of their parents. Many of these people probably aren\u2019t trying to have accurate views. And the situation would be much better if people gave more weight to the aggregated opinion of a broader coalition of perspectives.</p>\n<p class=\"MsoNormal\">I think a different problem arises in the LessWrong and <a href=\"http://effective-altruism.com/\">effective altruism</a> communities. In this case, people are much more reflectively choosing which sets of people to get their beliefs from, and I believe they are getting beliefs from some pretty good people. However, taking an outside perspective, it seems overwhelmingly likely that these communities are subject to their own biases and blind spots, and the people who are most attracted to these communities are most likely to suffer from the same biases and blind spots. I suspect elite common sense would take these communities more seriously than it currently does if it had access to more information about the communities, but I don\u2019t think it would take us sufficiently seriously to justify having high confidence in many of our more unusual views.</p>\n<p class=\"MsoNormal\"><strong>Being overconfident on open questions where we don\u2019t have a lot of evidence to work with: </strong>In my experience, it is common to give little weight to common sense takes on questions about which there is no generally accepted answer, even when it is impossible to use commonsense reasoning to arrive at conclusions that get broad support. Some less sophisticated people seem to see this as a license to think whatever they want, as Paul Graham <a href=\"http://www.paulgraham.com/identity.html\">has commented</a> in the case of politics and religion. I meet many more sophisticated people with unusual views about big picture philosophical, political, and economic questions in areas where they have very limited inside information and very limited information about the distribution of expert opinion. For example, I have now met a reasonably large number of non-experts who have very confident, detailed, unusual opinions about meta-ethics, libertarianism, and optimal methods of taxation. When I challenge people about this, I usually get some version of \u201cpeople are not good at thinking about this question\u201d but rarely a detailed explanation of why this person in particular is an exception to this generalization (more on this problem below).</p>\n<p class=\"MsoNormal\">There\u2019s an inverse version of this problem where people try to \u201csuspend judgment\u201d on questions where they don\u2019t have high-quality evidence, but actually end up taking very unusual stances without adequate justification. For example, I sometimes talk with people who say that improving the very long-term future would be <a href=\"https://sites.google.com/site/nbeckstead/research\">overwhelmingly important</a> if we could do it, but are skeptical about whether we can. In response, I sometimes run arguments of the form:<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;</span></p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">In expectation, it is possible to improve broad feature X of the world (education, governance quality, effectiveness of the scientific community, economic prosperity).</span></li>\n<li><span style=\"text-indent: -0.25in;\">If we improve feature X, it will help future people deal with various big challenges and opportunities better in expectation.</span></li>\n<li><span style=\"text-indent: -0.25in;\">If people deal with these challenges and opportunities better in expectation, the future will be better in expectation.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Therefore, it is possible to make the future better in expectation.</span></li>\n</ol>\n<p class=\"MsoNormal\">I\u2019ve presented some preliminary thoughts on related issues <a href=\"http://intelligence.org/wp-content/uploads/2013/07/Beckstead-Evaluating-Options-Using-Far-Future-Standards.pdf\">here</a>. Some people try to resist this argument on grounds of general skepticism about attempts at improving the world that haven\u2019t been documented with high-quality evidence. Peter Hurford\u2019s post on \u201c<a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">speculative causes</a>\u201d is the closest example that I can point to online, though I\u2019m not sure whether he still disagrees with me on this point. I believe that there can be some adjustment in the direction of skepticism in light of arguments that GiveWell has articulated <a href=\"http://blog.givewell.org/2013/04/04/deep-value-judgments-and-worldview-characteristics/\">here</a> under \u201cwe are relatively skeptical,\u201d but I consider rejecting the second premise on these grounds a significant departure from elite common sense. I would have a similar view about anyone who rejected any of the other premises\u2014at least if they rejected them for all values of X\u2014for such reasons. It\u2019s not that I think the presumption in favor of elite common sense can\u2019t be overcome\u2014I strongly favor thinking about such questions more carefully and am open to changing my mind\u2014it\u2019s just that I don\u2019t think it can be overcome by these types of skeptical considerations. Why not? These types of considerations seem like they could make the probability distribution over impact on the very long-term narrower, but I don\u2019t see how they could put it tightly around zero. And in any case, GiveWell articulates other considerations in <a href=\"http://blog.givewell.org/2013/04/04/deep-value-judgments-and-worldview-characteristics/\">that post</a> and <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">other posts</a> which point in favor of less skepticism about the second premise.</p>\n<p class=\"MsoNormal\">Part of the issue may be confusion about \u201crejecting\u201d a premise and \u201csuspending judgment.\u201d In my view, the question is \u201cWhat are the expected long-term effects of improving factor X?\u201d You can try not to think about this question or say \u201cI don\u2019t know,\u201d but when you make decisions you are implicitly committed to certain ranges of expected values on these questions. To justifiably ignore very long-term considerations, I think you probably need your implicit range to be close to zero. I often see people who say they are \u201csuspending judgment\u201d about these issues or who say they \u201cdon\u2019t know\u201d acting as if this ranger were very close to zero. I see this as a very strong, precise claim which is contrary to elite common sense, rather than an open-minded, \u201cwe\u2019ll wait until the evidence comes in\u201d type of view to have. Another way to put it is that my claim that improving some broad factor X has good long-run consequences is much more of an <a href=\"/lw/wm/disjunctions_antipredictions_etc/\">anti-prediction</a> than the claim that its expected effects are close to zero. (Independent point: I think that a more compelling argument than the argument that we can\u2019t affect the far future is the argument that that lots of ordinary actions have <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a> with astronomical expected impacts if anything does, so that people aiming explicitly at reducing <a href=\"/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/\">astronomical waste</a> are less privileged than one might think at first glance. I hope to write more about this issue in the future.)</p>\n<p class=\"MsoNormal\"><strong>Putting too much weight on your own opinions because you have better arguments on topics that interest you than other people, or the people you typically talk to:</strong> As mentioned above, I believe that some smart people, especially smart people who rely a lot on explicit reasoning, can become very good at developing strong arguments for their opinions without being very good at finding true beliefs. I think that in such instances, these people will generally not be very successful at getting a broad coalition of impressive people to accept their views (except perhaps by relying on non-rational methods of persuasion). Stress-testing your views by trying to actually convince others of your opinions, rather than just out-arguing them, can help you avoid this trap.</p>\n<p class=\"MsoNormal\"><strong>Putting too much weight on the opinions of single individuals who seem trustworthy to you personally but not to people in general, and have very unusual views: </strong>I have seen some people update significantly in favor of very unusual philosophical, scientific, and sociological claims when they encounter very intelligent advocates of these views. These people are often familiar with <a href=\"http://en.wikipedia.org/wiki/Aumann's_agreement_theorem\">Aumann\u2019s agreement theorem</a> and arguments for <a href=\"http://philsci-archive.pitt.edu/2940/1/refdis.pdf\">splitting the difference</a> with epistemic peers, and they are rightly troubled by the fact that someone fairly similar to them disagrees with them on an issue, so they try to correct for their own potential failures of rationality by giving additional weight to the advocates of these very unusual views.</p>\n<p class=\"MsoNormal\">However, I believe that taking disagreement seriously favors giving these very unusual views less weight, not more. The problem partly arises because philosophical discussion of disagreement often focuses on the simple case of two people sharing their evidence and opinions with each other. But what\u2019s more relevant is the distribution of quality-weighted opinion around the world <em>in general</em>, not the distribution of quality-weighted opinion of the people that you have had discussions with, and not the distribution of quality-weighted opinion of the people that seem trustworthy to you personally. The epistemically modest move here is to try to stay closer to elite common sense, not to split the difference.</p>\n<h1 id=\"Objections_to_this_approach\">Objections to this approach</h1>\n<h2 id=\"Objection__elite_common_sense_is_often_wrong\">Objection: elite common sense is often wrong</h2>\n<p class=\"MsoNormal\">One objection I often hear is that elite common sense is often wrong. I believe this is true, but not a problem for my framework. I make the comparative claim that elite common sense is more trustworthy than the idiosyncratic standards of the vast majority of individual people, not the claim that elite common sense is almost always right. A further consideration is that analogous objections to analogous views fail. For instance, \u201cmarkets are often wrong in their valuation of assets\u201d is not a good objection to the efficient markets hypothesis. As explained above, the argument that \u201cmarkets are often wrong\u201d needs to point to specific way in which one can do better than the market in order for it to make sense to place less weight on what the market says than on one\u2019s own judgments.</p>\n<h2 id=\"Objection__the_best_people_are_highly_unconventional\">Objection: the best people are highly unconventional</h2>\n<p class=\"MsoNormal\">Another objection I sometimes hear is that the most successful people often pay the least attention to conventional wisdom. I think this is true, but not a problem for my framework. One reason I believe this is that, according to my framework, when you go against elite common sense, what matters is whether elite common sense reasoning standards would justify your opinion if someone following those standards knew about your background, information, and analysis. Though I can\u2019t prove it, I suspect that the most successful people are often depart from elite common sense in ways that elite common sense would endorse if it had access to more information. I also believe that the most successful people tend to pay attention to elite common sense in many areas, and specifically bet against elite common sense in areas where they are most likely to be right.</p>\n<p class=\"MsoNormal\">A second consideration is that going against elite common sense may be a high-risk strategy, so that it is unsurprising if we see the most successful people pursuing it. People who give less weight to elite common sense are more likely to spend their time on pointless activities, join cults, and become crackpots, though they are also more likely to have revolutionary positive impacts. Consider an analogy: it may be that the gamblers who earned the most used the riskiest strategies, but this is not good evidence that you should use a risky strategy when gambling because the people who lost the most also played risky strategies.</p>\n<p class=\"MsoNormal\">A third consideration is that while it may be unreasonable to be too much of an independent thinker in a particular case, being an independent thinker helps you develop good epistemic habits. I think this point has a lot of merit, and could help explain why independent thinking is more common among the most successful people. This might seem like a good reason not to pay much attention to elite common sense. However, it seems to me that you can get the best of both worlds by being an independent thinker and keeping separate track of your own impressions and what elite common sense would make of your evidence. Where conflicts come up, you can try to use elite common sense to guide your decisions.</p>\n<p class=\"MsoNormal\">I feel my view is weakest in cases where there is a strong upside to disregarding elite common sense, there is little downside, and you\u2019ll find out whether your bet against conventional wisdom was right within a tolerable time limit. Perhaps many crazy-sounding entrepreneurial ideas and scientific hypotheses fit this description. I believe it may make sense to pick a relatively small number of these to bet on, even in cases where you can\u2019t convince elite common sense that you are on the right track. But I also believe that in cases where you really do have a great but unconventional idea, it will be possible to convince a reasonable chunk of elite common sense that your idea is worth trying out.</p>\n<h2 id=\"Objection__elite_common_sense_is_wrong_about_X__and_can_t_be_talked_out_of_it__so_your_framework_should_be_rejected_in_general\">Objection: elite common sense is wrong about X, and can\u2019t be talked out of it, so your framework should be rejected in general</h2>\n<p class=\"MsoNormal\">Another common objection takes the form: view X is true, but X is not a view which elite common sense would give much weight to. Eliezer makes a related argument <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">here</a>, though he is addressing a different kind of deference to common sense. He points to religious beliefs, beliefs about diet, and the rejection of cryonics as evidence that you shouldn\u2019t just follow what the majority believes. My position is closer to \u201cfollow the majority\u2019s epistemic standards\u201d than \u201cbelieve what the majority beliefs,\u201d and closer still to \u201cfollow the best people\u2019s epistemic standards without cherry picking \u201cbest\u201d to suit your biases,\u201d but objections of this form could have some force against the framework I have defended.</p>\n<p class=\"MsoNormal\">A first response is that unless one thinks there are many values of X in different areas where my framework fails, providing a few counterexamples is not very strong evidence that the framework isn\u2019t helpful in many cases. This is a general issue in philosophy which I think is underappreciated, and I\u2019ve made related arguments in chapter 2 of <a href=\"https://sites.google.com/site/nbeckstead/research\">my dissertation</a>. I think the most likely outcome of a careful version of this attack on my framework is that we identify some areas where the framework doesn\u2019t apply or has to be qualified.</p>\n<p class=\"MsoNormal\">But let\u2019s delve into the question about religion in greater detail. Yes, having some religious beliefs is generally more popular than being an atheist, and it would be hard to convince intelligent religious people to become atheists. However, my impression is that my framework does not recommend believing in God for the following reasons. Here are a number of weak arguments for this claim:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">My impression is that the people who are most trustworthy by clear and generally accepted standards are significantly more likely to be atheists than the general population. One illustration of my perspective is that in a 1998 </span><a style=\"text-indent: -0.25in;\" href=\"http://www.stephenjaygould.org/ctrl/news/file002.html\">survey</a><span style=\"text-indent: -0.25in;\"> of the National Academy of Sciences, only 7% of respondents reported that they believed in God. However, there is a flame war and people have pushed many arguments on this issue, and scientists are probably unrepresentative of many trustworthy people in this respect.</span></li>\n<li><span style=\"text-indent: -0.25in;\">While the world at large has broad agreement that some kind of higher power exists, there is very substantial disagreement about what this means, to the point where it isn\u2019t clear that these people are talking about the same thing.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In my experience, people generally do not try very hard to have accurate beliefs about religious questions and have little patience for people who want to carefully discuss arguments about religious questions at length. This makes it hard to stress-test one\u2019s views about religion by trying to get a broad coalition of impressive people to accept atheism, and makes it possible to give more weight to one\u2019s personal take if one has thought unusually carefully about religious questions.</span></li>\n<li><span style=\"text-indent: -0.25in;\">People are generally raised in religious families, and there are substantial social incentives to remain religious. Social incentives for atheists to remain non-religious generally seem weaker, though they can also be substantial. For example, given my current social network, I believe I would pay a significant cost if I wanted to become religious.</span></li>\n<li><span style=\"text-indent: -0.25in;\">Despite the above point, in my experience, it is much more common for religious people to become atheists than it is for atheists to become religious.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In my experience, among people who try very hard to have accurate beliefs about whether God exists, atheism is significantly more common than belief in God.</span></li>\n<li><span style=\"text-indent: -0.25in;\">In my experience, the most impressive people who are religious tend not to behave much differently from atheists or have different takes on scientific questions/questions about the future.</span></li>\n</ol>\n<p class=\"MsoNormal\">These points rely a lot on my personal experience, could stand to be researched more carefully, and feel uncomfortably close to lousy <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">contrarian excuses</a>, but I think they are nevertheless suggestive. In light of these points, I think my framework recommends that the vast majority of people with religious beliefs should be substantially less confident in their views, recommends modesty for atheists who haven\u2019t tried very hard to be right, and I suspect it allows reasonably high confidence that God doesn\u2019t exist for people who have strong indicators that they have thought carefully about the issue. I think it would be better if I saw a clear and principled way for the framework to push more strongly in the direction of atheism, but the case has enough unusual features that I don\u2019t see this as a major argument against the general helpfulness of the framework.</p>\n<p class=\"MsoNormal\">As a more general point, the framework seems less helpful in the case of religion and politics because people are generally unwilling to carefully consider arguments with the goal of having accurate beliefs. By and large, when people are unwilling to carefully consider arguments with the goal of having accurate beliefs, this is evidence that it is not useful to try to think carefully about this area. This follows from the idea mentioned above that people tend to try to have accurate views when it is in their present interests to have accurate views. So if this is the main way the framework breaks down, then the framework is mostly breaking down in cases where good epistemology is relatively unimportant.</p>\n<h1 id=\"Conclusion\">Conclusion</h1>\n<p class=\"MsoNormal\">I\u2019ve outlined a framework for taking account of the distribution of opinions and epistemic standards in the world and discussed some of its strengths and weaknesses. I think the largest strengths of the framework are that it can help you avoid falling prey to idiosyncratic personal biases, and that using it derives benefits from the \u201cwisdom of crowds\u201d effects. The framework is less helpful in:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">cases where there is a large upside to disregarding elite common sense, there is little downside, and you\u2019ll find out whether your bet against conventional wisdom was right within a tolerable time limit, and</span></li>\n<li><span style=\"text-indent: -0.25in;\">cases where people are unwilling to carefully consider arguments with the goal of having accurate beliefs.</span></li>\n</ol>\n<p class=\"MsoNormal\">Some questions for people who want to further develop the framework include:</p>\n<ol>\n<li><span style=\"text-indent: -0.25in;\">How sensitive is the framework to other reasonable choices of standards for selecting trustworthy people? Are there more helpful standards to use?</span></li>\n<li><span style=\"text-indent: -0.25in;\">How sensitive is the framework to reasonable choices of standards for aggregating opinions of trustworthy people?</span></li>\n<li><span style=\"text-indent: -0.25in;\">What are the best ways of getting a better grip on elite common sense?</span></li>\n<li><span style=\"text-indent: -0.25in;\">What other areas are there where the framework is particularly weak or particularly strong?</span></li>\n<li><span style=\"text-indent: -0.25in;\">Can the framework be developed in ways that make it more helpful in cases where it is weakest?</span></li>\n</ol>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "An outline of the framework and some guidelines for applying it effectively", "anchor": "An_outline_of_the_framework_and_some_guidelines_for_applying_it_effectively", "level": 1}, {"title": "Some further reasons to think that the framework is likely to be helpful", "anchor": "Some_further_reasons_to_think_that_the_framework_is_likely_to_be_helpful", "level": 1}, {"title": "Cases where people often don\u2019t follow the framework but I think they should", "anchor": "Cases_where_people_often_don_t_follow_the_framework_but_I_think_they_should", "level": 1}, {"title": "Objections to this approach", "anchor": "Objections_to_this_approach", "level": 1}, {"title": "Objection: elite common sense is often wrong", "anchor": "Objection__elite_common_sense_is_often_wrong", "level": 2}, {"title": "Objection: the best people are highly unconventional", "anchor": "Objection__the_best_people_are_highly_unconventional", "level": 2}, {"title": "Objection: elite common sense is wrong about X, and can\u2019t be talked out of it, so your framework should be rejected in general", "anchor": "Objection__elite_common_sense_is_wrong_about_X__and_can_t_be_talked_out_of_it__so_your_framework_should_be_rejected_in_general", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "215 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 215, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iyRpsScBa6y4rduEt", "NoYYBAaMRp9Y5Jnpo", "9W9P2snxu5Px746LD", "NLJ6NyHFZPJ2oNSZ8", "xYnnRmMmGRAwZoY25", "8DHqN85vab54LjLrM", "XiN948y5QDgNbuTXP", "yzzoWR33S9C3m75e8", "5czcpvqZ4RH7orcAa", "9KvefburLia7ptEE3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T02:55:04.976Z", "modifiedAt": null, "url": null, "title": "Some MIRI Workshop Stuff", "slug": "some-miri-workshop-stuff", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:34.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abramdemski", "createdAt": "2009-03-12T06:07:25.510Z", "isAdmin": false, "displayName": "abramdemski"}, "userId": "Q7NW4XaWQmfPfdcFj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fW2nt6yNZAHvLiYsq/some-miri-workshop-stuff", "pageUrlRelative": "/posts/fW2nt6yNZAHvLiYsq/some-miri-workshop-stuff", "linkUrl": "https://www.lesswrong.com/posts/fW2nt6yNZAHvLiYsq/some-miri-workshop-stuff", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20MIRI%20Workshop%20Stuff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20MIRI%20Workshop%20Stuff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfW2nt6yNZAHvLiYsq%2Fsome-miri-workshop-stuff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20MIRI%20Workshop%20Stuff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfW2nt6yNZAHvLiYsq%2Fsome-miri-workshop-stuff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfW2nt6yNZAHvLiYsq%2Fsome-miri-workshop-stuff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p>Hi all,</p>\n<p>I've written up some of my take-away thoughts from the MIRI workshop.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://lo-tho.blogspot.com/2013/08/progress-in-logical-priors.html\">http://lo-tho.blogspot.com/2013/08/progress-in-logical-priors.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fW2nt6yNZAHvLiYsq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 1.2993597373254664e-06, "legacy": true, "legacyId": "23721", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T03:14:28.601Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Lojban meetup", "slug": "meetup-washington-dc-lojban-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7ejLwr554xwMq6wxR/meetup-washington-dc-lojban-meetup", "pageUrlRelative": "/posts/7ejLwr554xwMq6wxR/meetup-washington-dc-lojban-meetup", "linkUrl": "https://www.lesswrong.com/posts/7ejLwr554xwMq6wxR/meetup-washington-dc-lojban-meetup", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Lojban%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Lojban%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ejLwr554xwMq6wxR%2Fmeetup-washington-dc-lojban-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Lojban%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ejLwr554xwMq6wxR%2Fmeetup-washington-dc-lojban-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ejLwr554xwMq6wxR%2Fmeetup-washington-dc-lojban-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pv'>Washington DC Lojban meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to learn Lojban!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pv'>Washington DC Lojban meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7ejLwr554xwMq6wxR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.2993759288179907e-06, "legacy": true, "legacyId": "23722", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Lojban_meetup\">Discussion article for the meetup : <a href=\"/meetups/pv\">Washington DC Lojban meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery courtyard, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to learn Lojban!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Lojban_meetup1\">Discussion article for the meetup : <a href=\"/meetups/pv\">Washington DC Lojban meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Lojban meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Lojban_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Lojban meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Lojban_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T06:46:49.581Z", "modifiedAt": null, "url": null, "title": "Open thread, August 12-18, 2013 ", "slug": "open-thread-august-12-18-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:51.117Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/omu5Bj2W9ruvtKTZ3/open-thread-august-12-18-2013", "pageUrlRelative": "/posts/omu5Bj2W9ruvtKTZ3/open-thread-august-12-18-2013", "linkUrl": "https://www.lesswrong.com/posts/omu5Bj2W9ruvtKTZ3/open-thread-august-12-18-2013", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20August%2012-18%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20August%2012-18%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fomu5Bj2W9ruvtKTZ3%2Fopen-thread-august-12-18-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20August%2012-18%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fomu5Bj2W9ruvtKTZ3%2Fopen-thread-august-12-18-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fomu5Bj2W9ruvtKTZ3%2Fopen-thread-august-12-18-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_i93\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "omu5Bj2W9ruvtKTZ3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "23724", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 125, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T06:59:48.120Z", "modifiedAt": null, "url": null, "title": "What Bayesianism taught me", "slug": "what-bayesianism-taught-me", "viewCount": null, "lastCommentedAt": "2020-09-02T19:50:07.501Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JBnaLpsrYXLXjFocu/what-bayesianism-taught-me", "pageUrlRelative": "/posts/JBnaLpsrYXLXjFocu/what-bayesianism-taught-me", "linkUrl": "https://www.lesswrong.com/posts/JBnaLpsrYXLXjFocu/what-bayesianism-taught-me", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Bayesianism%20taught%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Bayesianism%20taught%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBnaLpsrYXLXjFocu%2Fwhat-bayesianism-taught-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Bayesianism%20taught%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBnaLpsrYXLXjFocu%2Fwhat-bayesianism-taught-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBnaLpsrYXLXjFocu%2Fwhat-bayesianism-taught-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 967, "htmlBody": "<p>David Chapman <a href=\"http://meaningness.com/metablog/bayesianism-updating\">criticizes</a> \"pop Bayesianism\" as just common-sense rationality dressed up as intimidating math<sup>[1]</sup>:</p>\n<blockquote>Bayesianism boils down to &ldquo;don&rsquo;t be so sure of your beliefs; be less sure when you see contradictory evidence.&rdquo;\n<p>Now that is just common sense. Why does anyone need to be told this? And how does [Bayes'] formula help?</p>\n<p>[...]</p>\n<p>The leaders of the movement presumably do understand probability. But I&rsquo;m wondering whether they simply use Bayes&rsquo; formula to intimidate lesser minds into accepting &ldquo;don&rsquo;t be so sure of your beliefs.&rdquo; (In which case, Bayesianism is not about Bayes&rsquo; Rule, after all.)</p>\n<p>I don&rsquo;t think I&rsquo;d approve of that. &ldquo;Don&rsquo;t be so sure&rdquo; is a valuable lesson, but I&rsquo;d rather teach it in a way people can understand, rather than by invoking a Holy Mystery.</p>\n</blockquote>\n<p>What does Bayes's formula have to teach us about how to do epistemology, beyond obvious things like \"never be absolutely certain; update your credences when you see new evidence\"?</p>\n<p>I list below some of the specific things that I learned from Bayesianism. Some of these are examples of mistakes I'd made that Bayesianism corrected. Others are things that I just hadn't thought about explicitly before encountering Bayesianism, but which now seem important to me.</p>\n<p><a id=\"more\"></a>I'm interested in hearing what other people here would put on their own lists of things Bayesianism taught them. (Different people would make different lists, depending on how they had already thought about epistemology when they first encountered \"pop Bayesianism\".)</p>\n<p>I'm interested especially in those lessons that you think followed more-or-less directly from taking Bayesianism seriously as a normative epistemology (plus maybe the idea of making decisions based on expected utility). The LW memeplex contains many other valuable lessons (e.g., avoid the mind-projection fallacy, be mindful of inferential gaps, the MW interpretation of QM has a lot going for it, decision theory should take into account \"logical causation\", etc.). However, these seem further afield or more speculative than what I think of as \"bare-bones Bayesianism\".</p>\n<p>So, without further ado, here are some things that Bayesianism taught me.</p>\n<ol>\n<li><strong>Banish talk like \"There is absolutely no evidence for that belief\".</strong> P(E | H) &gt; P(E) if and only if P(H | E) &gt; P(H). The fact that there are myths about Zeus is evidence that Zeus exists. Zeus's existing would make it more likely for myths about him to arise, so the arising of myths about him must make it more likely that he exists. A related mistake I made was to be impressed by the cleverness of the aphorism \"The plural of 'anecdote' is not 'data'.\" There may be <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">a helpful distinction</a> between scientific evidence and Bayesian evidence. But anecdotal evidence is evidence, and it ought to sway my beliefs.</li>\n<li><strong>Banish talk like \"I don't know anything about that\".</strong> See the post <a href=\"/lw/gs/i_dont_know/\">\"I don't know.\"</a></li>\n<li><strong>Banish talk of \"thresholds of belief\".</strong> Probabilities go up or down, but there is no magic threshold beyond which they change qualitatively into \"knowledge\". I used to make the mistake of saying things like, \"I'm not absolutely certain that atheism is true, but it is my working hypothesis. I'm confident enough to act as though it's true.\" I assign a certain probability to atheism, which is less than 1.0. I ought to act as though I am just that confident, and no more. I should never <em>just assume</em> that I am in the possible world that I think is most likely, even if I think that that possible world is <em>overwhelmingly</em> likely. (However, perhaps I could be so confident that my behavior would not be practically discernible from absolute confidence.)</li>\n<li><strong>Absence of evidence <em>is</em> evidence of absence</strong>. <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">P(H | E) &gt; P(H) if and only if P(H | ~E) &lt; P(H)</a>. Absence of evidence may be very weak evidence of absence, but it is evidence nonetheless. (However, <a href=\"/lw/1ph/youre_entitled_to_arguments_but_not_that/\">you may not be entitled to a particular kind of evidence</a>.)</li>\n<li><strong>Many bits of \"common sense\" rationality can be <em>precisely stated</em> and <em>easily proved</em> within the austere framework of Bayesian probability.&nbsp; </strong>As noted by Jaynes in <em>Probability Theory: The Logic of Science</em>, \"[P]robability theory as extended logic reproduces many aspects of human mental activity, sometimes in surprising and even disturbing detail.\" While these things might be \"common knowledge\", the fact that they are readily deducible from a few simple premises is significant. Here are some examples: \n<ul>\n<li><strong>It is possible for the opinions of different people to <em>diverge</em> after they <em>rationally</em> update on the same evidence.</strong> Jaynes discusses this phenomenon in Section 5.3 of <em>PT:TLoS</em>.</li>\n<li><strong>Popper's falsification criterion</strong>, and other Popperian principles of \"good explanation\", such as that good explanations should be \"hard to vary\", follow from Bayes's formula. Eliezer discusses this in <a href=\"http://yudkowsky.net/rational/bayes\"><em>An Intuitive Explanation of Bayes' Theorem</em></a> and <a href=\"http://yudkowsky.net/rational/technical\"><em>A Technical Explanation of Technical Explanation</em></a>.</li>\n<li><strong>Occam's razor</strong>. This can be formalized using Solomonoff induction. (However, perhaps this shouldn't be on my list, because Solomonoff induction goes beyond just Bayes's formula. It also has <a href=\"/lw/cw1/open_problems_related_to_solomonoff_induction/\">several problems</a>.)</li>\n</ul>\n</li>\n<li><strong>You cannot expect<sup>[2]</sup> that future evidence will sway you in a particular direction.</strong> \"<a href=\"/lw/ii/conservation_of_expected_evidence/\">For every expectation of evidence, there is an equal and opposite expectation of counterevidence.</a>\"</li>\n<li><strong>Abandon all the meta-epistemological intuitions about the concept of knowledge on which <a href=\"https://en.wikipedia.org/wiki/Gettier_problem\">Gettier-style paradoxes</a> rely.</strong> Keep track of how confident your beliefs are when you update on the evidence. Keep track of the extent to which other people's beliefs are good evidence for what they believe. Don't worry about whether, in addition, these beliefs qualify as \"knowledge\".</li>\n</ol>\n<p>What items would you put on your list?</p>\n<p><strong>ETA:</strong><span class=\"author\"> ChrisHallquist's post <a href=\"/r/lesswrong/lw/iwb/bayesianism_for_humans/\">Bayesianism for Humans</a> lists other \"directly applicable corollaries to Bayesianism\". </span></p>\n<hr />\n<p>[1]&nbsp; See also Yvain's <a href=\"http://slatestarcodex.com/2013/08/06/on-first-looking-into-chapmans-pop-bayesianism/\">reaction</a> to David Chapman's criticisms.</p>\n<p>[2]&nbsp; <strong>ETA:</strong> My wording here is potentially misleading.&nbsp; See this <a href=\"/lw/iat/what_bayesianism_taught_me/9kko\">comment thread</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JBnaLpsrYXLXjFocu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 75, "baseScore": 111, "extendedScore": null, "score": 0.00032, "legacy": true, "legacyId": "23717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 111, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 204, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fhojYBGGiYAFcryHZ", "Pm83rA8MTYYeR4Ci4", "mnS2WYLCGJP2kQkRn", "vqbieD9PHG8RRJddu", "fC248GwrWLT4Dkjf6", "jiBFC7DcCrZjGmZnJ", "XCwPQzoe9hmQikMiY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T10:02:32.355Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-20", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:38.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9avJonnevzhkSnAti/meetup-melbourne-social-meetup-20", "pageUrlRelative": "/posts/9avJonnevzhkSnAti/meetup-melbourne-social-meetup-20", "linkUrl": "https://www.lesswrong.com/posts/9avJonnevzhkSnAti/meetup-melbourne-social-meetup-20", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9avJonnevzhkSnAti%2Fmeetup-melbourne-social-meetup-20%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9avJonnevzhkSnAti%2Fmeetup-melbourne-social-meetup-20", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9avJonnevzhkSnAti%2Fmeetup-melbourne-social-meetup-20", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pw'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 August 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5/52 Leicester Street, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on Friday evening this week. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Please ring the number 5 button when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pw'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9avJonnevzhkSnAti", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 1.2997166961291895e-06, "legacy": true, "legacyId": "23728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/pw\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 August 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5/52 Leicester Street, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on Friday evening this week. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Please ring the number 5 button when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/pw\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T16:11:23.263Z", "modifiedAt": null, "url": null, "title": "Update on establishment of Cambridge\u2019s Centre for Study of Existential Risk", "slug": "update-on-establishment-of-cambridge-s-centre-for-study-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:30.163Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sean_o_h", "createdAt": "2012-11-27T20:56:04.066Z", "isAdmin": false, "displayName": "Sean_o_h"}, "userId": "7ntNTAoctZqY9Nuwa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2idJBvzzj3dP36HSA/update-on-establishment-of-cambridge-s-centre-for-study-of", "pageUrlRelative": "/posts/2idJBvzzj3dP36HSA/update-on-establishment-of-cambridge-s-centre-for-study-of", "linkUrl": "https://www.lesswrong.com/posts/2idJBvzzj3dP36HSA/update-on-establishment-of-cambridge-s-centre-for-study-of", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Update%20on%20establishment%20of%20Cambridge%E2%80%99s%20Centre%20for%20Study%20of%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpdate%20on%20establishment%20of%20Cambridge%E2%80%99s%20Centre%20for%20Study%20of%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2idJBvzzj3dP36HSA%2Fupdate-on-establishment-of-cambridge-s-centre-for-study-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Update%20on%20establishment%20of%20Cambridge%E2%80%99s%20Centre%20for%20Study%20of%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2idJBvzzj3dP36HSA%2Fupdate-on-establishment-of-cambridge-s-centre-for-study-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2idJBvzzj3dP36HSA%2Fupdate-on-establishment-of-cambridge-s-centre-for-study-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 856, "htmlBody": "<div>Cambridge&rsquo;s high-profile launch of the <a href=\"http://www.cser.org\">Centre for Study of Existential Risk</a> last November received a lot of attention on LessWrong, and a number of people have been enquiring as to what&lsquo;s happened since. This post is meant to give a little explanation and update of what&rsquo;s been going on.</div>\n<div><br /></div>\n<div>Motivated by a common concern over human activity-related risks to humanity, Lord Martin Rees, Professor Huw Price, and Jaan Tallinn founded the Centre for Study of Existential Risk last year. &nbsp;However, this announcement was made before the establishment of a physical research centre or securement of long-term funding. The last 9 months have been focused on turning an important idea into a reality.</div>\n<div><br /></div>\n<div>Following the announcement in November, Professor Price contacted us at the Future of Humanity Institute regarding the possibility of collaboration on joint academic funding opportunities; the aim being both to raise the funds for CSER&rsquo;s research programmes and to support joint work by the FHI and CSER&rsquo;s researchers on anthropogenic existential risk. We submitted our first grant application in January to the European Research Council &ndash; an ambitious project to create &ldquo;A New Science of Existential Risk&rdquo; that, if successful, would provide enough funding for CSER&rsquo;s first research programme - a sizeable programme that will run for five years.</div>\n<div>We&rsquo;ve been successful in the first and second rounds, and we will hear a final round decision at the end of the year. It was also an opportunity for us to get some additional leading academics onto the project &ndash; Sir Partha Dasgupta, Professor of Economics at Cambridge and an expert in social choice theory, sustainability and intergenerational ethics, is a co-PI (along with Huw Price, Martin Rees and Nick Bostrom). In addition, a number of prominent academics concerned about technology-related risk &ndash; including Stephen Hawking, David Spiegelhalter, George Church and David Chalmers &ndash; have joined our advisory board.</div>\n<div><br /></div>\n<div>The FHI regards establishment of CSER as of the highest priority for a number of reasons including:</div>\n<div><br /></div>\n<div>1)<span style=\"white-space: pre;\"> </span>The value of the research the Centre will engage in</div>\n<div>2)<span style=\"white-space: pre;\"> </span>The reputational boost to the field of Existential Risk gained by the establishment of high-profile research centre in Cambridge.</div>\n<div>3)<span style=\"white-space: pre;\"> </span>The impact on policy and public perception that academic heavy-hitters like Rees and Price can have</div>\n<div><br /></div>\n<div>Therefore we&rsquo;ve been working with CSER behind the scenes over the last 9 months. Progress has been a little slow until now &ndash; Huw, Martin and Jaan are fully committed to this project, but due to their other responsibilities aren&rsquo;t in a position to work full-time on it yet.&nbsp;</div>\n<div><br /></div>\n<div>However, we&rsquo;re now in a position to make CSER&rsquo;s establishment official. Cambridge&rsquo;s new Centre for Research in the Arts, Social Sciences and Humanities (CRASSH) will host CSER and provide logistical support. I&rsquo;ll be acting manager of CSER&rsquo;s activities over the coming 6-12 months, under the guidance of Huw, Martin and Jaan. A generous seed funding donation from Jaan Tallinn is funding CSER&rsquo;s establishment and these activities &ndash; which will include a lecture series, workshops, public outreach, and staff time on grant-writing and fundraising. It&rsquo;ll also provide a buyout of a fraction of my time from FHI (providing funds for us to hire part-time staff to offload some of the FHI workload and help with some of the CSER work).</div>\n<div><br /></div>\n<div>At the moment and over the next couple of months we&rsquo;re going to be focused on identifying and working on additional academic funding opportunities for additional programmes, as well as chasing some promising leads in industry, private and philanthropic funding. I&rsquo;ll also be aiming to keep CSER&rsquo;s public profile active. There will be newsletters every three months (sign up <a href=\"http://cser.org/contact.html\">here</a>), the website&rsquo;s going to be fleshed out to contain more detail about our planned research and existing literature, and we&rsquo;ll be arranging regular high-quality media engagement. While we&rsquo;re unlikely to have time to answer every general query that comes in (though we&rsquo;ll try whenever possible: email:<span style=\"font-family: Calibri, sans-serif; font-size: 11pt; line-height: 115%;\">&nbsp;</span><a style=\"font-family: Calibri, sans-serif; font-size: 11pt; line-height: 115%;\" href=\"mailto:admin@cser.org\">admin@cser.org</a>), we&rsquo;ll aim to keep the existential risk community informed through the newsletters and posts such as these.</div>\n<div><br /></div>\n<div>We&rsquo;ve been lucky to get a lot of support from the academic and existential risk community for the CSER centre. In addition to CRASSH, Cambridge&rsquo;s Centre for Science and Policy will provide support in making policy-relevant links, and may co-host and co-publicise events. Luke Muehlhauser, MIRI&rsquo;s Executive Director, has been very supportive and has provided valuable advice, and has generously offered to direct some of MIRI&rsquo;s volunteer support towards CSER tasks. We also expect to get valuable support from the growing community around FHI.</div>\n<div><br /></div>\n<div>From where I&rsquo;m sitting, CSER&rsquo;s successful launch is looking very promising. The timeline on our research programmes, however, is still a little more uncertain. If we&rsquo;re successful with the European Research Council, we can expect to be hiring a full research team next spring. If not, it may take a little longer, but we&rsquo;re exploring a number of different opportunities in parallel and are feeling confident. The support of the existential risk community continues to be invaluable.</div>\n<div><br /></div>\n<div>Thanks,</div>\n<div><br /></div>\n<div>Se&aacute;n &Oacute; h&Eacute;igeartaigh</div>\n<div>Academic Manager, Future of Humanity Institute&nbsp;</div>\n<div>\n<p class=\"MsoNoSpacing\"><a href=\"http://www.fhi.ox.ac.uk/\">http://www.fhi.ox.ac.uk</a></p>\n</div>\n<div>Acting Academic Manager, Cambridge Centre for Study of Existential Risk.</div>\n<div>\n<p class=\"MsoNoSpacing\"><a href=\"http://www.cser.org/\">http://www.cser.org</a></p>\n</div>\n<div><br /></div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jZF2jwLnPKBv6m3Ag": 1, "Rz5jb3cYHTSRmqNnN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2idJBvzzj3dP36HSA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 60, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "23729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T16:30:29.192Z", "modifiedAt": null, "url": null, "title": "Course - Saving Millions of lives at a time [link]", "slug": "course-saving-millions-of-lives-at-a-time-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8GCNZCFQu2ji4mwpy/course-saving-millions-of-lives-at-a-time-link", "pageUrlRelative": "/posts/8GCNZCFQu2ji4mwpy/course-saving-millions-of-lives-at-a-time-link", "linkUrl": "https://www.lesswrong.com/posts/8GCNZCFQu2ji4mwpy/course-saving-millions-of-lives-at-a-time-link", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Course%20-%20Saving%20Millions%20of%20lives%20at%20a%20time%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACourse%20-%20Saving%20Millions%20of%20lives%20at%20a%20time%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GCNZCFQu2ji4mwpy%2Fcourse-saving-millions-of-lives-at-a-time-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Course%20-%20Saving%20Millions%20of%20lives%20at%20a%20time%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GCNZCFQu2ji4mwpy%2Fcourse-saving-millions-of-lives-at-a-time-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GCNZCFQu2ji4mwpy%2Fcourse-saving-millions-of-lives-at-a-time-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>Coursera is running this shortly.&nbsp;<br /><a href=\"https://www.coursera.org/course/globaldiseasecontrol\">https://www.coursera.org/course/globaldiseasecontrol</a></p>\n<p>Should be interesting to the folks interested in effective altruism here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8GCNZCFQu2ji4mwpy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.300040812195219e-06, "legacy": true, "legacyId": "23730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-12T21:30:53.487Z", "modifiedAt": null, "url": null, "title": "[LINK] Hyperloop officially announced \u2014 predictions, anyone?", "slug": "link-hyperloop-officially-announced-predictions-anyone", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malcolmocean", "createdAt": "2012-05-16T15:13:58.012Z", "isAdmin": false, "displayName": "MalcolmOcean"}, "userId": "urN5htciqMuEeghc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jre42ag5ejQgXEsAx/link-hyperloop-officially-announced-predictions-anyone", "pageUrlRelative": "/posts/Jre42ag5ejQgXEsAx/link-hyperloop-officially-announced-predictions-anyone", "linkUrl": "https://www.lesswrong.com/posts/Jre42ag5ejQgXEsAx/link-hyperloop-officially-announced-predictions-anyone", "postedAtFormatted": "Monday, August 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Hyperloop%20officially%20announced%20%E2%80%94%20predictions%2C%20anyone%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Hyperloop%20officially%20announced%20%E2%80%94%20predictions%2C%20anyone%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJre42ag5ejQgXEsAx%2Flink-hyperloop-officially-announced-predictions-anyone%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Hyperloop%20officially%20announced%20%E2%80%94%20predictions%2C%20anyone%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJre42ag5ejQgXEsAx%2Flink-hyperloop-officially-announced-predictions-anyone", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJre42ag5ejQgXEsAx%2Flink-hyperloop-officially-announced-predictions-anyone", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<p>I was studying in the <a title=\"LW Study Hall - 2 Month Update\" href=\"/lw/hen/lw_study_hall_2_month_update/\">LW Study Hall</a>, and during our break someone posted this link to the official hyperloop announcement:</p>\n<p><a href=\"http://www.spacex.com/sites/spacex/files/hyperloop_alpha-20130812.pdf\">http://www.spacex.com/sites/spacex/files/hyperloop_alpha-20130812.pdf</a></p>\n<p>One member was doubtful it would get past regulations, and another said \"tentative p&gt;0.05 that a hyperloop gets made by 2100\", which was met with \"p&gt;0.05 that uploading people and moving them between bodies will be available by 2100\".</p>\n<p>It struck me that people might be interested in betting on things like this, or at least having a conversation about it.</p>\n<p>A few predictions to start:</p>\n<ul>\n<li><a title=\"PredictionBook: Tesla Motors / SpaceX / Elon Musk will create a working hyperloop by 2100\" href=\"http://predictionbook.com/predictions/20946\" target=\"_blank\">Tesla Motors / SpaceX / Elon Musk will create a working hyperloop by <strong>2100</strong>.</a></li>\n<li><a title=\"PredictionBook: Tesla Motors / SpaceX / Elon Musk will create a working hyperloop by 2050\" href=\"http://predictionbook.com/predictions/20947\" target=\"_blank\">Tesla Motors / SpaceX / Elon Musk will create a working hyperloop by <strong>2050</strong>.</a></li>\n<li><a title=\"PredictionBook: Tesla Motors / SpaceX / Elon Musk will create a working hyperloop by 2030\" href=\"http://predictionbook.com/predictions/20948\" target=\"_blank\">Tesla Motors / SpaceX / Elon Musk will create a working hyperloop by <strong>2030</strong>.</a></li>\n</ul>\n<div>More predictions, based on comments:</div>\n<div>\n<ul>\n<li><a href=\"http://predictionbook.com/predictions/20985\" target=\"_blank\">The cost projections of the hyperloop are underestimates by at least an order of magnitude.</a></li>\n<li><a href=\"http://predictionbook.com/predictions/20986\" target=\"_blank\">When and if a hyperloop-like transit system is built (or not), the US will not be the first country to build it.</a></li>\n<li><a href=\"http://predictionbook.com/predictions/20987\" target=\"_blank\">One of the first really big (&gt;5bn$) hyperloops will go across a body of water.</a></li>\n<li><a href=\"http://predictionbook.com/predictions/20988\" target=\"_blank\">If a hyperloop is created, it will be predominately (&gt;50%) solar-powered.</a></li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jre42ag5ejQgXEsAx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 1.3002918880046723e-06, "legacy": true, "legacyId": "23732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2huJTJ2Fs9qbw3xR7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-13T02:09:41.984Z", "modifiedAt": null, "url": null, "title": "Does Checkers have simpler rules than Go?", "slug": "does-checkers-have-simpler-rules-than-go", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:07.441Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/buf87jaKck83EkNzH/does-checkers-have-simpler-rules-than-go", "pageUrlRelative": "/posts/buf87jaKck83EkNzH/does-checkers-have-simpler-rules-than-go", "linkUrl": "https://www.lesswrong.com/posts/buf87jaKck83EkNzH/does-checkers-have-simpler-rules-than-go", "postedAtFormatted": "Tuesday, August 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20Checkers%20have%20simpler%20rules%20than%20Go%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20Checkers%20have%20simpler%20rules%20than%20Go%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbuf87jaKck83EkNzH%2Fdoes-checkers-have-simpler-rules-than-go%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20Checkers%20have%20simpler%20rules%20than%20Go%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbuf87jaKck83EkNzH%2Fdoes-checkers-have-simpler-rules-than-go", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbuf87jaKck83EkNzH%2Fdoes-checkers-have-simpler-rules-than-go", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 400, "htmlBody": "<p>I've <a href=\"http://www.jefftk.com/p/aliens-dont-play-chess\">seen</a> various contenders for the title of simplest abstract game that's interesting enough that a professional community could reasonably play it full time. While <a href=\"http://en.wikipedia.org/wiki/Go_(board_game)\">Go</a> probably has the best ratio of interest to complexity, <a href=\"http://en.wikipedia.org/wiki/Draughts\">Checkers</a> and <a href=\"http://en.wikipedia.org/wiki/Dots_and_Boxes\">Dots and Boxes</a> might be simpler while remaining sufficiently interesting. [1] But is Checkers actually simpler than Go?  If so, how much?  How would we decide this?</p>\n<p>Initially you might approach this by writing out rules.  There's an <a href=\"http://homepages.cwi.nl/~tromp/go.html\">elegant set for Go</a> and I <a href=\"http://www.jefftk.com/p/aliens-dont-play-chess#fb-623983460022_785694\">wrote some for Checkers</a>, but English is a very flexible language. Perhaps my rules are underspecified?  Perhaps they're overly verbose? It's hard to say.</p>\n<p>A more objective test is to <a href=\"http://en.wikipedia.org/wiki/Kolmogorov_complexity\">write a computer program</a> that implements the rules.  It needs to determine whether moves are valid, and identify a winner.  The shorter the computer program, the simpler the rules of the game.  This only gives you an upper bound on the complexity, because someone could come along and write a shorter one, but in general we expect that shorter programs imply shorter possible programs.</p>\n<p>To investigate this, I wrote ones for each of the three games.  I wrote them quickly, and they're kind of terse, but they represent the rules as efficiently as I could figure out.  The one for Go is based off Tromp's definition of the rules while the other two implement the rules as they are in my head.  This probably gives an advantage to Go because those rules had a lot of care go into them, but I'm not sure how much of one.</p>\n<p>The programs as written have some excess information, such as comments, vaguely friendly error messages, whitespace, and meaningful variable names.  I took a <a href=\"https://developers.google.com/closure/compiler/\">jscompiler</a>-like pass over them to remove as much of this as possible, and making them nearly unreadable in the process.  Then I ran them through a lossless compressor, gzip, and computed their sizes:</p>\n<ul>\n<li>Checkers: 648 bytes </li>\n<li>Dots and Boxes: 505 bytes </li>\n<li>Go: 596 bytes </li>\n</ul>\n<p>(The programs are <a href=\"https://github.com/jeffkaufman/game-complexity\">on github</a>. If you have suggestions for simplifying them further, send me a pull request.)</p>\n<p><br /> [1] Go is the most interesting of the three, and has stood up to centuries of analysis and play, but Dots and Boxes is <a href=\"http://library.msri.org/books/Book29/files/westboxes.pdf\">surprisingly complex</a> (pdf) and there used to be professional Checkers players. (I'm having a remarkably hard time determining if there are still Checkers professionals.)</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/p/simplest-interesting-game\">on my blog</a>.</em></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb25c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "buf87jaKck83EkNzH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 1.3005249903191822e-06, "legacy": true, "legacyId": "23733", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-13T03:50:55.679Z", "modifiedAt": null, "url": null, "title": "Biases of Intuitive and Logical Thinkers", "slug": "biases-of-intuitive-and-logical-thinkers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:59.415Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pwno", "createdAt": "2009-02-27T06:17:31.584Z", "isAdmin": false, "displayName": "pwno"}, "userId": "SCgoHNxqc2agmDWEg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GSQsAReasGjETP3e9/biases-of-intuitive-and-logical-thinkers", "pageUrlRelative": "/posts/GSQsAReasGjETP3e9/biases-of-intuitive-and-logical-thinkers", "linkUrl": "https://www.lesswrong.com/posts/GSQsAReasGjETP3e9/biases-of-intuitive-and-logical-thinkers", "postedAtFormatted": "Tuesday, August 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Biases%20of%20Intuitive%20and%20Logical%20Thinkers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABiases%20of%20Intuitive%20and%20Logical%20Thinkers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGSQsAReasGjETP3e9%2Fbiases-of-intuitive-and-logical-thinkers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Biases%20of%20Intuitive%20and%20Logical%20Thinkers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGSQsAReasGjETP3e9%2Fbiases-of-intuitive-and-logical-thinkers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGSQsAReasGjETP3e9%2Fbiases-of-intuitive-and-logical-thinkers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2110, "htmlBody": "<p>Any intuition-dominant thinker who's struggled with math problems or logic-dominant thinker who's struggled with small-talk knows how difficult and hopeless the experience feels like. For a long time I was an intuition thinker, then I developed a logical thinking style and soon it ended up dominating -- granting me the luxury of experiencing both kinds of struggles. I eventually learned to apply the thinking style better optimized for the problem I was facing. Looking back, I realized why I kept sticking to one extreme. <br /><br />I hypothesize that one-sided thinkers develop biases and tendencies that prevent them from improving their weaker mode of thinking. These biases cause a positive feedback loop that further skews thinking styles in the same direction. <br /><br />The reasons why one style might be overdeveloped and the other underdeveloped vary greatly. Genes have a strong influence, but environment also plays a large part. A teacher may have inspired you to love learning science at a young age, causing you to foster to a thinking style better for learning science. Or maybe you grew up very physically attractive and found socializing with your peers a lot more rewarding than studying after school, causing you to foster a thinking style better for navigating social situations. Environment can be changed to help develop certain thinking styles, but it should be supplementary to exposing and understanding the biases you already have. Entering an environment that penalizes your thinking style can be uncomfortable, stressful and frustrating without being prepared. (Such a painful experience is part of why these biases cause a positive feedback loop, by making us avoid environments that require the opposite thinking style.) <br /><br />Despite genetic predisposition and environmental circumstances, there's room for improvement and exposing these biases and learning to account for them is a great first step. <br /><br />Below is a list of a few biases that worsen our ability to solve a certain class of problems and keep us from improving our underdeveloped thinking style.</p>\n<hr />\n<h2 style=\"margin-top: 3px;\">Intuition-dominant Biases</h2>\n<p><br /><strong>Overlooking crucial details</strong></p>\n<p>Details matter in order to understand technical concepts. Overlooking a word or sentence structure can cause complete misunderstanding -- a common blunder for intuition thinkers.</p>\n<p>Intuition is really good at making fairly accurate predictions without complete information, enabling us to navigate the world without having a <a href=\"/lw/1yq/understanding_your_understanding/\">deep understanding</a> of it. As a result, intuition trains us to experience the feeling we understand something without examining every detail. In most situations, paying close attention to detail is unnecessary and sometimes dangerous. When learning a technical concept, every detail matters and the premature feeling of understanding stops us from examining them. <br /><br />This bias is one that's more likely to go away once you realize it's there. You often don't know what details you're missing after you've missed them, so merely remembering that you tend to miss important details should prompt you to take closer examinations in the future. <br /><br /><strong>Expecting solutions to sound a certain way</strong><br /><br /><a href=\"http://www.imdb.com/title/tt2234155/\">The Internship</a> has a great example of this bias (and a few others) in action. The movie is about two middle-aged unemployed salesmen (intuition thinkers) trying to land an internship with Google. Part of Google's selection process has the two men participate in several technical challenges. One challenge required the men and their team to find a software bug. In a flash of insight, Vince Vaughn's character, Billy, shouts \"Maybe the answer is in the question! Maybe it has something to do with the word bug. A fly!\" After enthusiastically making several more word associations, he turns to his team and insists they take him seriously. <br /><br />Why is it believable to the audience that Billy can be so confident about his answer?<br /><br />Billy's intuition made an association between the challenge question and riddle-like questions he's heard in the past. When Billy used his intuition to find a solution, his confidence in a riddle-like answer grew. Intuition recklessly uses irrelevant associations as reasons for narrowing down the space of possible solutions to technical problems. When associations pop in your mind, it's a good idea to legitimize those associations with supporting reasons. <br /><br /><strong>Not recognizing precise language</strong><br /><br />Intuition thinkers are multi-channel learners -- all senses, thoughts and emotions are used to construct a complex database of clustered knowledge to predict and understand the world. With robust information-extracting ability, correct grammar/word-usage is, more often than not, unnecessary for meaningful communication. <br /><br />Communicating technical concepts in a meaningful way requires precise language. Connotation and subtext are stripped away so words and phrases can purely represent meaningful concepts inside a logical framework. Intuition thinkers communicate with imprecise language, gathering meaning from context to compensate. This makes it hard for them to recognize when to turn off their powerful information extractors. <br /><br />This bias explains part of why so many intuition thinkers dread math \"word problems\". Introducing words and phrases rich with meaning and connotation sends their intuition running wild. It's hard for them to find correspondences between words in the problem and variables in the theorems and formulas they've learned. <br /><br />The noise intuition brings makes it hard to think clearly. It's hard for intuition thinkers to tell whether their automatic associations should be taken seriously. Without a reliable way to discern, wrong interpretations of words go undetected. For example, without any physics background, an intuition thinker may read the statement \"Matter can have both wave and particle properties at once\" and believe they completely understand it. Unrelated associations of what matter, wave and particle mean, blindly take precedence over technical definitions. <br /><br />The slightest uncertainty about what a sentence means should raise a red flag. Going back and finding correspondence between each word and how it fits into a technical framework will eliminate any uncertainty. <br /><br /><strong>Believing their level of understanding is deeper than what it is</strong><br /><br />Intuition works on an unconscious level, making intuition thinkers unaware of how they know what they know. Not surprisingly, their best tool to learn what it means to understand is intuition. The concept \"understanding\" is a collection of associations from experience. You may have learned that part of understanding something means being able to answer questions on a test with memorized factoids, or knowing what to say to convince people you understand, or just knowing more facts than your friends. These are not good methods for gaining a deep understanding of technical concepts.<br /><br />When intuition thinkers optimize for understanding, they're really optimizing for a fuzzy idea of what they think understanding means. This often leaves them believing they understand a concept when all they've done is memorize some disconnected facts. Not knowing what it feels like to have deeper understanding, they become conditioned to always expect some amount of surprise. They can feel max understanding with less confidence than logical thinkers when they feel max understanding. This lower confidence disincentivizes intuition thinkers to invest in learning technical concepts, further keeping their logical thinking style underdeveloped.<br /><br />One way I overcame this tendency was to constantly ask myself \"why\" questions, like a curious child bothering their parents. The technique helped me uncover what used to be unknown unknowns that made me feel overconfident in my understanding.</p>\n<h2>\n<hr />\n</h2>\n<h2><strong>Logic-dominant Biases</strong></h2>\n<p><br /><strong>Ignoring information they cannot immediately fit into a framework</strong><br /><br />Logical thinkers have and use intuition -- problem is they don't feed it enough. They tend to ignore valuable intuition-building information if it doesn't immediately fit into a predictive model they deeply understand. While intuition thinkers don't filter out enough noise, logical thinkers filter too much. <br /><br />For example, if a logical thinker doesn't have a good framework for understanding human behavior, they're more likely to ignore visual input like body language and fashion, or auditory input like tone of voice and intonation. Human behavior is complicated, there's no framework to date that can make perfectly accurate predictions about it. Intuition can build powerful models despite working with many confounding variables.&nbsp;&nbsp; <br /><br />Bayesian probability enables logical thinkers to build predictive models from noisy data without having to use intuition. But even then, the first step of making a Bayesian update is data collection. <br /><br />Combatting this tendency requires you to pay attention to input you normally ignore. Supplement your broader attentional scope with a researched framework as a guide. Say you want to learn how storytelling works. Start by grabbing resources that teach storytelling and learn the basics. Out in the real-world, pay close attention to sights, sounds, and feelings when someone starts telling a story and try identifying sensory input to the storytelling elements you've learned about. Once the basics are subconsciously picked up by habit, your conscious attention will be freed up to make new and more subtle observations. <br /><br /><strong>Ignoring their emotions</strong><br /><br />Emotional input is difficult to factor, especially because you're emotional at the time. Logical thinkers are notorious for ignoring this kind of messy data, consequently starving their intuition of emotional data. Being able to \"go with your gut feelings\" is a major function of intuition that logical thinkers tend to miss out on. <br /><br />Your gut can predict if you'll get along long-term with a new SO, or what kind of outfit would give you more confidence in your workplace, or if learning tennis in your free time will make you happier, or whether you prefer eating a cheeseburger over tacos for lunch. Logical thinkers don't have enough data collected about their emotions to know what triggers them. They tend to get bogged down and mislead with objective, yet trivial details they manage to factor out. A weak understanding of their own emotions also leads to a weaker understanding of other's emotions. <a href=\"/lw/818/how_to_understand_people_better/\">You can become a better empathizer by better understanding yourself</a>. <br /><br />You could start from scratch and build your own framework, but self-assessment biases will impede productivity. Learning an existing framework is a more realistic solution. You can find resources with some light <a href=\"http://www.helpguide.org/toolkit/developing_emotional_awareness.htm\">googling</a> and I'm sure <a href=\"http://rationality.org/\">CFAR</a> teaches some good ones too. You can improve your gut feelings too. One way is making sure you're always consciously aware of the circumstances you're in when experiencing an emotion.<br /><br /><strong>Making rules too strict</strong><br /><br />Logical thinkers build frameworks in order to understand things. When adding a new rule to a framework, there's motivation to make the rule strict. The stricter the rule, the more predictive power, the better the framework. When the domain you're trying to understand has multivariable chaotic phenomena, strict rules are likely to break. The result is something like the current state of macroeconomics: a bunch of logical thinkers preoccupied by elegant models and theories that can only exist when useless in practice. <br /><br />Following rules that are too strict can have bad consequences. Imagine John the salesperson is learning how to make better first impressions and has built a rough framework so far. John has a rule that smiling always helps make people feel welcomed the first time they meet him. One day he makes a business trip to Russia to meet with a prospective client. The moment he meet his russian client, he flashes a big smile and continues to smile despite negative reactions. After a few hours of talking, his client reveals she felt he wasn't trustworthy at first and almost called off the meeting. Turns out that in Russia smiling to strangers is a sign of insincerity. John's strict rule didn't account for cultural differences, blindsiding him from updating on his clients reaction, putting him in a risky situation. <br /><br />The desire to hold onto strict rules can make logical thinkers susceptible to confirmation bias too. If John made an exception to his smiling rule, he'd feel less confident about his knowledge of making first impressions, subsequently making him feel bad. He may also have to amend some other rule that relates to the smiling rule, which would further hurt his framework and his feelings.<br /><br />When feeling the urge to add on a new rule, take note of circumstances in which the evidence for the rule was found in. Add exceptions that limit the rule's predictive power to similar circumstances. Another option is to entertain multiple conflicting rules simultaneously, shifting weight from one to the other after gathering more evidence.&nbsp; <br /><br /><a id=\"more\"></a><br /><br />Anyone have more biases/tendencies to add?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GSQsAReasGjETP3e9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 32, "extendedScore": null, "score": 0.000125, "legacy": true, "legacyId": "23736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Any intuition-dominant thinker who's struggled with math problems or logic-dominant thinker who's struggled with small-talk knows how difficult and hopeless the experience feels like. For a long time I was an intuition thinker, then I developed a logical thinking style and soon it ended up dominating -- granting me the luxury of experiencing both kinds of struggles. I eventually learned to apply the thinking style better optimized for the problem I was facing. Looking back, I realized why I kept sticking to one extreme. <br><br>I hypothesize that one-sided thinkers develop biases and tendencies that prevent them from improving their weaker mode of thinking. These biases cause a positive feedback loop that further skews thinking styles in the same direction. <br><br>The reasons why one style might be overdeveloped and the other underdeveloped vary greatly. Genes have a strong influence, but environment also plays a large part. A teacher may have inspired you to love learning science at a young age, causing you to foster to a thinking style better for learning science. Or maybe you grew up very physically attractive and found socializing with your peers a lot more rewarding than studying after school, causing you to foster a thinking style better for navigating social situations. Environment can be changed to help develop certain thinking styles, but it should be supplementary to exposing and understanding the biases you already have. Entering an environment that penalizes your thinking style can be uncomfortable, stressful and frustrating without being prepared. (Such a painful experience is part of why these biases cause a positive feedback loop, by making us avoid environments that require the opposite thinking style.) <br><br>Despite genetic predisposition and environmental circumstances, there's room for improvement and exposing these biases and learning to account for them is a great first step. <br><br>Below is a list of a few biases that worsen our ability to solve a certain class of problems and keep us from improving our underdeveloped thinking style.</p>\n<hr>\n<h2 style=\"margin-top: 3px;\" id=\"Intuition_dominant_Biases\">Intuition-dominant Biases</h2>\n<p><br><strong>Overlooking crucial details</strong></p>\n<p>Details matter in order to understand technical concepts. Overlooking a word or sentence structure can cause complete misunderstanding -- a common blunder for intuition thinkers.</p>\n<p>Intuition is really good at making fairly accurate predictions without complete information, enabling us to navigate the world without having a <a href=\"/lw/1yq/understanding_your_understanding/\">deep understanding</a> of it. As a result, intuition trains us to experience the feeling we understand something without examining every detail. In most situations, paying close attention to detail is unnecessary and sometimes dangerous. When learning a technical concept, every detail matters and the premature feeling of understanding stops us from examining them. <br><br>This bias is one that's more likely to go away once you realize it's there. You often don't know what details you're missing after you've missed them, so merely remembering that you tend to miss important details should prompt you to take closer examinations in the future. <br><br><strong>Expecting solutions to sound a certain way</strong><br><br><a href=\"http://www.imdb.com/title/tt2234155/\">The Internship</a> has a great example of this bias (and a few others) in action. The movie is about two middle-aged unemployed salesmen (intuition thinkers) trying to land an internship with Google. Part of Google's selection process has the two men participate in several technical challenges. One challenge required the men and their team to find a software bug. In a flash of insight, Vince Vaughn's character, Billy, shouts \"Maybe the answer is in the question! Maybe it has something to do with the word bug. A fly!\" After enthusiastically making several more word associations, he turns to his team and insists they take him seriously. <br><br>Why is it believable to the audience that Billy can be so confident about his answer?<br><br>Billy's intuition made an association between the challenge question and riddle-like questions he's heard in the past. When Billy used his intuition to find a solution, his confidence in a riddle-like answer grew. Intuition recklessly uses irrelevant associations as reasons for narrowing down the space of possible solutions to technical problems. When associations pop in your mind, it's a good idea to legitimize those associations with supporting reasons. <br><br><strong>Not recognizing precise language</strong><br><br>Intuition thinkers are multi-channel learners -- all senses, thoughts and emotions are used to construct a complex database of clustered knowledge to predict and understand the world. With robust information-extracting ability, correct grammar/word-usage is, more often than not, unnecessary for meaningful communication. <br><br>Communicating technical concepts in a meaningful way requires precise language. Connotation and subtext are stripped away so words and phrases can purely represent meaningful concepts inside a logical framework. Intuition thinkers communicate with imprecise language, gathering meaning from context to compensate. This makes it hard for them to recognize when to turn off their powerful information extractors. <br><br>This bias explains part of why so many intuition thinkers dread math \"word problems\". Introducing words and phrases rich with meaning and connotation sends their intuition running wild. It's hard for them to find correspondences between words in the problem and variables in the theorems and formulas they've learned. <br><br>The noise intuition brings makes it hard to think clearly. It's hard for intuition thinkers to tell whether their automatic associations should be taken seriously. Without a reliable way to discern, wrong interpretations of words go undetected. For example, without any physics background, an intuition thinker may read the statement \"Matter can have both wave and particle properties at once\" and believe they completely understand it. Unrelated associations of what matter, wave and particle mean, blindly take precedence over technical definitions. <br><br>The slightest uncertainty about what a sentence means should raise a red flag. Going back and finding correspondence between each word and how it fits into a technical framework will eliminate any uncertainty. <br><br><strong>Believing their level of understanding is deeper than what it is</strong><br><br>Intuition works on an unconscious level, making intuition thinkers unaware of how they know what they know. Not surprisingly, their best tool to learn what it means to understand is intuition. The concept \"understanding\" is a collection of associations from experience. You may have learned that part of understanding something means being able to answer questions on a test with memorized factoids, or knowing what to say to convince people you understand, or just knowing more facts than your friends. These are not good methods for gaining a deep understanding of technical concepts.<br><br>When intuition thinkers optimize for understanding, they're really optimizing for a fuzzy idea of what they think understanding means. This often leaves them believing they understand a concept when all they've done is memorize some disconnected facts. Not knowing what it feels like to have deeper understanding, they become conditioned to always expect some amount of surprise. They can feel max understanding with less confidence than logical thinkers when they feel max understanding. This lower confidence disincentivizes intuition thinkers to invest in learning technical concepts, further keeping their logical thinking style underdeveloped.<br><br>One way I overcame this tendency was to constantly ask myself \"why\" questions, like a curious child bothering their parents. The technique helped me uncover what used to be unknown unknowns that made me feel overconfident in my understanding.</p>\n<h2>\n<hr>\n</h2>\n<h2 id=\"Logic_dominant_Biases\"><strong>Logic-dominant Biases</strong></h2>\n<p><br><strong>Ignoring information they cannot immediately fit into a framework</strong><br><br>Logical thinkers have and use intuition -- problem is they don't feed it enough. They tend to ignore valuable intuition-building information if it doesn't immediately fit into a predictive model they deeply understand. While intuition thinkers don't filter out enough noise, logical thinkers filter too much. <br><br>For example, if a logical thinker doesn't have a good framework for understanding human behavior, they're more likely to ignore visual input like body language and fashion, or auditory input like tone of voice and intonation. Human behavior is complicated, there's no framework to date that can make perfectly accurate predictions about it. Intuition can build powerful models despite working with many confounding variables.&nbsp;&nbsp; <br><br>Bayesian probability enables logical thinkers to build predictive models from noisy data without having to use intuition. But even then, the first step of making a Bayesian update is data collection. <br><br>Combatting this tendency requires you to pay attention to input you normally ignore. Supplement your broader attentional scope with a researched framework as a guide. Say you want to learn how storytelling works. Start by grabbing resources that teach storytelling and learn the basics. Out in the real-world, pay close attention to sights, sounds, and feelings when someone starts telling a story and try identifying sensory input to the storytelling elements you've learned about. Once the basics are subconsciously picked up by habit, your conscious attention will be freed up to make new and more subtle observations. <br><br><strong>Ignoring their emotions</strong><br><br>Emotional input is difficult to factor, especially because you're emotional at the time. Logical thinkers are notorious for ignoring this kind of messy data, consequently starving their intuition of emotional data. Being able to \"go with your gut feelings\" is a major function of intuition that logical thinkers tend to miss out on. <br><br>Your gut can predict if you'll get along long-term with a new SO, or what kind of outfit would give you more confidence in your workplace, or if learning tennis in your free time will make you happier, or whether you prefer eating a cheeseburger over tacos for lunch. Logical thinkers don't have enough data collected about their emotions to know what triggers them. They tend to get bogged down and mislead with objective, yet trivial details they manage to factor out. A weak understanding of their own emotions also leads to a weaker understanding of other's emotions. <a href=\"/lw/818/how_to_understand_people_better/\">You can become a better empathizer by better understanding yourself</a>. <br><br>You could start from scratch and build your own framework, but self-assessment biases will impede productivity. Learning an existing framework is a more realistic solution. You can find resources with some light <a href=\"http://www.helpguide.org/toolkit/developing_emotional_awareness.htm\">googling</a> and I'm sure <a href=\"http://rationality.org/\">CFAR</a> teaches some good ones too. You can improve your gut feelings too. One way is making sure you're always consciously aware of the circumstances you're in when experiencing an emotion.<br><br><strong>Making rules too strict</strong><br><br>Logical thinkers build frameworks in order to understand things. When adding a new rule to a framework, there's motivation to make the rule strict. The stricter the rule, the more predictive power, the better the framework. When the domain you're trying to understand has multivariable chaotic phenomena, strict rules are likely to break. The result is something like the current state of macroeconomics: a bunch of logical thinkers preoccupied by elegant models and theories that can only exist when useless in practice. <br><br>Following rules that are too strict can have bad consequences. Imagine John the salesperson is learning how to make better first impressions and has built a rough framework so far. John has a rule that smiling always helps make people feel welcomed the first time they meet him. One day he makes a business trip to Russia to meet with a prospective client. The moment he meet his russian client, he flashes a big smile and continues to smile despite negative reactions. After a few hours of talking, his client reveals she felt he wasn't trustworthy at first and almost called off the meeting. Turns out that in Russia smiling to strangers is a sign of insincerity. John's strict rule didn't account for cultural differences, blindsiding him from updating on his clients reaction, putting him in a risky situation. <br><br>The desire to hold onto strict rules can make logical thinkers susceptible to confirmation bias too. If John made an exception to his smiling rule, he'd feel less confident about his knowledge of making first impressions, subsequently making him feel bad. He may also have to amend some other rule that relates to the smiling rule, which would further hurt his framework and his feelings.<br><br>When feeling the urge to add on a new rule, take note of circumstances in which the evidence for the rule was found in. Add exceptions that limit the rule's predictive power to similar circumstances. Another option is to entertain multiple conflicting rules simultaneously, shifting weight from one to the other after gathering more evidence.&nbsp; <br><br><a id=\"more\"></a><br><br>Anyone have more biases/tendencies to add?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Intuition-dominant Biases", "anchor": "Intuition_dominant_Biases", "level": 1}, {"title": "Logic-dominant Biases", "anchor": "Logic_dominant_Biases", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "84 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 84, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4gevjbK77NQS6hybY", "qy4DMkqNFakaZWYkR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-13T08:05:58.268Z", "modifiedAt": null, "url": null, "title": "Bets on an Extreme Future", "slug": "bets-on-an-extreme-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:32.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DXY5M9K6HZ4tXo6Tb/bets-on-an-extreme-future", "pageUrlRelative": "/posts/DXY5M9K6HZ4tXo6Tb/bets-on-an-extreme-future", "linkUrl": "https://www.lesswrong.com/posts/DXY5M9K6HZ4tXo6Tb/bets-on-an-extreme-future", "postedAtFormatted": "Tuesday, August 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bets%20on%20an%20Extreme%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABets%20on%20an%20Extreme%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXY5M9K6HZ4tXo6Tb%2Fbets-on-an-extreme-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bets%20on%20an%20Extreme%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXY5M9K6HZ4tXo6Tb%2Fbets-on-an-extreme-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXY5M9K6HZ4tXo6Tb%2Fbets-on-an-extreme-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1141, "htmlBody": "<p>Betting on the future is a good way to reveal true beliefs.</p>\n<p>As one example of such a bet on a key debate about a post-human future, I'd like to announce here that Robin Hanson and I have made the following agreement. (See also <a href=\"http://www.overcomingbias.com/2013/08/em-vs-non-agi-bet.html\">Robin's post</a> at Overcoming Bias):</p>\n<div style=\"font-size: 13px; font-family: arial, sans-serif; padding-left: 30px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">We, Robin Hanson and Joshua Fox, agree to bet on which kind of artificial general intelligence (AGI) will dominate first, once some kind of AGI dominates humans. If the AGI are closely based on or derived from emulations of human brains, Robin wins, otherwise Joshua wins. To be precise, we focus on the first point in time when more</span><span style=\"font-family: arial, helvetica, sans-serif;\">&nbsp;computing power (gate-operations-per-second) is (routinely, typically)&nbsp;controlled relatively-directly by non-biological human-level-or-higher general&nbsp;intelligence than by ordinary biological humans. (Human brains have gate-operation equivalents.)</span></div>\n<div style=\"font-size: 13px; font-family: arial, sans-serif; padding-left: 30px;\"><span style=\"font-family: arial, helvetica, sans-serif;\"><br /></span></div>\n<div style=\"font-size: 13px; font-family: arial, sans-serif; padding-left: 30px;\"><span style=\"font-family: arial, helvetica, sans-serif;\">If at that time more of that computing power is controlled by emulation-based AGI, Joshua owes Robin&nbsp;</span><span style=\"font-family: arial, helvetica, sans-serif;\">whatever $3000 invested today in S&amp;P500-like funds today is worth then. If more is controlled by AGI not closely based on emulations, Robin owes Joshua that amount. The bet is void if the terms of this bet make little sense then, such as if</span><span style=\"font-family: arial, helvetica, sans-serif;\">&nbsp;it becomes too hard to say if capable non-biological intelligence is general or human-level, if AGI is emulation-based, what devices contain computing power, or what devices control what other devices. But we intend to tolerate modest levels of ambiguity in such things.</span></div>\n<div style=\"font-size: 13px; font-family: arial, sans-serif; padding-left: 30px;\"><br /></div>\n<div style=\"font-size: 13px; font-family: arial, sans-serif; padding-left: 30px;\"><span style=\"font-family: arial, helvetica, sans-serif;\"><span style=\"font-family: arial, helvetica, sans-serif;\">[Added Aug. 17:] To judge if &ldquo;AGI are closely based on or derived from emulations of human brains,&rdquo; judge which end of the following spectrum is closer to the actual outcome. The two ends are 1) an emulation of the specific cell connections in a particular human brain, and 2) general algorithms of the sort that typically appear in AI journals today.</span></span></div>\n<div style=\"font-size: 13px; font-family: arial, sans-serif; padding-left: 30px;\"><span style=\"font-family: arial, helvetica, sans-serif;\"><span style=\"font-family: arial, helvetica, sans-serif;\"><br /></span></span></div>\n<p>It's a bet on the old question: ems vs. de novo AGI. Kurzweil and Kapor&nbsp;<a href=\"http://www.kurzweilai.net/a-wager-on-the-turing-test-the-rules\">bet</a>&nbsp;on another well-known debate: Will machines pass the Turing Test. It would be interesting to list some other key debates that we could bet on.&nbsp;</p>\n<p>But it's hard to make a bet when settling the bet may occur in extreme conditions:</p>\n<ul>\n<li>after human extinction,</li>\n<li>in an extreme utopia,</li>\n<li>in an extreme dystopia or,</li>\n<li>after the bettors' minds have been manipulated in ways that redefine their personhood: copied thousands of times, merged with other minds, etc.</li>\n</ul>\n<p>MIRI has a \"<a href=\"http://www.acceleratingfuture.com/steven/?p=62\">techno-volatile</a>\"&nbsp;world-view: We're not just optimistic or pessimistic about the impact of technology on our future. Instead, we predict that technology will have an extreme impa<span style=\"line-height: 21px;\">ct, good or bad, on the future of humanity.&nbsp;</span>In these extreme futures, the fundamental components of a bet--the bettors and the payment currency--may be missing or altered beyond recognition.</p>\n<p><span><span>So, how can we calibrate our probability estimates about extreme events? One way is by betting on how people will bet in the future when they are closer to the events, on the assumption that they'll know better than we do. </span><span>Though this is &nbsp;an indirect and imperfect method, it might be the best we have for</span></span>&nbsp;calibrating our beliefs about extreme futures.</p>\n<p><span><span style=\"; \">For example,&nbsp;</span>Robin Hanson has suggested a market on&nbsp;<a href=\"tis http://www.overcomingbias.com/2008/07/refuge-markets.html\">tickets to a survival shelter</a>&nbsp;as a way of betting on an apocalypse. However,&nbsp;this only relevant for futures where shelters can help; and where there is time to get to one while the ticket holder is alive, and while the social norm of honoring tickets still applies.</span></p>\n<p><span style=\";\">We could also define bets on the progress of MIRI and similar organizations. Looking back on the years since 2005, when I started tracking this, I&nbsp;</span>would have liked to bet on, or at least discuss, certain milestones before they happened. They&nbsp;<span style=\" \">served as (albeit weak)&nbsp;</span>arguments from authority or from social proof for the validity of MIRI's ideas. Some examples of milestones that have already been reached:</p>\n<ul>\n<li>SIAI's budget passing $500K per annum</li>\n<li>SIAI getting 4 full-time-equivalent employees</li>\n<li>SIAI publishing its fourth peer-reviewed paper</li>\n<li>The establishment of a university research center in relevant fields</li>\n<li>The first lecture on the core FAI thesis in an accredited university course</li>\n<li>The first article on the core FAI thesis in a popular science magazine</li>\n<li>The first mention of the core FAI thesis (or of SIAI as an organization) in various types of mainstream media, with a focus on the most prestigious (NPR for radio, New York Times for newspapers).</li>\n<li>The first&nbsp;(indirect/direct)&nbsp;government funding for SIAI</li>\n</ul>\n<p>Looking to the future, we can bet on some other FAI milestones. For example, we could bet on these coming true by a certain year.</p>\n<p><span style=\";\"> </span></p>\n<ul>\n<li><span style=\";\">FAI research in general (or: organization X) will have Y dollars in funding per annum (or: Z full-time researchers).</span></li>\n<li>Eliezer Yudkowsky will still be working on FAI.</li>\n<li>The intelligence explosion will be discussed on the floor of Congress (or: in some parliament; or: by a head of state somewhere in the world).</li>\n<li>The first academic monograph on the core FAI thesis will be published (apparently that will be Nick Bostrom's).</li>\n<li>The first master's thesis/PhD dissertation on the core FAI thesis will be completed.</li>\n<li><span style=\"; \">\"Bill Gates will read at least one of 'Our Final Invention' or 'Superintelligence' in the next 2 years\"</span>&nbsp;(This already appears on&nbsp;<a href=\"/predictionbazaar.com\">PredictionBazaar</a>.)</li>\n</ul>\n<p><span style=\"; \">(Some of these will need more refinement before we can bet on them.)</span></p>\n<p>Another approach is to bet on technology trends: brain scanning resolution; prices for computing power; etc. But these bets are about a&nbsp;<span style=\" \">Kurzweillian Law of Accelerating Returns, which may be quite distinct from the Intelligence Explosion and other extreme futures we are interested in. </span></p>\n<p><span style=\" \">Many bets only make sense if you believe that a soft takeoff is likely. If you believe that, you could bet on AI&nbsp;</span>events <span style=\" \">while still allowing the bettors a few years to enjoy their winnings.&nbsp;</span></p>\n<p><span style=\" \">You can make a bet on hard vs. soft takeoff simply by setting your discount rate. If you're 20 years old and think that the economy as we know it will end instantly in, for example, 2040,&nbsp;</span>then you won't save for your retirement. (See <a href=\"http://hplusmagazine.com/2011/11/18/planning-for-the-end-of-the-era/\">my article</a> at H+Magazine.) But such decisions don't pin down your beliefs very precisely:&nbsp;Most people who don't save for their retirement are simply being improvident. Not saving makes sense if the human race is about to go extinct, but also if we are going to enter an extreme utopia or dystopia where your savings have no meaning. Likewise, most people save for retirement simply out of old-fashioned prudence, but you might build up your wealth in order to enjoy it pre-Singularity, or in order to take it with you to a post-Singularity world in which \"old money\" is still valuable.</p>\n<p>I'd like to get your opinion: What are the best bets we can use for calibrating our beliefs about the extreme events we are interested in? Can you suggest some more of these indirect markers, or a different way of betting?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DXY5M9K6HZ4tXo6Tb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "23698", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-13T17:55:05.719Z", "modifiedAt": null, "url": null, "title": "Engaging Intellectual Elites at Less Wrong", "slug": "engaging-intellectual-elites-at-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:03.033Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dzPaFnf9cWiP3wbG9/engaging-intellectual-elites-at-less-wrong", "pageUrlRelative": "/posts/dzPaFnf9cWiP3wbG9/engaging-intellectual-elites-at-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/dzPaFnf9cWiP3wbG9/engaging-intellectual-elites-at-less-wrong", "postedAtFormatted": "Tuesday, August 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Engaging%20Intellectual%20Elites%20at%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEngaging%20Intellectual%20Elites%20at%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdzPaFnf9cWiP3wbG9%2Fengaging-intellectual-elites-at-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Engaging%20Intellectual%20Elites%20at%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdzPaFnf9cWiP3wbG9%2Fengaging-intellectual-elites-at-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdzPaFnf9cWiP3wbG9%2Fengaging-intellectual-elites-at-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 303, "htmlBody": "<p>Is Less Wrong, despite its flaws, the highest-quality relatively-general-interest forum on the web? It seems to me that, to find reliably higher-quality discussion, I must turn to more narrowly focused sites, e.g. <a href=\"http://mathoverflow.net/\">MathOverflow</a> and the <a href=\"http://blog.givewell.org/\">GiveWell blog</a>.</p>\n<p>Many people smarter than myself have reported the same impression. But if you know of any comparably high-quality relatively-general-interest forums, please link me to them!</p>\n<p>In the meantime: suppose it's true that Less Wrong is the highest-quality relatively-general-interest forum on the web. In that case, we're sitting on a big opportunity to grow Less Wrong into the \"standard\" general-interest discussion hub for people with high intelligence and high metacognition (shorthand: \"intellectual elites\").</p>\n<p>Earlier, Jonah Sinick lamented <a href=\"/lw/hky/the_paucity_of_elites_online/\">the scarcity of elites on the web</a>. How can we get more intellectual elites to engage on the web, and in particular at Less Wrong?</p>\n<p>Some projects to improve the situation are extremely costly:</p>\n<ol>\n<li>Pay some intellectual elites with unusually good writing skills (like Eliezer) to generate a constant stream of new, interesting content.</li>\n<li>Comb through Less Wrong to replace community-specific jargon with more universally comprehensible terms, and change community norms about jargon. (E.g. GiveWell's jargon tends to be more transparent, such as their phrase \"room for more funding.\")</li>\n</ol>\n<p>Code changes, however, could be significantly less costly. New features or site structure elements could increase engagement by intellectual elites. (To avoid <a href=\"/lw/k3/priming_and_contamination/\">priming and contamination</a>, I'll hold back from naming specific examples here.)</p>\n<p>To help us figure out which code changes are most likely to increase engagement on Less Wrong by intellectual elites, specific MIRI volunteers will be interviewing intellectual elites who (1) are familiar enough with Less Wrong to be able to simulate which code changes might cause them to engage more, but who (2) mostly just lurk, currently.</p>\n<p>In the meantime, I figured I'd throw these ideas to the community for feedback and suggestions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"a3W2TSzPuxKr3Hm9j": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dzPaFnf9cWiP3wbG9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 17, "extendedScore": null, "score": 1.3013159545692311e-06, "legacy": true, "legacyId": "23740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 136, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dugN5SjSvoCgXgmbh", "BaCWFCxBQYjJXSsah"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-13T23:13:38.214Z", "modifiedAt": null, "url": null, "title": "[LINK] Major guide to supplements", "slug": "link-major-guide-to-supplements", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:47.823Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vj2LKqrmvPzxasJ4g/link-major-guide-to-supplements", "pageUrlRelative": "/posts/vj2LKqrmvPzxasJ4g/link-major-guide-to-supplements", "linkUrl": "https://www.lesswrong.com/posts/vj2LKqrmvPzxasJ4g/link-major-guide-to-supplements", "postedAtFormatted": "Tuesday, August 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Major%20guide%20to%20supplements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Major%20guide%20to%20supplements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvj2LKqrmvPzxasJ4g%2Flink-major-guide-to-supplements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Major%20guide%20to%20supplements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvj2LKqrmvPzxasJ4g%2Flink-major-guide-to-supplements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvj2LKqrmvPzxasJ4g%2Flink-major-guide-to-supplements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p><a href=\"http://examine.com/supplements/\">Examine.com's Guide to Supplements</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vj2LKqrmvPzxasJ4g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 1.3015826538324284e-06, "legacy": true, "legacyId": "23741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-14T01:50:37.453Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC meetup: Robin Hanson visits again", "slug": "meetup-washington-dc-meetup-robin-hanson-visits-again", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Eu75yG5APMy3BnNp6/meetup-washington-dc-meetup-robin-hanson-visits-again", "pageUrlRelative": "/posts/Eu75yG5APMy3BnNp6/meetup-washington-dc-meetup-robin-hanson-visits-again", "linkUrl": "https://www.lesswrong.com/posts/Eu75yG5APMy3BnNp6/meetup-washington-dc-meetup-robin-hanson-visits-again", "postedAtFormatted": "Wednesday, August 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20meetup%3A%20Robin%20Hanson%20visits%20again&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20meetup%3A%20Robin%20Hanson%20visits%20again%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEu75yG5APMy3BnNp6%2Fmeetup-washington-dc-meetup-robin-hanson-visits-again%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20meetup%3A%20Robin%20Hanson%20visits%20again%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEu75yG5APMy3BnNp6%2Fmeetup-washington-dc-meetup-robin-hanson-visits-again", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEu75yG5APMy3BnNp6%2Fmeetup-washington-dc-meetup-robin-hanson-visits-again", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/px\">Robin Hanson visits to talk about prediction markets</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">08 September 2013 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">National Portrait Gallery</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>(This meetup is being posted a few weeks before it will occur.)</p>\n<p>Since we mostly talked about ems and AI last time he was here, Robin Hanson is visiting again, this time with more of a focus on economics and prediction markets. (Though as always, the discusssion will also be shaped by the interests of the participants.)</p>\n<p>The meetup will be in the courtyard adjacent to the National Portrait Gallery, as usual.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/px\">Robin Hanson visits to talk about prediction markets</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Eu75yG5APMy3BnNp6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3017141271491278e-06, "legacy": true, "legacyId": "23743", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Robin_Hanson_visits_to_talk_about_prediction_markets\">Discussion article for the meetup : <a href=\"/meetups/px\">Robin Hanson visits to talk about prediction markets</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">08 September 2013 03:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">National Portrait Gallery</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>(This meetup is being posted a few weeks before it will occur.)</p>\n<p>Since we mostly talked about ems and AI last time he was here, Robin Hanson is visiting again, this time with more of a focus on economics and prediction markets. (Though as always, the discusssion will also be shaped by the interests of the participants.)</p>\n<p>The meetup will be in the courtyard adjacent to the National Portrait Gallery, as usual.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Robin_Hanson_visits_to_talk_about_prediction_markets1\">Discussion article for the meetup : <a href=\"/meetups/px\">Robin Hanson visits to talk about prediction markets</a></h2>", "sections": [{"title": "Discussion article for the meetup : Robin Hanson visits to talk about prediction markets", "anchor": "Discussion_article_for_the_meetup___Robin_Hanson_visits_to_talk_about_prediction_markets", "level": 1}, {"title": "Discussion article for the meetup : Robin Hanson visits to talk about prediction markets", "anchor": "Discussion_article_for_the_meetup___Robin_Hanson_visits_to_talk_about_prediction_markets1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-14T08:39:58.848Z", "modifiedAt": null, "url": null, "title": "[LINK] EdTech startup hosts AI Hunger Games (cash prize $1k)", "slug": "link-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malcolmocean", "createdAt": "2012-05-16T15:13:58.012Z", "isAdmin": false, "displayName": "MalcolmOcean"}, "userId": "urN5htciqMuEeghc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TioXTy9bjQrceeSaw/link-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k", "pageUrlRelative": "/posts/TioXTy9bjQrceeSaw/link-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k", "linkUrl": "https://www.lesswrong.com/posts/TioXTy9bjQrceeSaw/link-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k", "postedAtFormatted": "Wednesday, August 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20EdTech%20startup%20hosts%20AI%20Hunger%20Games%20(cash%20prize%20%241k)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20EdTech%20startup%20hosts%20AI%20Hunger%20Games%20(cash%20prize%20%241k)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTioXTy9bjQrceeSaw%2Flink-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20EdTech%20startup%20hosts%20AI%20Hunger%20Games%20(cash%20prize%20%241k)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTioXTy9bjQrceeSaw%2Flink-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTioXTy9bjQrceeSaw%2Flink-edtech-startup-hosts-ai-hunger-games-cash-prize-usd1k", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 529, "htmlBody": "<p>TL;DR = write a python script to win this applied game theory contest for $1000. Based on Prisoner's Dilemma / Tragedy of the Commons but with a few twists. Deadline Sunday August 18.</p>\n<p><a href=\"https://brilliant.org/competitions/hunger-games/rules/\" target=\"_blank\">https://brilliant.org/competitions/hunger-games/rules/</a></p>\n<blockquote>\n<p><strong>I. Food and Winning</strong></p>\n<p>Each player begins the game with 300(P&minus;1) units of food, where P is the number of players.</p>\n<p>If after any round you have zero food, you will die and no longer be allowed to compete. All players who survive until the end of the game will receive the survivor's prize.</p>\n<p>The game can end in two ways. After a large number of rounds, there will be a small chance each additional round that the game ends. Alternatively, if there is only one person left with food then the game ends. In each case, the winner is the person who has the most food when the game ends.</p>\n<p><strong>II. Hunts</strong></p>\n<p>Each round is divided into hunts. A hunt is a game played between you and one other player. Each round you will have the opportunity to hunt with every other remaining player, so you will have P&minus;1 hunts per round, where P is the number of remaining players.</p>\n</blockquote>\n<p>The choices are H = hunt (cooperate) and S = slack (defect), and they use confusing wording here, but as far as I can tell the payoff matrix is (in units of food)</p>\n<table border=\"0\" width=\"200px;\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>H / C</td>\n<td>S / D</td>\n</tr>\n<tr>\n<td>H / C</td>\n<td>0:0</td>\n<td>-3:1</td>\n</tr>\n<tr>\n<td>S / D</td>\n<td>1:-3</td>\n<td>-2:-2</td>\n</tr>\n</tbody>\n</table>\n<p>What's interesting is you don't get the entirety of your partner's history (so strategies like Tit-Tit-Tit for Tat don't work) instead you get only their reputation, which is the fraction of times they've hunted.</p>\n<p>To further complicate the Nash equilibria, there's the option to overhunt: a random number m, 0 &lt; m &lt; P(P&minus;1) is chosen before each round (round consisting of P&minus;1 hunts, remember) and if the total number of hunt-choices is at least m, then each player is awarded 2(P&minus;1) food units (2 per hunt).</p>\n<p>Your python program has to decide at the start of each round whether or not to hunt with each opponent, based on:</p>\n<ul>\n<li>the round number</li>\n<li>your food</li>\n<li>your reputation</li>\n<li>m</li>\n<li>an array of the opponents' reputations</li>\n</ul>\n<div>Based on the fact they're giving you some values you'd already know if you had access to memory, I'm assuming it must be a memoryless script that gets run each round. (EDIT: I take that back, I looked at the sample code and while it says you don't need to track this stuff, it notes that you can use instance variables).</div>\n<div><br /></div>\n<div>Submissions close on the 18th.</div>\n<div><br /></div>\n<div>\n<hr />\n</div>\n<div>I think the contest could be both better designed and better explained in a number of ways, but thought I'd share it anyway because hey, money. Also, you'd be competing in an arena where they're giving explanations of what Nash equilibria are. Which is probably not really fair. But it's the Hunger Games, and of course it's not fair. (As far as I can tell, they are not enforcing anything related to fairness here.)</div>\n<div><br /></div>\n<div>I'd be curious to hear ideas for strategies and thoughts about the design of the game.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TioXTy9bjQrceeSaw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 1.3020570646000706e-06, "legacy": true, "legacyId": "23747", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>TL;DR = write a python script to win this applied game theory contest for $1000. Based on Prisoner's Dilemma / Tragedy of the Commons but with a few twists. Deadline Sunday August 18.</p>\n<p><a href=\"https://brilliant.org/competitions/hunger-games/rules/\" target=\"_blank\">https://brilliant.org/competitions/hunger-games/rules/</a></p>\n<blockquote>\n<p><strong id=\"I__Food_and_Winning\">I. Food and Winning</strong></p>\n<p>Each player begins the game with 300(P\u22121) units of food, where P is the number of players.</p>\n<p>If after any round you have zero food, you will die and no longer be allowed to compete. All players who survive until the end of the game will receive the survivor's prize.</p>\n<p>The game can end in two ways. After a large number of rounds, there will be a small chance each additional round that the game ends. Alternatively, if there is only one person left with food then the game ends. In each case, the winner is the person who has the most food when the game ends.</p>\n<p><strong id=\"II__Hunts\">II. Hunts</strong></p>\n<p>Each round is divided into hunts. A hunt is a game played between you and one other player. Each round you will have the opportunity to hunt with every other remaining player, so you will have P\u22121 hunts per round, where P is the number of remaining players.</p>\n</blockquote>\n<p>The choices are H = hunt (cooperate) and S = slack (defect), and they use confusing wording here, but as far as I can tell the payoff matrix is (in units of food)</p>\n<table border=\"0\" width=\"200px;\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>H / C</td>\n<td>S / D</td>\n</tr>\n<tr>\n<td>H / C</td>\n<td>0:0</td>\n<td>-3:1</td>\n</tr>\n<tr>\n<td>S / D</td>\n<td>1:-3</td>\n<td>-2:-2</td>\n</tr>\n</tbody>\n</table>\n<p>What's interesting is you don't get the entirety of your partner's history (so strategies like Tit-Tit-Tit for Tat don't work) instead you get only their reputation, which is the fraction of times they've hunted.</p>\n<p>To further complicate the Nash equilibria, there's the option to overhunt: a random number m, 0 &lt; m &lt; P(P\u22121) is chosen before each round (round consisting of P\u22121 hunts, remember) and if the total number of hunt-choices is at least m, then each player is awarded 2(P\u22121) food units (2 per hunt).</p>\n<p>Your python program has to decide at the start of each round whether or not to hunt with each opponent, based on:</p>\n<ul>\n<li>the round number</li>\n<li>your food</li>\n<li>your reputation</li>\n<li>m</li>\n<li>an array of the opponents' reputations</li>\n</ul>\n<div>Based on the fact they're giving you some values you'd already know if you had access to memory, I'm assuming it must be a memoryless script that gets run each round. (EDIT: I take that back, I looked at the sample code and while it says you don't need to track this stuff, it notes that you can use instance variables).</div>\n<div><br></div>\n<div>Submissions close on the 18th.</div>\n<div><br></div>\n<div>\n<hr>\n</div>\n<div>I think the contest could be both better designed and better explained in a number of ways, but thought I'd share it anyway because hey, money. Also, you'd be competing in an arena where they're giving explanations of what Nash equilibria are. Which is probably not really fair. But it's the Hunger Games, and of course it's not fair. (As far as I can tell, they are not enforcing anything related to fairness here.)</div>\n<div><br></div>\n<div>I'd be curious to hear ideas for strategies and thoughts about the design of the game.</div>", "sections": [{"title": "I. Food and Winning", "anchor": "I__Food_and_Winning", "level": 1}, {"title": "II. Hunts", "anchor": "II__Hunts", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-14T19:31:48.762Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup VI.", "slug": "meetup-bratislava-meetup-vi", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/su5vCRnRJnX5JEPsp/meetup-bratislava-meetup-vi", "pageUrlRelative": "/posts/su5vCRnRJnX5JEPsp/meetup-bratislava-meetup-vi", "linkUrl": "https://www.lesswrong.com/posts/su5vCRnRJnX5JEPsp/meetup-bratislava-meetup-vi", "postedAtFormatted": "Wednesday, August 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20VI.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20VI.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsu5vCRnRJnX5JEPsp%2Fmeetup-bratislava-meetup-vi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20VI.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsu5vCRnRJnX5JEPsp%2Fmeetup-bratislava-meetup-vi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsu5vCRnRJnX5JEPsp%2Fmeetup-bratislava-meetup-vi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/py'>Bratislava Meetup VI.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 August 2013 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to change yourself (in Slovak language).</p>\n\n<p>Stretneme sa o \u0161iestej v bistre The Peach na Heydukovej ulici (priamo oproti poliklinike). T\u00e9ma: ako sa zmeni\u0165 -- ako sa sta\u0165 optimistom, ako ma\u0165 viac \u0161\u0165astia, ako z\u00edska\u0165 motiv\u00e1ciu, ako v\u0161etko stihn\u00fa\u0165, a ako zmeni\u0165 n\u00e1zor.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/py'>Bratislava Meetup VI.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "su5vCRnRJnX5JEPsp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3026034652777133e-06, "legacy": true, "legacyId": "23748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_VI_\">Discussion article for the meetup : <a href=\"/meetups/py\">Bratislava Meetup VI.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 August 2013 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to change yourself (in Slovak language).</p>\n\n<p>Stretneme sa o \u0161iestej v bistre The Peach na Heydukovej ulici (priamo oproti poliklinike). T\u00e9ma: ako sa zmeni\u0165 -- ako sa sta\u0165 optimistom, ako ma\u0165 viac \u0161\u0165astia, ako z\u00edska\u0165 motiv\u00e1ciu, ako v\u0161etko stihn\u00fa\u0165, a ako zmeni\u0165 n\u00e1zor.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_VI_1\">Discussion article for the meetup : <a href=\"/meetups/py\">Bratislava Meetup VI.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup VI.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_VI_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup VI.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_VI_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-14T21:32:53.800Z", "modifiedAt": null, "url": null, "title": "Practical Limits to Giving Now and The Haste Consideration", "slug": "practical-limits-to-giving-now-and-the-haste-consideration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.167Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nAmeNGGuLcRW7tBjH/practical-limits-to-giving-now-and-the-haste-consideration", "pageUrlRelative": "/posts/nAmeNGGuLcRW7tBjH/practical-limits-to-giving-now-and-the-haste-consideration", "linkUrl": "https://www.lesswrong.com/posts/nAmeNGGuLcRW7tBjH/practical-limits-to-giving-now-and-the-haste-consideration", "postedAtFormatted": "Wednesday, August 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Practical%20Limits%20to%20Giving%20Now%20and%20The%20Haste%20Consideration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APractical%20Limits%20to%20Giving%20Now%20and%20The%20Haste%20Consideration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnAmeNGGuLcRW7tBjH%2Fpractical-limits-to-giving-now-and-the-haste-consideration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Practical%20Limits%20to%20Giving%20Now%20and%20The%20Haste%20Consideration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnAmeNGGuLcRW7tBjH%2Fpractical-limits-to-giving-now-and-the-haste-consideration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnAmeNGGuLcRW7tBjH%2Fpractical-limits-to-giving-now-and-the-haste-consideration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1278, "htmlBody": "<p>Quite a few people have talked about whether or not it's better to donate money or donate time now rather than save and donate more money later. &nbsp;While there's lots of nuance on these positions, there does seem to be roughly these two camps -- weighing in (mostly) on the give now side includes <a href=\"http://blog.givewell.org/2011/12/20/give-now-or-give-later/\">GiveWell</a>, <a href=\"http://www.givingwhatwecan.org/about-us/our-research/donating-vs-investing\">Giving What We Can</a>, <a href=\"http://slatestarcodex.com/2013/04/05/investment-and-inefficient-charity/\">Scott Alexander</a>, <a href=\"http://80000hours.org/blog/43-the-haste-consideration\">Matt Wage</a>, and <a href=\"http://www.everydayutilitarian.com/essays/giving-now-currently-seems-to-beat-giving-later/\">me</a>. &nbsp;Weighing in (mostly) on the &nbsp;give later side include <a href=\"http://effective-altruism.com/best-reason-give-later\">Paul Christiano</a>, <a href=\"http://www.overcomingbias.com/2013/04/more-now-means-less-later.html\">Robin Hanson</a>, and maybe <a href=\"http://felicifia.org/viewtopic.php?t=824\">Brian Tomasik</a>.</p>\n<p>The general idea in favor of giving/working now is what Matt Wage calls <a href=\"http://80000hours.org/blog/43-the-haste-consideration\">the haste consideration</a>, or the idea that we can spend resources now to get additional people involved in the movement, and doing so <a href=\"http://www.everydayutilitarian.com/essays/giving-now-currently-seems-to-beat-giving-later/\">multiplies our impact substantially</a>&nbsp;-- getting just one person to pursue altruism with the same effectiveness we would have done doubles our impact for the rest of our life.</p>\n<p>This haste consideration, if true, has strong implications. &nbsp;It means not only would we want to donate as soon as possible, but we'd want to make sure we earn money as soon as possible as well, meaning opportunities like law or medicine that have high incomes but also significantly more schooling may be less appealing from an altruistic perspective. &nbsp;It also would mean we should be donating higher percentages of our income instead of saving, even if this would lead to giving less money over the long term.</p>\n<p>&nbsp;</p>\n<p>However, I think the haste consideration might not be as powerful as it might appear, for three reasons I haven't seen discussed much:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Money and talent seems to encounter some bottlenecks in their usage and it seems difficult to make good use them at the rate they could be given.</li>\n<li>There could be significantly better opportunities in the future that need funding and talent, and we need resources around to take up these opportunities as well.</li>\n<li>If we take the haste consideration to it's logical conclusion, we face a potential <em>reductio ad absurdum</em>&nbsp;that we should be taking out large loans to finance the effective altruist movement.</li>\n</ol>\n<p>&nbsp;</p>\n<h2>Caveats</h2>\n<p>Before I begin to elaborate on these three points, I'd like to remind everyone that I'm speculating about funding needs that I know only a little bit about and it's quite possible I'm misrepresenting organization's needs for money or how they plan on using their funds.</p>\n<p>I offer these ideas in the spirit of \"putting things out there\" for discussion and reconsideration, but I think there's a good chance that I'm wrong and look forward to being corrected.</p>\n<p>&nbsp;</p>\n<h2>How quickly can money and talent be used?</h2>\n<p>Right now it seems that the effective altruist movement cannot make quick use of large amounts of funding and talent. &nbsp;There are a couple examples of this:</p>\n<p>&nbsp;</p>\n<p><strong>Funding the Against Malaria Foundation</strong></p>\n<p>First, the <a href=\"http://www.againstmalaria.com/\">Against Malaria Foundation</a>&nbsp;is said to be constrained entirely by a need for more money, but they're still holding on to <a href=\"http://www.againstmalaria.com/FinancialInformation.aspx\">more than $4M</a>&nbsp;from the previous year that they weren't able to immediately convert into nets. &nbsp;This leads me to believe, perhaps na&iuml;vely, that donating to AMF now versus donating to it a year ago would not make a difference in net distribution, therefore giving a larger length to the haste consideration than immediately thought.</p>\n<p>&nbsp;</p>\n<p><strong>Funding Effective Altruist Orgs</strong></p>\n<p>Second, organizations where the haste consideration is most immediately applicable, like <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a>, <a href=\"http://www.80000hours.org\">80,000 Hours</a>, or <a href=\"http://www.effectiveanimalactivism.org\">Effective Animal Activism</a>&nbsp;are constrained not just by money but also by the talent needed to spend that money well. &nbsp;Currently they're in large need of funding about six or seven staff positions between them. &nbsp;While there's definitely a need for this staff, funding them would take up, by my guess, less than $200K in the first year. &nbsp;Additional funding beyond that probably couldn't be spent immediately.</p>\n<p>And even then, the salaries for careers couldn't be spent immediately either, because a recruitment process needs to be run first and people need to be hired and settled into their positions. &nbsp;So even among the $200K that could be spent soon wouldn't be spent for several months, and therefore there would be little difference between donating now and donating a few months from now.</p>\n<p>&nbsp;</p>\n<p><strong>Working for Effective Altruist Orgs</strong></p>\n<p>Furthermore, while the haste consideration would favor trying to launch into work as fast as possible without spending time to gather experience or further education in unrelated fields. &nbsp;However, many of these organizations also lack the capacity to take on a large amount of staff for long periods of time and are only looking for the \"best of the best\". &nbsp;Therefore, if you don't fall into the upper limit of talent, it would be more advisable to ignore the haste consideration here and work on cultivating further talent first.</p>\n<p>&nbsp;</p>\n<p><strong>More Generally...</strong></p>\n<p>Most generally, it seems like effective altruist organizations, like all organizations, are constrained by logistics and management and cannot absorb money and talent as quickly as it can be flung at them. &nbsp;Perhaps this is optimistic of how many resources are being channelled into the effective altruist movement and it turns out that everything could be used up, but excess money and talent would be better invested until it can be used well.</p>\n<p>&nbsp;</p>\n<h2>Could there be significantly better opportunities just a few years from now?</h2>\n<p>On the flip side of things, it could turn out that we end up with enormously better opportunities to spend money in just a few years. &nbsp;Given that <a href=\"http://www.jefftk.com/p/the-unintuitive-power-laws-of-giving\">non-profit cost-effectiveness varies widely</a>, the discovery of a new giving opportunity (by say <a href=\"http://blog.givewell.org/category/givewell-labs/\">GiveWell Labs</a>) could be huge and it's quite plausible that the returns from saving now and donating to this much better opportunity are higher than donating to whatever we can best find now.</p>\n<p>It might make sense to give now if your giving time or money will make us find this opportunity quicker (<a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">value of information in giving</a>), but it seems like many people are not in the position for this to be the case. &nbsp;For example, GiveWell labs is already fully funded and working about as fast as they can, and there might not be anything we can do to speed things up but wait.</p>\n<p>&nbsp;</p>\n<h2>Should we take out loans to give now?</h2>\n<p>While perhaps unfair or infeasible for a variety of reasons, a possible <em>reductio ad absurdum</em>&nbsp;of the idea of giving as fast as possible is the idea that we would be in the best position to give now if we took out large loans and donated the proceeds. &nbsp;Interest rates for loans are very frequently much less than the returns on giving now that some people have suggested, which could be 20% or more. &nbsp;Therefore, one could take out a loan now, donate it all, and use the rest of the income they earn over their life to repay the loan.</p>\n<p>I have an intuitive aversion to this idea of taking out a loan and I suspect other people do as well. &nbsp;Exploring why we're averse to this idea could explain why it might be a bad idea to donate as much as possible now (or lead us to all take out loans).</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>Overall, it seems like the haste consideration is not as \"smooth\" or \"linear\" as it may seem. &nbsp;Non-profits rarely spend every dollar right as it comes in, instead gathering together many donations and making large purchases all at once. &nbsp;Therefore, whether you donate today or a year from now might not make much difference at all.</p>\n<p>Given these constraints, I think it's worth taking the haste consideration somewhat seriously, but perhaps less seriously than it may seem on face value. &nbsp;For example, I think it often would be a bad idea to do things like eschew further education or refuse to save portions of your income. &nbsp;Though, luckily, I haven't heard of anyone taking the haste consideration so seriously that they would do this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nAmeNGGuLcRW7tBjH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 1.3027050090616283e-06, "legacy": true, "legacyId": "23749", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Quite a few people have talked about whether or not it's better to donate money or donate time now rather than save and donate more money later. &nbsp;While there's lots of nuance on these positions, there does seem to be roughly these two camps -- weighing in (mostly) on the give now side includes <a href=\"http://blog.givewell.org/2011/12/20/give-now-or-give-later/\">GiveWell</a>, <a href=\"http://www.givingwhatwecan.org/about-us/our-research/donating-vs-investing\">Giving What We Can</a>, <a href=\"http://slatestarcodex.com/2013/04/05/investment-and-inefficient-charity/\">Scott Alexander</a>, <a href=\"http://80000hours.org/blog/43-the-haste-consideration\">Matt Wage</a>, and <a href=\"http://www.everydayutilitarian.com/essays/giving-now-currently-seems-to-beat-giving-later/\">me</a>. &nbsp;Weighing in (mostly) on the &nbsp;give later side include <a href=\"http://effective-altruism.com/best-reason-give-later\">Paul Christiano</a>, <a href=\"http://www.overcomingbias.com/2013/04/more-now-means-less-later.html\">Robin Hanson</a>, and maybe <a href=\"http://felicifia.org/viewtopic.php?t=824\">Brian Tomasik</a>.</p>\n<p>The general idea in favor of giving/working now is what Matt Wage calls <a href=\"http://80000hours.org/blog/43-the-haste-consideration\">the haste consideration</a>, or the idea that we can spend resources now to get additional people involved in the movement, and doing so <a href=\"http://www.everydayutilitarian.com/essays/giving-now-currently-seems-to-beat-giving-later/\">multiplies our impact substantially</a>&nbsp;-- getting just one person to pursue altruism with the same effectiveness we would have done doubles our impact for the rest of our life.</p>\n<p>This haste consideration, if true, has strong implications. &nbsp;It means not only would we want to donate as soon as possible, but we'd want to make sure we earn money as soon as possible as well, meaning opportunities like law or medicine that have high incomes but also significantly more schooling may be less appealing from an altruistic perspective. &nbsp;It also would mean we should be donating higher percentages of our income instead of saving, even if this would lead to giving less money over the long term.</p>\n<p>&nbsp;</p>\n<p>However, I think the haste consideration might not be as powerful as it might appear, for three reasons I haven't seen discussed much:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Money and talent seems to encounter some bottlenecks in their usage and it seems difficult to make good use them at the rate they could be given.</li>\n<li>There could be significantly better opportunities in the future that need funding and talent, and we need resources around to take up these opportunities as well.</li>\n<li>If we take the haste consideration to it's logical conclusion, we face a potential <em>reductio ad absurdum</em>&nbsp;that we should be taking out large loans to finance the effective altruist movement.</li>\n</ol>\n<p>&nbsp;</p>\n<h2 id=\"Caveats\">Caveats</h2>\n<p>Before I begin to elaborate on these three points, I'd like to remind everyone that I'm speculating about funding needs that I know only a little bit about and it's quite possible I'm misrepresenting organization's needs for money or how they plan on using their funds.</p>\n<p>I offer these ideas in the spirit of \"putting things out there\" for discussion and reconsideration, but I think there's a good chance that I'm wrong and look forward to being corrected.</p>\n<p>&nbsp;</p>\n<h2 id=\"How_quickly_can_money_and_talent_be_used_\">How quickly can money and talent be used?</h2>\n<p>Right now it seems that the effective altruist movement cannot make quick use of large amounts of funding and talent. &nbsp;There are a couple examples of this:</p>\n<p>&nbsp;</p>\n<p><strong id=\"Funding_the_Against_Malaria_Foundation\">Funding the Against Malaria Foundation</strong></p>\n<p>First, the <a href=\"http://www.againstmalaria.com/\">Against Malaria Foundation</a>&nbsp;is said to be constrained entirely by a need for more money, but they're still holding on to <a href=\"http://www.againstmalaria.com/FinancialInformation.aspx\">more than $4M</a>&nbsp;from the previous year that they weren't able to immediately convert into nets. &nbsp;This leads me to believe, perhaps na\u00efvely, that donating to AMF now versus donating to it a year ago would not make a difference in net distribution, therefore giving a larger length to the haste consideration than immediately thought.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Funding_Effective_Altruist_Orgs\">Funding Effective Altruist Orgs</strong></p>\n<p>Second, organizations where the haste consideration is most immediately applicable, like <a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a>, <a href=\"http://www.80000hours.org\">80,000 Hours</a>, or <a href=\"http://www.effectiveanimalactivism.org\">Effective Animal Activism</a>&nbsp;are constrained not just by money but also by the talent needed to spend that money well. &nbsp;Currently they're in large need of funding about six or seven staff positions between them. &nbsp;While there's definitely a need for this staff, funding them would take up, by my guess, less than $200K in the first year. &nbsp;Additional funding beyond that probably couldn't be spent immediately.</p>\n<p>And even then, the salaries for careers couldn't be spent immediately either, because a recruitment process needs to be run first and people need to be hired and settled into their positions. &nbsp;So even among the $200K that could be spent soon wouldn't be spent for several months, and therefore there would be little difference between donating now and donating a few months from now.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Working_for_Effective_Altruist_Orgs\">Working for Effective Altruist Orgs</strong></p>\n<p>Furthermore, while the haste consideration would favor trying to launch into work as fast as possible without spending time to gather experience or further education in unrelated fields. &nbsp;However, many of these organizations also lack the capacity to take on a large amount of staff for long periods of time and are only looking for the \"best of the best\". &nbsp;Therefore, if you don't fall into the upper limit of talent, it would be more advisable to ignore the haste consideration here and work on cultivating further talent first.</p>\n<p>&nbsp;</p>\n<p><strong id=\"More_Generally___\">More Generally...</strong></p>\n<p>Most generally, it seems like effective altruist organizations, like all organizations, are constrained by logistics and management and cannot absorb money and talent as quickly as it can be flung at them. &nbsp;Perhaps this is optimistic of how many resources are being channelled into the effective altruist movement and it turns out that everything could be used up, but excess money and talent would be better invested until it can be used well.</p>\n<p>&nbsp;</p>\n<h2 id=\"Could_there_be_significantly_better_opportunities_just_a_few_years_from_now_\">Could there be significantly better opportunities just a few years from now?</h2>\n<p>On the flip side of things, it could turn out that we end up with enormously better opportunities to spend money in just a few years. &nbsp;Given that <a href=\"http://www.jefftk.com/p/the-unintuitive-power-laws-of-giving\">non-profit cost-effectiveness varies widely</a>, the discovery of a new giving opportunity (by say <a href=\"http://blog.givewell.org/category/givewell-labs/\">GiveWell Labs</a>) could be huge and it's quite plausible that the returns from saving now and donating to this much better opportunity are higher than donating to whatever we can best find now.</p>\n<p>It might make sense to give now if your giving time or money will make us find this opportunity quicker (<a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">value of information in giving</a>), but it seems like many people are not in the position for this to be the case. &nbsp;For example, GiveWell labs is already fully funded and working about as fast as they can, and there might not be anything we can do to speed things up but wait.</p>\n<p>&nbsp;</p>\n<h2 id=\"Should_we_take_out_loans_to_give_now_\">Should we take out loans to give now?</h2>\n<p>While perhaps unfair or infeasible for a variety of reasons, a possible <em>reductio ad absurdum</em>&nbsp;of the idea of giving as fast as possible is the idea that we would be in the best position to give now if we took out large loans and donated the proceeds. &nbsp;Interest rates for loans are very frequently much less than the returns on giving now that some people have suggested, which could be 20% or more. &nbsp;Therefore, one could take out a loan now, donate it all, and use the rest of the income they earn over their life to repay the loan.</p>\n<p>I have an intuitive aversion to this idea of taking out a loan and I suspect other people do as well. &nbsp;Exploring why we're averse to this idea could explain why it might be a bad idea to donate as much as possible now (or lead us to all take out loans).</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>Overall, it seems like the haste consideration is not as \"smooth\" or \"linear\" as it may seem. &nbsp;Non-profits rarely spend every dollar right as it comes in, instead gathering together many donations and making large purchases all at once. &nbsp;Therefore, whether you donate today or a year from now might not make much difference at all.</p>\n<p>Given these constraints, I think it's worth taking the haste consideration somewhat seriously, but perhaps less seriously than it may seem on face value. &nbsp;For example, I think it often would be a bad idea to do things like eschew further education or refuse to save portions of your income. &nbsp;Though, luckily, I haven't heard of anyone taking the haste consideration so seriously that they would do this.</p>", "sections": [{"title": "Caveats", "anchor": "Caveats", "level": 1}, {"title": "How quickly can money and talent be used?", "anchor": "How_quickly_can_money_and_talent_be_used_", "level": 1}, {"title": "Funding the Against Malaria Foundation", "anchor": "Funding_the_Against_Malaria_Foundation", "level": 2}, {"title": "Funding Effective Altruist Orgs", "anchor": "Funding_Effective_Altruist_Orgs", "level": 2}, {"title": "Working for Effective Altruist Orgs", "anchor": "Working_for_Effective_Altruist_Orgs", "level": 2}, {"title": "More Generally...", "anchor": "More_Generally___", "level": 2}, {"title": "Could there be significantly better opportunities just a few years from now?", "anchor": "Could_there_be_significantly_better_opportunities_just_a_few_years_from_now_", "level": 1}, {"title": "Should we take out loans to give now?", "anchor": "Should_we_take_out_loans_to_give_now_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-15T02:18:54.850Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 26, chapter 97", "slug": "harry-potter-and-the-methods-of-rationality-discussion-23", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:34.270Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4hKoG4e248yuXtG47/harry-potter-and-the-methods-of-rationality-discussion-23", "pageUrlRelative": "/posts/4hKoG4e248yuXtG47/harry-potter-and-the-methods-of-rationality-discussion-23", "linkUrl": "https://www.lesswrong.com/posts/4hKoG4e248yuXtG47/harry-potter-and-the-methods-of-rationality-discussion-23", "postedAtFormatted": "Thursday, August 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2026%2C%20chapter%2097&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2026%2C%20chapter%2097%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hKoG4e248yuXtG47%2Fharry-potter-and-the-methods-of-rationality-discussion-23%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2026%2C%20chapter%2097%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hKoG4e248yuXtG47%2Fharry-potter-and-the-methods-of-rationality-discussion-23", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hKoG4e248yuXtG47%2Fharry-potter-and-the-methods-of-rationality-discussion-23", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 231, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/chapter/97\">chapter 97</a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/i4r/harry_potter_and_the_methods_of_rationality/\">The previous thread&nbsp;</a>is at nearly 500 comments.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/fyv/harry_potter_and_the_methods_of_rationality/\">17</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/g1q/harry_potter_and_the_methods_of_rationality/\">18</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/huq/harry_potter_and_the_methods_of_rationality/\">19</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hvg/harry_potter_and_the_methods_of_rationality/\">20</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hwf/harry_potter_and_the_methods_of_rationality/\">21</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hws/harry_potter_and_the_methods_of_rationality/\">22</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">23</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/i19/harry_potter_and_the_methods_of_rationality/\">24</a>, &nbsp;<a href=\"/r/discussion/lw/i4r/harry_potter_and_the_methods_of_rationality/\">25</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4hKoG4e248yuXtG47", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 505, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ey8yGkFnT7Gcgnt5r", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3", "QkhX5YeuYHzPW7Waz", "4sY9rqAqty8rHWGSW", "35GjH7tDvNJWSHQ3H", "Pxiu5SG8gjhCh2jYd", "CEd85FLRbQWsbkrmf", "CcnpbKuRaYMjpFmQq", "smKK6yrKBehxvQq5i", "bMxxf7Wtic298LcNx", "uBpSaxteqitApiJJs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-15T15:31:45.682Z", "modifiedAt": "2021-05-22T23:51:09.000Z", "url": null, "title": "\"Mind reading\" - how is this done?", "slug": "mind-reading-how-is-this-done", "viewCount": null, "lastCommentedAt": "2015-06-28T17:04:37.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CWG", "createdAt": "2012-05-25T08:54:53.298Z", "isAdmin": false, "displayName": "CWG"}, "userId": "WuY6KvdLfaq3FPsK4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v3NR8FmK5awk2A63G/mind-reading-how-is-this-done", "pageUrlRelative": "/posts/v3NR8FmK5awk2A63G/mind-reading-how-is-this-done", "linkUrl": "https://www.lesswrong.com/posts/v3NR8FmK5awk2A63G/mind-reading-how-is-this-done", "postedAtFormatted": "Thursday, August 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Mind%20reading%22%20-%20how%20is%20this%20done%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Mind%20reading%22%20-%20how%20is%20this%20done%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv3NR8FmK5awk2A63G%2Fmind-reading-how-is-this-done%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Mind%20reading%22%20-%20how%20is%20this%20done%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv3NR8FmK5awk2A63G%2Fmind-reading-how-is-this-done", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv3NR8FmK5awk2A63G%2Fmind-reading-how-is-this-done", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 715, "htmlBody": "<p><em>I just want to burn him at a stake and watch his witch's heart bubble. It&rsquo;s extraordinary. Great trick. - Stephen Fry</em></p>\n<p>Derren Brown does many amazing tricks - I want to focus here on his \"mind reading\". This is way beyond any cold reading I've seen, but he insists that he uses no actors or stooges. He's also a skeptic, very clear about not being psychic. He does reveal some of his tricks, but maintains a lot of mystery.</p>\n<p><a href=\"http://youtu.be/1LqZHyANzns\">Reading David Frost's mind</a> - unusually, he struggles and gets the first one wrong, and seems to reveal tiny glimpses of his technique. Then at the end he gives more hints about his technique than usual.&nbsp;</p>\n<p><a href=\"http://youtu.be/gWm6GA_TgLo\">Pet name</a>&nbsp;- getting someone on the street to read another person's mind. In the full version (from the DVD of Trick of the Mind, series one) the segment starts with Derren telling the guy (the pet owner) that sorry, it won't work on you, then later changing his mind and bringing him in.</p>\n<p><a href=\"http://youtu.be/GyOrQmoXVpc\">Creepy clown</a>&nbsp;- the detail here is extraordinary.&nbsp;</p>\n<p>Watch the videos then scroll down, if you want to watch it without being influenced by me... I have a few thoughts, but they don't go very far in explaining it...&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp;- - - - - -&nbsp;</p>\n<p>Whatever he's doing, he's extraordinarily good at it. Some speculations:</p>\n<p>\n<ul>\n<li>Derren Brown uses suggestion&nbsp;and \"subliminal\" messages&nbsp;very heavily in his tricks. Often he will have written down the person's choice long before they've chosen, and subtly gets them thinking about what he wants.&nbsp;In the examples above he doesn't have much opportunity to direct the thought, I think... except that in the case of David Frost choosing a place, Frost is looking in the direction of the city scape behind Derren, which presumably influences his choice.&nbsp;</li>\n<li>Micromuscle reading: When he or a participant tries to read a thought, there's often something about picking up the sound of a letter. Perhaps he reads involuntary micromuscle movements related to the mouth and throat that happen while saying something loudly in one's head, but suppressing it. (I would have guessed that was impossible... it still seems unlikely, but much more likely than \"he's psychic\".)</li>\n<li>He narrows down the field of possibilities, often through suggestion, or sometimes (as with David Frost) asking them for something more specific.&nbsp;</li>\n<li>He usually selects the participant, making sure he's got someone suitable. Perhaps all TV show hosts are suitable. (Except in his stage shows, where he throws a teddy bear into the audience, and asks the audience to throw it again. Perhaps if you've bought an expensive ticket to his show, you're invested in it and ready to go along with him, and that's enough for those particular tricks.) He says that a few things increase suggestibility, including the presence of a camera.</li>\n<li>He's extremely observant and good at making connections - in one trick, he tells a man a lot about himself, by holding his hands through holes in a panel, but not able to see the man. E.g. by the roughness of the man's hands, he guesses what sport he's interested in. He pulls his hands back, smells them deeply, then declares that he has terriers (correct - he attributes that to having learnt to tell the difference between breeds of dogs by their smell), probably 3 of them (correct - no idea how).&nbsp;</li>\n<li>Any of these things might be misdirection.&nbsp;</li>\n<li>He may be lying, and actually using actors &amp; stooges in some of his tricks. But that doesn't explain everything very well (e.g. I doubt that David Frost, Jamie Oliver or other celebs were paid to go along with him, but again, it's more believable that they're all lying than that he's psychic.)</li>\n</ul>\n<div>I think he's using a lot of different methods at once, with great skill... but I'm sure my speculations are a long way short of explaining what he does. I'd love to hear any further insights on his mind-reading.&nbsp;</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v3NR8FmK5awk2A63G", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 1.3036103882864556e-06, "legacy": true, "legacyId": "23756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-08-15T15:31:45.682Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-15T21:20:23.641Z", "modifiedAt": null, "url": null, "title": "Optimizing optimizing LessWrong", "slug": "optimizing-optimizing-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.314Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "yxAh6rCnmpzQccRPn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SD6vmtZJMHgD3KfPD/optimizing-optimizing-lesswrong", "pageUrlRelative": "/posts/SD6vmtZJMHgD3KfPD/optimizing-optimizing-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/SD6vmtZJMHgD3KfPD/optimizing-optimizing-lesswrong", "postedAtFormatted": "Thursday, August 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimizing%20optimizing%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimizing%20optimizing%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSD6vmtZJMHgD3KfPD%2Foptimizing-optimizing-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimizing%20optimizing%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSD6vmtZJMHgD3KfPD%2Foptimizing-optimizing-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSD6vmtZJMHgD3KfPD%2Foptimizing-optimizing-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Our optimization process sucks.</p>\n<p>Every now and then someone makes up a thread concerning the optimization of LW, we have a nice discussion, one or the other idea gets a number of upvotes, sometimes quite a lot, but in the end, nothing happens.</p>\n<p>What's more, it is predictable that nothing will happen, which kills the motivation to contribute.</p>\n<ul>\n<li>The LW staff (those who hold the authority to implement major changes!) needs to commit a considerable amount of time for the optimization of LW. If necessary, hire someone and make his main job LW optimization.</li>\n<li>The people contributing to optimization threads need to get serious feedback from the staff. It is just frustrating to feel that those who are in charge of implementing changes will not even take the time to think my arguments through.</li>\n</ul>\n<p>It is just a tragic waste to put so much time and effort into LW, and about nothing into optimizing it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SD6vmtZJMHgD3KfPD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -7, "extendedScore": null, "score": 1.3039031975539582e-06, "legacy": true, "legacyId": "23759", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-16T07:09:54.834Z", "modifiedAt": null, "url": null, "title": "Where I've Changed My Mind on My Approach to Speculative Causes", "slug": "where-i-ve-changed-my-mind-on-my-approach-to-speculative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:39.049Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tX93igpzE5spGkpmp/where-i-ve-changed-my-mind-on-my-approach-to-speculative", "pageUrlRelative": "/posts/tX93igpzE5spGkpmp/where-i-ve-changed-my-mind-on-my-approach-to-speculative", "linkUrl": "https://www.lesswrong.com/posts/tX93igpzE5spGkpmp/where-i-ve-changed-my-mind-on-my-approach-to-speculative", "postedAtFormatted": "Friday, August 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20I've%20Changed%20My%20Mind%20on%20My%20Approach%20to%20Speculative%20Causes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20I've%20Changed%20My%20Mind%20on%20My%20Approach%20to%20Speculative%20Causes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtX93igpzE5spGkpmp%2Fwhere-i-ve-changed-my-mind-on-my-approach-to-speculative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20I've%20Changed%20My%20Mind%20on%20My%20Approach%20to%20Speculative%20Causes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtX93igpzE5spGkpmp%2Fwhere-i-ve-changed-my-mind-on-my-approach-to-speculative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtX93igpzE5spGkpmp%2Fwhere-i-ve-changed-my-mind-on-my-approach-to-speculative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1179, "htmlBody": "<p><em>Follow up to <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">Why I'm Skeptical About Unproven Causes (And You Should Be Too)</a></em></p>\n<p>Previously, I wrote <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">\"Why I'm Skeptical About Unproven Causes (And You Should Be Too)\"</a>&nbsp;and a follow up essay <a href=\"/r/discussion/lw/i9u/what_would_it_take_to_prove_a_speculative_cause/\">\"What Would It Take to Prove a Speculative Cause?\"</a>. &nbsp;Both of these sparked a lot of discussion on LessWrong, <a href=\"http://effective-altruism.com/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">on the Effective Altruist blog</a>, and <a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">my own blog</a>, as well as many hours of in person conversation.</p>\n<p>After all this extended conversation with people, I've changed my mind on a few things that I will elaborate here. &nbsp;I hope in doing so I can (1) clarify my original position and (2) explain where I now stand in light of all the debate so people can engage with my <em>current ideas</em> as opposed to the ideas I no longer hold. &nbsp;My opinions on things tend to change quickly, so I think updates like this will help.</p>\n<p>&nbsp;</p>\n<h2>My Argument, As It Currently Stands</h2>\n<p>If I were to communicate one main point of my essay, based on what I believe now, it would be <strong>when you're in a position of high uncertainty, the best response is to use a strategy of exploration rather than a strategy of exploitation.</strong></p>\n<p>What I mean by this is that given the high uncertainty of impact we see now, especially with regard to the far future, we're better off trying to find more information about impact and reduce our uncertainty (exploration) rather than pursuing whatever we think is best (exploitation).</p>\n<p>The implications of this would mean that:</p>\n<ul>\n<li>We should develop more of an attitude that our case for impact is neither clear nor proven.</li>\n</ul>\n<ul>\n<li>We should apply more skepticism to our causes and more self-skepticism to our personal beliefs about impact.</li>\n</ul>\n<ul>\n<li>We should use the language of \"information\" and \"exploration\" more often than the language of \"impact\" and \"exploitation\".</li>\n</ul>\n<ul>\n<li>We should focus more on finding specific and concrete attempts to ensure we're making progress and figure out our impact (whether it be surveys, experiments, soliciting external review from relevant experts, etc.).</li>\n</ul>\n<ul>\n<li>We should focus more on transparency about what we're doing and thinking and why, when relevant and not exceedingly costly.</li>\n</ul>\n<p>&nbsp;</p>\n<p>And to be clear, here are specific statements that address misconceptions about what I have argued:</p>\n<ul>\n<li>I do think it is wrong to ignore unproven causes completely and stop pursuing them.</li>\n</ul>\n<ul>\n<li>I don't think we should be donating everything to the Against Malaria Foundation instead of speculative causes.</li>\n</ul>\n<ul>\n<li>I don't think the Against Malaria Foundation has the highest impact of all current opportunities to donate.</li>\n</ul>\n<ul>\n<li>I do think we can say useful things about the far future.</li>\n</ul>\n<ul>\n<li>I don't think the correct way to think about high uncertainty and low evidence is to \"suspend judgement\". &nbsp;Rather, I think we should make a judgement that we expect the estimate to be much lower than initially claimed in light of all the things I've said earlier about <a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">the history of past cost-effectiveness estimates</a>.</li>\n</ul>\n<div><br /></div>\n<p>&nbsp;</p>\n<p>And, lastly, if I were to make a second important point it would be <strong>it's difficult to find good opportunities to buy information.</strong>&nbsp; It's easy to think that any donation to an organization will generate good information or that we'll automatically make progress just by working. &nbsp;I think some element of random pursuit is important (see below), but all things considered I think we're doing too much random pursuit right now.</p>\n<p>&nbsp;</p>\n<h2>Specific Things I Changed My Mind About</h2>\n<p>Here are the specific places I changed my mind on:</p>\n<p>&nbsp;</p>\n<p><strong>I used to think donating to AMF, at least in part, was important for me. &nbsp;Now I don't.</strong></p>\n<p>I underestimated the power of exploring and the existing opportunities, so I think that 100% of my donations should be going to trying to assess impact. &nbsp;I've been persuaded that there is already quite a lot of money going toward AMF and <a href=\"/r/discussion/lw/ibp/practical_limits_to_giving_now_and_the_haste/\">we might not need more money as quickly as thought</a>, so for the time being it's probably more appropriate to save and then donate to opportunities to buy information as they come up.</p>\n<p>&nbsp;</p>\n<p><strong>I now agree that there are relevant economies of scale in pursuing information that I hadn't taken into account.</strong></p>\n<p>What I mean by this is it might not be appropriate for individuals to work on purchasing information themselves. &nbsp;Instead, this could end up splitting up the time of organizations unnecessarily as they provide information to a bunch of different people. &nbsp;Also, many people don't have the time to do this themselves.</p>\n<p>I think this has two implications:</p>\n<ul>\n<li>We should put more trust in larger scale organizations who are doing exploring, like <a href=\"http://www.givewell.org\">GiveWell</a>, and pool our resources.</li>\n</ul>\n<ul>\n<li>Individuals should work harder to put relevant information about information we gather online.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>I was partially mistaken in thinking about how to \"prove\" speculative causes.</strong></p>\n<p>I think there was some value in my essay <a href=\"/r/discussion/lw/i9u/what_would_it_take_to_prove_a_speculative_cause/\">\"What Would It Take to Prove a Speculative Cause?\"</a>&nbsp;because it talked concretely about strategies some organizations could take to get more information about their impact.</p>\n<p>But the overall concept is mistaken -- there is no arbitrary threshold of evidence at which a speculative cause needs to cross and I was wasting my time by trying to come up with one. &nbsp;Instead, I think it's appropriate to continue doing expected value calculations as long as we maintain a self-skeptical, pro-measurement mindset.</p>\n<p>&nbsp;</p>\n<p><strong>I had previously not fully taken into account the cost of acquiring further information.</strong></p>\n<p>The important question in value of information is not \"what does this information get me in terms of changing my beliefs and actions?\" but actually \"how valuable is this information?\", as in, do the benefits of gathering this information outweigh all the costs? &nbsp;In some cases, I think the benefits of further proving a cause probably don't outweigh the costs.</p>\n<p>For one possibly extreme example, while I don't know the rationale for doing a 23rd randomized controlled trial on anti-malaria bednets after performing the previous 22, it's likely that doing that RCT would have to be testing something more specific than the general effectiveness of bednets to justify the high cost of doing an RCT.</p>\n<p>Likewise, there are costs on organizations to devoting resources to measuring themselves and being more transparent. &nbsp;I don't think these costs are particularly high or defeat the idea of devoting more resources to this area, but I hadn't really taken them into account before.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>I'm slightly more in favor of acting randomly (trial and error).</strong></p>\n<p>I still think it's difficult to acquire good value of information and it's very easy to get caught \"spinning our wheels\" in research, especially when that research has no clear feedback loops. &nbsp;One example, perhaps somewhat controversial, would be to point to&nbsp;<a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">the multi-century lack of progress on some problems in philosophy</a>&nbsp;(think meta-ethics) as an example of what can happen to a field when there aren't good feedback loops to ground yourself.</p>\n<p>However, I underestimated the amount of information that comes forward just doing ones normal activities. &nbsp;The implication here is that it's more worthwhile than I initially thought to fund speculative causes just to have them continue to scale and operate.</p>\n<p>-</p>\n<p><em>(This was also <a href=\"http://www.everydayutilitarian.com/essays/where-ive-changed-my-mind-on-my-approach-to-speculative-causes/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3RnEKrsNgNEDxuNnw": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tX93igpzE5spGkpmp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 59, "extendedScore": null, "score": 0.000154, "legacy": true, "legacyId": "23760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Follow up to <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">Why I'm Skeptical About Unproven Causes (And You Should Be Too)</a></em></p>\n<p>Previously, I wrote <a href=\"/lw/i6f/why_im_skeptical_about_unproven_causes_and_you/\">\"Why I'm Skeptical About Unproven Causes (And You Should Be Too)\"</a>&nbsp;and a follow up essay <a href=\"/r/discussion/lw/i9u/what_would_it_take_to_prove_a_speculative_cause/\">\"What Would It Take to Prove a Speculative Cause?\"</a>. &nbsp;Both of these sparked a lot of discussion on LessWrong, <a href=\"http://effective-altruism.com/why-im-skeptical-about-unproven-causes-and-you-should-be-too\">on the Effective Altruist blog</a>, and <a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">my own blog</a>, as well as many hours of in person conversation.</p>\n<p>After all this extended conversation with people, I've changed my mind on a few things that I will elaborate here. &nbsp;I hope in doing so I can (1) clarify my original position and (2) explain where I now stand in light of all the debate so people can engage with my <em>current ideas</em> as opposed to the ideas I no longer hold. &nbsp;My opinions on things tend to change quickly, so I think updates like this will help.</p>\n<p>&nbsp;</p>\n<h2 id=\"My_Argument__As_It_Currently_Stands\">My Argument, As It Currently Stands</h2>\n<p>If I were to communicate one main point of my essay, based on what I believe now, it would be <strong>when you're in a position of high uncertainty, the best response is to use a strategy of exploration rather than a strategy of exploitation.</strong></p>\n<p>What I mean by this is that given the high uncertainty of impact we see now, especially with regard to the far future, we're better off trying to find more information about impact and reduce our uncertainty (exploration) rather than pursuing whatever we think is best (exploitation).</p>\n<p>The implications of this would mean that:</p>\n<ul>\n<li>We should develop more of an attitude that our case for impact is neither clear nor proven.</li>\n</ul>\n<ul>\n<li>We should apply more skepticism to our causes and more self-skepticism to our personal beliefs about impact.</li>\n</ul>\n<ul>\n<li>We should use the language of \"information\" and \"exploration\" more often than the language of \"impact\" and \"exploitation\".</li>\n</ul>\n<ul>\n<li>We should focus more on finding specific and concrete attempts to ensure we're making progress and figure out our impact (whether it be surveys, experiments, soliciting external review from relevant experts, etc.).</li>\n</ul>\n<ul>\n<li>We should focus more on transparency about what we're doing and thinking and why, when relevant and not exceedingly costly.</li>\n</ul>\n<p>&nbsp;</p>\n<p>And to be clear, here are specific statements that address misconceptions about what I have argued:</p>\n<ul>\n<li>I do think it is wrong to ignore unproven causes completely and stop pursuing them.</li>\n</ul>\n<ul>\n<li>I don't think we should be donating everything to the Against Malaria Foundation instead of speculative causes.</li>\n</ul>\n<ul>\n<li>I don't think the Against Malaria Foundation has the highest impact of all current opportunities to donate.</li>\n</ul>\n<ul>\n<li>I do think we can say useful things about the far future.</li>\n</ul>\n<ul>\n<li>I don't think the correct way to think about high uncertainty and low evidence is to \"suspend judgement\". &nbsp;Rather, I think we should make a judgement that we expect the estimate to be much lower than initially claimed in light of all the things I've said earlier about <a href=\"http://www.everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">the history of past cost-effectiveness estimates</a>.</li>\n</ul>\n<div><br></div>\n<p>&nbsp;</p>\n<p>And, lastly, if I were to make a second important point it would be <strong>it's difficult to find good opportunities to buy information.</strong>&nbsp; It's easy to think that any donation to an organization will generate good information or that we'll automatically make progress just by working. &nbsp;I think some element of random pursuit is important (see below), but all things considered I think we're doing too much random pursuit right now.</p>\n<p>&nbsp;</p>\n<h2 id=\"Specific_Things_I_Changed_My_Mind_About\">Specific Things I Changed My Mind About</h2>\n<p>Here are the specific places I changed my mind on:</p>\n<p>&nbsp;</p>\n<p><strong id=\"I_used_to_think_donating_to_AMF__at_least_in_part__was_important_for_me___Now_I_don_t_\">I used to think donating to AMF, at least in part, was important for me. &nbsp;Now I don't.</strong></p>\n<p>I underestimated the power of exploring and the existing opportunities, so I think that 100% of my donations should be going to trying to assess impact. &nbsp;I've been persuaded that there is already quite a lot of money going toward AMF and <a href=\"/r/discussion/lw/ibp/practical_limits_to_giving_now_and_the_haste/\">we might not need more money as quickly as thought</a>, so for the time being it's probably more appropriate to save and then donate to opportunities to buy information as they come up.</p>\n<p>&nbsp;</p>\n<p><strong id=\"I_now_agree_that_there_are_relevant_economies_of_scale_in_pursuing_information_that_I_hadn_t_taken_into_account_\">I now agree that there are relevant economies of scale in pursuing information that I hadn't taken into account.</strong></p>\n<p>What I mean by this is it might not be appropriate for individuals to work on purchasing information themselves. &nbsp;Instead, this could end up splitting up the time of organizations unnecessarily as they provide information to a bunch of different people. &nbsp;Also, many people don't have the time to do this themselves.</p>\n<p>I think this has two implications:</p>\n<ul>\n<li>We should put more trust in larger scale organizations who are doing exploring, like <a href=\"http://www.givewell.org\">GiveWell</a>, and pool our resources.</li>\n</ul>\n<ul>\n<li>Individuals should work harder to put relevant information about information we gather online.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong id=\"I_was_partially_mistaken_in_thinking_about_how_to__prove__speculative_causes_\">I was partially mistaken in thinking about how to \"prove\" speculative causes.</strong></p>\n<p>I think there was some value in my essay <a href=\"/r/discussion/lw/i9u/what_would_it_take_to_prove_a_speculative_cause/\">\"What Would It Take to Prove a Speculative Cause?\"</a>&nbsp;because it talked concretely about strategies some organizations could take to get more information about their impact.</p>\n<p>But the overall concept is mistaken -- there is no arbitrary threshold of evidence at which a speculative cause needs to cross and I was wasting my time by trying to come up with one. &nbsp;Instead, I think it's appropriate to continue doing expected value calculations as long as we maintain a self-skeptical, pro-measurement mindset.</p>\n<p>&nbsp;</p>\n<p><strong id=\"I_had_previously_not_fully_taken_into_account_the_cost_of_acquiring_further_information_\">I had previously not fully taken into account the cost of acquiring further information.</strong></p>\n<p>The important question in value of information is not \"what does this information get me in terms of changing my beliefs and actions?\" but actually \"how valuable is this information?\", as in, do the benefits of gathering this information outweigh all the costs? &nbsp;In some cases, I think the benefits of further proving a cause probably don't outweigh the costs.</p>\n<p>For one possibly extreme example, while I don't know the rationale for doing a 23rd randomized controlled trial on anti-malaria bednets after performing the previous 22, it's likely that doing that RCT would have to be testing something more specific than the general effectiveness of bednets to justify the high cost of doing an RCT.</p>\n<p>Likewise, there are costs on organizations to devoting resources to measuring themselves and being more transparent. &nbsp;I don't think these costs are particularly high or defeat the idea of devoting more resources to this area, but I hadn't really taken them into account before.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"I_m_slightly_more_in_favor_of_acting_randomly__trial_and_error__\">I'm slightly more in favor of acting randomly (trial and error).</strong></p>\n<p>I still think it's difficult to acquire good value of information and it's very easy to get caught \"spinning our wheels\" in research, especially when that research has no clear feedback loops. &nbsp;One example, perhaps somewhat controversial, would be to point to&nbsp;<a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">the multi-century lack of progress on some problems in philosophy</a>&nbsp;(think meta-ethics) as an example of what can happen to a field when there aren't good feedback loops to ground yourself.</p>\n<p>However, I underestimated the amount of information that comes forward just doing ones normal activities. &nbsp;The implication here is that it's more worthwhile than I initially thought to fund speculative causes just to have them continue to scale and operate.</p>\n<p>-</p>\n<p><em>(This was also <a href=\"http://www.everydayutilitarian.com/essays/where-ive-changed-my-mind-on-my-approach-to-speculative-causes/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "sections": [{"title": "My Argument, As It Currently Stands", "anchor": "My_Argument__As_It_Currently_Stands", "level": 1}, {"title": "Specific Things I Changed My Mind About", "anchor": "Specific_Things_I_Changed_My_Mind_About", "level": 1}, {"title": "I used to think donating to AMF, at least in part, was important for me. \u00a0Now I don't.", "anchor": "I_used_to_think_donating_to_AMF__at_least_in_part__was_important_for_me___Now_I_don_t_", "level": 2}, {"title": "I now agree that there are relevant economies of scale in pursuing information that I hadn't taken into account.", "anchor": "I_now_agree_that_there_are_relevant_economies_of_scale_in_pursuing_information_that_I_hadn_t_taken_into_account_", "level": 2}, {"title": "I was partially mistaken in thinking about how to \"prove\" speculative causes.", "anchor": "I_was_partially_mistaken_in_thinking_about_how_to__prove__speculative_causes_", "level": 2}, {"title": "I had previously not fully taken into account the cost of acquiring further information.", "anchor": "I_had_previously_not_fully_taken_into_account_the_cost_of_acquiring_further_information_", "level": 2}, {"title": "I'm slightly more in favor of acting randomly (trial and error).", "anchor": "I_m_slightly_more_in_favor_of_acting_randomly__trial_and_error__", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XiN948y5QDgNbuTXP", "JvvGJxrfCgCiRLraq", "nAmeNGGuLcRW7tBjH", "FwiPfF8Woe5JrzqEu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-16T08:28:59.125Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: The Sunday Meetup", "slug": "meetup-moscow-the-sunday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5rzekAjzYaChkZMcg/meetup-moscow-the-sunday-meetup", "pageUrlRelative": "/posts/5rzekAjzYaChkZMcg/meetup-moscow-the-sunday-meetup", "linkUrl": "https://www.lesswrong.com/posts/5rzekAjzYaChkZMcg/meetup-moscow-the-sunday-meetup", "postedAtFormatted": "Friday, August 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20The%20Sunday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20The%20Sunday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rzekAjzYaChkZMcg%2Fmeetup-moscow-the-sunday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20The%20Sunday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rzekAjzYaChkZMcg%2Fmeetup-moscow-the-sunday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rzekAjzYaChkZMcg%2Fmeetup-moscow-the-sunday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/pz'>Moscow: The Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 August 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall or just look for group of geek-looking people.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Applied rationality exercises.</li>\n<li>Short presentation about self deception.</li>\n<li>Game session.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_reports+20130818_meet_up&amp;utm_content=20130818_meet_up&amp;utm_campaign=moscow_meetups\">here, in Russian</a>, now with more photos!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/pz'>Moscow: The Sunday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5rzekAjzYaChkZMcg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3044650591819388e-06, "legacy": true, "legacyId": "23761", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/pz\">Moscow: The Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 August 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall or just look for group of geek-looking people.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Applied rationality exercises.</li>\n<li>Short presentation about self deception.</li>\n<li>Game session.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>Reports from previous sessions can be found <a href=\"http://lesswrong.ru/forum/index.php/topic,71.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_reports+20130818_meet_up&amp;utm_content=20130818_meet_up&amp;utm_campaign=moscow_meetups\">here, in Russian</a>, now with more photos!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/pz\">Moscow: The Sunday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: The Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Moscow: The Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-16T08:31:57.117Z", "modifiedAt": null, "url": null, "title": "[LINK] Radio interview with Daniel Kahneman", "slug": "link-radio-interview-with-daniel-kahneman", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KKeEDAkmpKFBZphdT/link-radio-interview-with-daniel-kahneman", "pageUrlRelative": "/posts/KKeEDAkmpKFBZphdT/link-radio-interview-with-daniel-kahneman", "linkUrl": "https://www.lesswrong.com/posts/KKeEDAkmpKFBZphdT/link-radio-interview-with-daniel-kahneman", "postedAtFormatted": "Friday, August 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Radio%20interview%20with%20Daniel%20Kahneman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Radio%20interview%20with%20Daniel%20Kahneman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKeEDAkmpKFBZphdT%2Flink-radio-interview-with-daniel-kahneman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Radio%20interview%20with%20Daniel%20Kahneman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKeEDAkmpKFBZphdT%2Flink-radio-interview-with-daniel-kahneman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKeEDAkmpKFBZphdT%2Flink-radio-interview-with-daniel-kahneman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Daniel_Kahneman\">Daniel Kahneman</a> is being interviewed on <a href=\"http://www.bbc.co.uk/programmes/b0381l2v\">Desert Island Discs</a>&nbsp;on BBC Radio 4 right now (09:00-09:45 BST). The recording should be permanently available at that link from an hour after the programme ends.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KKeEDAkmpKFBZphdT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.3044675530969962e-06, "legacy": true, "legacyId": "23762", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-16T15:38:27.877Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Saskatoon", "slug": "new-lw-meetup-saskatoon", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JwsqG8n6jaL4yMHkf/new-lw-meetup-saskatoon", "pageUrlRelative": "/posts/JwsqG8n6jaL4yMHkf/new-lw-meetup-saskatoon", "linkUrl": "https://www.lesswrong.com/posts/JwsqG8n6jaL4yMHkf/new-lw-meetup-saskatoon", "postedAtFormatted": "Friday, August 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Saskatoon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Saskatoon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwsqG8n6jaL4yMHkf%2Fnew-lw-meetup-saskatoon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Saskatoon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwsqG8n6jaL4yMHkf%2Fnew-lw-meetup-saskatoon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwsqG8n6jaL4yMHkf%2Fnew-lw-meetup-saskatoon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 581, "htmlBody": "<p><strong>This summary was posted to LW main on August 9th. The following week's summary is <a href=\"/lw/ic4/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/pk\">Saskatoon's First Meetup!:&nbsp;<span class=\"date\">17 August 2013 01:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/pr\">Atlanta LessWrong: Games Night:&nbsp;<span class=\"date\">24 August 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/p7\">Brussels meetup:&nbsp;<span class=\"date\">17 August 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/pn\">LessWrong Israel September meetup:&nbsp;<span class=\"date\">12 September 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/pg\">[Lyon] Picnic au Parc de la T&ecirc;te d'Or:&nbsp;<span class=\"date\">17 August 2013 12:30PM</span></a></li>\n<li><a href=\"/meetups/pl\">LW Munich Meetup in August:&nbsp;<span class=\"date\">10 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pp\">Vancouver!:&nbsp;<span class=\"date\">11 August 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">10 August 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/po\">[Cambridge MA] Using Causal Graphs to Understand Bias in the Medical Literature:&nbsp;<span class=\"date\">11 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/pd\">Durham/RTLW HPMoR discussion, ch. 79-81:&nbsp;<span class=\"date\">10 August 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/pq\">London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight:&nbsp;<span class=\"date\">18 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ph\">Vienna Meetup:&nbsp;<span class=\"date\">17 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pj\">Washington DC books meetup:&nbsp;<span class=\"date\">11 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ps\">West LA Meetup&mdash;Confess Your Unpopular Opinion:&nbsp;<span class=\"date\">14 August 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JwsqG8n6jaL4yMHkf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3048262049778057e-06, "legacy": true, "legacyId": "23699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kEJtovPxjeX3QZRL4", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-16T21:05:28.195Z", "modifiedAt": null, "url": null, "title": "Humans are utility monsters", "slug": "humans-are-utility-monsters", "viewCount": null, "lastCommentedAt": "2021-02-04T19:50:15.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XfpJ6WQBDcEcC8Mu4/humans-are-utility-monsters", "pageUrlRelative": "/posts/XfpJ6WQBDcEcC8Mu4/humans-are-utility-monsters", "linkUrl": "https://www.lesswrong.com/posts/XfpJ6WQBDcEcC8Mu4/humans-are-utility-monsters", "postedAtFormatted": "Friday, August 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humans%20are%20utility%20monsters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumans%20are%20utility%20monsters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXfpJ6WQBDcEcC8Mu4%2Fhumans-are-utility-monsters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humans%20are%20utility%20monsters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXfpJ6WQBDcEcC8Mu4%2Fhumans-are-utility-monsters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXfpJ6WQBDcEcC8Mu4%2Fhumans-are-utility-monsters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 471, "htmlBody": "<p>When someone complains that utilitarianism<sup>1</sup> leads to the <a href=\"/lw/kn/torture_vs_dust_specks/\">dust speck paradox</a> or the trolley-car problem, I tell them that's a feature, not a bug. I'm not ready to say that respecting the <a href=\"http://en.wikipedia.org/wiki/Utility_monster\">utility monster</a> is also a feature of utilitarianism, but it <em>is</em> what most people everywhere have always done. A model that doesn't allow for utility monsters can't model human behavior, and certainly shouldn't provoke indignant responses from philosophers who keep right on respecting their own utility monsters.</p>\n<p><a id=\"more\"></a></p>\n<p>The utility monster is a creature that is somehow more capable of experiencing pleasure (or positive utility) than all others combined. Most people consider sacrificing everyone else's small utilities for the benefits of this monster to be repugnant.</p>\n<p>Let's suppose the utility monster is a utility monster because it has a more highly-developed brain capable of making finer discriminations, higher-level abstractions, and more associations than all the lesser minds around it. Does that make it less repugnant? (If so, I lose you here. I invite you to post a comment explaining why utility-monster-by-smartness is an exception.) Suppose we have one utility monster and one million others. Everything we do, we do for the one utility monster. Repugnant?</p>\n<p>Multiply by nine billion. We now have nine billion utility monsters and 9x10<sup>15</sup> others. Still repugnant?</p>\n<p>Yet these same enlightened, democratic societies whose philosophers decry the utility monster give approximately zero weight to the well-being of non-humans. We might try not to drive a species <em>extinct</em>, but when contemplating a new hydroelectric dam, nobody adds up the disutility to all the squirrels in the valley to be flooded.</p>\n<p>If you believe the utility monster is a problem with utilitarianism, how do you take into account the well-being of squirrels? How about ants? Worms? Bacteria? You've gone to 10<sup>15</sup> others&nbsp;just with ants.<sup>2</sup> Maybe 10<sup>20</sup> with nematodes.</p>\n<p>\"But humans are different!\" our anti-utilitarian complains. \"They're so much more intelligent and emotionally complex than nematodes that it would be repugnant to wipe out all humans to save any number of nematodes.\"</p>\n<p>Well, that's what a real utility monster looks like.</p>\n<p>The same people who believe this then turn around and say there's a problem with utilitarianism because (when unpacked into a plausible real-life example) it might kill all the nematodes to save one human. Given their beliefs, they should complain about the opposite \"problem\": For a sufficient number of nematodes, an instantiation of utilitarianism might say <em>not</em> to kill all the nematodes to save one human.</p>\n<p>&nbsp;</p>\n<p>1. I use the term in a very general way, meaning any action selection system that uses a utility function&mdash;which in practice means any rational, deterministic action selection system in which action preferences are well-ordered.</p>\n<p>2.&nbsp;<a href=\"http://www.utilitarian-essays.com/number-of-wild-animals.html\">This</a> recent attempt to estimate the number of different living beings of different kinds gives some numbers. The web has many pages claiming there are 10<sup>15</sup> ants, but I haven't found a citation of any original source.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XfpJ6WQBDcEcC8Mu4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 94, "baseScore": 104, "extendedScore": null, "score": 0.000264, "legacy": true, "legacyId": "23765", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 104, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 217, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 27, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-16T23:29:02.445Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, August 16-31", "slug": "group-rationality-diary-august-16-31-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:02.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TDZbs6Ah34PTd9Txw/group-rationality-diary-august-16-31-0", "pageUrlRelative": "/posts/TDZbs6Ah34PTd9Txw/group-rationality-diary-august-16-31-0", "linkUrl": "https://www.lesswrong.com/posts/TDZbs6Ah34PTd9Txw/group-rationality-diary-august-16-31-0", "postedAtFormatted": "Friday, August 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20August%2016-31&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20August%2016-31%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDZbs6Ah34PTd9Txw%2Fgroup-rationality-diary-august-16-31-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20August%2016-31%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDZbs6Ah34PTd9Txw%2Fgroup-rationality-diary-august-16-31-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDZbs6Ah34PTd9Txw%2Fgroup-rationality-diary-august-16-31-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<div id=\"entry_t3_i7u\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_i05\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hvy\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hqf\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>T<span style=\"color: #333333;\">his is the public group instrumental rationality diary for August 16-31. <br /></span></p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/r/discussion/lw/iho/group_rationality_diary_september_115/\">Next diary</a>:&nbsp; September 1-15</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/lw/i7u/group_rationality_diary_august_115/\">Immediate past diary</a>:&nbsp; August 1-15<a href=\"/lw/hvy/group_rationality_diary_july_115/\"> </a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality Diaries archive</a></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TDZbs6Ah34PTd9Txw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "23766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XPsft9gKw2tPQBuC2", "BqgA8tZYsDrBMQjbn", "QXYsonygGQRc6fjJy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-17T03:39:53.619Z", "modifiedAt": null, "url": null, "title": "Meetup : New Meetup: Urbana-Champaign, Illinois.", "slug": "meetup-new-meetup-urbana-champaign-illinois", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a6wfLA77SBT2uAYEt/meetup-new-meetup-urbana-champaign-illinois", "pageUrlRelative": "/posts/a6wfLA77SBT2uAYEt/meetup-new-meetup-urbana-champaign-illinois", "linkUrl": "https://www.lesswrong.com/posts/a6wfLA77SBT2uAYEt/meetup-new-meetup-urbana-champaign-illinois", "postedAtFormatted": "Saturday, August 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20New%20Meetup%3A%20Urbana-Champaign%2C%20Illinois.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20New%20Meetup%3A%20Urbana-Champaign%2C%20Illinois.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6wfLA77SBT2uAYEt%2Fmeetup-new-meetup-urbana-champaign-illinois%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20New%20Meetup%3A%20Urbana-Champaign%2C%20Illinois.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6wfLA77SBT2uAYEt%2Fmeetup-new-meetup-urbana-champaign-illinois", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa6wfLA77SBT2uAYEt%2Fmeetup-new-meetup-urbana-champaign-illinois", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q0'>New Meetup: Urbana-Champaign, Illinois.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 August 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Illini Union South Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup topic will be determined by popular consensus at the actual meetup. The expectations of active LessWrongers in the area about future meetup topics will probably be affected by the meetup topic at this first meetup. If you have strong preferences, be there on the 25th and make them known. I will bring Zendo and cards from Wits and Wagers (trivia with quantitative answers, good for Fermi estimates), meaning we can do calibration, or play prediction-based games.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-urbana-champaign\">Here is a mailing list for the meetup group I&#39;m attempting to create</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q0'>New Meetup: Urbana-Champaign, Illinois.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a6wfLA77SBT2uAYEt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3054332474321465e-06, "legacy": true, "legacyId": "23770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___New_Meetup__Urbana_Champaign__Illinois_\">Discussion article for the meetup : <a href=\"/meetups/q0\">New Meetup: Urbana-Champaign, Illinois.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 August 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Illini Union South Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup topic will be determined by popular consensus at the actual meetup. The expectations of active LessWrongers in the area about future meetup topics will probably be affected by the meetup topic at this first meetup. If you have strong preferences, be there on the 25th and make them known. I will bring Zendo and cards from Wits and Wagers (trivia with quantitative answers, good for Fermi estimates), meaning we can do calibration, or play prediction-based games.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-urbana-champaign\">Here is a mailing list for the meetup group I'm attempting to create</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___New_Meetup__Urbana_Champaign__Illinois_1\">Discussion article for the meetup : <a href=\"/meetups/q0\">New Meetup: Urbana-Champaign, Illinois.</a></h2>", "sections": [{"title": "Discussion article for the meetup : New Meetup: Urbana-Champaign, Illinois.", "anchor": "Discussion_article_for_the_meetup___New_Meetup__Urbana_Champaign__Illinois_", "level": 1}, {"title": "Discussion article for the meetup : New Meetup: Urbana-Champaign, Illinois.", "anchor": "Discussion_article_for_the_meetup___New_Meetup__Urbana_Champaign__Illinois_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-17T14:17:45.028Z", "modifiedAt": null, "url": null, "title": "LW Melbourne: Report on Public Rationality Lecture", "slug": "lw-melbourne-report-on-public-rationality-lecture", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:39.730Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CeTijoorgtNBeSrZG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gWgiGfaunx5g4KYWt/lw-melbourne-report-on-public-rationality-lecture", "pageUrlRelative": "/posts/gWgiGfaunx5g4KYWt/lw-melbourne-report-on-public-rationality-lecture", "linkUrl": "https://www.lesswrong.com/posts/gWgiGfaunx5g4KYWt/lw-melbourne-report-on-public-rationality-lecture", "postedAtFormatted": "Saturday, August 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Melbourne%3A%20Report%20on%20Public%20Rationality%20Lecture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Melbourne%3A%20Report%20on%20Public%20Rationality%20Lecture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWgiGfaunx5g4KYWt%2Flw-melbourne-report-on-public-rationality-lecture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Melbourne%3A%20Report%20on%20Public%20Rationality%20Lecture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWgiGfaunx5g4KYWt%2Flw-melbourne-report-on-public-rationality-lecture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWgiGfaunx5g4KYWt%2Flw-melbourne-report-on-public-rationality-lecture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1341, "htmlBody": "<p style=\"text-indent: 0px;\"><span style=\"text-indent: -1.1pt; font-size: small;\"><strong>Introduction</strong></span></p>\n<p style=\"text-indent: 0px;\"><span style=\"text-indent: -1.1pt; font-size: small;\"><strong></strong></span><span style=\"text-indent: -1.1pt; font-size: small;\">In the past couple of months, Melbourne LW has been working to expand our activities and community, as well generally promoting rationality. A huge shout out goes to <a href=\"/user/BraydenM/overview/\">BraydenM</a>&nbsp;who is responsible for spearheading these efforts. So far we have opened up some of our </span><span style=\"text-indent: -1.1pt;\">meetups to </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/131672592/\" target=\"_blank\">new-comers</a><span style=\"text-indent: -1.1pt; font-size: small;\">, held a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"/lw/hq1/meetup_melbourne_winter_solstice_dinner_party_and/\">winter solstice</a><span style=\"text-indent: -1.1pt;\"> dinner party</span><span style=\"text-indent: -1.1pt; font-size: small;\">, held a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"/lw/i1x/meetup_melbourne_excursion_comfort_zone_expansion/\">COZE event</a><span style=\"text-indent: -1.1pt; font-size: small;\">, created a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"http://www.meetup.com/Melbourne-Less-Wrong/\">Meetup.com group</a><span style=\"text-indent: -1.1pt; font-size: small;\"> for Melbourne LW, presented at other Meetup.com groups, distributed copies of HPMoR, conducted a rationality-vox pop, and presented a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"http://oi41.tinypic.com/8vt0l2.jpg\">public lecture</a><span style=\"text-indent: -1.1pt;\"> on LW content</span><span style=\"text-indent: -1.1pt; font-size: small;\">.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">The public lecture was my project, and here is my report.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong style=\"text-indent: -1.1pt;\">Initial Planning</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">In early July, we held a planning session for expanding the community and promoting rationality. Holding public rationality lectures was deemed a possibility worth trying.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Being students in our early twenties, we judged the most receptive market would be our peers</span><span style=\"text-indent: -1.1pt;\">. We considered who we should target: (1) High IQ, heavily academic students with technical backgrounds in maths/science/philosophy of the sort who might join LW, or (2) everyone else as well.&nbsp;Having visions of a world where everyone was more rational, I opted for the latter.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">I was ambitious at this point and wanted to run a series of six lectures progressing through core sequences content. Wiser heads suggested we run a single, stand-alone lecture to get data. Since I still hoped to run a whole series, the first lecture had to be given at the beginning of new semester (start of August here).</p>\n<p>&nbsp;</p>\n<p><span style=\"text-indent: -1.1pt;\"><strong>Content </strong></span></p>\n<h5><span style=\"text-indent: -1.1pt;\"><strong><a href=\"https://docs.google.com/file/d/0B-Qk4CQwYau0bVdjUGR2c1lJSDA/edit?usp=sharing\">(slides: open in PP to view presenter's notes)</a></strong></span></h5>\n<p><span style=\"text-indent: -1.1pt;\"><strong></strong></span><span style=\"text-indent: -1.1pt;\">To meet the deadline, I developed the lecture material unaided and without much planning or revision. Speed came at the price of quality.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">The lecture could have focused on either of a) cognitive biases or b) core sequences content such as </span><a style=\"text-indent: -1.1pt;\" href=\"http://yudkowsky.net/rational/the-simple-truth\">simple truth</a><span style=\"text-indent: -1.1pt;\">, </span><a style=\"text-indent: -1.1pt;\" href=\"http://slatestarcodex.com/2013/08/06/on-first-looking-into-chapmans-pop-bayesianism/\">probabilistic reasoning</a><span style=\"text-indent: -1.1pt;\">, </span><a style=\"text-indent: -1.1pt;\" href=\"/lw/is/fake_causality/\">beliefs</a><span style=\"text-indent: -1.1pt;\"> </span><a style=\"text-indent: -1.1pt;\" href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">paying rent</a><span style=\"text-indent: -1.1pt;\">, and evidence.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Reasons in favour of and against:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\"><em>Cognitive Biases</em><br /> + I have a prior that biases would be more appealing and get better attendance</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\">- requires more research to present on</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\">- teaching people about biases can be dangerous</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\"><em>Core LW Material</em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\">+ I know the content well, easy to prepare material on</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\">+ provides motivation for overcoming biases</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\">- more abstract, philosophical, and less interesting to most</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\">&nbsp;</p>\n<p><span style=\"text-indent: -1.1pt;\">Overwhelmingly, I decided to focus on core sequence material because of the ability to prepare lecture material quickly without reading/rereading material. It may have been better to relax the deadline and lose the time constraint, but it is also I likely that I would have not completed the project if had taken longer.</span></p>\n<p><span style=\"text-indent: -1.1pt;\">Delivering the lecture, I realised that by opting to have a lecture suitable for everyone, I was caught trying to please both groups (1) and (2) above. One demands more depth, complexity, and theoretical justification, and the other needs more basic content explained even slower. It's like trying to teach high school and primary school kids together. 'Obvious in hindsight' and all that.</span></p>\n<p><a href=\"/lw/i1k/making_rationality_generalinterest/9g1c\">Assuming we have an option</a>, targeting (1) would be better. They have the ability to understand and appreciate the material, and are more likely to be receptive. Needless to say, we haven't exhausted the pool of high IQ, mathy-sciency people.&nbsp;</p>\n<p>In this lecture I tabooed the word 'rationality'. To the average student at an Australian university it does not mean at all what it means to us (and I don't see why they'd be special). The rationality vox pop we conducted demonstrated this and I'll report soon what we learnt from it, namely, how typical students perceive 'rationality'.</p>\n<p>&nbsp;</p>\n<p><strong style=\"text-indent: -1.1pt;\">Venue</strong></p>\n<p><strong style=\"text-indent: -1.1pt;\"></strong><span style=\"text-indent: -1.1pt;\">Running an event for students on campus is easy for student clubs. With LW members on club committees, the lecture was hosted by the University of Melbourne Sceptics Society and the Rationalist Association of Monash at their respective campuses. Both clubs provided assistance in promoting and running the events.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong>Advertising</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong></strong><span style=\"text-indent: -1.1pt;\">We advertised the lecture through the university clubs&rsquo; facebook groups, meetup.com, flyers, and posters around campus. The cost of printing the posters was not trivial, but judging from feedback, they did not increase attendance.</span></p>\n<div><br /></div>\n<div><span style=\"text-indent: -1.1pt;\"><br /></span></div>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong>Attendance and Reception</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">I was surprised by the number of people who expressed enthusiasm for the lecture topic, I had assumed belief and evidence seemed like completely mundane and boring topics to most people. Disappointingly, that did not translate into attendance. About 15-20 people attended each of the lectures, including members of the hosting clubs's committees and personal friends.&nbsp;<span style=\"text-indent: -1.1pt;\">A few friends requested a recording of the lecture.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Feedback was positive. We issued </span><a style=\"text-indent: -1.1pt;\" href=\"http://is.gd/isitsofeedbackgraph\">feedback</a><span style=\"text-indent: -1.1pt;\"> </span><a style=\"text-indent: -1.1pt;\" href=\"http://is.gd/isitsofeedback\">forms</a><span style=\"text-indent: -1.1pt;\"> at the end of each lecture.&nbsp;</span><span style=\"text-indent: -1.1pt;\">On a 1-5 scale of unlikely to likely, the mean response for both attending a similar event and for recommending a similar event to a friend was 4. Comments offered criticisms, recommendations, and thanks.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Conversations with attendees afterwards showed that they had understood and taken on board the idea of probabilistic reasoning. I assign a greater than 50% chance that if nothing else, the explicit idea that belief comes on sliding scale of certainty and gets pushed up and down by evidence will stick with people.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong style=\"text-indent: -1.1pt;\">Lessons Learnt</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">While I never made an explicit estimate of how long this project would take, it exceeded my expectations. Planning fallacy strikes again. I also assumed I would get more assistance than I did. There were no replies to my request for assistance making a poster in either the LW Melbourne or LW global facebook group.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Crucially, I learnt that I should have finalised the content before commencing advertising. Once I'd committed to talking about certain topics I felt constrained to include them. Afterwards </span><a style=\"text-indent: -1.1pt;\" href=\"/user/RyanCarey/overview/\">the point was made</a><span style=\"text-indent: -1.1pt;\"> to me that it may have been better to violate the advertising than give a sub-optimal lecture, and in hindsight I'm inclined to agree.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">The personal experience gained in presenting a lengthy lecture to a public audience was valuable. Notable lessons were to spread content out over more slides rather than concentrating it, and to generate the exact wording on the spot, rather than rehearsing a script. I relied heavily on a script the first time I gave the lecture (and duly received criticism). For the second I used only the slides as a prompt, resulting in more pauses, but a far more natural presenation. Having tried both extremes, it seems that the classic 'prompts on palm cards' is ideal.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong style=\"text-indent: -1.1pt;\">Conclusion</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Reviewing the whole process of producing this rationality lecture, I feel rather irrational. I started with a<a href=\"http://effective-altruism.com/what-effective-altruism\"> 'what' and not a 'why'</a>. I started off with a plan I wanted to implement and an assumption that this plan was the best way to achieve my goal, rather than with the goal itself. The sequence of bad judgements and decisions followed from there:&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">1) conclude without any real reasoning that delivering a lecture series this semester on campus is the best way to promote rationality</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\"><span style=\"text-indent: -1.1pt;\">2) giving a series of lectures this semester requires starting soon, imposing a short deadline</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">3) to meet the deadline I have to work fast</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">4) to work fast, I have to stick to only delivering I content already know very well</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">5) to work fast there is no time to work with others, get feedback, and/or revise the material.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">My decisions were further biased by the fact that that having a reason to work quickly meant I had an excuse to not work with others, to do things all my way, and to avoid the unpleasant experience of receiving criticism and having to alter my work.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Tsuyoku naritai!</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">As much as I regret the lack of planning that went into this project, I am glad that I didn't fall into the opposite trap: trying to plan so exhaustively that I never got out there and did something. At some stage (later than I did), we need to satisfice. Despite being imperfect, this lecture had value and did good.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">Thankfully, I was directed to <a href=\"/lw/i1k/making_rationality_generalinterest/\">Swimmer963&rsquo;s post</a>. Her approach is the right one, and I plan to discuss the project of increasing the sanity waterline with all others who are keen before taking further action.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><a href=\"/lw/i1k/making_rationality_generalinterest/9g19\">RobbBB's</a> suggestion of starting a new website as a hub for HPMoR fans seems seriously worthy of consideration, and I'd be very willing to put time and effort into making it happen awesomely.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 1, "T57Qd9J3AfxmwhQtY": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gWgiGfaunx5g4KYWt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 21, "baseScore": 21, "extendedScore": null, "score": 1.3059703848548237e-06, "legacy": true, "legacyId": "23773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"text-indent: 0px;\"><span style=\"text-indent: -1.1pt; font-size: small;\"><strong>Introduction</strong></span></p>\n<p style=\"text-indent: 0px;\"><span style=\"text-indent: -1.1pt; font-size: small;\"><strong></strong></span><span style=\"text-indent: -1.1pt; font-size: small;\">In the past couple of months, Melbourne LW has been working to expand our activities and community, as well generally promoting rationality. A huge shout out goes to <a href=\"/user/BraydenM/overview/\">BraydenM</a>&nbsp;who is responsible for spearheading these efforts. So far we have opened up some of our </span><span style=\"text-indent: -1.1pt;\">meetups to </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/131672592/\" target=\"_blank\">new-comers</a><span style=\"text-indent: -1.1pt; font-size: small;\">, held a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"/lw/hq1/meetup_melbourne_winter_solstice_dinner_party_and/\">winter solstice</a><span style=\"text-indent: -1.1pt;\"> dinner party</span><span style=\"text-indent: -1.1pt; font-size: small;\">, held a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"/lw/i1x/meetup_melbourne_excursion_comfort_zone_expansion/\">COZE event</a><span style=\"text-indent: -1.1pt; font-size: small;\">, created a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"http://www.meetup.com/Melbourne-Less-Wrong/\">Meetup.com group</a><span style=\"text-indent: -1.1pt; font-size: small;\"> for Melbourne LW, presented at other Meetup.com groups, distributed copies of HPMoR, conducted a rationality-vox pop, and presented a </span><a style=\"text-indent: -1.1pt; font-size: small;\" href=\"http://oi41.tinypic.com/8vt0l2.jpg\">public lecture</a><span style=\"text-indent: -1.1pt;\"> on LW content</span><span style=\"text-indent: -1.1pt; font-size: small;\">.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">The public lecture was my project, and here is my report.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong style=\"text-indent: -1.1pt;\" id=\"Initial_Planning\">Initial Planning</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">In early July, we held a planning session for expanding the community and promoting rationality. Holding public rationality lectures was deemed a possibility worth trying.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Being students in our early twenties, we judged the most receptive market would be our peers</span><span style=\"text-indent: -1.1pt;\">. We considered who we should target: (1) High IQ, heavily academic students with technical backgrounds in maths/science/philosophy of the sort who might join LW, or (2) everyone else as well.&nbsp;Having visions of a world where everyone was more rational, I opted for the latter.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">I was ambitious at this point and wanted to run a series of six lectures progressing through core sequences content. Wiser heads suggested we run a single, stand-alone lecture to get data. Since I still hoped to run a whole series, the first lecture had to be given at the beginning of new semester (start of August here).</p>\n<p>&nbsp;</p>\n<p><span style=\"text-indent: -1.1pt;\"><strong>Content </strong></span></p>\n<h5><span style=\"text-indent: -1.1pt;\"><strong><a href=\"https://docs.google.com/file/d/0B-Qk4CQwYau0bVdjUGR2c1lJSDA/edit?usp=sharing\">(slides: open in PP to view presenter's notes)</a></strong></span></h5>\n<p><span style=\"text-indent: -1.1pt;\"><strong></strong></span><span style=\"text-indent: -1.1pt;\">To meet the deadline, I developed the lecture material unaided and without much planning or revision. Speed came at the price of quality.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">The lecture could have focused on either of a) cognitive biases or b) core sequences content such as </span><a style=\"text-indent: -1.1pt;\" href=\"http://yudkowsky.net/rational/the-simple-truth\">simple truth</a><span style=\"text-indent: -1.1pt;\">, </span><a style=\"text-indent: -1.1pt;\" href=\"http://slatestarcodex.com/2013/08/06/on-first-looking-into-chapmans-pop-bayesianism/\">probabilistic reasoning</a><span style=\"text-indent: -1.1pt;\">, </span><a style=\"text-indent: -1.1pt;\" href=\"/lw/is/fake_causality/\">beliefs</a><span style=\"text-indent: -1.1pt;\"> </span><a style=\"text-indent: -1.1pt;\" href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">paying rent</a><span style=\"text-indent: -1.1pt;\">, and evidence.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Reasons in favour of and against:</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\"><em>Cognitive Biases</em><br> + I have a prior that biases would be more appealing and get better attendance</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\">- requires more research to present on</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\">- teaching people about biases can be dangerous</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\"><em>Core LW Material</em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt; text-indent: -1.1pt;\">+ I know the content well, easy to prepare material on</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\">+ provides motivation for overcoming biases</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\">- more abstract, philosophical, and less interesting to most</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\">&nbsp;</p>\n<p><span style=\"text-indent: -1.1pt;\">Overwhelmingly, I decided to focus on core sequence material because of the ability to prepare lecture material quickly without reading/rereading material. It may have been better to relax the deadline and lose the time constraint, but it is also I likely that I would have not completed the project if had taken longer.</span></p>\n<p><span style=\"text-indent: -1.1pt;\">Delivering the lecture, I realised that by opting to have a lecture suitable for everyone, I was caught trying to please both groups (1) and (2) above. One demands more depth, complexity, and theoretical justification, and the other needs more basic content explained even slower. It's like trying to teach high school and primary school kids together. 'Obvious in hindsight' and all that.</span></p>\n<p><a href=\"/lw/i1k/making_rationality_generalinterest/9g1c\">Assuming we have an option</a>, targeting (1) would be better. They have the ability to understand and appreciate the material, and are more likely to be receptive. Needless to say, we haven't exhausted the pool of high IQ, mathy-sciency people.&nbsp;</p>\n<p>In this lecture I tabooed the word 'rationality'. To the average student at an Australian university it does not mean at all what it means to us (and I don't see why they'd be special). The rationality vox pop we conducted demonstrated this and I'll report soon what we learnt from it, namely, how typical students perceive 'rationality'.</p>\n<p>&nbsp;</p>\n<p><strong style=\"text-indent: -1.1pt;\" id=\"Venue\">Venue</strong></p>\n<p><strong style=\"text-indent: -1.1pt;\"></strong><span style=\"text-indent: -1.1pt;\">Running an event for students on campus is easy for student clubs. With LW members on club committees, the lecture was hosted by the University of Melbourne Sceptics Society and the Rationalist Association of Monash at their respective campuses. Both clubs provided assistance in promoting and running the events.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong id=\"Advertising\">Advertising</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong></strong><span style=\"text-indent: -1.1pt;\">We advertised the lecture through the university clubs\u2019 facebook groups, meetup.com, flyers, and posters around campus. The cost of printing the posters was not trivial, but judging from feedback, they did not increase attendance.</span></p>\n<div><br></div>\n<div><span style=\"text-indent: -1.1pt;\"><br></span></div>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong id=\"Attendance_and_Reception\">Attendance and Reception</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">I was surprised by the number of people who expressed enthusiasm for the lecture topic, I had assumed belief and evidence seemed like completely mundane and boring topics to most people. Disappointingly, that did not translate into attendance. About 15-20 people attended each of the lectures, including members of the hosting clubs's committees and personal friends.&nbsp;<span style=\"text-indent: -1.1pt;\">A few friends requested a recording of the lecture.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Feedback was positive. We issued </span><a style=\"text-indent: -1.1pt;\" href=\"http://is.gd/isitsofeedbackgraph\">feedback</a><span style=\"text-indent: -1.1pt;\"> </span><a style=\"text-indent: -1.1pt;\" href=\"http://is.gd/isitsofeedback\">forms</a><span style=\"text-indent: -1.1pt;\"> at the end of each lecture.&nbsp;</span><span style=\"text-indent: -1.1pt;\">On a 1-5 scale of unlikely to likely, the mean response for both attending a similar event and for recommending a similar event to a friend was 4. Comments offered criticisms, recommendations, and thanks.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Conversations with attendees afterwards showed that they had understood and taken on board the idea of probabilistic reasoning. I assign a greater than 50% chance that if nothing else, the explicit idea that belief comes on sliding scale of certainty and gets pushed up and down by evidence will stick with people.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong style=\"text-indent: -1.1pt;\" id=\"Lessons_Learnt\">Lessons Learnt</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">While I never made an explicit estimate of how long this project would take, it exceeded my expectations. Planning fallacy strikes again. I also assumed I would get more assistance than I did. There were no replies to my request for assistance making a poster in either the LW Melbourne or LW global facebook group.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Crucially, I learnt that I should have finalised the content before commencing advertising. Once I'd committed to talking about certain topics I felt constrained to include them. Afterwards </span><a style=\"text-indent: -1.1pt;\" href=\"/user/RyanCarey/overview/\">the point was made</a><span style=\"text-indent: -1.1pt;\"> to me that it may have been better to violate the advertising than give a sub-optimal lecture, and in hindsight I'm inclined to agree.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">The personal experience gained in presenting a lengthy lecture to a public audience was valuable. Notable lessons were to spread content out over more slides rather than concentrating it, and to generate the exact wording on the spot, rather than rehearsing a script. I relied heavily on a script the first time I gave the lecture (and duly received criticism). For the second I used only the slides as a prompt, resulting in more pauses, but a far more natural presenation. Having tried both extremes, it seems that the classic 'prompts on palm cards' is ideal.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><strong style=\"text-indent: -1.1pt;\" id=\"Conclusion\">Conclusion</strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Reviewing the whole process of producing this rationality lecture, I feel rather irrational. I started with a<a href=\"http://effective-altruism.com/what-effective-altruism\"> 'what' and not a 'why'</a>. I started off with a plan I wanted to implement and an assumption that this plan was the best way to achieve my goal, rather than with the goal itself. The sequence of bad judgements and decisions followed from there:&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">1) conclude without any real reasoning that delivering a lecture series this semester on campus is the best way to promote rationality</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\"><span style=\"text-indent: -1.1pt;\">2) giving a series of lectures this semester requires starting soon, imposing a short deadline</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">3) to meet the deadline I have to work fast</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">4) to work fast, I have to stick to only delivering I content already know very well</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; text-indent: -1.1pt; padding-left: 30px;\">5) to work fast there is no time to work with others, get feedback, and/or revise the material.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">My decisions were further biased by the fact that that having a reason to work quickly meant I had an excuse to not work with others, to do things all my way, and to avoid the unpleasant experience of receiving criticism and having to alter my work.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">Tsuyoku naritai!</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\">As much as I regret the lack of planning that went into this project, I am glad that I didn't fall into the opposite trap: trying to plan so exhaustively that I never got out there and did something. At some stage (later than I did), we need to satisfice. Despite being imperfect, this lecture had value and did good.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><span style=\"text-indent: -1.1pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">Thankfully, I was directed to <a href=\"/lw/i1k/making_rationality_generalinterest/\">Swimmer963\u2019s post</a>. Her approach is the right one, and I plan to discuss the project of increasing the sanity waterline with all others who are keen before taking further action.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt;text-indent: -1.1pt\"><a href=\"/lw/i1k/making_rationality_generalinterest/9g19\">RobbBB's</a> suggestion of starting a new website as a hub for HPMoR fans seems seriously worthy of consideration, and I'd be very willing to put time and effort into making it happen awesomely.</p>", "sections": [{"title": "Initial Planning", "anchor": "Initial_Planning", "level": 1}, {"title": "Venue", "anchor": "Venue", "level": 1}, {"title": "Advertising", "anchor": "Advertising", "level": 1}, {"title": "Attendance and Reception", "anchor": "Attendance_and_Reception", "level": 1}, {"title": "Lessons Learnt", "anchor": "Lessons_Learnt", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ySAN6rQm76T5ALCEz", "zYqLsd6gaoAnQn8Js", "RgkqLqkg8vLhsYpfh", "a7n8GdKiAZRX86T5A", "R38jwJcWKH6S7hpFJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-18T11:54:00.596Z", "modifiedAt": null, "url": null, "title": "My daily reflection routine", "slug": "my-daily-reflection-routine", "viewCount": null, "lastCommentedAt": "2018-09-15T05:18:56.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Beckstead", "createdAt": "2011-08-19T23:58:47.870Z", "isAdmin": false, "displayName": "Nick_Beckstead"}, "userId": "Sjm96fPXwa2x4eHHv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xJ57sF8eEoAPJHzaq/my-daily-reflection-routine", "pageUrlRelative": "/posts/xJ57sF8eEoAPJHzaq/my-daily-reflection-routine", "linkUrl": "https://www.lesswrong.com/posts/xJ57sF8eEoAPJHzaq/my-daily-reflection-routine", "postedAtFormatted": "Sunday, August 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20daily%20reflection%20routine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20daily%20reflection%20routine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxJ57sF8eEoAPJHzaq%2Fmy-daily-reflection-routine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20daily%20reflection%20routine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxJ57sF8eEoAPJHzaq%2Fmy-daily-reflection-routine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxJ57sF8eEoAPJHzaq%2Fmy-daily-reflection-routine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1296, "htmlBody": "<p class=\"MsoNormal\">In <a href=\"/r/lesswrong/lw/iao/common_sense_as_a_prior/\">Common sense as a prior</a>, I used the example of prayer as a practice that is probably adaptive but the people who adopt it may not know why it is adaptive. I wrote:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">Another striking example is bedtime prayer. In many Christian traditions I am aware of, it is common to pray before going to sleep. And in the tradition I was raised in, the main components of prayer were listing things you were grateful for, asking for forgiveness for all the mistakes you made that day and thinking about what you would do to avoid similar mistakes in the future, and asking God for things. Christians might say the point of this is that it is a duty to God, that repentance is a requirement for entry to heaven, or that asking God for things makes God more likely to intervene and create miracles. However, I think these activities are reasonable for different reasons:&nbsp;<a href=\"/lw/i0c/for_happiness_keep_a_gratitude_journal/\">gratitude journals</a>&nbsp;are great, reflecting on mistakes is a great way to learn and overcome weaknesses, and it is a good idea to get clear about what you really want out of life in the short-term and the long-term.</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\"><br /> &hellip;I think it would be better still to introduce a different routine that serves similar functions&mdash;this is something I have done in my own life&hellip;</p>\n<p class=\"MsoNormal\">Someone recently wrote to me asking about my routine. I wrote this person an answer, so I thought I might as well share it with others. I have a number of structured routines like this that I find helpful and have considered sharing more widely, so this post will also serve as a test for whether I should share these routines. (These routines include: planning the day and tracking your time, planning and evaluating a project, doing a literature search, keeping a record of personal principles, reading and evaluating a paper, weekly review, and a few others that are less developed.)</p>\n<p class=\"MsoNormal\">Below, I offer and explanation of my routine, a template for following it, and give examples of what it looks like when I have used it. I have been doing this for about 6 weeks and I spend 5-15 minutes doing this per day. I was raised in a very religious family, and did something pretty similar for about the first 18 years of my life. I think it is good, but I don&rsquo;t think the effect size/my tracking ability combo allows me to confidently distinguish between &ldquo;it&rsquo;s a placebo&rdquo; and &ldquo;it actually works&rdquo; on the basis of my personal experience. I do it because it intuitively makes sense to me, it fits with some practices that I think are likely to be adaptive, it seems good so far, it seems good from a common sense perspective, some impressive people I know do similar things, and I&rsquo;ve been told that psychological research on gratitude journals supports the idea. (Also, I don't mind benefits from \"mere\" placebos.)</p>\n<p class=\"MsoNormal\">One quick point of caution is that I would be careful about framing this as &ldquo;atheist prayers&rdquo; in your head. I framed it that way for a while and thought it would be a good idea to do it, but &ldquo;atheist prayers&rdquo; just sounds silly. On the other hand, &ldquo;daily reflection&rdquo; just sounds reasonable. I found framing it this way made me substantially more motivated to actually follow the process.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h1>A detailed explanation of my process</h1>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->1.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Getting started</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Download the document &ldquo;<a href=\"https://dl.dropboxusercontent.com/u/6287771/Daily%20reflection.docx\">Daily reflection</a>.&rdquo;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->2.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Step by step</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->At the end of the work day or before going to sleep, open up &ldquo;Daily reflection.&rdquo;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Copy and paste the template for today&rsquo;s entry.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Fill in today&rsquo;s date, e.g. 18 August 2013.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->d.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Under &ldquo;What went well today/what am I grateful for?&rdquo;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l2 level3 lfo3;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List things you feel good about doing recently or things you enjoyed today.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l2 level3 lfo3;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List general things you have noticed lately and appreciate, even if they are not recent.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l2 level3 lfo3;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->(This is supposed to help you notice good things in life and seek out more of the good things.)</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->e.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Under &ldquo;Where would I like to improve? What principles could I follow in the future in order to improve?&rdquo;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List any mistakes you think you made today.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Try to think about principles you could follow to avoid similar mistakes in the future.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->If any of the principles seem useful or generally applicable, save them in another document, titled e.g. &ldquo;My Principles.&rdquo; I review my principles roughly monthly, and get reminders when I add new ones. I took this idea from <a href=\"http://www.bwater.com/Uploads/FileManager/Principles/Bridgewater-Associates-Ray-Dalio-Principles.pdf\">Ray Dalio</a>.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->(This is supposed to help you learn from mistakes and identify, manage, and/or overcome personal weaknesses.)</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l1 level2 lfo2;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Under &ldquo;What do I hope for in the future?&rdquo;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List upcoming challenges and opportunities that you hope go well.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List long-term priorities that you hope go well, especially ones you haven&rsquo;t thought about lately or have been neglecting.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->(This is supposed to help you keep track of what you really want out of life in the short-term and the long-term.)</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->3.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Other notes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->If it is inconvenient to make notes in this document and I&rsquo;m not going to open up the document, I will sometimes do my daily reflection in my head.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I generally list 2-5 things under each category per day.</p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I find this complements well with tracking your time. If you do track your time, you can look at how you spent your day and think about what was productive and what was unproductive. This helps with identifying items for the first and second steps.</p>\n<h1>Daily entries (template)</h1>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->1.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Date:</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->2.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Date:</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->3.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Date:</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<h1>Examples of daily entries (stripped of anything personal or embarrassing)</h1>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->1.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->9 August 2013</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Got my post drafted</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Great talk with [friend]</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Enjoyed dinner</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Talking to [other friend] was useful</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Spending too much on dinners</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Spending too much time criticizing people with dumb views</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Evaluate my projects well after they are done</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Use my framework for evaluating topics to work on</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->2.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->13 August 2013</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Really enjoyed the weekly review</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Glad I e-mailed a number of people to engage them on their perspectives</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Glad I came up with the idea that 80K say what it is going to change and be held accountable for making the changes</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I spent too much time checking the LW blog in response to my stuff</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I&rsquo;m not sure how useful it was for me to be involved with this prioritization institute stuff</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I didn&rsquo;t do a good job filling out my time tracker</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Maybe [friend] is right that I didn&rsquo;t do a good job as I think defending my common sense prior post</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Get my GCRI talk outline done</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Get to the point where I can do 10 pull-ups</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Review my common sense prior project</p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Share my productivity procedures with others</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xJ57sF8eEoAPJHzaq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "23775", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\">In <a href=\"/r/lesswrong/lw/iao/common_sense_as_a_prior/\">Common sense as a prior</a>, I used the example of prayer as a practice that is probably adaptive but the people who adopt it may not know why it is adaptive. I wrote:</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">Another striking example is bedtime prayer. In many Christian traditions I am aware of, it is common to pray before going to sleep. And in the tradition I was raised in, the main components of prayer were listing things you were grateful for, asking for forgiveness for all the mistakes you made that day and thinking about what you would do to avoid similar mistakes in the future, and asking God for things. Christians might say the point of this is that it is a duty to God, that repentance is a requirement for entry to heaven, or that asking God for things makes God more likely to intervene and create miracles. However, I think these activities are reasonable for different reasons:&nbsp;<a href=\"/lw/i0c/for_happiness_keep_a_gratitude_journal/\">gratitude journals</a>&nbsp;are great, reflecting on mistakes is a great way to learn and overcome weaknesses, and it is a good idea to get clear about what you really want out of life in the short-term and the long-term.</p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\"><br> \u2026I think it would be better still to introduce a different routine that serves similar functions\u2014this is something I have done in my own life\u2026</p>\n<p class=\"MsoNormal\">Someone recently wrote to me asking about my routine. I wrote this person an answer, so I thought I might as well share it with others. I have a number of structured routines like this that I find helpful and have considered sharing more widely, so this post will also serve as a test for whether I should share these routines. (These routines include: planning the day and tracking your time, planning and evaluating a project, doing a literature search, keeping a record of personal principles, reading and evaluating a paper, weekly review, and a few others that are less developed.)</p>\n<p class=\"MsoNormal\">Below, I offer and explanation of my routine, a template for following it, and give examples of what it looks like when I have used it. I have been doing this for about 6 weeks and I spend 5-15 minutes doing this per day. I was raised in a very religious family, and did something pretty similar for about the first 18 years of my life. I think it is good, but I don\u2019t think the effect size/my tracking ability combo allows me to confidently distinguish between \u201cit\u2019s a placebo\u201d and \u201cit actually works\u201d on the basis of my personal experience. I do it because it intuitively makes sense to me, it fits with some practices that I think are likely to be adaptive, it seems good so far, it seems good from a common sense perspective, some impressive people I know do similar things, and I\u2019ve been told that psychological research on gratitude journals supports the idea. (Also, I don't mind benefits from \"mere\" placebos.)</p>\n<p class=\"MsoNormal\">One quick point of caution is that I would be careful about framing this as \u201catheist prayers\u201d in your head. I framed it that way for a while and thought it would be a good idea to do it, but \u201catheist prayers\u201d just sounds silly. On the other hand, \u201cdaily reflection\u201d just sounds reasonable. I found framing it this way made me substantially more motivated to actually follow the process.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h1 id=\"A_detailed_explanation_of_my_process\">A detailed explanation of my process</h1>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->1.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Getting started</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Download the document \u201c<a href=\"https://dl.dropboxusercontent.com/u/6287771/Daily%20reflection.docx\">Daily reflection</a>.\u201d</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->2.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Step by step</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->At the end of the work day or before going to sleep, open up \u201cDaily reflection.\u201d</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Copy and paste the template for today\u2019s entry.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Fill in today\u2019s date, e.g. 18 August 2013.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->d.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Under \u201cWhat went well today/what am I grateful for?\u201d</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l2 level3 lfo3;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List things you feel good about doing recently or things you enjoyed today.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l2 level3 lfo3;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List general things you have noticed lately and appreciate, even if they are not recent.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l2 level3 lfo3;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->(This is supposed to help you notice good things in life and seek out more of the good things.)</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->e.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Under \u201cWhere would I like to improve? What principles could I follow in the future in order to improve?\u201d</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List any mistakes you think you made today.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Try to think about principles you could follow to avoid similar mistakes in the future.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->If any of the principles seem useful or generally applicable, save them in another document, titled e.g. \u201cMy Principles.\u201d I review my principles roughly monthly, and get reminders when I add new ones. I took this idea from <a href=\"http://www.bwater.com/Uploads/FileManager/Principles/Bridgewater-Associates-Ray-Dalio-Principles.pdf\">Ray Dalio</a>.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->(This is supposed to help you learn from mistakes and identify, manage, and/or overcome personal weaknesses.)</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l1 level2 lfo2;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Under \u201cWhat do I hope for in the future?\u201d</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List upcoming challenges and opportunities that you hope go well.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->List long-term priorities that you hope go well, especially ones you haven\u2019t thought about lately or have been neglecting.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l1 level3 lfo2;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->(This is supposed to help you keep track of what you really want out of life in the short-term and the long-term.)</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->3.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Other notes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->If it is inconvenient to make notes in this document and I\u2019m not going to open up the document, I will sometimes do my daily reflection in my head.</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I generally list 2-5 things under each category per day.</p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l2 level2 lfo3;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I find this complements well with tracking your time. If you do track your time, you can look at how you spent your day and think about what was productive and what was unproductive. This helps with identifying items for the first and second steps.</p>\n<h1 id=\"Daily_entries__template_\">Daily entries (template)</h1>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->1.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Date:</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->2.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Date:</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->3.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Date:</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l0 level2 lfo4;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<h1 id=\"Examples_of_daily_entries__stripped_of_anything_personal_or_embarrassing_\">Examples of daily entries (stripped of anything personal or embarrassing)</h1>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->1.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->9 August 2013</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Got my post drafted</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Great talk with [friend]</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Enjoyed dinner</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Talking to [other friend] was useful</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Spending too much on dinners</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Spending too much time criticizing people with dumb views</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Evaluate my projects well after they are done</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Use my framework for evaluating topics to work on</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; padding-left: 30px;\"><!--[if !supportLists]-->2.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->13 August 2013</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->a.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What went well today/what am I grateful for?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Really enjoyed the weekly review</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Glad I e-mailed a number of people to engage them on their perspectives</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Glad I came up with the idea that 80K say what it is going to change and be held accountable for making the changes</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->b.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Where would I like to improve? What principles could I follow in the future in order to improve?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I spent too much time checking the LW blog in response to my stuff</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I\u2019m not sure how useful it was for me to be involved with this prioritization institute stuff</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->I didn\u2019t do a good job filling out my time tracker</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Maybe [friend] is right that I didn\u2019t do a good job as I think defending my common sense prior post</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.0in; mso-add-space: auto; text-indent: -.25in; mso-list: l3 level2 lfo1;\"><!--[if !supportLists]-->c.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->What do I hope for in the future?</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>i.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Get my GCRI talk outline done</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>ii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Get to the point where I can do 10 pull-ups</p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iii.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Review my common sense prior project</p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"margin-left: 1.5in; mso-add-space: auto; text-indent: -1.5in; mso-text-indent-alt: -9.0pt; mso-list: l3 level3 lfo1;\"><!--[if !supportLists]--><span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>iv.<span style=\"font-size: 7pt; font-family: 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><!--[endif]-->Share my productivity procedures with others</p>", "sections": [{"title": "A detailed explanation of my process", "anchor": "A_detailed_explanation_of_my_process", "level": 1}, {"title": "Daily entries (template)", "anchor": "Daily_entries__template_", "level": 1}, {"title": "Examples of daily entries (stripped of anything personal or embarrassing)", "anchor": "Examples_of_daily_entries__stripped_of_anything_personal_or_embarrassing_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wgdfxQJ2DQuju73zC", "xYnnRmMmGRAwZoY25"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-18T22:40:34.868Z", "modifiedAt": null, "url": null, "title": "Effective Altruist Job Board?", "slug": "effective-altruist-job-board", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tuLv5zLmB9iz4FKdG/effective-altruist-job-board", "pageUrlRelative": "/posts/tuLv5zLmB9iz4FKdG/effective-altruist-job-board", "linkUrl": "https://www.lesswrong.com/posts/tuLv5zLmB9iz4FKdG/effective-altruist-job-board", "postedAtFormatted": "Sunday, August 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Effective%20Altruist%20Job%20Board%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEffective%20Altruist%20Job%20Board%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtuLv5zLmB9iz4FKdG%2Feffective-altruist-job-board%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Effective%20Altruist%20Job%20Board%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtuLv5zLmB9iz4FKdG%2Feffective-altruist-job-board", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtuLv5zLmB9iz4FKdG%2Feffective-altruist-job-board", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>An idea I had while talking to Xio Kikauka and Joey Savoie is to run a jobs board for people interested in <a href=\"http://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a>. &nbsp;It seems like it actually would be relatively easy to have <a href=\"http://en.wikipedia.org/wiki/CURL\">a script</a>&nbsp;automatically monitor various job pages and synthesize them all into one area that looks nice.</p>\n<p>Would this be useful to create? And if so, where should we get jobs from?</p>\n<p>Some potential ideas:</p>\n<p>&nbsp;</p>\n<ul>\n<li>The Humane League (<a href=\"http://www.thehumaneleague.com/jobs.htm\">Jobs</a>, <a href=\"http://www.thehumaneleague.com/internships.htm\">Internships</a>)</li>\n<li><a href=\"http://www.effectiveanimalactivism.org/about/get-involved\">Effective Animal Activism</a></li>\n<li><a href=\"http://givingwhatwecan.org/getting-involved/work-with-us\">Giving What We Can</a></li>\n<li><a href=\"http://www.80000hours.org/recruitment\">80,000 Hours</a></li>\n<li><a href=\"http://www.givewell.org/about/jobs\">GiveWell</a></li>\n<li>Innovations for Poverty Action (<a href=\"https://www.poverty-action.org/getinvolved/jobs\">Jobs</a>, <a href=\"https://www.poverty-action.org/getinvolved/internships\">Internships</a>)</li>\n<li><a href=\"http://careers.gatesfoundation.org/search/?q=&amp;location\">Bill and Melinda Gates Foundation</a></li>\n</ul>\n<div><strong>[Added 19 Aug --</strong></div>\n<div>\n<ul>\n<li>Oxfam (<a href=\"https://www.oxfam.org/en/jobs/secretariat\">International Secretariat Jobs</a>, <a href=\"http://www.oxfam.org/en/jobs/affiliates\">Affiliate Jobs</a>)</li>\n<li><a href=\"http://www.nyayahealth.org/join-us/\">Nyaya Health</a></li>\n<li>The Hunger Project (<a href=\"http://www.thp.org/get_involved/work_with_us/employment_opportunities\">Jobs</a>, <a href=\"http://www.thp.org/get_involved/work_with_us/internship_opportunities\">Internships</a>)</li>\n<li><a href=\"https://2xrecruit.kenexa.com/kr/cc/jsp/public/landingPage.jsf\">Population Services International</a></li>\n<li><a href=\"http://www.goodventures.org/about-us/jobs\">Good Ventures</a></li>\n<li>Farm Sanctuary (<a href=\"http://www.farmsanctuary.org/get-involved/jobs-intern-volunteer/jobs/current-job-listings/\">Jobs</a>, <a href=\"http://www.farmsanctuary.org/get-involved/jobs-intern-volunteer/internships/\">Internships</a>)</li>\n</ul>\n</div>\n<div><strong>]</strong></div>\n<p>&nbsp;</p>\n<p>I'd be willing to make this happen if people were interested.</p>\n<p>-</p>\n<p><em>(Also <a href=\"http://www.everydayutilitarian.com/essays/effective-altruist-job-board/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tuLv5zLmB9iz4FKdG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "23776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-19T05:47:10.920Z", "modifiedAt": null, "url": null, "title": "One way to manipulate your level of abstraction related to a task", "slug": "one-way-to-manipulate-your-level-of-abstraction-related-to-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.036Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andy_McKenzie", "createdAt": "2009-02-28T21:46:45.283Z", "isAdmin": false, "displayName": "Andy_McKenzie"}, "userId": "7PFnr3J3uCGSfjnZJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AgPwLmqChrWCYcKus/one-way-to-manipulate-your-level-of-abstraction-related-to-a", "pageUrlRelative": "/posts/AgPwLmqChrWCYcKus/one-way-to-manipulate-your-level-of-abstraction-related-to-a", "linkUrl": "https://www.lesswrong.com/posts/AgPwLmqChrWCYcKus/one-way-to-manipulate-your-level-of-abstraction-related-to-a", "postedAtFormatted": "Monday, August 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20way%20to%20manipulate%20your%20level%20of%20abstraction%20related%20to%20a%20task&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20way%20to%20manipulate%20your%20level%20of%20abstraction%20related%20to%20a%20task%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAgPwLmqChrWCYcKus%2Fone-way-to-manipulate-your-level-of-abstraction-related-to-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20way%20to%20manipulate%20your%20level%20of%20abstraction%20related%20to%20a%20task%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAgPwLmqChrWCYcKus%2Fone-way-to-manipulate-your-level-of-abstraction-related-to-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAgPwLmqChrWCYcKus%2Fone-way-to-manipulate-your-level-of-abstraction-related-to-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 390, "htmlBody": "<p>In construal level theory, ideas can be classified along a spectrum from concrete (<a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">\"near\" in Robin Hanson's terminology</a>) to abstract (\"far\"). As a summary, here is the abstract from a 2010 review (<a href=\"http://www.psych.nyu.edu/trope/Trope_Liberman_2010.pdf\">pdf</a>):&nbsp;</p>\n<blockquote>\n<p>People are capable of thinking about the future, the past, remote locations, another person&rsquo;s perspective, and counterfactual alternatives. Without denying the uniqueness of each process, it is proposed that they constitute different forms of traversing psychological distance. Psychological distance is egocentric: Its reference point is the self in the here and now, and the different ways in which an object might be removed from that point&mdash;in time, in space, in social distance, and in hypotheticality&mdash; constitute different distance dimensions. Transcending the self in the here and now entails mental construal, and the farther removed an object is from direct experience, the higher (more abstract) the level of construal of that object. Supporting this analysis, research shows (a) that the various distances are cognitively related to each other, (b) that they similarly influence and are influenced by level of mental construal, and (c) that they similarly affect prediction, preference, and action.</p>\n</blockquote>\n<p>Now, what if you want to think about some thing in a more or less near or far way? Here's one well-studied strategy to do so (e.g., see pdf <a href=\"http://psych.nyu.edu/tropelab/publications/LibermanTropeMcCraeSherman2007.pdf\">here</a>).</p>\n<p>To think about a task in more concrete terms, ask yourself <em>how</em> you would do it. Then, however you answer that question, ask yourself how would you do <em>that</em>. Do this two (or so) more times, and you will be thinking about that task significantly more concretely.&nbsp;</p>\n<p>To think about a task in more abstract terms, ask yourself <em>why</em> you would do it. Then ask yourself why you would want that 3 (or so) more times.&nbsp;</p>\n<p>An excerpt from the 2007 study in the second link to give an example of how this would work:&nbsp;</p>\n<blockquote>\n<p>Suppose you indicate &ldquo;taking a vacation&rdquo; as one of your goals. Please write the goal in the uppermost square. Then, think why you would like to go on vacation, and write your answer in the square underneath. Suppose that you write &ldquo;in order to rest.&rdquo; Now, please think why you would like to rest, and write your answer in the third square. Suppose that you write &ldquo;in order to renew your energy.&rdquo; Finally, write in the last square why you would like to renew your energy.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5GYzBE6q89w74dqfk": 1, "DWWZwkxTJs4d5WrcX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AgPwLmqChrWCYcKus", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 36, "extendedScore": null, "score": 1.307969104979464e-06, "legacy": true, "legacyId": "23778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-19T06:26:26.697Z", "modifiedAt": null, "url": null, "title": "Engaging First Introductions to AI Risk", "slug": "engaging-first-introductions-to-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:01.578Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BdXeZC8hFvLMcN495/engaging-first-introductions-to-ai-risk", "pageUrlRelative": "/posts/BdXeZC8hFvLMcN495/engaging-first-introductions-to-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/BdXeZC8hFvLMcN495/engaging-first-introductions-to-ai-risk", "postedAtFormatted": "Monday, August 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Engaging%20First%20Introductions%20to%20AI%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEngaging%20First%20Introductions%20to%20AI%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdXeZC8hFvLMcN495%2Fengaging-first-introductions-to-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Engaging%20First%20Introductions%20to%20AI%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdXeZC8hFvLMcN495%2Fengaging-first-introductions-to-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBdXeZC8hFvLMcN495%2Fengaging-first-introductions-to-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 850, "htmlBody": "<p>I'm putting together a list of short and sweet <strong>introductions to the dangers of artificial superintelligence</strong>.</p>\n<p>My target audience is intelligent, broadly philosophical <a href=\"/lw/hzt/writing_style_and_the_typical_mind_fallacy/\">narrative</a> thinkers, who can evaluate arguments well but who don't know a lot of the relevant background or jargon.</p>\n<p>My method is to construct a <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequence</a> mix tape &mdash;&nbsp;a collection of short and enlightening texts, meant to be read in a specified order. I've chosen them for their persuasive and pedagogical punchiness, and for their flow in the list. I'll also (separately) list somewhat longer or less essential follow-up texts below that are still meant to be accessible to astute visitors and laypeople.</p>\n<p>The first half focuses on <em><strong>intelligence</strong></em>, answering 'What is Artificial General Intelligence (AGI)?'. The second half focuses on <em style=\"font-weight: bold;\">friendliness</em>, answering 'How can we make AGI safe, and why does it matter?'. Since the topics of some posts aren't obvious from their titles, I've summarized them using questions they address.</p>\n<p>&nbsp;</p>\n<hr />\n<p><strong>Part I. Building intelligence.</strong></p>\n<p style=\"padding-left: 30px;\">1. <a href=\"http://yudkowsky.net/singularity/power\">Power of Intelligence</a>. Why is intelligence important?</p>\n<p style=\"padding-left: 30px;\">2. <a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a>. Is building an intelligence from scratch like talking to a person?</p>\n<p style=\"padding-left: 30px;\">3. <a href=\"/lw/l9/artificial_addition/\">Artificial Addition</a>. What can we conclude about the nature of intelligence from the fact that we don't yet understand it?</p>\n<p style=\"padding-left: 30px;\">4. <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a>. How do human goals relate to the 'goals' of evolution?</p>\n<p style=\"padding-left: 30px;\">5. <a href=\"/lw/6ha/the_blueminimizing_robot/\">The Blue-Minimizing Robot</a>.&nbsp;What are the shortcomings of thinking of things as 'agents', 'intelligences', or 'optimizers' with defined values/goals/preferences?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong>Part II. Intelligence explosion.</strong></p>\n<p style=\"padding-left: 30px;\">6. <a href=\"/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a>. What is optimization? As optimization processes, how do evolution, humans, and self-modifying AGI differ?</p>\n<p style=\"padding-left: 30px;\">7. <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Efficient Cross-Domain Optimization</a>. What is intelligence?</p>\n<p style=\"padding-left: 30px;\">8. <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a>. What else is universally true of intelligences?</p>\n<p style=\"padding-left: 30px;\">9. <a href=\"http://intelligenceexplosion.com/2011/plenty-of-room-above-us/\">Plenty of Room Above Us</a>. Why should we expect self-improving AGI to quickly become superintelligent?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong>Part III. AI risk.</strong></p>\n<p style=\"padding-left: 30px;\">10. <a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a>. What kind of jerk would Defect even knowing the other side Cooperated?</p>\n<p style=\"padding-left: 30px;\">11. <a href=\"http://wiki.lesswrong.com/wiki/Basic_AI_drives\">Basic AI drives</a>. Why are AGIs dangerous even when they're indifferent to us?</p>\n<p style=\"padding-left: 30px;\">12. <a href=\"/lw/st/anthropomorphic_optimism/\">Anthropomorphic Optimism</a>. Why do we think things we hope happen are likelier?</p>\n<p style=\"padding-left: 30px;\">13. <a href=\"/lw/ld/the_hidden_complexity_of_wishes\">The Hidden Complexity of Wishes</a>. How hard is it to directly program an alien intelligence to enact my values?</p>\n<p style=\"padding-left: 30px;\">14. <a href=\"/lw/td/magical_categories/\">Magical Categories</a>. How hard is it to program an alien intelligence to reconstruct my values from observed patterns?</p>\n<p style=\"padding-left: 30px;\">15. <a href=\"http://intelligenceexplosion.com/2012/ai-the-problem-with-solutions/\">The AI Problem, with Solutions</a>. How hard is it to give AGI predictable values of any sort? More generally, why does AGI risk matter so much?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong>Part IV. Ends.</strong></p>\n<p style=\"padding-left: 30px;\">16. <a href=\"/lw/sb/could_anything_be_right/\">Could Anything Be Right?</a> What do we mean by 'good', or 'valuable', or 'moral'?</p>\n<p style=\"padding-left: 30px;\">17. <a href=\"/lw/sw/morality_as_fixed_computation/\">Morality as Fixed Computation</a>. Is it enough to have an AGI improve the fit between my preferences and the world?</p>\n<p style=\"padding-left: 30px;\">18. <a href=\"/lw/xi/serious_stories/\">Serious Stories</a>. What would a true utopia be like?</p>\n<p style=\"padding-left: 30px;\">19. <a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a>. If we just sit back and let the universe do its thing, will it still produce value? If we don't take charge of our future, won't it still turn out interesting and beautiful on some deeper level?</p>\n<p style=\"padding-left: 30px;\">20. <a href=\"http://wiki.lesswrong.com/wiki/User:RobbBB/Tomorrow\">The Gift We Give To Tomorrow</a>. In explaining value, are we explaining it away? Are we making our goals less important?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong>Summary</strong>:&nbsp;<a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five theses, two lemmas, and a couple of strategic implications</a>.</p>\n<hr />\n<p>&nbsp;</p>\n<p>All of the above were written by Eliezer Yudkowsky, with the exception of The Blue-Minimizing Robot (by Yvain), Plenty of Room Above Us and The AI Problem (by Luke Muehlhauser), and Basic AI Drives (a wiki collaboration). Seeking a powerful conclusion, I ended up making a compromise between Eliezer's original <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">The Gift We Give To Tomorrow</a> and Raymond Arnold's <a href=\"https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf\">Solstice Ritual Book</a> version. It's on the wiki, so you can further improve it with edits.</p>\n<p>&nbsp;</p>\n<p><strong>Further reading</strong>:</p>\n<ul>\n<li><a href=\"https://dl.dropboxusercontent.com/u/12787472/philosophy/three-worlds-collide-short.pdf\">Three Worlds Collide</a> (Normal), by Eliezer Yudkowsky \n<ul>\n<li>a short story vividly illustrating how alien values can evolve.</li>\n</ul>\n</li>\n<li><a href=\"/lw/91c/so_you_want_to_save_the_world/\">So You Want to Save the World</a>, by Luke Muehlhauser \n<ul>\n<li>an introduction to the open problems in Friendly Artificial Intelligence.</li>\n</ul>\n</li>\n<li><a href=\"http://intelligence.org/ie-faq/\">Intelligence Explosion FAQ</a>, by Luke Muehlhauser \n<ul>\n<li>a broad overview of likely misconceptions about AI risk.</li>\n</ul>\n</li>\n<li><a href=\"http://consc.net/papers/singularity.pdf\">The Singularity: A Philosophical Analysis</a>, by David Chalmers \n<ul>\n<li>a detailed but non-technical argument for expecting intelligence explosion, with an assessment of the moral significance of synthetic human and non-human intelligence.</li>\n</ul>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>I'm posting this to get more feedback for improving it, to isolate topics for which we <em>don't</em> yet have high-quality, non-technical stand-alone introductions, and to reintroduce LessWrongers to exceptionally useful posts I haven't seen sufficiently discussed, linked, or upvoted. I'd especially like feedback on how the list I provided flows as a unit, and what inferential gaps it fails to address. My goals are:</p>\n<p style=\"padding-left: 30px;\"><strong>A.</strong>&nbsp;Via lucid and anti-anthropomorphic vignettes, to explain AGI in a way that encourages clear thought.</p>\n<p style=\"padding-left: 30px;\"><strong>B.</strong>&nbsp;Via the Five Theses, to demonstrate the importance of Friendly AI research.</p>\n<p style=\"padding-left: 30px;\"><strong>C.</strong> Via down-to-earth meta-ethics, humanistic poetry, and pragmatic strategizing, to combat any nihilisms, relativisms, and defeatisms that might be triggered by recognizing the possibility (or probability) of Unfriendly AI.</p>\n<p style=\"padding-left: 30px;\"><strong>D.</strong> Via an accessible, substantive, entertaining presentation, to introduce the&nbsp;raison d'&ecirc;tre of LessWrong to sophisticated newcomers in a way that encourages further engagement with LessWrong's community and/or content.</p>\n<p>What do you think? What would you add, remove, or alter?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1, "7ow6EFpypbH4hzFuz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BdXeZC8hFvLMcN495", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 1.3080022705772936e-06, "legacy": true, "legacyId": "23774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm putting together a list of short and sweet <strong>introductions to the dangers of artificial superintelligence</strong>.</p>\n<p>My target audience is intelligent, broadly philosophical <a href=\"/lw/hzt/writing_style_and_the_typical_mind_fallacy/\">narrative</a> thinkers, who can evaluate arguments well but who don't know a lot of the relevant background or jargon.</p>\n<p>My method is to construct a <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequence</a> mix tape \u2014&nbsp;a collection of short and enlightening texts, meant to be read in a specified order. I've chosen them for their persuasive and pedagogical punchiness, and for their flow in the list. I'll also (separately) list somewhat longer or less essential follow-up texts below that are still meant to be accessible to astute visitors and laypeople.</p>\n<p>The first half focuses on <em><strong>intelligence</strong></em>, answering 'What is Artificial General Intelligence (AGI)?'. The second half focuses on <em style=\"font-weight: bold;\">friendliness</em>, answering 'How can we make AGI safe, and why does it matter?'. Since the topics of some posts aren't obvious from their titles, I've summarized them using questions they address.</p>\n<p>&nbsp;</p>\n<hr>\n<p><strong id=\"Part_I__Building_intelligence_\">Part I. Building intelligence.</strong></p>\n<p style=\"padding-left: 30px;\">1. <a href=\"http://yudkowsky.net/singularity/power\">Power of Intelligence</a>. Why is intelligence important?</p>\n<p style=\"padding-left: 30px;\">2. <a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a>. Is building an intelligence from scratch like talking to a person?</p>\n<p style=\"padding-left: 30px;\">3. <a href=\"/lw/l9/artificial_addition/\">Artificial Addition</a>. What can we conclude about the nature of intelligence from the fact that we don't yet understand it?</p>\n<p style=\"padding-left: 30px;\">4. <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a>. How do human goals relate to the 'goals' of evolution?</p>\n<p style=\"padding-left: 30px;\">5. <a href=\"/lw/6ha/the_blueminimizing_robot/\">The Blue-Minimizing Robot</a>.&nbsp;What are the shortcomings of thinking of things as 'agents', 'intelligences', or 'optimizers' with defined values/goals/preferences?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong id=\"Part_II__Intelligence_explosion_\">Part II. Intelligence explosion.</strong></p>\n<p style=\"padding-left: 30px;\">6. <a href=\"/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a>. What is optimization? As optimization processes, how do evolution, humans, and self-modifying AGI differ?</p>\n<p style=\"padding-left: 30px;\">7. <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Efficient Cross-Domain Optimization</a>. What is intelligence?</p>\n<p style=\"padding-left: 30px;\">8. <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a>. What else is universally true of intelligences?</p>\n<p style=\"padding-left: 30px;\">9. <a href=\"http://intelligenceexplosion.com/2011/plenty-of-room-above-us/\">Plenty of Room Above Us</a>. Why should we expect self-improving AGI to quickly become superintelligent?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong id=\"Part_III__AI_risk_\">Part III. AI risk.</strong></p>\n<p style=\"padding-left: 30px;\">10. <a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a>. What kind of jerk would Defect even knowing the other side Cooperated?</p>\n<p style=\"padding-left: 30px;\">11. <a href=\"http://wiki.lesswrong.com/wiki/Basic_AI_drives\">Basic AI drives</a>. Why are AGIs dangerous even when they're indifferent to us?</p>\n<p style=\"padding-left: 30px;\">12. <a href=\"/lw/st/anthropomorphic_optimism/\">Anthropomorphic Optimism</a>. Why do we think things we hope happen are likelier?</p>\n<p style=\"padding-left: 30px;\">13. <a href=\"/lw/ld/the_hidden_complexity_of_wishes\">The Hidden Complexity of Wishes</a>. How hard is it to directly program an alien intelligence to enact my values?</p>\n<p style=\"padding-left: 30px;\">14. <a href=\"/lw/td/magical_categories/\">Magical Categories</a>. How hard is it to program an alien intelligence to reconstruct my values from observed patterns?</p>\n<p style=\"padding-left: 30px;\">15. <a href=\"http://intelligenceexplosion.com/2012/ai-the-problem-with-solutions/\">The AI Problem, with Solutions</a>. How hard is it to give AGI predictable values of any sort? More generally, why does AGI risk matter so much?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong id=\"Part_IV__Ends_\">Part IV. Ends.</strong></p>\n<p style=\"padding-left: 30px;\">16. <a href=\"/lw/sb/could_anything_be_right/\">Could Anything Be Right?</a> What do we mean by 'good', or 'valuable', or 'moral'?</p>\n<p style=\"padding-left: 30px;\">17. <a href=\"/lw/sw/morality_as_fixed_computation/\">Morality as Fixed Computation</a>. Is it enough to have an AGI improve the fit between my preferences and the world?</p>\n<p style=\"padding-left: 30px;\">18. <a href=\"/lw/xi/serious_stories/\">Serious Stories</a>. What would a true utopia be like?</p>\n<p style=\"padding-left: 30px;\">19. <a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a>. If we just sit back and let the universe do its thing, will it still produce value? If we don't take charge of our future, won't it still turn out interesting and beautiful on some deeper level?</p>\n<p style=\"padding-left: 30px;\">20. <a href=\"http://wiki.lesswrong.com/wiki/User:RobbBB/Tomorrow\">The Gift We Give To Tomorrow</a>. In explaining value, are we explaining it away? Are we making our goals less important?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p><strong>Summary</strong>:&nbsp;<a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five theses, two lemmas, and a couple of strategic implications</a>.</p>\n<hr>\n<p>&nbsp;</p>\n<p>All of the above were written by Eliezer Yudkowsky, with the exception of The Blue-Minimizing Robot (by Yvain), Plenty of Room Above Us and The AI Problem (by Luke Muehlhauser), and Basic AI Drives (a wiki collaboration). Seeking a powerful conclusion, I ended up making a compromise between Eliezer's original <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">The Gift We Give To Tomorrow</a> and Raymond Arnold's <a href=\"https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf\">Solstice Ritual Book</a> version. It's on the wiki, so you can further improve it with edits.</p>\n<p>&nbsp;</p>\n<p><strong>Further reading</strong>:</p>\n<ul>\n<li><a href=\"https://dl.dropboxusercontent.com/u/12787472/philosophy/three-worlds-collide-short.pdf\">Three Worlds Collide</a> (Normal), by Eliezer Yudkowsky \n<ul>\n<li>a short story vividly illustrating how alien values can evolve.</li>\n</ul>\n</li>\n<li><a href=\"/lw/91c/so_you_want_to_save_the_world/\">So You Want to Save the World</a>, by Luke Muehlhauser \n<ul>\n<li>an introduction to the open problems in Friendly Artificial Intelligence.</li>\n</ul>\n</li>\n<li><a href=\"http://intelligence.org/ie-faq/\">Intelligence Explosion FAQ</a>, by Luke Muehlhauser \n<ul>\n<li>a broad overview of likely misconceptions about AI risk.</li>\n</ul>\n</li>\n<li><a href=\"http://consc.net/papers/singularity.pdf\">The Singularity: A Philosophical Analysis</a>, by David Chalmers \n<ul>\n<li>a detailed but non-technical argument for expecting intelligence explosion, with an assessment of the moral significance of synthetic human and non-human intelligence.</li>\n</ul>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>I'm posting this to get more feedback for improving it, to isolate topics for which we <em>don't</em> yet have high-quality, non-technical stand-alone introductions, and to reintroduce LessWrongers to exceptionally useful posts I haven't seen sufficiently discussed, linked, or upvoted. I'd especially like feedback on how the list I provided flows as a unit, and what inferential gaps it fails to address. My goals are:</p>\n<p style=\"padding-left: 30px;\"><strong>A.</strong>&nbsp;Via lucid and anti-anthropomorphic vignettes, to explain AGI in a way that encourages clear thought.</p>\n<p style=\"padding-left: 30px;\"><strong>B.</strong>&nbsp;Via the Five Theses, to demonstrate the importance of Friendly AI research.</p>\n<p style=\"padding-left: 30px;\"><strong>C.</strong> Via down-to-earth meta-ethics, humanistic poetry, and pragmatic strategizing, to combat any nihilisms, relativisms, and defeatisms that might be triggered by recognizing the possibility (or probability) of Unfriendly AI.</p>\n<p style=\"padding-left: 30px;\"><strong>D.</strong> Via an accessible, substantive, entertaining presentation, to introduce the&nbsp;raison d'\u00eatre of LessWrong to sophisticated newcomers in a way that encourages further engagement with LessWrong's community and/or content.</p>\n<p>What do you think? What would you add, remove, or alter?</p>", "sections": [{"title": "Part I. Building intelligence.", "anchor": "Part_I__Building_intelligence_", "level": 1}, {"title": "Part II. Intelligence explosion.", "anchor": "Part_II__Intelligence_explosion_", "level": 1}, {"title": "Part III. AI risk.", "anchor": "Part_III__AI_risk_", "level": 1}, {"title": "Part IV. Ends.", "anchor": "Part_IV__Ends_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7Q3MoE9YzFMxGZR7F", "cnYHFNBF3kZEyx24v", "YhgjmCxcQXixStWMC", "XPErvb8m9FapXCjhA", "hQHuXuRGZxxWXaPgg", "HFTn3bAT6uXSNwv4m", "yLeEPFnnB9wE7KLx2", "tnWRXkcDi5Tw9rzXw", "HFyWNBnDNEDsDNLrZ", "RcZeZt8cPk48xxiQ8", "4ARaTpNX62uaL86j6", "PoDAyQMWEXBBBEJ5P", "vy9nnPdwTjSmt5qdb", "FnJPa8E9ZG5xiLLp5", "6qS9q5zHafFXsB6hf", "GNnHHmm8EzePmKzPk", "pGvyqAQw6yqTjpKf4", "5BJvusxdwNXYQ4L9L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-19T06:58:15.174Z", "modifiedAt": null, "url": null, "title": "Open thread, August 19-25, 2013 ", "slug": "open-thread-august-19-25-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:06.813Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gW9GHdp2czQGsDvDn/open-thread-august-19-25-2013", "pageUrlRelative": "/posts/gW9GHdp2czQGsDvDn/open-thread-august-19-25-2013", "linkUrl": "https://www.lesswrong.com/posts/gW9GHdp2czQGsDvDn/open-thread-august-19-25-2013", "postedAtFormatted": "Monday, August 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20August%2019-25%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20August%2019-25%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW9GHdp2czQGsDvDn%2Fopen-thread-august-19-25-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20August%2019-25%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW9GHdp2czQGsDvDn%2Fopen-thread-august-19-25-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW9GHdp2czQGsDvDn%2Fopen-thread-august-19-25-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_ib0\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_i93\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gW9GHdp2czQGsDvDn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "23779", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 326, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-19T21:00:51.099Z", "modifiedAt": null, "url": null, "title": "[LINK] Slashdot interview with David Ettinger of the Cryonics Institute", "slug": "link-slashdot-interview-with-david-ettinger-of-the-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:57.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "moreati", "createdAt": "2011-03-31T22:21:28.991Z", "isAdmin": false, "displayName": "moreati"}, "userId": "rxrassLcfMeHxYm3S", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4drqDGPqroxp6M9yd/link-slashdot-interview-with-david-ettinger-of-the-cryonics", "pageUrlRelative": "/posts/4drqDGPqroxp6M9yd/link-slashdot-interview-with-david-ettinger-of-the-cryonics", "linkUrl": "https://www.lesswrong.com/posts/4drqDGPqroxp6M9yd/link-slashdot-interview-with-david-ettinger-of-the-cryonics", "postedAtFormatted": "Monday, August 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Slashdot%20interview%20with%20David%20Ettinger%20of%20the%20Cryonics%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Slashdot%20interview%20with%20David%20Ettinger%20of%20the%20Cryonics%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4drqDGPqroxp6M9yd%2Flink-slashdot-interview-with-david-ettinger-of-the-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Slashdot%20interview%20with%20David%20Ettinger%20of%20the%20Cryonics%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4drqDGPqroxp6M9yd%2Flink-slashdot-interview-with-david-ettinger-of-the-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4drqDGPqroxp6M9yd%2Flink-slashdot-interview-with-david-ettinger-of-the-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://science.slashdot.org/story/13/08/19/1834210/the-cryonics-institute-offers-a-chance-at-immortality-video\">http://science.slashdot.org/story/13/08/19/1834210/the-cryonics-institute-offers-a-chance-at-immortality-video</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4drqDGPqroxp6M9yd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.3087412715579653e-06, "legacy": true, "legacyId": "23780", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-20T12:06:57.000Z", "modifiedAt": null, "url": null, "title": "Torture vs Dust Specks Yet Again", "slug": "torture-vs-dust-specks-yet-again", "viewCount": null, "lastCommentedAt": "2019-04-20T16:50:51.062Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sentientplatypus", "createdAt": "2013-05-01T02:43:34.377Z", "isAdmin": false, "displayName": "sentientplatypus"}, "userId": "vNepEav2ET9qeYM9y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rNBqHAkukptH59sMq/torture-vs-dust-specks-yet-again", "pageUrlRelative": "/posts/rNBqHAkukptH59sMq/torture-vs-dust-specks-yet-again", "linkUrl": "https://www.lesswrong.com/posts/rNBqHAkukptH59sMq/torture-vs-dust-specks-yet-again", "postedAtFormatted": "Tuesday, August 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Torture%20vs%20Dust%20Specks%20Yet%20Again&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATorture%20vs%20Dust%20Specks%20Yet%20Again%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNBqHAkukptH59sMq%2Ftorture-vs-dust-specks-yet-again%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Torture%20vs%20Dust%20Specks%20Yet%20Again%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNBqHAkukptH59sMq%2Ftorture-vs-dust-specks-yet-again", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNBqHAkukptH59sMq%2Ftorture-vs-dust-specks-yet-again", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 745, "htmlBody": "<p>The first time I read <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Specks</a> about a year ago I didn't read a single comment because I assumed the article was making a point that simply multiplying can sometimes get you the wrong answer to a problem. I seem to have had a different \"obvious answer\" in mind.</p>\n<p>And don't get me wrong, I generally agree with the idea that math can do better than moral intuition in deciding questions of ethics. Take this example from Eliezer&rsquo;s post <a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a> which made me realize that I had assumed wrong:</p>\n<blockquote>\n<p>Suppose that a disease, or a monster, or a war, or something, is killing people. And suppose you only have enough resources to implement one of the following two options:<br /> 1. Save 400 lives, with certainty.<br /> 2. Save 500 lives, with 90% probability; save no lives, 10% probability.</p>\n</blockquote>\n<p>I agree completely that you pick number 2. For me that was just manifestly obvious, of course the math trumps the feeling that you shouldn't gamble with people&rsquo;s lives&hellip;but then we get to torture vs. dust specks and that just did not compute. So I've read most every argument I could find in favor of torture(there are a great deal and I might have missed something critical), but...while I totally understand the argument (I think) I'm still horrified that people would choose torture over dust specks.</p>\n<p>I feel that the way that math predominates intuition begins to fall apart when you the problem compares trivial individual suffering with massive individual suffering, in a way very much analogous to the way in which Pascal&rsquo;s Mugging stops working when you make the credibility really low but the threat really high. Like <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">this</a>. Except I find the answer to torture vs. dust specks to be much easier...</p>\n<p>&nbsp;</p>\n<p>Let me give some examples to illustrate my point.</p>\n<p>Can you imagine Harry killing Hermione because Voldemort threatened to plague all sentient life with one barely noticed dust speck each day for the rest of time? Can you imagine killing your own best friend/significant other/loved one to stop the powers of the Matrix from hitting 3^^^3 sentient beings with nearly inconsquential dust specks? Of course not. No. Snap decision.</p>\n<p>Eliezer, would you seriously, given the choice by Alpha, the Alien superintelligence that always carries out its threats, give up all your work, and horribly torture some innocent person, all day for fifty years in the face of the threat of a 3^^^3 insignificant dust specks barely inconveniencing sentient beings? Or be tortured for fifty years to avoid the dust specks?</p>\n<p>I realize that this is much more personally specific than the original question: but it is <em>someone's</em> loved one, <em>someone's </em>life. And if you wouldn't make the sacrifice what right do you have to say someone else should make it? I feel as though if you want to argue that torture for fifty years is better than 3^^^3 barely noticeable inconveniences you had better well be willing to make that sacrifice yourself.</p>\n<p>And I can&rsquo;t conceive of anyone actually sacrificing their life, or themselves to save the world from <em>dust specks</em>. Maybe I'm committing the typical mind fallacy in believing that no one is <em>that</em> ridiculously altruistic, but does anyone want an Artificial Intelligence that will potentially sacrifice them if it will deal with the universe&rsquo;s dust speck problem or some equally widespread and trivial equivalent? I most certainly object to the creation of <em>that</em> AI. An AI that sacrifices me to save two others - I wouldn't like that, certainly, but I still think the AI should probably do it if it thinks their lives are of more value. But dust specks on the other hand....</p>\n<p>This example made me immediately think that some sort of rule is needed to limit morality coming from math in the development of any AI program. When the problem reaches a certain low level of suffering and is multiplied it by an unreasonably large number it needs to take some kind of huge penalty because otherwise to an AI it would be vastly preferable the whole of Earth be blown up than 3^^^3 people suffer a mild slap to the face.</p>\n<p>And really, I don&rsquo;t think we want to create an Artificial Intelligence that would do that.</p>\n<p>I&rsquo;m mainly just concerned that some factor be incorporated into the design of any Artificial Intelligence that prevents it from murdering myself and others for trivial but widespread causes. Because that just sounds like a sci-fi book of how superintelligence could go horribly wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rNBqHAkukptH59sMq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": -2, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "23784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "4ZzefKQwAtMo5yp99", "a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-20T22:42:06.583Z", "modifiedAt": null, "url": null, "title": "[LINK] Cochrane on Existential Risk", "slug": "link-cochrane-on-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:59.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Salemicus", "createdAt": "2012-05-10T20:50:25.455Z", "isAdmin": false, "displayName": "Salemicus"}, "userId": "D8cdrPXwhhiPdqkSz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mXrNs8cRfWEARgLLc/link-cochrane-on-existential-risk", "pageUrlRelative": "/posts/mXrNs8cRfWEARgLLc/link-cochrane-on-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/mXrNs8cRfWEARgLLc/link-cochrane-on-existential-risk", "postedAtFormatted": "Tuesday, August 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Cochrane%20on%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Cochrane%20on%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXrNs8cRfWEARgLLc%2Flink-cochrane-on-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Cochrane%20on%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXrNs8cRfWEARgLLc%2Flink-cochrane-on-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXrNs8cRfWEARgLLc%2Flink-cochrane-on-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 394, "htmlBody": "<p>The finance professor <a href=\"http://en.wikipedia.org/wiki/John_H._Cochrane\">John Cochrane</a> recently posted an interesting blog post. The piece is about existential risk in the context of global warming, but it is really a discussion of existential risk generally; many of his points are highly relevant to AI risk.</p>\n<blockquote>\n<p>If we [respond strongly to all low-probability threats], we spend 10 times GDP.</p>\n<p>It's a interesting case of framing bias. If you worry only about climate, it seems sensible to pay a pretty stiff price to avoid a small uncertain catastrophe. But if you worry about small uncertain catastrophes, you spend all you have and more, and it's not clear that climate is the highest on the list...</p>\n<p>All in all, I'm not convinced our political system is ready to do a very good job of prioritizing outsize expenditures on small ambiguous-probability events.</p>\n</blockquote>\n<p>He also points out that the threat from global warming has a negative beta - i.e. higher future growth rates are likely to be associated with greater risk of global warming, but also the richer our descendants will be. This means both that they will be more able to cope with the threat, and that the damage is less important from a utilitarian point of view. Attempting to stop global warming therefore has positive beta, and therefore requires higher rates of return than simple time-discounting.</p>\n<p>It strikes me that this argument applies equally to AI risk, as fruitful artificial intelligence research is likely to be associated with higher economic growth. Moreover:</p>\n<blockquote>\n<p>The economic case for cutting carbon emissions now is that by paying a bit now, we will make our descendants better off in 100 years.</p>\n<p>Once stated this way, carbon taxes are just an investment. But is investing in carbon reduction the most profitable way to transfer wealth to our descendants? Instead of spending say $1 trillion in carbon abatement costs, why don't we invest $1 trillion in stocks? If the 100 year rate of return on stocks is higher than the 100 year rate of return on carbon abatement -- likely -- they come out better off. With a gazillion dollars or so, they can rebuild Manhattan on higher ground. They can afford whatever carbon capture or geoengineering technology crops up to clean up our messes.</p>\n</blockquote>\n<p>So should we close down MIRI and invest the funds in an index tracker?</p>\n<p>The full post can be found <a href=\"http://johnhcochrane.blogspot.co.uk/2013/08/litterman-on-carbon-finance.html\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mXrNs8cRfWEARgLLc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -1, "extendedScore": null, "score": 1.3100456595418807e-06, "legacy": true, "legacyId": "23793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T06:35:45.478Z", "modifiedAt": null, "url": null, "title": "Internal and external criticism", "slug": "internal-and-external-criticism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q3BpSzw9iWvdXy9Bh/internal-and-external-criticism", "pageUrlRelative": "/posts/q3BpSzw9iWvdXy9Bh/internal-and-external-criticism", "linkUrl": "https://www.lesswrong.com/posts/q3BpSzw9iWvdXy9Bh/internal-and-external-criticism", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Internal%20and%20external%20criticism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInternal%20and%20external%20criticism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3BpSzw9iWvdXy9Bh%2Finternal-and-external-criticism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Internal%20and%20external%20criticism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3BpSzw9iWvdXy9Bh%2Finternal-and-external-criticism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3BpSzw9iWvdXy9Bh%2Finternal-and-external-criticism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<p>I notice that if you want to persuade me away from a position, it sometimes works to have me talk with two kinds of people: 1) people who have good reasons for disagreeing with my position, and 2) people who agree with my position for similar reasons and hold it even more strongly than I do.<br /><br />In both cases, the difference in opinion forces me to re-examine my reasons for believing in something, but the direction of the examination is different. Case 1 makes me think \"are these criticisms valid, or do I (should I) support this position because of some reason that the criticisms do not take into account?\". Case 2 makes me think \"this person believes in this thing for basically the same reasons that I do, so why haven't those reasons pushed me to a similar extreme? Do I actually have unacknowledged reasons for doubting the validity of those reasons, which would deserve further consideration?\"<br /><br />In case 1, I am being presented with criticisms that came from outside my own thought process. In case 2, I am searching my own thought process for criticisms that have been generated within it. So the source of the criticism is either external or internal, respectively. A combination of both may prove decisive in situations where just one isn't enough.<br /><br />Now the question is, are there reliable ways for inducing one of the cases in situations where only the other is present, and I have reason to suspect that I'm being overconfident about something?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wygybwY9SMdaPepZr": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q3BpSzw9iWvdXy9Bh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "23804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T11:14:41.588Z", "modifiedAt": null, "url": null, "title": "[LINK] \"Preventing Human Extinction\" by Nick Beckstead, Peter Singer, and Matt Wage", "slug": "link-preventing-human-extinction-by-nick-beckstead-peter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Sz3NNYBWC2ZTAenkG/link-preventing-human-extinction-by-nick-beckstead-peter", "pageUrlRelative": "/posts/Sz3NNYBWC2ZTAenkG/link-preventing-human-extinction-by-nick-beckstead-peter", "linkUrl": "https://www.lesswrong.com/posts/Sz3NNYBWC2ZTAenkG/link-preventing-human-extinction-by-nick-beckstead-peter", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20%22Preventing%20Human%20Extinction%22%20by%20Nick%20Beckstead%2C%20Peter%20Singer%2C%20and%20Matt%20Wage&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20%22Preventing%20Human%20Extinction%22%20by%20Nick%20Beckstead%2C%20Peter%20Singer%2C%20and%20Matt%20Wage%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSz3NNYBWC2ZTAenkG%2Flink-preventing-human-extinction-by-nick-beckstead-peter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20%22Preventing%20Human%20Extinction%22%20by%20Nick%20Beckstead%2C%20Peter%20Singer%2C%20and%20Matt%20Wage%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSz3NNYBWC2ZTAenkG%2Flink-preventing-human-extinction-by-nick-beckstead-peter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSz3NNYBWC2ZTAenkG%2Flink-preventing-human-extinction-by-nick-beckstead-peter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<blockquote>\n<p>Fortunately, the odds of an extinction-sized asteroid hitting the earth this century are low, on the order of one in a million. Unfortunately, asteroids aren&rsquo;t the only threats to humanity&rsquo;s survival. Other potential threats stem from bio-engineered diseases, nuclear war, extreme climate change, and dangerous future technologies.</p>\n<p>Given that there is some risk of humanity going extinct over the next couple of centuries, the next question is whether we can do anything about it. We will first explain what we can do about it, and then ask the deeper ethical question: how bad would human extinction be?</p>\n</blockquote>\n<p>Read more at <a href=\"http://effective-altruism.com/preventing-human-extinction\">\"Preventing Human Extinction\"</a>&nbsp;at the Effective Altruist blog.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Sz3NNYBWC2ZTAenkG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.310683417138718e-06, "legacy": true, "legacyId": "23807", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T15:05:08.440Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 82-85", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-82-85", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mgx2qE6kRLMvGgg7w/meetup-durham-rtlw-hpmor-discussion-ch-82-85", "pageUrlRelative": "/posts/Mgx2qE6kRLMvGgg7w/meetup-durham-rtlw-hpmor-discussion-ch-82-85", "linkUrl": "https://www.lesswrong.com/posts/Mgx2qE6kRLMvGgg7w/meetup-durham-rtlw-hpmor-discussion-ch-82-85", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2082-85&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2082-85%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMgx2qE6kRLMvGgg7w%2Fmeetup-durham-rtlw-hpmor-discussion-ch-82-85%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2082-85%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMgx2qE6kRLMvGgg7w%2Fmeetup-durham-rtlw-hpmor-discussion-ch-82-85", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMgx2qE6kRLMvGgg7w%2Fmeetup-durham-rtlw-hpmor-discussion-ch-82-85", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q1'>Durham/RTLW HPMoR discussion, ch. 82-85</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 August 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Fullsteam for discussion of HPMoR 82-85!</p>\n\n<p>Bring a question, observation, or topic for discussion -- anything you found particularly interesting in the reading.</p>\n\n<p>No time to read or reread?  No worries; come hang out anyway.</p>\n\n<p>12:00 gather food and coffee <br />\n12:30 discussion begins <br />\n2:00 beers, SpaceTeam, or other antics</p>\n\n<p>To be notified about other upcoming meetups &amp; events, join the RTLW mailing list: <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a> !</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q1'>Durham/RTLW HPMoR discussion, ch. 82-85</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mgx2qE6kRLMvGgg7w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23808", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__82_85\">Discussion article for the meetup : <a href=\"/meetups/q1\">Durham/RTLW HPMoR discussion, ch. 82-85</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 August 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at Fullsteam for discussion of HPMoR 82-85!</p>\n\n<p>Bring a question, observation, or topic for discussion -- anything you found particularly interesting in the reading.</p>\n\n<p>No time to read or reread?  No worries; come hang out anyway.</p>\n\n<p>12:00 gather food and coffee <br>\n12:30 discussion begins <br>\n2:00 beers, SpaceTeam, or other antics</p>\n\n<p>To be notified about other upcoming meetups &amp; events, join the RTLW mailing list: <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a> !</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__82_851\">Discussion article for the meetup : <a href=\"/meetups/q1\">Durham/RTLW HPMoR discussion, ch. 82-85</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 82-85", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__82_85", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 82-85", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__82_851", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T15:16:20.469Z", "modifiedAt": null, "url": null, "title": "Meetup : [Boston] Estimation Experiment & Discussion", "slug": "meetup-boston-estimation-experiment-and-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ben_LandauTaylor", "createdAt": "2013-07-25T17:40:48.283Z", "isAdmin": false, "displayName": "Ben_LandauTaylor"}, "userId": "ZvoQwr4zZjPuC2oNJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MmHccJnBPhLzvat8K/meetup-boston-estimation-experiment-and-discussion", "pageUrlRelative": "/posts/MmHccJnBPhLzvat8K/meetup-boston-estimation-experiment-and-discussion", "linkUrl": "https://www.lesswrong.com/posts/MmHccJnBPhLzvat8K/meetup-boston-estimation-experiment-and-discussion", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BBoston%5D%20Estimation%20Experiment%20%26%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BBoston%5D%20Estimation%20Experiment%20%26%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmHccJnBPhLzvat8K%2Fmeetup-boston-estimation-experiment-and-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BBoston%5D%20Estimation%20Experiment%20%26%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmHccJnBPhLzvat8K%2Fmeetup-boston-estimation-experiment-and-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMmHccJnBPhLzvat8K%2Fmeetup-boston-estimation-experiment-and-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<h2><strong style=\"font-size: small;\">WHEN:</strong><span style=\"font-size: small;\"> </span><span class=\"date\" style=\"font-size: small;\">25 August 2013 02:00:00PM (-0400)</span></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHERE:</strong> <span class=\"address\">25 Ames St, Cambridge, MA</span></p>\n<p><span class=\"address\"><a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/134404242/\">Meetup event page</a></span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We're running a test of an estimation method to see whether it helps form accurate beliefs. Afterwards, we'll talk about the method and its implications. This experiment was designed with input from the Center for Applied Rationality, and they're eager to see the outcome. Please come if you're able; the bigger our sample size, the better our data will be.</p>\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n<p>Our default schedule is as follows:</p>\n<p>&mdash;Phase 1: Arrival, greetings, unstructured conversation.</p>\n<p>&mdash;Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes.</p>\n<p>&mdash;Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n<p>&mdash;Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MmHccJnBPhLzvat8K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.310888312165275e-06, "legacy": true, "legacyId": "23809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T15:20:31.875Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!", "slug": "meetup-durham-nc-triangle-area-cognitive-biases-and-where-to", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6dioECLNaf3dPKHPj/meetup-durham-nc-triangle-area-cognitive-biases-and-where-to", "pageUrlRelative": "/posts/6dioECLNaf3dPKHPj/meetup-durham-nc-triangle-area-cognitive-biases-and-where-to", "linkUrl": "https://www.lesswrong.com/posts/6dioECLNaf3dPKHPj/meetup-durham-nc-triangle-area-cognitive-biases-and-where-to", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%20and%20Where%20to%20Find%20Them%20%2B%20games!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%20and%20Where%20to%20Find%20Them%20%2B%20games!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6dioECLNaf3dPKHPj%2Fmeetup-durham-nc-triangle-area-cognitive-biases-and-where-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%20and%20Where%20to%20Find%20Them%20%2B%20games!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6dioECLNaf3dPKHPj%2Fmeetup-durham-nc-triangle-area-cognitive-biases-and-where-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6dioECLNaf3dPKHPj%2Fmeetup-durham-nc-triangle-area-cognitive-biases-and-where-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q3'>Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 August 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll do a minidiscussion on cognitive biases, then follow with The Hypothesis Testing Game Sometimes Known As Zendo.</p>\n\n<p>7:00 gather, chat, coffee <br />\n7:30 discussion <br />\n8:30? Pyramids o'clock! <br />\n9:30ish Adjourn (probably to Fullsteam for beers.)</p>\n\n<p>To be notified about other upcoming meetups &amp; events, join the RTLW mailing list: <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a> !</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q3'>Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6dioECLNaf3dPKHPj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases_and_Where_to_Find_Them___games_\">Discussion article for the meetup : <a href=\"/meetups/q3\">Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 August 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll do a minidiscussion on cognitive biases, then follow with The Hypothesis Testing Game Sometimes Known As Zendo.</p>\n\n<p>7:00 gather, chat, coffee <br>\n7:30 discussion <br>\n8:30? Pyramids o'clock! <br>\n9:30ish Adjourn (probably to Fullsteam for beers.)</p>\n\n<p>To be notified about other upcoming meetups &amp; events, join the RTLW mailing list: <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a> !</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases_and_Where_to_Find_Them___games_1\">Discussion article for the meetup : <a href=\"/meetups/q3\">Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases_and_Where_to_Find_Them___games_", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases_and_Where_to_Find_Them___games_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T19:10:01.886Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NThwAdbPCwThsQxLZ/meetup-helsinki-meetup-4", "pageUrlRelative": "/posts/NThwAdbPCwThsQxLZ/meetup-helsinki-meetup-4", "linkUrl": "https://www.lesswrong.com/posts/NThwAdbPCwThsQxLZ/meetup-helsinki-meetup-4", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNThwAdbPCwThsQxLZ%2Fmeetup-helsinki-meetup-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNThwAdbPCwThsQxLZ%2Fmeetup-helsinki-meetup-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNThwAdbPCwThsQxLZ%2Fmeetup-helsinki-meetup-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q4'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 September 2013 03:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">J\u00e4mer\u00e4ntaival 3, 02150 Espoo, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019ll be trying a new location this time: <a href=\"http://ayy.fi/jasenille/tilojen-vuokraaminen/sitsi-peli-ja-kokoustilat/takkakabinetti/\" rel=\"nofollow\">Takkakabinetti</a> in Otaniemi.</p>\n\n<p>In this meetup, Pyry will give us a brief introduction to Lojban, which has been described as \"the constructed language with the most complete grammar \u2013 a language created to reflect the principles of logic\u201d.</p>\n\n<p>We\u2019ll also try personal problem solving in smaller groups. Get your problems solved, or help others!</p>\n\n<p>Various board games will be available as well.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q4'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NThwAdbPCwThsQxLZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3110865136663116e-06, "legacy": true, "legacyId": "23811", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/q4\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 September 2013 03:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">J\u00e4mer\u00e4ntaival 3, 02150 Espoo, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019ll be trying a new location this time: <a href=\"http://ayy.fi/jasenille/tilojen-vuokraaminen/sitsi-peli-ja-kokoustilat/takkakabinetti/\" rel=\"nofollow\">Takkakabinetti</a> in Otaniemi.</p>\n\n<p>In this meetup, Pyry will give us a brief introduction to Lojban, which has been described as \"the constructed language with the most complete grammar \u2013 a language created to reflect the principles of logic\u201d.</p>\n\n<p>We\u2019ll also try personal problem solving in smaller groups. Get your problems solved, or help others!</p>\n\n<p>Various board games will be available as well.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/q4\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-21T22:06:32.148Z", "modifiedAt": null, "url": null, "title": "Meetup : VA Callibration/Biased games meetup", "slug": "meetup-va-callibration-biased-games-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gc7YhQaczzvHxbex3/meetup-va-callibration-biased-games-meetup", "pageUrlRelative": "/posts/gc7YhQaczzvHxbex3/meetup-va-callibration-biased-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/gc7YhQaczzvHxbex3/meetup-va-callibration-biased-games-meetup", "postedAtFormatted": "Wednesday, August 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20VA%20Callibration%2FBiased%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20VA%20Callibration%2FBiased%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgc7YhQaczzvHxbex3%2Fmeetup-va-callibration-biased-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20VA%20Callibration%2FBiased%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgc7YhQaczzvHxbex3%2Fmeetup-va-callibration-biased-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgc7YhQaczzvHxbex3%2Fmeetup-va-callibration-biased-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q5'>VA Callibration/Biased games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4238 Wilson Boulevard, Ballston Common Mall, Arlington, VA 22203, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting in the food court to play calibration games and and biased boardgames.</p>\n\n<p>Please note that this is NOT THE NORMAL LOCATION: we're experimenting with a different place to see how attendance fares in VA.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q5'>VA Callibration/Biased games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gc7YhQaczzvHxbex3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3112362488217389e-06, "legacy": true, "legacyId": "23812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___VA_Callibration_Biased_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/q5\">VA Callibration/Biased games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 August 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4238 Wilson Boulevard, Ballston Common Mall, Arlington, VA 22203, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting in the food court to play calibration games and and biased boardgames.</p>\n\n<p>Please note that this is NOT THE NORMAL LOCATION: we're experimenting with a different place to see how attendance fares in VA.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___VA_Callibration_Biased_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/q5\">VA Callibration/Biased games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : VA Callibration/Biased games meetup", "anchor": "Discussion_article_for_the_meetup___VA_Callibration_Biased_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : VA Callibration/Biased games meetup", "anchor": "Discussion_article_for_the_meetup___VA_Callibration_Biased_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-22T01:14:45.109Z", "modifiedAt": null, "url": null, "title": "Use Your Identity Carefully", "slug": "use-your-identity-carefully", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:38.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ben_LandauTaylor", "createdAt": "2013-07-25T17:40:48.283Z", "isAdmin": false, "displayName": "Ben_LandauTaylor"}, "userId": "ZvoQwr4zZjPuC2oNJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zupr296Zy74wpihXT/use-your-identity-carefully", "pageUrlRelative": "/posts/Zupr296Zy74wpihXT/use-your-identity-carefully", "linkUrl": "https://www.lesswrong.com/posts/Zupr296Zy74wpihXT/use-your-identity-carefully", "postedAtFormatted": "Thursday, August 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Use%20Your%20Identity%20Carefully&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUse%20Your%20Identity%20Carefully%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZupr296Zy74wpihXT%2Fuse-your-identity-carefully%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Use%20Your%20Identity%20Carefully%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZupr296Zy74wpihXT%2Fuse-your-identity-carefully", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZupr296Zy74wpihXT%2Fuse-your-identity-carefully", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 527, "htmlBody": "<p>&nbsp;</p>\n<p>In <a href=\"http://www.paulgraham.com/identity.html\">Keep Your Identity Small</a>, Paul Graham argues against associating yourself with labels (i.e. &ldquo;libertarian,&rdquo; &ldquo;feminist,&rdquo; &ldquo;gamer,&rdquo; &ldquo;American&rdquo;) because labels constrain what you&rsquo;ll let yourself believe. It&rsquo;s a wonderful essay that&rsquo;s led me to make concrete changes in my life. That said, it&rsquo;s only about 90% correct. I have two issues with Graham&rsquo;s argument; one is a semantic quibble, but it leads into the bigger issue, which is a tactic I&rsquo;ve used to become a better person.</p>\n<p>Graham talks about the importance of identity in determining beliefs. This isn&rsquo;t quite the right framework. I&rsquo;m a fanatical consequentialist, so I care what actions people take. Beliefs can constrain actions, but identity can also constrain actions directly.</p>\n<p>To give a trivial example from the past week in which beliefs didn&rsquo;t matter: I had a self-image as someone who didn&rsquo;t wear jeans or t-shirts. As it happens, there are times when wearing jeans is completely fine, and when other people wore jeans in casual settings, I knew it was appropriate. Nevertheless, I wasn&rsquo;t able to act on this belief because of my identity. (I finally realized this was silly, consciously discarded that useless bit of identity, and made a point of wearing jeans to a social event.)</p>\n<p>Why is this distinction important? If we&rsquo;re looking at identify from an action-centered framework, this recommends a different approach from Graham&rsquo;s.</p>\n<p>Do you want to constrain your beliefs? No; you want to go wherever the evidence pushes you. &ldquo;If X is true, I desire to believe that X is true. If X is not true, I desire to believe that X is not true.&rdquo; Identity will only get in the way.</p>\n<p>Do you want to constrain your actions? Yes! Ten thousand times yes! <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">Akrasia</a> exists. <a href=\"http://en.wikipedia.org/wiki/Commitment_device\">Commitment devices</a> are useful. <a href=\"https://www.beeminder.com/\">Beeminder</a> is successful. Identity is one of the most effective tools for the job, if you wield it deliberately.</p>\n<p>I&rsquo;ve cultivated an identity as a person who makes events happen. It took months to instill, but now, when I think &ldquo;I wish people were doing X,&rdquo; I instinctively start putting together a group to do X. This manifests in minor ways, like the tree-climbing expedition I put together at the Effective Altruism Summit, and in big ways, like the megameetup we held in Boston. If I hadn&rsquo;t used my identity to motivate myself, neither of those things would&rsquo;ve happened, and my life would be poorer.</p>\n<p>Identity is powerful. Powerful things are dangerous, like backhoes and bandsaws. People use them anyway, because sometimes they&rsquo;re the best tools for the job, and because safety precautions can minimize the danger.</p>\n<p>Identity is hard to change. Identity can be difficult to notice. Identity has unintended consequences. Use this tool only after careful deliberation. What would this identity do to your actions? What would it do to your beliefs? What social consequences would it have? Can you do the same thing with a less dangerous tool? Think twice, and then think again, before you add to your identity. Most identities are a hindrance.</p>\n<p>But please, don&rsquo;t discard this tool just because some things might go wrong. If you are willful, and careful, and wise, then you can cultivate the identity of the person you always wanted to be.</p>\n<p><span style=\"line-height: 23px; color: #333333;\"><em></em></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zupr296Zy74wpihXT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 99, "baseScore": 130, "extendedScore": null, "score": 0.000313, "legacy": true, "legacyId": "23815", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 130, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-22T07:15:54.919Z", "modifiedAt": null, "url": null, "title": "Simple investing for a complete beginner? (Just\u2026 developing world index funds?)", "slug": "simple-investing-for-a-complete-beginner-just-developing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.966Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Owen_Richardson", "createdAt": "2011-04-08T22:03:10.482Z", "isAdmin": false, "displayName": "Owen_Richardson"}, "userId": "uR7QxXK65gYZQ4PrL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5FcK3hCG25eDiAsWL/simple-investing-for-a-complete-beginner-just-developing", "pageUrlRelative": "/posts/5FcK3hCG25eDiAsWL/simple-investing-for-a-complete-beginner-just-developing", "linkUrl": "https://www.lesswrong.com/posts/5FcK3hCG25eDiAsWL/simple-investing-for-a-complete-beginner-just-developing", "postedAtFormatted": "Thursday, August 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simple%20investing%20for%20a%20complete%20beginner%3F%20(Just%E2%80%A6%20developing%20world%20index%20funds%3F)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimple%20investing%20for%20a%20complete%20beginner%3F%20(Just%E2%80%A6%20developing%20world%20index%20funds%3F)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5FcK3hCG25eDiAsWL%2Fsimple-investing-for-a-complete-beginner-just-developing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simple%20investing%20for%20a%20complete%20beginner%3F%20(Just%E2%80%A6%20developing%20world%20index%20funds%3F)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5FcK3hCG25eDiAsWL%2Fsimple-investing-for-a-complete-beginner-just-developing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5FcK3hCG25eDiAsWL%2Fsimple-investing-for-a-complete-beginner-just-developing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 613, "htmlBody": "<div id=\"markdown-here-wrapper-259418\" class=\"markdown-here-wrapper\">\n<p style=\"margin: 1.2em 0px ! important;\"><strong>[EDIT: Through <a href=\"/r/discussion/lw/idq/simple_investing_for_a_complete_beginner_just/9mi2?context=1#9mi2\">conversation with Rolf Andreassen below</a>, it has been brought to my attention that I am simply <em>completely and irretrievably insane</em>. </strong></p>\n<p><strong>Sane and well-measured advice is therefore wasted on me, and I just wanted to edit in this notice here so that other  well-meaning folks don't get tricked into wasting their time  trying to talk sense into a total nutcase like me. :)</strong></p>\n<strong> </strong>\n<p style=\"margin: 1.2em 0px ! important;\"><strong>(I appreciate all y'all, though. ^^ ) ]</strong></p>\n<p style=\"margin: 1.2em 0px ! important;\">So my dad set up a trust fund for me when I was a kid, and I've got 13k (CAD) now, which I am going to be taking direct control of.</p>\n<p style=\"margin: 1.2em 0px ! important;\">Now, I have no interest in making a deep study of investment. I have a life to live and dealing with money is <em>boring</em>.</p>\n<p style=\"margin: 1.2em 0px ! important;\">The only thing more boring than dealing with money, is dealing with a <em>lack</em> of money, and so I want to optimize the time and thought I spend avoiding that down to a minimum.</p>\n<p style=\"margin: 1.2em 0px ! important;\">Four things occur to me:</p>\n<p style=\"margin: 1.2em 0px ! important;\">1) Taking the naive and sparse knowledge I have of this area, basically just stuff I&lsquo;ve randomly osmosis&rsquo;d up, this is my train of thought:</p>\n<p style=\"margin: 1.2em 0px ! important;\">Markets are essentially random walks with an upward trend?</p>\n<p style=\"margin: 1.2em 0px ! important;\">&ldquo;Index funds&rdquo; are magic boxes that you put money in and your money will grow at the same rate of the market that the fund &ldquo;indexes&rdquo;?</p>\n<p style=\"margin: 1.2em 0px ! important;\">&ldquo;Developing world&rdquo; economies generally grow a <em>lot</em> quicker than those in the &ldquo;developed world&rdquo;?</p>\n<p style=\"margin: 1.2em 0px ! important;\">(This makes sense to me. Places like the US, Canada Europe, etc, already have mature transportation and communication infrastructure. You can't get much economic growth out of doing basic stuff like building a new highway <em>here</em>, but in, like, some African region that has previously been served by, I dunno, jungle-donkeys, it makes a proportionally <em>much</em> bigger difference.)</p>\n<p style=\"margin: 1.2em 0px ! important;\">There are a few countries where &ldquo;developing&rdquo; is a euphemism for &ldquo;totally messed up&rdquo;, but in general it really <em>does</em> mean &ldquo;growing&rdquo;?</p>\n<p style=\"margin: 1.2em 0px ! important;\">And there are enough of these places over the world, and they're independent enough, that natural disasters/political trouble/etc in a few of them still leave a consistent and high rate of average growth?</p>\n<p style=\"margin: 1.2em 0px ! important;\">So shouldn't I just put all my money in a fund that &ldquo;indexes&rdquo; all these \"<em>developing</em>\" economies together?</p>\n<p style=\"margin: 1.2em 0px ! important;\">2) My dad set up this trust fund with a bank that has a bunch of big expensive physical buildings for some reason. I recently read a letter from them saying that they will charge a $100 yearly fee for having less than 15k in an account.</p>\n<p style=\"margin: 1.2em 0px ! important;\">Are there better options I should be taking than opening my own account with an institution that thinks it makes sense to charge me a hundred bucks for not being rich?</p>\n<p style=\"margin: 1.2em 0px ! important;\">3) Me and muflax are actually going to go work full time on developing [this totally amazing educational technology that will completely revolutionize human civilization but you have no reason to care about that until you've seen a demo in action so nevermind].</p>\n<p style=\"margin: 1.2em 0px ! important;\">We might spend as much as a year (yeah, that's outside-view calibrated) working on it until we have something we can make a living off of while continuing development.</p>\n<p style=\"margin: 1.2em 0px ! important;\">We think we can get total living costs for a year down into the 5k..10k range&hellip; maybe even lower. We're going to be living in the UK, because of reasons.</p>\n<p style=\"margin: 1.2em 0px ! important;\">So&hellip; can I leave this measly 13k in an investment account and still draw out of it for monthly costs?</p>\n<p style=\"margin: 1.2em 0px ! important;\">4) Or is this whole &ldquo;investing&rdquo; thing something I should even be bothering with at all right now?</p>\n<p style=\"margin: 1.2em 0px ! important;\">Should I just pop out the whole sum into a savings account that I can draw from as I need, and worry about reinvesting whatever is left over then, a year from now, after we have obviously started on our way to becoming rich and famous?</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5FcK3hCG25eDiAsWL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -8, "extendedScore": null, "score": 1.311702501231308e-06, "legacy": true, "legacyId": "23822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"markdown-here-wrapper-259418\" class=\"markdown-here-wrapper\">\n<p style=\"margin: 1.2em 0px ! important;\"><strong id=\"_EDIT__Through_conversation_with_Rolf_Andreassen_below__it_has_been_brought_to_my_attention_that_I_am_simply_completely_and_irretrievably_insane__\">[EDIT: Through <a href=\"/r/discussion/lw/idq/simple_investing_for_a_complete_beginner_just/9mi2?context=1#9mi2\">conversation with Rolf Andreassen below</a>, it has been brought to my attention that I am simply <em>completely and irretrievably insane</em>. </strong></p>\n<p><strong id=\"Sane_and_well_measured_advice_is_therefore_wasted_on_me__and_I_just_wanted_to_edit_in_this_notice_here_so_that_other__well_meaning_folks_don_t_get_tricked_into_wasting_their_time__trying_to_talk_sense_into_a_total_nutcase_like_me____\">Sane and well-measured advice is therefore wasted on me, and I just wanted to edit in this notice here so that other  well-meaning folks don't get tricked into wasting their time  trying to talk sense into a total nutcase like me. :)</strong></p>\n<strong> </strong>\n<p style=\"margin: 1.2em 0px ! important;\"><strong id=\"_I_appreciate_all_y_all__though________\">(I appreciate all y'all, though. ^^ ) ]</strong></p>\n<p style=\"margin: 1.2em 0px ! important;\">So my dad set up a trust fund for me when I was a kid, and I've got 13k (CAD) now, which I am going to be taking direct control of.</p>\n<p style=\"margin: 1.2em 0px ! important;\">Now, I have no interest in making a deep study of investment. I have a life to live and dealing with money is <em>boring</em>.</p>\n<p style=\"margin: 1.2em 0px ! important;\">The only thing more boring than dealing with money, is dealing with a <em>lack</em> of money, and so I want to optimize the time and thought I spend avoiding that down to a minimum.</p>\n<p style=\"margin: 1.2em 0px ! important;\">Four things occur to me:</p>\n<p style=\"margin: 1.2em 0px ! important;\">1) Taking the naive and sparse knowledge I have of this area, basically just stuff I\u2018ve randomly osmosis\u2019d up, this is my train of thought:</p>\n<p style=\"margin: 1.2em 0px ! important;\">Markets are essentially random walks with an upward trend?</p>\n<p style=\"margin: 1.2em 0px ! important;\">\u201cIndex funds\u201d are magic boxes that you put money in and your money will grow at the same rate of the market that the fund \u201cindexes\u201d?</p>\n<p style=\"margin: 1.2em 0px ! important;\">\u201cDeveloping world\u201d economies generally grow a <em>lot</em> quicker than those in the \u201cdeveloped world\u201d?</p>\n<p style=\"margin: 1.2em 0px ! important;\">(This makes sense to me. Places like the US, Canada Europe, etc, already have mature transportation and communication infrastructure. You can't get much economic growth out of doing basic stuff like building a new highway <em>here</em>, but in, like, some African region that has previously been served by, I dunno, jungle-donkeys, it makes a proportionally <em>much</em> bigger difference.)</p>\n<p style=\"margin: 1.2em 0px ! important;\">There are a few countries where \u201cdeveloping\u201d is a euphemism for \u201ctotally messed up\u201d, but in general it really <em>does</em> mean \u201cgrowing\u201d?</p>\n<p style=\"margin: 1.2em 0px ! important;\">And there are enough of these places over the world, and they're independent enough, that natural disasters/political trouble/etc in a few of them still leave a consistent and high rate of average growth?</p>\n<p style=\"margin: 1.2em 0px ! important;\">So shouldn't I just put all my money in a fund that \u201cindexes\u201d all these \"<em>developing</em>\" economies together?</p>\n<p style=\"margin: 1.2em 0px ! important;\">2) My dad set up this trust fund with a bank that has a bunch of big expensive physical buildings for some reason. I recently read a letter from them saying that they will charge a $100 yearly fee for having less than 15k in an account.</p>\n<p style=\"margin: 1.2em 0px ! important;\">Are there better options I should be taking than opening my own account with an institution that thinks it makes sense to charge me a hundred bucks for not being rich?</p>\n<p style=\"margin: 1.2em 0px ! important;\">3) Me and muflax are actually going to go work full time on developing [this totally amazing educational technology that will completely revolutionize human civilization but you have no reason to care about that until you've seen a demo in action so nevermind].</p>\n<p style=\"margin: 1.2em 0px ! important;\">We might spend as much as a year (yeah, that's outside-view calibrated) working on it until we have something we can make a living off of while continuing development.</p>\n<p style=\"margin: 1.2em 0px ! important;\">We think we can get total living costs for a year down into the 5k..10k range\u2026 maybe even lower. We're going to be living in the UK, because of reasons.</p>\n<p style=\"margin: 1.2em 0px ! important;\">So\u2026 can I leave this measly 13k in an investment account and still draw out of it for monthly costs?</p>\n<p style=\"margin: 1.2em 0px ! important;\">4) Or is this whole \u201cinvesting\u201d thing something I should even be bothering with at all right now?</p>\n<p style=\"margin: 1.2em 0px ! important;\">Should I just pop out the whole sum into a savings account that I can draw from as I need, and worry about reinvesting whatever is left over then, a year from now, after we have obviously started on our way to becoming rich and famous?</p>\n</div>", "sections": [{"title": "[EDIT: Through conversation with Rolf Andreassen below, it has been brought to my attention that I am simply completely and irretrievably insane. ", "anchor": "_EDIT__Through_conversation_with_Rolf_Andreassen_below__it_has_been_brought_to_my_attention_that_I_am_simply_completely_and_irretrievably_insane__", "level": 1}, {"title": "Sane and well-measured advice is therefore wasted on me, and I just wanted to edit in this notice here so that other  well-meaning folks don't get tricked into wasting their time  trying to talk sense into a total nutcase like me. :)", "anchor": "Sane_and_well_measured_advice_is_therefore_wasted_on_me__and_I_just_wanted_to_edit_in_this_notice_here_so_that_other__well_meaning_folks_don_t_get_tricked_into_wasting_their_time__trying_to_talk_sense_into_a_total_nutcase_like_me____", "level": 1}, {"title": "(I appreciate all y'all, though. ^^ ) ]", "anchor": "_I_appreciate_all_y_all__though________", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "65 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-22T08:58:37.532Z", "modifiedAt": null, "url": null, "title": "Are \u2018Evidence-based\u2019 policies damaging policymaking?", "slug": "are-evidence-based-policies-damaging-policymaking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:58.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "quantropy", "createdAt": "2010-10-03T15:59:45.882Z", "isAdmin": false, "displayName": "quantropy"}, "userId": "LvHrBB8YZXS5sk68B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sEeo84MkidXcJLe5C/are-evidence-based-policies-damaging-policymaking", "pageUrlRelative": "/posts/sEeo84MkidXcJLe5C/are-evidence-based-policies-damaging-policymaking", "linkUrl": "https://www.lesswrong.com/posts/sEeo84MkidXcJLe5C/are-evidence-based-policies-damaging-policymaking", "postedAtFormatted": "Thursday, August 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20%E2%80%98Evidence-based%E2%80%99%20policies%20damaging%20policymaking%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20%E2%80%98Evidence-based%E2%80%99%20policies%20damaging%20policymaking%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEeo84MkidXcJLe5C%2Fare-evidence-based-policies-damaging-policymaking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20%E2%80%98Evidence-based%E2%80%99%20policies%20damaging%20policymaking%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEeo84MkidXcJLe5C%2Fare-evidence-based-policies-damaging-policymaking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEeo84MkidXcJLe5C%2Fare-evidence-based-policies-damaging-policymaking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 286, "htmlBody": "<blockquote>\n<p>Both the government and the public should be far more sceptical about policies which are purported to be &lsquo;evidence-based&rsquo;, argues new research released today by the Institute of Economic Affairs.</p>\n<p>In <em>Quack Policy &ndash; Abusing Science in the Cause of Paternalism</em>, Jamie Whyte exposes how politicians promote regulations and taxes under the justification of scientific evidence, yet the experts promoting these policies often make basic errors and have little or no grasp of economics.</p>\n</blockquote>\n<p>http://www.iea.org.uk/in-the-media/press-release/%E2%80%98evidence-based%E2%80%99-policies-are-damaging-uk-policymaking</p>\n<p>Those in favour of evidenced based policies have tended to be dismissive, arguing that this is just a case of lack of evidence, rather than a problem with evidence-based policies per se.</p>\n<p>https://twitter.com/bengoldacre/status/370159847985381376</p>\n<p>I'm not fully convinced though.&nbsp; I've started reading <em>Taking the medicine: a short history of medicine's beautiful idea and our difficulty swallowing it</em> by Druin Burch, and this has plenty of examples of people saying something like \"Of course, up till now medicine has been totally wrong, but I've got a new way of doing it right\" - and then going on to make the same old mistakes. Maybe evidence based practice is the same, just a way of convincing ourselves that we've got it right this time.</p>\n<p>As I see it, the trouble with evidence based practice is that what counts as evidence based today doesn't meet tomorrows higher standards.&nbsp; This means</p>\n<ol>\n<li>How can we trust evidence based practice if it might be overturned tomorrow?</li>\n<li>As less gets to count as evidence, evidence based practice gets harder and harder to do.</li>\n</ol>\n<p>At the root of this is the insistence that evidence must meet some 'gold standard'.&nbsp; This seems to be too much the frequentist viewpoint, but in the end we have to be Bayesians, taking all evidence into account.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sEeo84MkidXcJLe5C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -15, "extendedScore": null, "score": -3.3e-05, "legacy": true, "legacyId": "23823", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-22T19:31:09.513Z", "modifiedAt": null, "url": null, "title": "Another Anki deck for Less Wrong content", "slug": "another-anki-deck-for-less-wrong-content", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:59.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MondSemmel", "createdAt": "2013-01-17T18:33:25.683Z", "isAdmin": false, "displayName": "MondSemmel"}, "userId": "6tLz8Q6yaejBENSzC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u53R6AmLHBcCgaA52/another-anki-deck-for-less-wrong-content", "pageUrlRelative": "/posts/u53R6AmLHBcCgaA52/another-anki-deck-for-less-wrong-content", "linkUrl": "https://www.lesswrong.com/posts/u53R6AmLHBcCgaA52/another-anki-deck-for-less-wrong-content", "postedAtFormatted": "Thursday, August 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20Anki%20deck%20for%20Less%20Wrong%20content&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20Anki%20deck%20for%20Less%20Wrong%20content%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu53R6AmLHBcCgaA52%2Fanother-anki-deck-for-less-wrong-content%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20Anki%20deck%20for%20Less%20Wrong%20content%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu53R6AmLHBcCgaA52%2Fanother-anki-deck-for-less-wrong-content", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu53R6AmLHBcCgaA52%2Fanother-anki-deck-for-less-wrong-content", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 404, "htmlBody": "<p>Anki decks of Less Wrong content have been shared here <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition#SR_decks\">before</a>. However, they felt a bit huge (one deck was &gt;1500 cards) and/or not helpful to me. As I go through the sequences, I create Anki cards, and I've decided they are at a point where I can share them. Maybe someone else will benefit from them.</p>\n<p>Current content: <a href=\"https://ankiweb.net/shared/info/1691010092\">The deck</a> currently consists of 186 Anki cards (82 Q&amp;A, 104 cloze deletion), covering the following Less Wrong sequences: The Map and the Territory, Mysterious Answers to Mysterious Questions, How to Actually Change Your Mind, A Human's Guide to Words, and Reductionism.<br />All cards contain an extra field for their source, usually 1-2 Less Wrong posts, rarely a link to Wikipedia. Some mathy cards use LaTeX. I don't know what happens if you don't have LateX installed. Though if this is a problem, I think I can convert the LaTeX code to images with an Anki plugin.</p>\n<p>Important caveats:</p>\n<ol>\n<li>My cards tend to have more context than those I've seen in most other decks, to the point that one might consider them overloaded with information. That's partly due to personal preference, and partly because I need as much context as possible so I memorize more than just a teacher's <a href=\"/lw/iq/guessing_the_teachers_password/\">password</a>.</li>\n<li>In contrast to previously shared Anki decks of Less Wrong content, I do not aim to make this deck comprehensive. Rather, I create cards for content which I understood <em>and </em>which seems suitable for memorization <em>and </em>which seemed particularly useful to me. Conversely, I did not create cards when I couldn't think of a way to memorize something, or when I did not understand (the usefulness of) something. (For instance, <a href=\"/lw/k7/original_seeing/\">Original Seeing</a> and <a href=\"/lw/k3/priming_and_contamination/\">Priming and Contamination</a> did not work for me.)</li>\n<li>I've tried a few shared decks so far, and everybody seems to create cards differently. So I'm not sure to which extent this deck can be useful to anyone who isn't me.</li>\n</ol>\n<p>Open question: I'm still not sure to which extent I'm memorizing internalized and understood <em>knowledge</em> with these cards, and to which extent they are just fake explanations or attempts to guess at passwords.</p>\n<p>And a final disclaimer: The content is mostly taken verbatim from Yudkowsky's sequences, though I've often edited the text so it fit better as an Anki card. I checked the cards thoroughly before making the deck public, but any remaining errors are mine.</p>\n<p>I'm thankful for suggestions and other feedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u53R6AmLHBcCgaA52", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "23826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMoLJuDJEms7Ku9XS", "SA79JMXKWke32A3hG", "BaCWFCxBQYjJXSsah"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-23T00:41:46.330Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia - Humans are not automatically strategic", "slug": "meetup-philadelphia-humans-are-not-automatically-strategic", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mare-of-night", "createdAt": "2013-04-06T13:26:03.532Z", "isAdmin": false, "displayName": "mare-of-night"}, "userId": "6thzLTpEnEpcZF8bf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yZ47fjTHJvmg438Bd/meetup-philadelphia-humans-are-not-automatically-strategic", "pageUrlRelative": "/posts/yZ47fjTHJvmg438Bd/meetup-philadelphia-humans-are-not-automatically-strategic", "linkUrl": "https://www.lesswrong.com/posts/yZ47fjTHJvmg438Bd/meetup-philadelphia-humans-are-not-automatically-strategic", "postedAtFormatted": "Friday, August 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia%20-%20Humans%20are%20not%20automatically%20strategic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia%20-%20Humans%20are%20not%20automatically%20strategic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZ47fjTHJvmg438Bd%2Fmeetup-philadelphia-humans-are-not-automatically-strategic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia%20-%20Humans%20are%20not%20automatically%20strategic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZ47fjTHJvmg438Bd%2Fmeetup-philadelphia-humans-are-not-automatically-strategic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZ47fjTHJvmg438Bd%2Fmeetup-philadelphia-humans-are-not-automatically-strategic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q6'>Philadelphia - Humans are not automatically strategic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 August 2013 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1515 Chestnut St, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting at Giovani Pizza at 4 PM on Sunday to discuss the article <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>. If you don't have time to read the article, come out anyway. We welcome your thoughts on the subject regardless. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q6'>Philadelphia - Humans are not automatically strategic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yZ47fjTHJvmg438Bd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3125909199639385e-06, "legacy": true, "legacyId": "23828", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Philadelphia___Humans_are_not_automatically_strategic\">Discussion article for the meetup : <a href=\"/meetups/q6\">Philadelphia - Humans are not automatically strategic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 August 2013 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1515 Chestnut St, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting at Giovani Pizza at 4 PM on Sunday to discuss the article <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>. If you don't have time to read the article, come out anyway. We welcome your thoughts on the subject regardless. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Philadelphia___Humans_are_not_automatically_strategic1\">Discussion article for the meetup : <a href=\"/meetups/q6\">Philadelphia - Humans are not automatically strategic</a></h2>", "sections": [{"title": "Discussion article for the meetup : Philadelphia - Humans are not automatically strategic", "anchor": "Discussion_article_for_the_meetup___Philadelphia___Humans_are_not_automatically_strategic", "level": 1}, {"title": "Discussion article for the meetup : Philadelphia - Humans are not automatically strategic", "anchor": "Discussion_article_for_the_meetup___Philadelphia___Humans_are_not_automatically_strategic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-23T07:56:28.490Z", "modifiedAt": null, "url": null, "title": "Rationalist households: What can London learn from its predecessors?", "slug": "rationalist-households-what-can-london-learn-from-its", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:35.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KnaveOfAllTrades", "createdAt": "2012-07-20T02:08:23.538Z", "isAdmin": false, "displayName": "KnaveOfAllTrades"}, "userId": "FuACexYrBpyrMmz5C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2LzLPjex8krLdTZwo/rationalist-households-what-can-london-learn-from-its", "pageUrlRelative": "/posts/2LzLPjex8krLdTZwo/rationalist-households-what-can-london-learn-from-its", "linkUrl": "https://www.lesswrong.com/posts/2LzLPjex8krLdTZwo/rationalist-households-what-can-london-learn-from-its", "postedAtFormatted": "Friday, August 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20households%3A%20What%20can%20London%20learn%20from%20its%20predecessors%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20households%3A%20What%20can%20London%20learn%20from%20its%20predecessors%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LzLPjex8krLdTZwo%2Frationalist-households-what-can-london-learn-from-its%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20households%3A%20What%20can%20London%20learn%20from%20its%20predecessors%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LzLPjex8krLdTZwo%2Frationalist-households-what-can-london-learn-from-its", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LzLPjex8krLdTZwo%2Frationalist-households-what-can-london-learn-from-its", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>At our <a href=\"/lw/ia1/meetup_london_social_the_unwelcome_but_probable/\">most recent</a> meetup, the London LessWrongers began discussion of setting up one or more houses in the capital. This thread is intended for discussion and advice on planning &lsquo;rationalist households&rsquo; and on making them thrive. You can also register your interest in being part of a London, UK rationalist house <a href=\"/r/discussion/lw/idx/rationalist_households_what_can_london_learn_from/9mmh\">here</a>.<strong><br /></strong><strong><br />Those who currently live in or have previously lived in rationalist households</strong>, or who have relevant experience, are particularly encouraged to share their experiences, and any data on house setups is most welcome. It would be great if we could get case studies of several rationalist households, to compare approaches and aid other organizers.<br /><br />We&rsquo;re considering having a room for visitors and people who are only in the city for part of the year, with an <a href=\"http://en.wikipedia.org/wiki/Airbnb\">Airbnb</a>-type arrangement for that room at other times. Therefore, we are <strong>seeking advice from Airbnb hosts</strong> on setting this up, as well as on its advantages and disadvantages.<br /><br />We would also <strong>like to hear about the common pitfalls</strong> of group living in order to avoid making basic errors.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2LzLPjex8krLdTZwo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "23829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3fHmtFKNMpHbedQH8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-23T13:15:56.214Z", "modifiedAt": null, "url": null, "title": "Lesswrong Philosophy and Personal Identity", "slug": "lesswrong-philosophy-and-personal-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CKWL7yjwAwnZri2o4/lesswrong-philosophy-and-personal-identity", "pageUrlRelative": "/posts/CKWL7yjwAwnZri2o4/lesswrong-philosophy-and-personal-identity", "linkUrl": "https://www.lesswrong.com/posts/CKWL7yjwAwnZri2o4/lesswrong-philosophy-and-personal-identity", "postedAtFormatted": "Friday, August 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lesswrong%20Philosophy%20and%20Personal%20Identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALesswrong%20Philosophy%20and%20Personal%20Identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKWL7yjwAwnZri2o4%2Flesswrong-philosophy-and-personal-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lesswrong%20Philosophy%20and%20Personal%20Identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKWL7yjwAwnZri2o4%2Flesswrong-philosophy-and-personal-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCKWL7yjwAwnZri2o4%2Flesswrong-philosophy-and-personal-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 225, "htmlBody": "<p>Although Elizier has dealt with personal identity questions (in terms of ruling out the body theory), he has not actually, as far as&nbsp;I know, \"solved\" the problem of Personal Identity as it is understood in philosophy. Nor,&nbsp;as far as I know, has any thinker (Robin Hanson, Yvain, etc) broadly in the same school of thought. &nbsp;</p>\r\n<p>Why do I think it worth solving? One- Lesswrong has a tradition of trying to solve all of philosophy through thinking better than philosophers do. Even when I don't agree with it,&nbsp;the result is often enlightening.&nbsp;Two-&nbsp;What counts as 'same person'&nbsp;could easily&nbsp;have&nbsp;significant implications for large numbers of ethical dilemnas, and thus for Lesswrongian ethics.&nbsp;&nbsp;&nbsp;</p>\r\n<p>Three- most importantly of all, the correct theory has practical implications for cryonics. I don't know enough to assert any theory as actually true, but if, say, Identity as Continuity of Form rather than of Matter were the true theory it would mean that preserving only the mental data would not be enough. What kind of preservation is necessary also varies somewhat- the difference in requirement based on a Continuity of Consciousness v.s a Continuity of Psyche theory, for example&nbsp;should be obvious.</p>\r\n<p>I'm curious what people here think. What is the correct answer? No-self theory? Psyche theory? Derek Parfit's theory in some manner? Or if there is a correct way to dissolve the question, what is that correct way?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2fa": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CKWL7yjwAwnZri2o4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 13, "extendedScore": null, "score": 1.313232217361924e-06, "legacy": true, "legacyId": "23831", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-23T13:59:04.896Z", "modifiedAt": null, "url": null, "title": "Meetup : Comfort Zone Expansion outing - London", "slug": "meetup-comfort-zone-expansion-outing-london", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:03.501Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ndJpbw73GAMnPhkHS/meetup-comfort-zone-expansion-outing-london", "pageUrlRelative": "/posts/ndJpbw73GAMnPhkHS/meetup-comfort-zone-expansion-outing-london", "linkUrl": "https://www.lesswrong.com/posts/ndJpbw73GAMnPhkHS/meetup-comfort-zone-expansion-outing-london", "postedAtFormatted": "Friday, August 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Comfort%20Zone%20Expansion%20outing%20-%20London&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Comfort%20Zone%20Expansion%20outing%20-%20London%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FndJpbw73GAMnPhkHS%2Fmeetup-comfort-zone-expansion-outing-london%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Comfort%20Zone%20Expansion%20outing%20-%20London%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FndJpbw73GAMnPhkHS%2Fmeetup-comfort-zone-expansion-outing-london", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FndJpbw73GAMnPhkHS%2Fmeetup-comfort-zone-expansion-outing-london", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 503, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q7'>Comfort Zone Expansion outing - London</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 September 2013 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Westfield Shopping Center, Shepherds Bush, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LW London will be running a Comfort Zone Expansion (CoZE) outing on 1st September.</p>\n\n<p>A lot of things seem scarier than, on reflection, they should do. CoZE is a technique to recalibrate our fears, especially related to social situations. The way to expand one's comfort zone is to do things that are just on the edge of it, so that's what we'll be doing. In an imitation of part of the CFAR workshops, we'll be heading to a mall to split up, do slightly scary things, and then meet back together to debrief and discuss.</p>\n\n<p>Prior to the mall, we'll be meeting somewhere to discuss CoZE in more depth: what we're trying to do, why we're trying to do it, some things we're absolutely not trying to do, etc. Safety briefing.</p>\n\n<p>We'll meet at <strong>11am</strong>, but I haven't yet worked out exactly where that will be; if anyone has suggestions for a somewhat-private space near the mall we can go to, please let me know. (Basically we want to be able to discuss things that make us uncomfortable, and the fewer people nearby, the better. For people who've been coming to the social meetups, somewhere like the dungeons in the Old Star would probably be fine. If the weather's nice then Shepherds Bush Common might work, but I can't rely on that.) I'll work somewhere out this week and update before the day of the outing.</p>\n\n<p>[<strong>Update</strong>: The weather for tomorrow looks good, so we'll meet somewhere on Shepherds Bush Common: <a href=\"http://goo.gl/maps/byGhz\" rel=\"nofollow\">map</a>. If we're not easy to find, give me a call and I can be more specific.</p>\n\n<p>In the event that the weather does fail, we'll meet in the Defector's Weld, which is right on the north-west corner of the common.]</p>\n\n<p>After that, hopefully we'll be able to play some improv games as a warm-up. This depends on time and whether or not we're likely to annoy lots of people if we play.</p>\n\n<p>In any case, we'll head to the mall no later than 1.30pm. (Probably sooner; I'm keeping times flexible because I don't really know how long the discussion will take.) Then we'll split up for the outing itself. We'll meet back up after an hour and a half, and head somewhere else to debrief: success stories, things that didn't work, anything that seems relevant.</p>\n\n<p>Please bring a mobile phone so I can get in touch with everyone if necessary.</p>\n\n<p>If you feel nervous, that's understandable. Remember you'll never have to do anything you don't want to; and feel free to contact me privately, by PM, text message or facebook.</p>\n\n<p>RSVPs are appreciated, so that I have some idea of numbers; if you don't know whether or not you're coming, feel free to say that, optionally with a probability estimate; but if you want to come and haven't RSVPed, please come anyway.</p>\n\n<p>My phone number is 07792009646.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q7'>Comfort Zone Expansion outing - London</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ndJpbw73GAMnPhkHS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Comfort_Zone_Expansion_outing___London\">Discussion article for the meetup : <a href=\"/meetups/q7\">Comfort Zone Expansion outing - London</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 September 2013 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Westfield Shopping Center, Shepherds Bush, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LW London will be running a Comfort Zone Expansion (CoZE) outing on 1st September.</p>\n\n<p>A lot of things seem scarier than, on reflection, they should do. CoZE is a technique to recalibrate our fears, especially related to social situations. The way to expand one's comfort zone is to do things that are just on the edge of it, so that's what we'll be doing. In an imitation of part of the CFAR workshops, we'll be heading to a mall to split up, do slightly scary things, and then meet back together to debrief and discuss.</p>\n\n<p>Prior to the mall, we'll be meeting somewhere to discuss CoZE in more depth: what we're trying to do, why we're trying to do it, some things we're absolutely not trying to do, etc. Safety briefing.</p>\n\n<p>We'll meet at <strong>11am</strong>, but I haven't yet worked out exactly where that will be; if anyone has suggestions for a somewhat-private space near the mall we can go to, please let me know. (Basically we want to be able to discuss things that make us uncomfortable, and the fewer people nearby, the better. For people who've been coming to the social meetups, somewhere like the dungeons in the Old Star would probably be fine. If the weather's nice then Shepherds Bush Common might work, but I can't rely on that.) I'll work somewhere out this week and update before the day of the outing.</p>\n\n<p>[<strong>Update</strong>: The weather for tomorrow looks good, so we'll meet somewhere on Shepherds Bush Common: <a href=\"http://goo.gl/maps/byGhz\" rel=\"nofollow\">map</a>. If we're not easy to find, give me a call and I can be more specific.</p>\n\n<p>In the event that the weather does fail, we'll meet in the Defector's Weld, which is right on the north-west corner of the common.]</p>\n\n<p>After that, hopefully we'll be able to play some improv games as a warm-up. This depends on time and whether or not we're likely to annoy lots of people if we play.</p>\n\n<p>In any case, we'll head to the mall no later than 1.30pm. (Probably sooner; I'm keeping times flexible because I don't really know how long the discussion will take.) Then we'll split up for the outing itself. We'll meet back up after an hour and a half, and head somewhere else to debrief: success stories, things that didn't work, anything that seems relevant.</p>\n\n<p>Please bring a mobile phone so I can get in touch with everyone if necessary.</p>\n\n<p>If you feel nervous, that's understandable. Remember you'll never have to do anything you don't want to; and feel free to contact me privately, by PM, text message or facebook.</p>\n\n<p>RSVPs are appreciated, so that I have some idea of numbers; if you don't know whether or not you're coming, feel free to say that, optionally with a probability estimate; but if you want to come and haven't RSVPed, please come anyway.</p>\n\n<p>My phone number is 07792009646.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Comfort_Zone_Expansion_outing___London1\">Discussion article for the meetup : <a href=\"/meetups/q7\">Comfort Zone Expansion outing - London</a></h2>", "sections": [{"title": "Discussion article for the meetup : Comfort Zone Expansion outing - London", "anchor": "Discussion_article_for_the_meetup___Comfort_Zone_Expansion_outing___London", "level": 1}, {"title": "Discussion article for the meetup : Comfort Zone Expansion outing - London", "anchor": "Discussion_article_for_the_meetup___Comfort_Zone_Expansion_outing___London1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-23T15:38:23.693Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-129", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:57.497Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kEJtovPxjeX3QZRL4/weekly-lw-meetups-129", "pageUrlRelative": "/posts/kEJtovPxjeX3QZRL4/weekly-lw-meetups-129", "linkUrl": "https://www.lesswrong.com/posts/kEJtovPxjeX3QZRL4/weekly-lw-meetups-129", "postedAtFormatted": "Friday, August 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkEJtovPxjeX3QZRL4%2Fweekly-lw-meetups-129%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkEJtovPxjeX3QZRL4%2Fweekly-lw-meetups-129", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkEJtovPxjeX3QZRL4%2Fweekly-lw-meetups-129", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 567, "htmlBody": "<p><strong>This summary was posted to LW Main on August 16th. The following week's summary is <a href=\"/lw/ie2/new_lw_meetup_urbanachampaign/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/pk\">Saskatoon's First Meetup!:&nbsp;<span class=\"date\">17 August 2013 01:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/pr\">Atlanta LessWrong: Games Night:&nbsp;<span class=\"date\">24 August 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/py\">Bratislava Meetup VI.:&nbsp;<span class=\"date\">19 August 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/p7\">Brussels meetup:&nbsp;<span class=\"date\">17 August 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/pu\">Helsinki Meetup:&nbsp;<span class=\"date\">18 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/pn\">LessWrong Israel September meetup:&nbsp;<span class=\"date\">12 September 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/pg\">[Lyon] Picnic au Parc de la T&ecirc;te d'Or:&nbsp;<span class=\"date\">17 August 2013 12:30PM</span></a></li>\n<li><a href=\"/meetups/pz\">Moscow: The Sunday Meetup:&nbsp;<span class=\"date\">18 August 2013 04:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">17 August 2019 01:30PM</span></a><a href=\"/meetups/pr\"></a></li>\n<li><a href=\"/meetups/pq\">London Social - The Unwelcome but Probable Decline and Fall of Direct Sunlight:&nbsp;<span class=\"date\">18 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ph\">Vienna Meetup:&nbsp;<span class=\"date\">17 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pv\">Washington DC Lojban meetup:&nbsp;<span class=\"date\">18 August 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/px\">[Washington DC] Robin Hanson visits to talk about prediction markets:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kEJtovPxjeX3QZRL4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3133534174409104e-06, "legacy": true, "legacyId": "23764", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8R7qMeggx3Ms9aZAA", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-23T21:29:58.530Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014Nothing in Particular", "slug": "meetup-west-la-nothing-in-particular", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6zqxq66Q5PfeRz4ey/meetup-west-la-nothing-in-particular", "pageUrlRelative": "/posts/6zqxq66Q5PfeRz4ey/meetup-west-la-nothing-in-particular", "linkUrl": "https://www.lesswrong.com/posts/6zqxq66Q5PfeRz4ey/meetup-west-la-nothing-in-particular", "postedAtFormatted": "Friday, August 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94Nothing%20in%20Particular&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94Nothing%20in%20Particular%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zqxq66Q5PfeRz4ey%2Fmeetup-west-la-nothing-in-particular%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94Nothing%20in%20Particular%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zqxq66Q5PfeRz4ey%2Fmeetup-west-la-nothing-in-particular", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zqxq66Q5PfeRz4ey%2Fmeetup-west-la-nothing-in-particular", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q8'>West LA\u2014Nothing in Particular</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 August 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you apply some advanced trickery.</p>\n\n<p><strong>Discussion</strong>: We will talk about whatever. No topic will be enforced. No presentation will be given. No one will be \"playing host\", and no games will be played.</p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://www.quora.com/What-are-some-must-read-Less-Wrong-sequences/answer/George-Koleszarik/\" rel=\"nofollow\">The Less Wrong Sequences</a></li>\n<li><a href=\"http://www.cs.tut.fi/~jkorpela/wiio.html\" rel=\"nofollow\">Wiio&#39;s Laws</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Metabooks/G%C3%B6del%2C%20Escher%2C%20Bach.pdf\" rel=\"nofollow\">G\u00f6del, Escher, Bach: An Eternal Golden Braid</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Game%20theory/Thomas_C._Schelling_The_Strategy_of_Conflict__1981.pdf\" rel=\"nofollow\">The Strategy of Conflict</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/self-help/How%20to%20Win%20Friends%20and%20Influence%20People.epub\" rel=\"nofollow\">How to Win Friends and Influence People</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Mathematics/Penrose__The%20Road%20to%20Reality.pdf\" rel=\"nofollow\">The Road to Reality</a></li>\n<li><a href=\"http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-4.html#%_toc_start\" rel=\"nofollow\">The Structure and Interpretation of Computer Programs</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. In fact, due to the Zen nature of this week's topic, it is unlikely that anyone will even be able to identify us as a Less Wrong meetup.</p>\n\n<p>There will definitely not be a whiteboard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q8'>West LA\u2014Nothing in Particular</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6zqxq66Q5PfeRz4ey", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3136526192569815e-06, "legacy": true, "legacyId": "23836", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Nothing_in_Particular\">Discussion article for the meetup : <a href=\"/meetups/q8\">West LA\u2014Nothing in Particular</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 August 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you apply some advanced trickery.</p>\n\n<p><strong>Discussion</strong>: We will talk about whatever. No topic will be enforced. No presentation will be given. No one will be \"playing host\", and no games will be played.</p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://www.quora.com/What-are-some-must-read-Less-Wrong-sequences/answer/George-Koleszarik/\" rel=\"nofollow\">The Less Wrong Sequences</a></li>\n<li><a href=\"http://www.cs.tut.fi/~jkorpela/wiio.html\" rel=\"nofollow\">Wiio's Laws</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Metabooks/G%C3%B6del%2C%20Escher%2C%20Bach.pdf\" rel=\"nofollow\">G\u00f6del, Escher, Bach: An Eternal Golden Braid</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Game%20theory/Thomas_C._Schelling_The_Strategy_of_Conflict__1981.pdf\" rel=\"nofollow\">The Strategy of Conflict</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/self-help/How%20to%20Win%20Friends%20and%20Influence%20People.epub\" rel=\"nofollow\">How to Win Friends and Influence People</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Mathematics/Penrose__The%20Road%20to%20Reality.pdf\" rel=\"nofollow\">The Road to Reality</a></li>\n<li><a href=\"http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-4.html#%_toc_start\" rel=\"nofollow\">The Structure and Interpretation of Computer Programs</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. In fact, due to the Zen nature of this week's topic, it is unlikely that anyone will even be able to identify us as a Less Wrong meetup.</p>\n\n<p>There will definitely not be a whiteboard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Nothing_in_Particular1\">Discussion article for the meetup : <a href=\"/meetups/q8\">West LA\u2014Nothing in Particular</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014Nothing in Particular", "anchor": "Discussion_article_for_the_meetup___West_LA_Nothing_in_Particular", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014Nothing in Particular", "anchor": "Discussion_article_for_the_meetup___West_LA_Nothing_in_Particular1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-24T05:57:48.169Z", "modifiedAt": null, "url": null, "title": "How Efficient is the Charitable Market?", "slug": "how-efficient-is-the-charitable-market", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JBKrSNEejyE7nYmqq/how-efficient-is-the-charitable-market", "pageUrlRelative": "/posts/JBKrSNEejyE7nYmqq/how-efficient-is-the-charitable-market", "linkUrl": "https://www.lesswrong.com/posts/JBKrSNEejyE7nYmqq/how-efficient-is-the-charitable-market", "postedAtFormatted": "Saturday, August 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Efficient%20is%20the%20Charitable%20Market%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Efficient%20is%20the%20Charitable%20Market%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBKrSNEejyE7nYmqq%2Fhow-efficient-is-the-charitable-market%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Efficient%20is%20the%20Charitable%20Market%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBKrSNEejyE7nYmqq%2Fhow-efficient-is-the-charitable-market", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBKrSNEejyE7nYmqq%2Fhow-efficient-is-the-charitable-market", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 711, "htmlBody": "<p>When I talk about the <a href=\"http://www.amazon.com/With-Charity-All-Charities-Failing/dp/038553471X/\">poor distribution of funds in charity</a>, people in the <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">effective altruism</a> movement sometimes say, \"Didn't Holden Karnofsky show that charity is an efficient market in his post <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">Broad Market Efficiency</a>?\"</p>\n<p>My reply is \"No. Holden never said, and doesn't believe, that charity is an efficient market.\"</p>\n<p>&nbsp;</p>\n<h4>What is an efficient market?</h4>\n<p>An efficient market is one <a href=\"http://en.wikipedia.org/wiki/Efficient-market_hypothesis\">in which</a> \"one cannot consistently achieve returns in excess of average market returns... given the information available at the time the investment is made.\" (Details <a href=\"http://highered.mcgraw-hill.com/sites/dl/free/007338240x/773409/Sample_Chapter_8_New.pdf\">here</a>.)</p>\n<p>Of course, market efficiency is a spectrum, not a yes/no question. As Holden writes, \"The most efficient markets can be consistently beaten only by the most talented/dedicated players, while the least efficient [markets] can be beaten with fairly little in the way of talent and dedication.\"</p>\n<p>Moreover, market efficiency is multi-dimensional. Any particular market may be efficient in some ways, and in some domains, while highly inefficient in other ways and other domains.</p>\n<p><a id=\"more\"></a></p>\n<h4><br /></h4>\n<h4>Charity as an inefficient market</h4>\n<p>Financial markets are relatively efficient. It's rare for players to consistently beat the market by a large margin. You can beat the average by investing in a <a href=\"https://personal.vanguard.com/us/whatweoffer/mutualfundinvesting/indexfunds\">low-fee index fund</a>, but not by a lot, and it's hard to beat hedge funds.</p>\n<p>Philanthropic markets appear to be less efficient than financial markets in many ways. In charity, one can consistently beat the market by a wide margin simply by giving to <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's recommended charities</a>, which achieve far greater returns (in social value) per marginal dollar than the average charity does. However, Holden <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">points out</a> that it has been surprisingly difficult for GiveWell to find ways to beat <em>well-run large foundations</em> like the <a href=\"http://www.gatesfoundation.org/\">Gates Foundation</a>'s work in global health.</p>\n<p>Why should we expect charity to be less efficient than financial markets?</p>\n<p>For one thing, most people giving to charity don't even seem to <em>care</em> what returns (in social value) they're getting with their investments. That's why, when proto-GiveWell initially contacted a bunch of charities to ask for evidence of positive impact, some of those charities reported that nobody who gave them money had ever <em>asked</em> that question before. And when charities sent proto-GiveWell their internal reports about effectiveness, they were so inadequate that they \"led [proto-GiveWell] to understand that the charities <em>themselves</em> did not know whether they were helping or hurting a given situation\" (<a href=\"http://www.amazon.com/With-Charity-All-Charities-Failing/dp/038553471X/\">Stern 2012</a>).</p>\n<p>For another thing, \"market incentives of the nonprofit world push charities toward happy anecdote and inspiring narrative rather than toward careful planning, research, and evidence-based investments\" (details <a href=\"http://www.amazon.com/With-Charity-All-Charities-Failing/dp/038553471X/\">here</a>).</p>\n<p>Also, as Brian Tomasik <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/comment-page-1/#comment-542070\">notes</a>, \"Efficiency in the realm of charity is inherently less plausible than in financial markets because in charity there&rsquo;s not a common unit of what 'good' means... Indeed, one man&rsquo;s good may be another man&rsquo;s bad (e.g., abortion, gun control, extinction risks).\" But even when we focus on relatively common units of 'good' (e.g. human welfare, QALYs, or DALYs), charity is still relatively inefficient: we can easily purchase more QALYs per dollar via <a href=\"http://www.givewell.org/international/top-charities/AMF\">AMF</a> than via, say, the popular <a href=\"http://en.wikipedia.org/wiki/Make-A-Wish_Foundation\">Make-a-Wish Foundation</a>.</p>\n<p>&nbsp;</p>\n<h4>What is \"broad market efficiency\", then?</h4>\n<p>If Holden agrees that philanthropic markets are relatively inefficient in the sense that it's easy to consistently and substantially beat average market returns by giving to GiveWell's recommended charities, then what does he mean by \"broad market efficiency\"? Holden introduces \"broad market efficiency\" as a term for the <em>spectrum</em> of market efficiency, but remains uncertain as to where charity falls on that spectrum of market efficiency.</p>\n<p>Brian Tomasik <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/comment-page-1/#comment-542070\">worried</a> that the term \"broad market efficiency\" would confuse some readers into thinking Holden was claiming that philanthropic markets are relatively efficient and thus that \"it doesn&rsquo;t really matter where you donate.\" Holden <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/comment-page-1/#comment-542975\">said</a> he wasn't worried about this, saying, \"I don&rsquo;t think 'broad market efficiency' is a common phrase or one with a clear meaning.\" But I think the phrase <em>is</em> confusing, that many readers interpret it as meaning \"market efficiency,\" and indeed that people in economics and finance sometimes use it that way: search for the phrase \"broad market efficiency\" <a href=\"[https://www2.blackrock.com/webcore/litService/search/getDocument.seam?venue=PUB_INS&amp;source=CONTENT&amp;ServiceName=PublicServiceView&amp;ContentID=1111183580](http://commonsenseatheism.com/wp-content/uploads/2013/08/Singal-Beyond-the-Random-Walk-Preface.pdf\">here</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/Blackrock-Securities-Lending-unlocking-the-full-potential-of-investment-porfolios.pdf\">here</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/Ooi-et-al-Extrapolation-theory-and-hte-pricing-of-REIT-stocks.pdf\">here</a>, and <a href=\"http://www.greaterfool.ca/2011/04/10/depraved/#comment-94887\">here</a>.</p>\n<p>&nbsp;</p>\n<h4>The research ahead</h4>\n<p>So how efficient <em>is</em> the charitable market, and in which ways? My own guess is that it's far less efficient than financial markets, but GiveWell's research has <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">provided</a> valuable and surprising (to me) information on this topic, and I look forward to future discoveries.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 2, "JsJPrdgRGRqnci8cZ": 2, "pnSDArjzAjkvAF5Jo": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JBKrSNEejyE7nYmqq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 25, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "23839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>When I talk about the <a href=\"http://www.amazon.com/With-Charity-All-Charities-Failing/dp/038553471X/\">poor distribution of funds in charity</a>, people in the <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">effective altruism</a> movement sometimes say, \"Didn't Holden Karnofsky show that charity is an efficient market in his post <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">Broad Market Efficiency</a>?\"</p>\n<p>My reply is \"No. Holden never said, and doesn't believe, that charity is an efficient market.\"</p>\n<p>&nbsp;</p>\n<h4 id=\"What_is_an_efficient_market_\">What is an efficient market?</h4>\n<p>An efficient market is one <a href=\"http://en.wikipedia.org/wiki/Efficient-market_hypothesis\">in which</a> \"one cannot consistently achieve returns in excess of average market returns... given the information available at the time the investment is made.\" (Details <a href=\"http://highered.mcgraw-hill.com/sites/dl/free/007338240x/773409/Sample_Chapter_8_New.pdf\">here</a>.)</p>\n<p>Of course, market efficiency is a spectrum, not a yes/no question. As Holden writes, \"The most efficient markets can be consistently beaten only by the most talented/dedicated players, while the least efficient [markets] can be beaten with fairly little in the way of talent and dedication.\"</p>\n<p>Moreover, market efficiency is multi-dimensional. Any particular market may be efficient in some ways, and in some domains, while highly inefficient in other ways and other domains.</p>\n<p><a id=\"more\"></a></p>\n<h4><br></h4>\n<h4 id=\"Charity_as_an_inefficient_market\">Charity as an inefficient market</h4>\n<p>Financial markets are relatively efficient. It's rare for players to consistently beat the market by a large margin. You can beat the average by investing in a <a href=\"https://personal.vanguard.com/us/whatweoffer/mutualfundinvesting/indexfunds\">low-fee index fund</a>, but not by a lot, and it's hard to beat hedge funds.</p>\n<p>Philanthropic markets appear to be less efficient than financial markets in many ways. In charity, one can consistently beat the market by a wide margin simply by giving to <a href=\"http://www.givewell.org/charities/top-charities\">GiveWell's recommended charities</a>, which achieve far greater returns (in social value) per marginal dollar than the average charity does. However, Holden <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">points out</a> that it has been surprisingly difficult for GiveWell to find ways to beat <em>well-run large foundations</em> like the <a href=\"http://www.gatesfoundation.org/\">Gates Foundation</a>'s work in global health.</p>\n<p>Why should we expect charity to be less efficient than financial markets?</p>\n<p>For one thing, most people giving to charity don't even seem to <em>care</em> what returns (in social value) they're getting with their investments. That's why, when proto-GiveWell initially contacted a bunch of charities to ask for evidence of positive impact, some of those charities reported that nobody who gave them money had ever <em>asked</em> that question before. And when charities sent proto-GiveWell their internal reports about effectiveness, they were so inadequate that they \"led [proto-GiveWell] to understand that the charities <em>themselves</em> did not know whether they were helping or hurting a given situation\" (<a href=\"http://www.amazon.com/With-Charity-All-Charities-Failing/dp/038553471X/\">Stern 2012</a>).</p>\n<p>For another thing, \"market incentives of the nonprofit world push charities toward happy anecdote and inspiring narrative rather than toward careful planning, research, and evidence-based investments\" (details <a href=\"http://www.amazon.com/With-Charity-All-Charities-Failing/dp/038553471X/\">here</a>).</p>\n<p>Also, as Brian Tomasik <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/comment-page-1/#comment-542070\">notes</a>, \"Efficiency in the realm of charity is inherently less plausible than in financial markets because in charity there\u2019s not a common unit of what 'good' means... Indeed, one man\u2019s good may be another man\u2019s bad (e.g., abortion, gun control, extinction risks).\" But even when we focus on relatively common units of 'good' (e.g. human welfare, QALYs, or DALYs), charity is still relatively inefficient: we can easily purchase more QALYs per dollar via <a href=\"http://www.givewell.org/international/top-charities/AMF\">AMF</a> than via, say, the popular <a href=\"http://en.wikipedia.org/wiki/Make-A-Wish_Foundation\">Make-a-Wish Foundation</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"What_is__broad_market_efficiency___then_\">What is \"broad market efficiency\", then?</h4>\n<p>If Holden agrees that philanthropic markets are relatively inefficient in the sense that it's easy to consistently and substantially beat average market returns by giving to GiveWell's recommended charities, then what does he mean by \"broad market efficiency\"? Holden introduces \"broad market efficiency\" as a term for the <em>spectrum</em> of market efficiency, but remains uncertain as to where charity falls on that spectrum of market efficiency.</p>\n<p>Brian Tomasik <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/comment-page-1/#comment-542070\">worried</a> that the term \"broad market efficiency\" would confuse some readers into thinking Holden was claiming that philanthropic markets are relatively efficient and thus that \"it doesn\u2019t really matter where you donate.\" Holden <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/comment-page-1/#comment-542975\">said</a> he wasn't worried about this, saying, \"I don\u2019t think 'broad market efficiency' is a common phrase or one with a clear meaning.\" But I think the phrase <em>is</em> confusing, that many readers interpret it as meaning \"market efficiency,\" and indeed that people in economics and finance sometimes use it that way: search for the phrase \"broad market efficiency\" <a href=\"[https://www2.blackrock.com/webcore/litService/search/getDocument.seam?venue=PUB_INS&amp;source=CONTENT&amp;ServiceName=PublicServiceView&amp;ContentID=1111183580](http://commonsenseatheism.com/wp-content/uploads/2013/08/Singal-Beyond-the-Random-Walk-Preface.pdf\">here</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/Blackrock-Securities-Lending-unlocking-the-full-potential-of-investment-porfolios.pdf\">here</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/08/Ooi-et-al-Extrapolation-theory-and-hte-pricing-of-REIT-stocks.pdf\">here</a>, and <a href=\"http://www.greaterfool.ca/2011/04/10/depraved/#comment-94887\">here</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"The_research_ahead\">The research ahead</h4>\n<p>So how efficient <em>is</em> the charitable market, and in which ways? My own guess is that it's far less efficient than financial markets, but GiveWell's research has <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">provided</a> valuable and surprising (to me) information on this topic, and I look forward to future discoveries.</p>", "sections": [{"title": "What is an efficient market?", "anchor": "What_is_an_efficient_market_", "level": 1}, {"title": "Charity as an inefficient market", "anchor": "Charity_as_an_inefficient_market", "level": 1}, {"title": "What is \"broad market efficiency\", then?", "anchor": "What_is__broad_market_efficiency___then_", "level": 1}, {"title": "The research ahead", "anchor": "The_research_ahead", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JmmA2Mf5GrY9D6nQD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-24T20:55:47.307Z", "modifiedAt": null, "url": null, "title": "The 50 Shades of Grey Book Club", "slug": "the-50-shades-of-grey-book-club", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YzHGW97SrAovxDPcF/the-50-shades-of-grey-book-club", "pageUrlRelative": "/posts/YzHGW97SrAovxDPcF/the-50-shades-of-grey-book-club", "linkUrl": "https://www.lesswrong.com/posts/YzHGW97SrAovxDPcF/the-50-shades-of-grey-book-club", "postedAtFormatted": "Saturday, August 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%2050%20Shades%20of%20Grey%20Book%20Club&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%2050%20Shades%20of%20Grey%20Book%20Club%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzHGW97SrAovxDPcF%2Fthe-50-shades-of-grey-book-club%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%2050%20Shades%20of%20Grey%20Book%20Club%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzHGW97SrAovxDPcF%2Fthe-50-shades-of-grey-book-club", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzHGW97SrAovxDPcF%2Fthe-50-shades-of-grey-book-club", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>I think it would be a useful rationality exercise to take something that millions of people love, that you have contempt for, and sincerely try to appreciate it. The purpose would be to get practice imagining a different point of view, and to see whether you're able to do so.</p>\n<p><em>50 Shades of Grey</em> might not be the best choice for this exercise. I haven't read it. Maybe a better choice would be <em>The da Vinci Code</em>, NASCAR, or professional wrestling. But a book has a definite length that you have to get through to be allowed to say that you honestly tried.</p>\n<p>The idea is to start a thread for people to discuss 50 Shades, or something else perceived as trashy, and try--not to find what you might like in it, but to get inside someone else's head and imagine why they might like it. (Of course it could backfire, and leave you less open-minded than before, if you always conclude that your contempt was simply right in the first place.)</p>\n<p>I think the biggest problem is that if people succeed at finding something to appreciate in it, they would feel terribly embarrassed to say so. So this can be done with alternate accounts.</p>\n<p>Anyone interested? What do you hold in contempt that you might be willing to take a closer look at?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YzHGW97SrAovxDPcF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 3, "extendedScore": null, "score": 1.3148501987773938e-06, "legacy": true, "legacyId": "23849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-25T18:52:07.757Z", "modifiedAt": null, "url": null, "title": "Transparency in safety-critical systems", "slug": "transparency-in-safety-critical-systems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9xLMeCix89i8ozuqS/transparency-in-safety-critical-systems", "pageUrlRelative": "/posts/9xLMeCix89i8ozuqS/transparency-in-safety-critical-systems", "linkUrl": "https://www.lesswrong.com/posts/9xLMeCix89i8ozuqS/transparency-in-safety-critical-systems", "postedAtFormatted": "Sunday, August 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transparency%20in%20safety-critical%20systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATransparency%20in%20safety-critical%20systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xLMeCix89i8ozuqS%2Ftransparency-in-safety-critical-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transparency%20in%20safety-critical%20systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xLMeCix89i8ozuqS%2Ftransparency-in-safety-critical-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xLMeCix89i8ozuqS%2Ftransparency-in-safety-critical-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>I've just posted an analysis to MIRI's blog called <a href=\"http://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/\">Transparency in Safety-Critical Systems</a>. Its aim is to explain a common view about transparency and system reliability, and then open a dialogue about which parts of that view are wrong, or don't apply well to AGI.</p>\n<p>The \"common view\" (not universal by any means) explained in the post is, roughly:</p>\n<blockquote>\n<p>Black box testing can provide some confidence that a system will behave as intended, but if a system is built such that it is transparent to human inspection, then additional methods of reliability verification are available. Unfortunately, many of AI&rsquo;s most useful methods are among its least transparent. Logic-based systems are typically more transparent than statistical methods, but statistical methods are more widely used. There are exceptions to this general rule, and some people are working to make statistical methods more transparent.</p>\n</blockquote>\n<p>Three caveats / open problems listed at the end of the post are:</p>\n<p><ol>\n<li>How does the transparency of a method change with scale? A 200-rules logical AI might be more transparent than a 200-node Bayes net, but what if we&rsquo;re comparing 100,000 rules vs. 100,000 nodes? At least we can query the Bayes net to ask &ldquo;what it believes about X,&rdquo; whereas we can&rsquo;t necessarily do so with the logic-based system.</li>\n<li>Do the categories above really &ldquo;carve reality at its joints&rdquo; with respect to transparency? Does a system&rsquo;s status as a logic-based system or a Bayes net reliably predict its transparency, given that in principle we can use either one to express a probabilistic model of the world?</li>\n<li>How much of a system&rsquo;s transparency is &ldquo;intrinsic&rdquo; to the system, and how much of it depends on the quality of the user interface used to inspect it? How much of a &ldquo;transparency boost&rdquo; can different kinds of systems get from excellently designed user interfaces?</li>\n</ol></p>\n<p>The MIRI blog has only recently begun to regularly host substantive, non-news content, so it doesn't get much commenting action yet. Thus, I figured I'd post here and try to start a dialogue. Comment away!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9xLMeCix89i8ozuqS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 1.3159733116899761e-06, "legacy": true, "legacyId": "23850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-25T19:29:33.808Z", "modifiedAt": null, "url": null, "title": "To what degree do you model people as agents?", "slug": "to-what-degree-do-you-model-people-as-agents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:37.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cenSWez9Ddgsjd5Fc/to-what-degree-do-you-model-people-as-agents", "pageUrlRelative": "/posts/cenSWez9Ddgsjd5Fc/to-what-degree-do-you-model-people-as-agents", "linkUrl": "https://www.lesswrong.com/posts/cenSWez9Ddgsjd5Fc/to-what-degree-do-you-model-people-as-agents", "postedAtFormatted": "Sunday, August 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20what%20degree%20do%20you%20model%20people%20as%20agents%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20what%20degree%20do%20you%20model%20people%20as%20agents%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcenSWez9Ddgsjd5Fc%2Fto-what-degree-do-you-model-people-as-agents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20what%20degree%20do%20you%20model%20people%20as%20agents%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcenSWez9Ddgsjd5Fc%2Fto-what-degree-do-you-model-people-as-agents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcenSWez9Ddgsjd5Fc%2Fto-what-degree-do-you-model-people-as-agents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1425, "htmlBody": "<p>The idea for this post came out of a conversation during one of the Less Wrong Ottawa events. A joke about being solipsist turned into a genuine question&ndash;if you wanted to assume that people were figments of your imagination, how much of a problem would this be? (Being told \"you would be problematic if I were a solipsist\" is a surprising compliment.)&nbsp;</p>\n<p>You can rephrase the question as \"do you model people as agents versus complex systems?\" or \"do you model people as <a href=\"http://en.wikipedia.org/wiki/Player_character\">PCs</a>&nbsp;versus <a href=\"http://en.wikipedia.org/wiki/Non-player_character\">NPCs</a>?\" (To me these seem like a reframing of the same question, with a different connotation/focus; to other people they might seem like different questions entirely). Almost everyone at the table immediately recognized what we were talking about and agreed that modelling some people as agents and some people as complex systems was a thing they did. However, pretty much everything else varied&ndash;how much they modelled people as agents overall, how much it varied in between different people they knew, and how much this impacted the moral value that they assigned to other people. I suspect that another variable is \"how much you model <em>yourself </em>as an agent\"; this probably varies between people and impacts how they model others.&nbsp;</p>\n<p><strong>What does it mean to model someone as an agent?</strong></p>\n<p>The conversation didn't go here in huge amounts of detail, but I expect that due to typical mind fallacy, it's a fascinating discussion to have&ndash;that the distinctions that seem clear and self-evident to me probably aren't what other people use at all. I'll explain mine here.&nbsp;</p>\n<p>1. <em>Reliability and responsibility</em>. Agenty people are people I feel I can rely on, who I trust to take&nbsp;<a href=\"http://www.reddit.com/r/HPMOR/comments/yj2kb/ethical_solipsism_chapter_75/\">heroic responsibility</a>. If I have an unsolved problem and no idea what to do, I can go to them in tears and say \"fix this please!\" And they will do it. They'll pull out a solution that surprises me and that works. If the first solution doesn't work, they will keep trying.&nbsp;</p>\n<p>In this sense, I model my parents strongly as agents&ndash;I have close to 100% confidence that they will do whatever it takes to solve a problem for me. There are other people who I trust to execute a pre-defined solution for me, once I've thought of it, like \"could you do me a huge favour and drive me to the bike shop tomorrow at noon?\" but whom I wouldn't go to with \"AAAAH my bike is broken, help!\" There are other people who I wouldn't ask for help, period. Some of them are people I get along with well and like a lot, but they aren't reliable, and they're further down the mental gradient towards NPC.&nbsp;</p>\n<p>The end result of this is that I'm more likely to model people as agents if I know them well and have some kind of relationship where I would expect them to want to help me. Of course, this is incomplete, because there are brilliant, original people who I respect hugely, but who I don't know well, and I wouldn't ask or expect them to solve a problem in my day-to-day life. So this isn't the only factor.&nbsp;</p>\n<p>2. <em>Intellectual formidability</em>. To what extent someone comes up with ideas that surprise me and seem like things I would never have thought of on my own. This also includes people who have accomplished things that I can't imagine myself succeeding at, like startups. In this sense, there are a lot of bloggers, LW posters, and people on the CFAR mailing list who are major PCs in my mental classification system, but who I may not know personally at all.&nbsp;</p>\n<p>3. <em>Conventional \"agentiness\"</em>. The degree to which a person's behaviour can be described by \"they wanted X, so they took action Y and got what they wanted\", as opposed to \"they did X kind of at random, and Y happened.\" When people seem highly agenty to me, I model their mental processes like this&ndash;my brother is one of them. I take the inside view, imagining that I wanted the thing they want and had their characteristics, i.e. relative intelligence, domain-specific expertise, social support, etc, and this gives better predictions than past behaviour. There are other people whose behaviour I predict based on how they've behaved in the past, using the <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a>, while barely taking into account what they say they want in the future, and this is what gives useful predictions.&nbsp;</p>\n<p>This category also includes the degree to which people have a <a href=\"http://en.wikipedia.org/wiki/Mindset_(book)\">growth mindset</a>, which approximates how much they expect themselves to behave in an agenty way. My parents are a good example of people who are totally 100% reliable, but don't expect or want to change their attitudes or beliefs much in the next twenty years.</p>\n<p>These three categories probably don't include all the subconscious criteria I use, but they're the main ones I can think of.&nbsp;</p>\n<p><strong>How does this affect relationships with people?</strong></p>\n<p>With people who I model as agents, I'm more likely to invoke phrases like \"it was your fault that X happened\" or \"you said you would do Y, why didn't you?\" The degree to which I feel blame or judgement towards people for not doing things they said they would do is almost directly proportional to how much I model them as agents. For people who I consider less agenty, whom I model more as complex systems, I'm more likely to skip the blaming step and jump right to \"what are the things that made it hard for you to do Y? Can we fix them?\"</p>\n<p>On reflection, it seems like the latter is a healthier way to treat <em>myself</em>, and I know this (and consistently fail at doing this). However, I want to be treated like an agent by other people, not a complex system; I want people to give me the benefit of the doubt and assume that I know what I want and am capable of planning to get it. I'm not sure what this means for how I should treat other people.&nbsp;</p>\n<p><strong>How does this affect moral value judgements?</strong></p>\n<p>For me, not at all. My default, probably hammered in by years of nursing school, is to treat <em>every </em>human as worthy of dignity and respect. (On a gut level, it doesn't include animals, although it probably should. On an intellectual level, I don't think animals should be mistreated, but animal suffering doesn't upset me on the same visceral level that human suffering does. I think that on a gut level, my \"circle of empathy\" includes human dead bodies more than it includes animals).&nbsp;</p>\n<p>One of my friends asked me recently if I got frustrated at work, taking care of people who had \"brought their illness on themselves\", i.e. by smoking, alcohol, drug use, eating junk food for 50 years, or whatever else people usually put in the category of \"lifestyle choices.\" Honestly, I don't; it's not a distinction my brain makes. Some of my patients will recover, go home, and make heroic efforts to stay healthy; others won't, and will turn up back in the ICU at regular intervals. It doesn't affect how I feel about treating them; it feels meaningful either way. The one time I'm liable to get frustrated is when I have to spend hours of hard work on patients who are severely neurologically damaged and are, in a sense, dead already, or at least not people anymore. I hate this. But my default is still to talk to them, keep them looking tidy and comfortable, et cetera...</p>\n<p>In that sense, I don't know if modelling different people differently is, for me, a morally a right or a wrong thing to do. However, I spoke to someone whose default is <em>not </em>to assign people moral value, unless he models them as agents. I can see this being problematic, since it's a high standard.&nbsp;</p>\n<p><strong>Conclusion</strong></p>\n<p>As usual for when I notice something new about my thinking, I expect to pay a <em>lot </em>of attention to this over the next few weeks, and probably notice some interesting things, and quite possibly change the way I think and behave. I think I've already succeeded in finding the source of some mysterious frustration with my roommate; I want to model her as an agent because of #1&ndash;she's my best friend and we've been through a lot together&ndash;but in the sense of #3, she's one of the least agenty people I know. So I consistently, <em>predictably </em>get mad at her for things like saying she'll do the dishes and then not doing them, and getting mad doesn't help either of us at all.&nbsp;</p>\n<p>I'm curious to hear what other people think of this idea.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "3uE2pXvbcnS9nnZRE": 1, "mip7tdAN87Jarkcew": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cenSWez9Ddgsjd5Fc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 47, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "23798", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The idea for this post came out of a conversation during one of the Less Wrong Ottawa events. A joke about being solipsist turned into a genuine question\u2013if you wanted to assume that people were figments of your imagination, how much of a problem would this be? (Being told \"you would be problematic if I were a solipsist\" is a surprising compliment.)&nbsp;</p>\n<p>You can rephrase the question as \"do you model people as agents versus complex systems?\" or \"do you model people as <a href=\"http://en.wikipedia.org/wiki/Player_character\">PCs</a>&nbsp;versus <a href=\"http://en.wikipedia.org/wiki/Non-player_character\">NPCs</a>?\" (To me these seem like a reframing of the same question, with a different connotation/focus; to other people they might seem like different questions entirely). Almost everyone at the table immediately recognized what we were talking about and agreed that modelling some people as agents and some people as complex systems was a thing they did. However, pretty much everything else varied\u2013how much they modelled people as agents overall, how much it varied in between different people they knew, and how much this impacted the moral value that they assigned to other people. I suspect that another variable is \"how much you model <em>yourself </em>as an agent\"; this probably varies between people and impacts how they model others.&nbsp;</p>\n<p><strong id=\"What_does_it_mean_to_model_someone_as_an_agent_\">What does it mean to model someone as an agent?</strong></p>\n<p>The conversation didn't go here in huge amounts of detail, but I expect that due to typical mind fallacy, it's a fascinating discussion to have\u2013that the distinctions that seem clear and self-evident to me probably aren't what other people use at all. I'll explain mine here.&nbsp;</p>\n<p>1. <em>Reliability and responsibility</em>. Agenty people are people I feel I can rely on, who I trust to take&nbsp;<a href=\"http://www.reddit.com/r/HPMOR/comments/yj2kb/ethical_solipsism_chapter_75/\">heroic responsibility</a>. If I have an unsolved problem and no idea what to do, I can go to them in tears and say \"fix this please!\" And they will do it. They'll pull out a solution that surprises me and that works. If the first solution doesn't work, they will keep trying.&nbsp;</p>\n<p>In this sense, I model my parents strongly as agents\u2013I have close to 100% confidence that they will do whatever it takes to solve a problem for me. There are other people who I trust to execute a pre-defined solution for me, once I've thought of it, like \"could you do me a huge favour and drive me to the bike shop tomorrow at noon?\" but whom I wouldn't go to with \"AAAAH my bike is broken, help!\" There are other people who I wouldn't ask for help, period. Some of them are people I get along with well and like a lot, but they aren't reliable, and they're further down the mental gradient towards NPC.&nbsp;</p>\n<p>The end result of this is that I'm more likely to model people as agents if I know them well and have some kind of relationship where I would expect them to want to help me. Of course, this is incomplete, because there are brilliant, original people who I respect hugely, but who I don't know well, and I wouldn't ask or expect them to solve a problem in my day-to-day life. So this isn't the only factor.&nbsp;</p>\n<p>2. <em>Intellectual formidability</em>. To what extent someone comes up with ideas that surprise me and seem like things I would never have thought of on my own. This also includes people who have accomplished things that I can't imagine myself succeeding at, like startups. In this sense, there are a lot of bloggers, LW posters, and people on the CFAR mailing list who are major PCs in my mental classification system, but who I may not know personally at all.&nbsp;</p>\n<p>3. <em>Conventional \"agentiness\"</em>. The degree to which a person's behaviour can be described by \"they wanted X, so they took action Y and got what they wanted\", as opposed to \"they did X kind of at random, and Y happened.\" When people seem highly agenty to me, I model their mental processes like this\u2013my brother is one of them. I take the inside view, imagining that I wanted the thing they want and had their characteristics, i.e. relative intelligence, domain-specific expertise, social support, etc, and this gives better predictions than past behaviour. There are other people whose behaviour I predict based on how they've behaved in the past, using the <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a>, while barely taking into account what they say they want in the future, and this is what gives useful predictions.&nbsp;</p>\n<p>This category also includes the degree to which people have a <a href=\"http://en.wikipedia.org/wiki/Mindset_(book)\">growth mindset</a>, which approximates how much they expect themselves to behave in an agenty way. My parents are a good example of people who are totally 100% reliable, but don't expect or want to change their attitudes or beliefs much in the next twenty years.</p>\n<p>These three categories probably don't include all the subconscious criteria I use, but they're the main ones I can think of.&nbsp;</p>\n<p><strong id=\"How_does_this_affect_relationships_with_people_\">How does this affect relationships with people?</strong></p>\n<p>With people who I model as agents, I'm more likely to invoke phrases like \"it was your fault that X happened\" or \"you said you would do Y, why didn't you?\" The degree to which I feel blame or judgement towards people for not doing things they said they would do is almost directly proportional to how much I model them as agents. For people who I consider less agenty, whom I model more as complex systems, I'm more likely to skip the blaming step and jump right to \"what are the things that made it hard for you to do Y? Can we fix them?\"</p>\n<p>On reflection, it seems like the latter is a healthier way to treat <em>myself</em>, and I know this (and consistently fail at doing this). However, I want to be treated like an agent by other people, not a complex system; I want people to give me the benefit of the doubt and assume that I know what I want and am capable of planning to get it. I'm not sure what this means for how I should treat other people.&nbsp;</p>\n<p><strong id=\"How_does_this_affect_moral_value_judgements_\">How does this affect moral value judgements?</strong></p>\n<p>For me, not at all. My default, probably hammered in by years of nursing school, is to treat <em>every </em>human as worthy of dignity and respect. (On a gut level, it doesn't include animals, although it probably should. On an intellectual level, I don't think animals should be mistreated, but animal suffering doesn't upset me on the same visceral level that human suffering does. I think that on a gut level, my \"circle of empathy\" includes human dead bodies more than it includes animals).&nbsp;</p>\n<p>One of my friends asked me recently if I got frustrated at work, taking care of people who had \"brought their illness on themselves\", i.e. by smoking, alcohol, drug use, eating junk food for 50 years, or whatever else people usually put in the category of \"lifestyle choices.\" Honestly, I don't; it's not a distinction my brain makes. Some of my patients will recover, go home, and make heroic efforts to stay healthy; others won't, and will turn up back in the ICU at regular intervals. It doesn't affect how I feel about treating them; it feels meaningful either way. The one time I'm liable to get frustrated is when I have to spend hours of hard work on patients who are severely neurologically damaged and are, in a sense, dead already, or at least not people anymore. I hate this. But my default is still to talk to them, keep them looking tidy and comfortable, et cetera...</p>\n<p>In that sense, I don't know if modelling different people differently is, for me, a morally a right or a wrong thing to do. However, I spoke to someone whose default is <em>not </em>to assign people moral value, unless he models them as agents. I can see this being problematic, since it's a high standard.&nbsp;</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>As usual for when I notice something new about my thinking, I expect to pay a <em>lot </em>of attention to this over the next few weeks, and probably notice some interesting things, and quite possibly change the way I think and behave. I think I've already succeeded in finding the source of some mysterious frustration with my roommate; I want to model her as an agent because of #1\u2013she's my best friend and we've been through a lot together\u2013but in the sense of #3, she's one of the least agenty people I know. So I consistently, <em>predictably </em>get mad at her for things like saying she'll do the dishes and then not doing them, and getting mad doesn't help either of us at all.&nbsp;</p>\n<p>I'm curious to hear what other people think of this idea.&nbsp;</p>", "sections": [{"title": "What does it mean to model someone as an agent?", "anchor": "What_does_it_mean_to_model_someone_as_an_agent_", "level": 1}, {"title": "How does this affect relationships with people?", "anchor": "How_does_this_affect_relationships_with_people_", "level": 1}, {"title": "How does this affect moral value judgements?", "anchor": "How_does_this_affect_moral_value_judgements_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "132 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 133, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-25T19:29:42.541Z", "modifiedAt": null, "url": null, "title": "Reality is weirdly normal", "slug": "reality-is-weirdly-normal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:04.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jntmpKiyg6oHFhLcP/reality-is-weirdly-normal", "pageUrlRelative": "/posts/jntmpKiyg6oHFhLcP/reality-is-weirdly-normal", "linkUrl": "https://www.lesswrong.com/posts/jntmpKiyg6oHFhLcP/reality-is-weirdly-normal", "postedAtFormatted": "Sunday, August 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reality%20is%20weirdly%20normal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReality%20is%20weirdly%20normal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjntmpKiyg6oHFhLcP%2Freality-is-weirdly-normal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reality%20is%20weirdly%20normal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjntmpKiyg6oHFhLcP%2Freality-is-weirdly-normal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjntmpKiyg6oHFhLcP%2Freality-is-weirdly-normal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1528, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">When Anthropomorphism Became Stupid</a>, <a href=\"/lw/on/reductionism/\">Reductionism</a>, <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">How to Convince Me That 2 + 2 = 3</a></p>\n<p>\"<strong><a href=\"http://wiki.lesswrong.com/wiki/Reality_is_normal\">Reality is normal.</a></strong>\" That is: Surprise, confusion, and mystery are features&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">of maps, not of territories</a>. If you would&nbsp;<a href=\"/lw/hs/think_like_reality/\">think like reality</a>, cultivate outrage at yourself for failing to intuit the data, not resentment at the data for being counter-intuitive.</p>\n<p>\"<strong><a href=\"/lw/hr/universal_law/\">Not one unusual thing has ever happened.</a></strong>\" That is: Ours is a tight-knit and monochrome country. The cosmos is simple, tidy, lawful.&nbsp;\"[T]here is <a href=\"/lw/s6/probability_is_subjectively_objective/\">no surprise</a> from a causal <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">viewpoint</a>&nbsp;&mdash; no disruption of the physical order of the universe.\"</p>\n<p>\"<strong><a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">It all adds up to normality.</a></strong>\" That is: Whatever is true of fundamental reality does not exist in a separate universe from our everyday activities. It <em>composes</em>&nbsp;those activities. The perfected description of our universe must in principle allow us to reproduce the appearances we started with.</p>\n<p>These maxims are remedies to&nbsp;<a href=\"/lw/ix/say_not_complexity/\">magical mereology</a>, <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">anthropocentrism</a>, and all manner of&nbsp;<a href=\"/lw/eqn/the_useful_idea_of_truth/\">philosophical panic</a>.&nbsp;But reading too much (or too little) into them can lead seekers from the Path. For instance, they may be wrongly taken to mean that the world is obliged to validate our initial impressions or our untrained intuitions. As a further corrective, I suggest:&nbsp;<strong>R</strong><strong>eality is&nbsp;<em>weirdly</em>&nbsp;normal</strong>. It's \"normal\" in odd&nbsp;ways, by strange&nbsp;means, in surprising&nbsp;senses.</p>\n<p>At the risk of vivisecting poetry, and maybe of stating the obvious, I'll point out that the maxims mean different things by \"normal\". In the first two, what's \"normal\" or \"usual\" is the universe taken on its own terms &mdash;&nbsp;the cosmos as it sees itself, or as an ideally calibrated <a href=\"http://en.wikipedia.org/wiki/Laplace's_demon\">demon</a> would see it. In the third maxim, what's \"normal\" is the universe <em>humanity</em>&nbsp;perceives &mdash;&nbsp;though this still doesn't identify normality with what's <em>believed</em>&nbsp;or <em>expected</em>. Actually,&nbsp;it will take some philosophical work to articulate just what Egan's \"normality\" should amount to. I'll start with Copernicanism and reductionism, and then I'll revisit that question.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<hr />\n<p><strong></strong></p>\n<p><strong>Everything is usual. Very nearly nothing is familiar.</strong></p>\n<ul>\n<li>Spell 1: <em><span style=\"color: #888888;\">Since the beginning, not one unusual thing has ever happened. And <a href=\"/r/discussion/lw/ice/engaging_first_introductions_to_ai_risk/\">intelligence explosion</a>&nbsp;sounds... well, 'unusual' is the mildest word that comes to mind. So it won't happen.</span></em></li>\n<li>Counterspell: Reality is <em>weirdly</em>&nbsp;normal. It's not the kind of normal that <em>sounds</em>&nbsp;normal.</li>\n</ul>\n<p>Relative to the universe's laws, everything is par for the course. But relative to <em>human</em>&nbsp;standards of normality &mdash;&nbsp;what I'll call<strong>&nbsp;</strong>the<em>&nbsp;</em><strong>familiar</strong>&nbsp;&mdash;&nbsp;<em>barely anything</em>&nbsp;that actually exists is par for the course. Nearly all events are senseless, bizarre, inhuman. But that's because our mapping hardware is adapted to a very specific environment; if anything's objectively weird, it's we, not the other denizens of Everythingland. That's much of why <a href=\"/lw/j4/absurdity_heuristic_absurdity_bias/\">absurdity is a poor guide to probability</a>.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">Egan's Law</a> reminds us that beneath our human surface weirdness lies a deeper regularity, a deeper unity with the rest of Nature. But to call this alien order 'regular' already assumes a shift in perspective from what a person off the street&nbsp;would initially think of&nbsp;as 'regular', to the hidden patterns of the very large and very small. We should prize the ability to shift between these points of view, while carefully avoiding conflating them.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Spell 2: <em><span style=\"color: #888888;\">Reality is normal. So it shouldn't be difficult for me to <a href=\"/lw/hs/think_like_reality/\">think like it</a>.</span></em></li>\n<li>Counterspell: What's normal from a God's-eye view is wont to be weird from a human's. And vice versa.</li>\n</ul>\n<p>Thinking like reality requires <em>understanding</em>&nbsp;reality. You can't just&nbsp;<a href=\"/lw/je/doublethink_choosing_to_be_biased/\">will</a>&nbsp;yourself into becoming a high-fidelity map. If nothing else, you'll be too vulnerable to <a href=\"/lw/la/truly_part_of_you/\">knowledge gaps</a>.&nbsp;<em>Pretending</em>&nbsp;you think like quantum physics is even worse than rebelling against the physics for being too confusing. At least in the latter case you've&nbsp;<a href=\"/lw/if/your_strength_as_a_rationalist/\">noticed</a>&nbsp;the disparity between your map and the territory.</p>\n<p>To take pride in one's confusion at the quantum, rather than striving mightily to understand,&nbsp;is an epistemic sin. But to <em>deny</em>&nbsp;one's confusion at the quantum, to push it from one's mind and play-act at wisdom, is a graver sin. The goal isn't to do away with one's confusion; it's to do away with one's best&nbsp;<em>reasons</em>&nbsp;to be confused.</p>\n<hr />\n<p>&nbsp;</p>\n<hr />\n<p><strong>What adds up to the familiar needn't be familiar.</strong></p>\n<ul>\n<li>Spell 3: <em><span style=\"color: #888888;\">It all adds up to normality. But relativity sure sounds abnormal! So relativity will be replaced by something normaler.</span></em></li>\n<li>Counterspell: Physics is the weird addenda, not the normal sum.</li>\n</ul>\n<p>x + y + z + ... = Normality. But x &ne; Normality. Nor does y. In fact, even the complete quantum-mechanical description of the human world would not <em>look</em>&nbsp;normal, from a human perspective.&nbsp;But it would be a description of <em>exactly </em>the world we live in.</p>\n<p>This is another reminder that surprise is a feature of maps, not of territories. Very different maps &mdash;&nbsp;a wave function scribbled in silicon, a belief, a gesture, a child's drawing &mdash; can represent the same territory. The drawing is normal (familiar), and the wave function isn't. But<em>&nbsp;</em>the <em>referent</em>&nbsp;of the two maps may well be the same. In asserting \"<a href=\"/lw/qz/living_in_many_worlds/\">quantum mechanics adds up to normality</a>\", we're really asserting that our maps of familiar objects, to the extent they're accurate, would co-refer with portions of the maps of an ideal Finished Science. They're two different languages, but faithful translation is possible.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Spell 4: <em><span style=\"color: #888888;\">It all adds up to normality. So I should be able to readily intuit how QM yields the familiar.</span></em></li>\n<li>Counterspell: Reality forms what's normal, but by weird means.</li>\n</ul>\n<p>The&nbsp;<em>relationship</em>&nbsp;between the quantum and the familiar can be thoroughly unfamiliar. The way in which the unintuitive fundamental truths yield an intuitive&nbsp;<a href=\"http://www.youtube.com/watch?v=1APOxsp1VFw\">Middle World</a> is not itself intuitive.</p>\n<p>We could call this \"<a href=\"/lw/iv/the_futility_of_emergence/\">emergence</a>\", if we wished to <a href=\"http://consc.net/papers/emergence.pdf\">craft</a> a territory predicate out of mapstuff. Otherwise, I suggest calling this predicate\"wow-sometimes-I'm-not-very-good-at-keeping-track-of-lots-of-small-things-ence\". Understanding the logical or mathematical implications of simple patterns is not humanity's specialty. Causal and part-whole relations are no exception to this rule.</p>\n<hr />\n<p>&nbsp;</p>\n<hr />\n<p><strong>It all adds up to <em>the phenomenon</em>.</strong></p>\n<ul>\n<li>Spell 5: <em><span style=\"font-family: mceinline;\">It all adds up to normality. It's common sense that <a href=\"http://plato.stanford.edu/entries/time/\">time 'flows'.</a>&nbsp;So science will ultimately show time is <a href=\"/lw/qp/timeless_physics/\">not at all like space</a>.</span></em></li>\n<li>Counterspell: We'd find even the familiar alien, if we but understood it.</li>\n</ul>\n<p><a href=\"/lw/gux/dont_get_offended/8lz8?context=3#comments\"><em>Normality isn't common sense</em>.</a> Normality isn't our beliefs, expectations, assumptions, or strongest convictions. Normality, in the sense relevant to a <em>true and binding</em>&nbsp;\"It all adds up to normality\", is the phenomenon, the human world as it appears. (Not the human world as it is <em>believed</em>&nbsp;to appear. The human world as we <em>actually</em>&nbsp;encounter it.)</p>\n<p>What's \"the phenomenon\"?</p>\n<p>I'm going to be willfully cagey about that. What I really mean by \"the phenomenon\" is \"whatever's going on\". Or whatever's going on that we're cognitively accessing or representing. It needn't all add up to a world that makes my map true; but it <em>will</em>&nbsp;add up to a world that <em>explains why my map looks the way it does</em>.</p>\n<p>Even saying that much risks overly restricting the shape explanations are allowed to take. Egan's Law can be used as a general constraint on successful explanations &mdash; they must account for the <em>explanandum</em>&nbsp;&mdash; but only if we treat the <em>explanandum</em>&nbsp;loosely enough to permit <a href=\"/lw/of/dissolving_the_question/\">dissolutions</a> and <a href=\"/lw/6pm/secrets_of_the_eliminati/\">eliminations</a> alongside run-of-the-mill reductions. It all adds up to an explanation&nbsp;of how our beliefs arose, though not necessarily a validation of them.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Spell 6: <em><span style=\"font-family: mceinline;\">It all adds up to normality. I have an immediate epistemic acquaintance with the <a href=\"http://plato.stanford.edu/entries/qualia-knowledge/\">irreducible phenomenal character</a> of my experience. So, whatever be the final theory, it will certainly include qualia.</span></em></li>\n<li>Counterspell: Every map is in a territory. But no meta-map is beyond suspicion.</li>\n</ul>\n<p>It's obvious that appearances can deceive us about underlying reality. But a variety of perceptual&nbsp;<a href=\"http://www.michaelbach.de/ot/sze_shepardTables/\">illusions</a>&nbsp;demonstrate that experiences can also consistently mislead us about&nbsp;<em>themselves</em>. It all adds up to normality, but normality <em>lies</em>.</p>\n<p>In some cases, we <a href=\"/lw/5ee/being_wrong_about_your_own_subjective_experience/\">can't accurately describe our&nbsp;<em>surface impressions</em></a>&nbsp;until we've understood their underlying mechanism.&nbsp;Since so much of science is revisionary, we mustn't interpret Egan's Law in a way that unduly privileges first-pass descriptions. When data and theory conflict, it's sometimes more likely that <a href=\"/lw/ig/i_defy_the_data/\">the data has been misrepresented</a>&nbsp;than that the theory is false.</p>\n<p>How far does this openness to map revision extend? As <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">far</a> as the dynamics of one's brain allows.&nbsp;From the sidelines, it may appear to me that&nbsp;<em>in principle</em>&nbsp;a thinker should be able to have certainty about some propositions &mdash; for instance, 'an experience is occurring'. But when I actually find myself living through such a thought, I don't in fact&nbsp;<em>experience</em>&nbsp;<a href=\"/lw/mo/infinite_certainty/\">infinite confidence</a>. It remains physically possible for me to <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">be persuaded otherwise</a>.</p>\n<p>This point ought to be a bit controversial, and more defense of it is needed. But I do insist on treating 'phenomenal experience' and 'phenomenon' as <em>prima facie</em>&nbsp;independent concepts. Egan's Law really is <em>the&nbsp;law</em>, whereas claims about some inerrant mode of reasoning or perception will at best qualify as <a href=\"/lw/k2/a_priori/\">well-supported hypotheses</a>.</p>\n<hr />\n<p>&nbsp;</p>\n<p>Egan's Law is not about saving conscious experience, or our theories, or our axioms, or our interpretations or descriptions of the phenomenon. It's about <strong>saving <em>the phenomenon itself</em></strong>&nbsp;&mdash; the piece of the world in which we are, in fact, submerged. Egan's Law can be restated: The part of reality that (under its familiar description) puzzled or surprised us is identical to (or otherwise lawfully derivable from) the part of reality that (under its more fundamental description) explains it.</p>\n<p>The human piece of the universe is <a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">of a piece with everything else</a>. And the Everything Else gets explanatory priority. Of all our science's findings to date, that may well be the most startling.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 1, "wMPYFGmhcFg4bSb4Z": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jntmpKiyg6oHFhLcP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 55, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "23783", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to: </strong><a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">When Anthropomorphism Became Stupid</a>, <a href=\"/lw/on/reductionism/\">Reductionism</a>, <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">How to Convince Me That 2 + 2 = 3</a></p>\n<p>\"<strong><a href=\"http://wiki.lesswrong.com/wiki/Reality_is_normal\">Reality is normal.</a></strong>\" That is: Surprise, confusion, and mystery are features&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">of maps, not of territories</a>. If you would&nbsp;<a href=\"/lw/hs/think_like_reality/\">think like reality</a>, cultivate outrage at yourself for failing to intuit the data, not resentment at the data for being counter-intuitive.</p>\n<p>\"<strong><a href=\"/lw/hr/universal_law/\">Not one unusual thing has ever happened.</a></strong>\" That is: Ours is a tight-knit and monochrome country. The cosmos is simple, tidy, lawful.&nbsp;\"[T]here is <a href=\"/lw/s6/probability_is_subjectively_objective/\">no surprise</a> from a causal <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">viewpoint</a>&nbsp;\u2014 no disruption of the physical order of the universe.\"</p>\n<p>\"<strong><a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">It all adds up to normality.</a></strong>\" That is: Whatever is true of fundamental reality does not exist in a separate universe from our everyday activities. It <em>composes</em>&nbsp;those activities. The perfected description of our universe must in principle allow us to reproduce the appearances we started with.</p>\n<p>These maxims are remedies to&nbsp;<a href=\"/lw/ix/say_not_complexity/\">magical mereology</a>, <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">anthropocentrism</a>, and all manner of&nbsp;<a href=\"/lw/eqn/the_useful_idea_of_truth/\">philosophical panic</a>.&nbsp;But reading too much (or too little) into them can lead seekers from the Path. For instance, they may be wrongly taken to mean that the world is obliged to validate our initial impressions or our untrained intuitions. As a further corrective, I suggest:&nbsp;<strong>R</strong><strong>eality is&nbsp;<em>weirdly</em>&nbsp;normal</strong>. It's \"normal\" in odd&nbsp;ways, by strange&nbsp;means, in surprising&nbsp;senses.</p>\n<p>At the risk of vivisecting poetry, and maybe of stating the obvious, I'll point out that the maxims mean different things by \"normal\". In the first two, what's \"normal\" or \"usual\" is the universe taken on its own terms \u2014&nbsp;the cosmos as it sees itself, or as an ideally calibrated <a href=\"http://en.wikipedia.org/wiki/Laplace's_demon\">demon</a> would see it. In the third maxim, what's \"normal\" is the universe <em>humanity</em>&nbsp;perceives \u2014&nbsp;though this still doesn't identify normality with what's <em>believed</em>&nbsp;or <em>expected</em>. Actually,&nbsp;it will take some philosophical work to articulate just what Egan's \"normality\" should amount to. I'll start with Copernicanism and reductionism, and then I'll revisit that question.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<hr>\n<p><strong></strong></p>\n<p><strong id=\"Everything_is_usual__Very_nearly_nothing_is_familiar_\">Everything is usual. Very nearly nothing is familiar.</strong></p>\n<ul>\n<li>Spell 1: <em><span style=\"color: #888888;\">Since the beginning, not one unusual thing has ever happened. And <a href=\"/r/discussion/lw/ice/engaging_first_introductions_to_ai_risk/\">intelligence explosion</a>&nbsp;sounds... well, 'unusual' is the mildest word that comes to mind. So it won't happen.</span></em></li>\n<li>Counterspell: Reality is <em>weirdly</em>&nbsp;normal. It's not the kind of normal that <em>sounds</em>&nbsp;normal.</li>\n</ul>\n<p>Relative to the universe's laws, everything is par for the course. But relative to <em>human</em>&nbsp;standards of normality \u2014&nbsp;what I'll call<strong>&nbsp;</strong>the<em>&nbsp;</em><strong>familiar</strong>&nbsp;\u2014&nbsp;<em>barely anything</em>&nbsp;that actually exists is par for the course. Nearly all events are senseless, bizarre, inhuman. But that's because our mapping hardware is adapted to a very specific environment; if anything's objectively weird, it's we, not the other denizens of Everythingland. That's much of why <a href=\"/lw/j4/absurdity_heuristic_absurdity_bias/\">absurdity is a poor guide to probability</a>.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">Egan's Law</a> reminds us that beneath our human surface weirdness lies a deeper regularity, a deeper unity with the rest of Nature. But to call this alien order 'regular' already assumes a shift in perspective from what a person off the street&nbsp;would initially think of&nbsp;as 'regular', to the hidden patterns of the very large and very small. We should prize the ability to shift between these points of view, while carefully avoiding conflating them.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Spell 2: <em><span style=\"color: #888888;\">Reality is normal. So it shouldn't be difficult for me to <a href=\"/lw/hs/think_like_reality/\">think like it</a>.</span></em></li>\n<li>Counterspell: What's normal from a God's-eye view is wont to be weird from a human's. And vice versa.</li>\n</ul>\n<p>Thinking like reality requires <em>understanding</em>&nbsp;reality. You can't just&nbsp;<a href=\"/lw/je/doublethink_choosing_to_be_biased/\">will</a>&nbsp;yourself into becoming a high-fidelity map. If nothing else, you'll be too vulnerable to <a href=\"/lw/la/truly_part_of_you/\">knowledge gaps</a>.&nbsp;<em>Pretending</em>&nbsp;you think like quantum physics is even worse than rebelling against the physics for being too confusing. At least in the latter case you've&nbsp;<a href=\"/lw/if/your_strength_as_a_rationalist/\">noticed</a>&nbsp;the disparity between your map and the territory.</p>\n<p>To take pride in one's confusion at the quantum, rather than striving mightily to understand,&nbsp;is an epistemic sin. But to <em>deny</em>&nbsp;one's confusion at the quantum, to push it from one's mind and play-act at wisdom, is a graver sin. The goal isn't to do away with one's confusion; it's to do away with one's best&nbsp;<em>reasons</em>&nbsp;to be confused.</p>\n<hr>\n<p>&nbsp;</p>\n<hr>\n<p><strong id=\"What_adds_up_to_the_familiar_needn_t_be_familiar_\">What adds up to the familiar needn't be familiar.</strong></p>\n<ul>\n<li>Spell 3: <em><span style=\"color: #888888;\">It all adds up to normality. But relativity sure sounds abnormal! So relativity will be replaced by something normaler.</span></em></li>\n<li>Counterspell: Physics is the weird addenda, not the normal sum.</li>\n</ul>\n<p>x + y + z + ... = Normality. But x \u2260 Normality. Nor does y. In fact, even the complete quantum-mechanical description of the human world would not <em>look</em>&nbsp;normal, from a human perspective.&nbsp;But it would be a description of <em>exactly </em>the world we live in.</p>\n<p>This is another reminder that surprise is a feature of maps, not of territories. Very different maps \u2014&nbsp;a wave function scribbled in silicon, a belief, a gesture, a child's drawing \u2014 can represent the same territory. The drawing is normal (familiar), and the wave function isn't. But<em>&nbsp;</em>the <em>referent</em>&nbsp;of the two maps may well be the same. In asserting \"<a href=\"/lw/qz/living_in_many_worlds/\">quantum mechanics adds up to normality</a>\", we're really asserting that our maps of familiar objects, to the extent they're accurate, would co-refer with portions of the maps of an ideal Finished Science. They're two different languages, but faithful translation is possible.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Spell 4: <em><span style=\"color: #888888;\">It all adds up to normality. So I should be able to readily intuit how QM yields the familiar.</span></em></li>\n<li>Counterspell: Reality forms what's normal, but by weird means.</li>\n</ul>\n<p>The&nbsp;<em>relationship</em>&nbsp;between the quantum and the familiar can be thoroughly unfamiliar. The way in which the unintuitive fundamental truths yield an intuitive&nbsp;<a href=\"http://www.youtube.com/watch?v=1APOxsp1VFw\">Middle World</a> is not itself intuitive.</p>\n<p>We could call this \"<a href=\"/lw/iv/the_futility_of_emergence/\">emergence</a>\", if we wished to <a href=\"http://consc.net/papers/emergence.pdf\">craft</a> a territory predicate out of mapstuff. Otherwise, I suggest calling this predicate\"wow-sometimes-I'm-not-very-good-at-keeping-track-of-lots-of-small-things-ence\". Understanding the logical or mathematical implications of simple patterns is not humanity's specialty. Causal and part-whole relations are no exception to this rule.</p>\n<hr>\n<p>&nbsp;</p>\n<hr>\n<p><strong id=\"It_all_adds_up_to_the_phenomenon_\">It all adds up to <em>the phenomenon</em>.</strong></p>\n<ul>\n<li>Spell 5: <em><span style=\"font-family: mceinline;\">It all adds up to normality. It's common sense that <a href=\"http://plato.stanford.edu/entries/time/\">time 'flows'.</a>&nbsp;So science will ultimately show time is <a href=\"/lw/qp/timeless_physics/\">not at all like space</a>.</span></em></li>\n<li>Counterspell: We'd find even the familiar alien, if we but understood it.</li>\n</ul>\n<p><a href=\"/lw/gux/dont_get_offended/8lz8?context=3#comments\"><em>Normality isn't common sense</em>.</a> Normality isn't our beliefs, expectations, assumptions, or strongest convictions. Normality, in the sense relevant to a <em>true and binding</em>&nbsp;\"It all adds up to normality\", is the phenomenon, the human world as it appears. (Not the human world as it is <em>believed</em>&nbsp;to appear. The human world as we <em>actually</em>&nbsp;encounter it.)</p>\n<p>What's \"the phenomenon\"?</p>\n<p>I'm going to be willfully cagey about that. What I really mean by \"the phenomenon\" is \"whatever's going on\". Or whatever's going on that we're cognitively accessing or representing. It needn't all add up to a world that makes my map true; but it <em>will</em>&nbsp;add up to a world that <em>explains why my map looks the way it does</em>.</p>\n<p>Even saying that much risks overly restricting the shape explanations are allowed to take. Egan's Law can be used as a general constraint on successful explanations \u2014 they must account for the <em>explanandum</em>&nbsp;\u2014 but only if we treat the <em>explanandum</em>&nbsp;loosely enough to permit <a href=\"/lw/of/dissolving_the_question/\">dissolutions</a> and <a href=\"/lw/6pm/secrets_of_the_eliminati/\">eliminations</a> alongside run-of-the-mill reductions. It all adds up to an explanation&nbsp;of how our beliefs arose, though not necessarily a validation of them.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Spell 6: <em><span style=\"font-family: mceinline;\">It all adds up to normality. I have an immediate epistemic acquaintance with the <a href=\"http://plato.stanford.edu/entries/qualia-knowledge/\">irreducible phenomenal character</a> of my experience. So, whatever be the final theory, it will certainly include qualia.</span></em></li>\n<li>Counterspell: Every map is in a territory. But no meta-map is beyond suspicion.</li>\n</ul>\n<p>It's obvious that appearances can deceive us about underlying reality. But a variety of perceptual&nbsp;<a href=\"http://www.michaelbach.de/ot/sze_shepardTables/\">illusions</a>&nbsp;demonstrate that experiences can also consistently mislead us about&nbsp;<em>themselves</em>. It all adds up to normality, but normality <em>lies</em>.</p>\n<p>In some cases, we <a href=\"/lw/5ee/being_wrong_about_your_own_subjective_experience/\">can't accurately describe our&nbsp;<em>surface impressions</em></a>&nbsp;until we've understood their underlying mechanism.&nbsp;Since so much of science is revisionary, we mustn't interpret Egan's Law in a way that unduly privileges first-pass descriptions. When data and theory conflict, it's sometimes more likely that <a href=\"/lw/ig/i_defy_the_data/\">the data has been misrepresented</a>&nbsp;than that the theory is false.</p>\n<p>How far does this openness to map revision extend? As <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">far</a> as the dynamics of one's brain allows.&nbsp;From the sidelines, it may appear to me that&nbsp;<em>in principle</em>&nbsp;a thinker should be able to have certainty about some propositions \u2014 for instance, 'an experience is occurring'. But when I actually find myself living through such a thought, I don't in fact&nbsp;<em>experience</em>&nbsp;<a href=\"/lw/mo/infinite_certainty/\">infinite confidence</a>. It remains physically possible for me to <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">be persuaded otherwise</a>.</p>\n<p>This point ought to be a bit controversial, and more defense of it is needed. But I do insist on treating 'phenomenal experience' and 'phenomenon' as <em>prima facie</em>&nbsp;independent concepts. Egan's Law really is <em>the&nbsp;law</em>, whereas claims about some inerrant mode of reasoning or perception will at best qualify as <a href=\"/lw/k2/a_priori/\">well-supported hypotheses</a>.</p>\n<hr>\n<p>&nbsp;</p>\n<p>Egan's Law is not about saving conscious experience, or our theories, or our axioms, or our interpretations or descriptions of the phenomenon. It's about <strong>saving <em>the phenomenon itself</em></strong>&nbsp;\u2014 the piece of the world in which we are, in fact, submerged. Egan's Law can be restated: The part of reality that (under its familiar description) puzzled or surprised us is identical to (or otherwise lawfully derivable from) the part of reality that (under its more fundamental description) explains it.</p>\n<p>The human piece of the universe is <a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">of a piece with everything else</a>. And the Everything Else gets explanatory priority. Of all our science's findings to date, that may well be the most startling.</p>", "sections": [{"title": "Everything is usual. Very nearly nothing is familiar.", "anchor": "Everything_is_usual__Very_nearly_nothing_is_familiar_", "level": 1}, {"title": "What adds up to the familiar needn't be familiar.", "anchor": "What_adds_up_to_the_familiar_needn_t_be_familiar_", "level": 1}, {"title": "It all adds up to the phenomenon.", "anchor": "It_all_adds_up_to_the_phenomenon_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "81 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f4RJtHBPvDRJcCTva", "tPqQdLCuxanjhoaNs", "6FmqiAgS8h4EJm86s", "tWLFWAndSZSYN6rPB", "7iTwGquBFZKttpEdE", "XhaKvQyHzeXdNnFKy", "pGvyqAQw6yqTjpKf4", "kpRSCH7ALLcb6ucWM", "QsMJQSFj7WfoTMNgW", "XqvnWFtRD2keJdwjX", "BdXeZC8hFvLMcN495", "P792Z4QA9dzcLdKkE", "Hs3ymqypvhgFMkgLb", "fg9fXrHpeaDD6pEPL", "5JDkW4MYXit2CquLs", "qcYCAxYZT4Xp9iMZY", "8QzZKw9WHRxjR4948", "rrW7yf42vQYDf8AcH", "Mc6QcrsbH5NRXbCRX", "WQWhXzALcrzrJtqRh", "J55XeCNeF7wNwgCj9", "vrHRcEDMjZcx5Yfru", "QGkYCwyC7wTDyt3yT", "ooypcn7qFzsMcy53R", "qmqLxvtsPzZ2s6mpY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-25T22:30:01.903Z", "modifiedAt": null, "url": null, "title": "[LINK] Attention Schema Theory of Consciousness", "slug": "link-attention-schema-theory-of-consciousness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:39.417Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ESRogs", "createdAt": "2011-01-05T21:50:53.170Z", "isAdmin": false, "displayName": "ESRogs"}, "userId": "dRGmZYGDzf5LFNjtz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PJJg5oArXNa5pTg6F/link-attention-schema-theory-of-consciousness", "pageUrlRelative": "/posts/PJJg5oArXNa5pTg6F/link-attention-schema-theory-of-consciousness", "linkUrl": "https://www.lesswrong.com/posts/PJJg5oArXNa5pTg6F/link-attention-schema-theory-of-consciousness", "postedAtFormatted": "Sunday, August 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Attention%20Schema%20Theory%20of%20Consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Attention%20Schema%20Theory%20of%20Consciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJJg5oArXNa5pTg6F%2Flink-attention-schema-theory-of-consciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Attention%20Schema%20Theory%20of%20Consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJJg5oArXNa5pTg6F%2Flink-attention-schema-theory-of-consciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJJg5oArXNa5pTg6F%2Flink-attention-schema-theory-of-consciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p>I found this theory pretty interesting, and it reminded me of Gary Drescher's explanation of consciousness in <em>Good and Real</em>:</p>\n<blockquote>\n<p><strong><em>How the light gets out</em></strong></p>\n<p><em>Consciousness is the &lsquo;hard problem&rsquo;, the mystery that confounds scientists and philosophers. Has a new theory cracked it?</em></p>\n<p>[...]</p>\n<p>Attention requires control. In the modern study of robotics there is something called control theory, and it teaches us that, if a machine such as a brain is to control something, it helps to have an internal model of that thing. Think of a military general with his model armies arrayed on a map: they provide a simple but useful representation &mdash; not always perfectly accurate, but close enough to help formulate strategy. Likewise, to control its own state of attention, the brain needs a constantly updated simulation or model of that state. Like the general&rsquo;s toy armies, the model will be schematic and short on detail. The brain will attribute a property to itself and that property will be a simplified proxy for attention. It won&rsquo;t be precisely accurate, but it will convey useful information. What exactly is that property? When it is paying attention to thing X, we know that the brain usually attributes an experience of X to itself &mdash; the property of being conscious, or aware, of something. Why? Because that attribution helps to keep track of the ever-changing focus of attention.</p>\n<p>I call this the &lsquo;attention schema theory&rsquo;. It has a very simple idea at its heart: that consciousness is a schematic model of one&rsquo;s state of attention. Early in evolution, perhaps hundreds of millions of years ago, brains evolved a specific set of computations to construct that model. At that point, &lsquo;I am aware of X&rsquo; entered their repertoire of possible computations.</p>\n</blockquote>\n<p>- Princeton neuroscientist, Michael Graziano, <a href=\"http://www.aeonmagazine.com/being-human/how-consciousness-works/\">writing in Aeon Magazine</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PJJg5oArXNa5pTg6F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.316159391517011e-06, "legacy": true, "legacyId": "23851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-26T04:47:20.167Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign, Illinois", "slug": "meetup-urbana-champaign-illinois-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pRw3YzZiEq5byCm25/meetup-urbana-champaign-illinois-0", "pageUrlRelative": "/posts/pRw3YzZiEq5byCm25/meetup-urbana-champaign-illinois-0", "linkUrl": "https://www.lesswrong.com/posts/pRw3YzZiEq5byCm25/meetup-urbana-champaign-illinois-0", "postedAtFormatted": "Monday, August 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%2C%20Illinois&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%2C%20Illinois%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRw3YzZiEq5byCm25%2Fmeetup-urbana-champaign-illinois-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%2C%20Illinois%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRw3YzZiEq5byCm25%2Fmeetup-urbana-champaign-illinois-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRw3YzZiEq5byCm25%2Fmeetup-urbana-champaign-illinois-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/q9'>Urbana-Champaign, Illinois</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Illini Union South Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup topic will again be determined by popular consensus at the actual meetup. I will have Zendo, Wits and Wagers (with cards that can be used independently for calibration games), and Pandemic, a cooperative strategy board game. 2PM Sunday in the Illini Union South Lounge seems to have worked for everyone this week, so I chose the same time next week, and this will probably become our permanent place and weekly time. Cross posted on <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/39kfb4IhB2c\">the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/q9'>Urbana-Champaign, Illinois</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pRw3YzZiEq5byCm25", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3164817042786024e-06, "legacy": true, "legacyId": "23855", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Illinois\">Discussion article for the meetup : <a href=\"/meetups/q9\">Urbana-Champaign, Illinois</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Illini Union South Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup topic will again be determined by popular consensus at the actual meetup. I will have Zendo, Wits and Wagers (with cards that can be used independently for calibration games), and Pandemic, a cooperative strategy board game. 2PM Sunday in the Illini Union South Lounge seems to have worked for everyone this week, so I chose the same time next week, and this will probably become our permanent place and weekly time. Cross posted on <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/39kfb4IhB2c\">the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Illinois1\">Discussion article for the meetup : <a href=\"/meetups/q9\">Urbana-Champaign, Illinois</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign, Illinois", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Illinois", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign, Illinois", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Illinois1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-26T06:21:17.996Z", "modifiedAt": null, "url": null, "title": "How sure are you that brain emulations would be conscious?", "slug": "how-sure-are-you-that-brain-emulations-would-be-conscious", "viewCount": null, "lastCommentedAt": "2021-01-03T08:26:06.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6nbuf6ZDiQe5RWwqv/how-sure-are-you-that-brain-emulations-would-be-conscious", "pageUrlRelative": "/posts/6nbuf6ZDiQe5RWwqv/how-sure-are-you-that-brain-emulations-would-be-conscious", "linkUrl": "https://www.lesswrong.com/posts/6nbuf6ZDiQe5RWwqv/how-sure-are-you-that-brain-emulations-would-be-conscious", "postedAtFormatted": "Monday, August 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20sure%20are%20you%20that%20brain%20emulations%20would%20be%20conscious%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20sure%20are%20you%20that%20brain%20emulations%20would%20be%20conscious%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6nbuf6ZDiQe5RWwqv%2Fhow-sure-are-you-that-brain-emulations-would-be-conscious%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20sure%20are%20you%20that%20brain%20emulations%20would%20be%20conscious%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6nbuf6ZDiQe5RWwqv%2Fhow-sure-are-you-that-brain-emulations-would-be-conscious", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6nbuf6ZDiQe5RWwqv%2Fhow-sure-are-you-that-brain-emulations-would-be-conscious", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 585, "htmlBody": "<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Or the converse problem - an agent that contains all the aspects of human value,&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">except&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">the valuation of subjective experience.&nbsp; So that the result is a nonsentient optimizer that goes around making genuine discoveries, but the discoveries are not savored and enjoyed, because there is no one there to do so.&nbsp; This, I admit, I don't quite know to be possible.&nbsp; Consciousness does still confuse me to some extent.&nbsp; But a universe with no one to bear witness to it, might as well not be.</span></p>\n</blockquote>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">- Eliezer Yudkowsky, <a href=\"/lw/y3/value_is_fragile/\">\"Value is Fragile\"</a></span></span></p>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">I had meant to try to write a long post for LessWrong on consciousness, but I'm getting stuck on it, partly because I'm not sure how well I know my audience here. So instead, I'm writing a short post, with my main purpose being just to informally poll the LessWrong community on one question: how sure are you that <a href=\"http://wiki.lesswrong.com/wiki/Whole_brain_emulation\">whole brain emulations</a> would be conscious?</span></span></p>\n<p style=\"text-align: justify;\">There's actually a fair amount of philosophical literature about issues in this vicinity; David Chalmers' paper <a href=\"http://consc.net/papers/singularity.pdf\">\"The Singularity: A Philosophical Analysis\"</a> has a good introduction to the debate in section 9, including some relevant terminology:</p>\n<blockquote>\n<p style=\"text-align: justify;\">Biological theorists of consciousness hold that consciousness is essentially biological and that no nonbiological system can be conscious. Functionalist theorists of consciousness hold that what matters to consciousness is not biological makeup but causal structure and causal role, so that a nonbiological system can be conscious as long as it is organized correctly.</p>\n</blockquote>\n<p style=\"text-align: justify;\">So, on the functionalist view, emulations would be conscious, while on the biological view, they would not be.</p>\n<p style=\"text-align: justify;\">Personally, I think there are good arguments for the functionalist view, and the biological view seems problematic: \"biological\" is a fuzzy, high-level category that doesn't seem like it could be of any fundamental importance. So <em>probably </em>emulations will be conscious--but I'm not too sure of that. Consciousness confuses me a great deal, and seems to confuse other people a great deal, and because of that I'd caution against being too sure of much of anything about consciousness. I'm worried not so much that the biological view will turn out to be right, but that the truth might be some third option no one has thought of, which might or might not entail emulations are conscious.</p>\n<p style=\"text-align: justify;\">Uncertainty about whether emulations would be conscious is potentially of great practical concern. I don't think it's much of an argument against uploading-as-life-extension; better to <em>probably </em>survive as an up than do nothing and die for sure. But it's worrisome if you think about the possibility, say, of an intended-to-be-Friendly AI deciding we'd all be better off if we were forcibly uploaded (or <em>persuaded, </em>using its superhuman intelligence, to \"voluntarily\" upload...) Uncertainty about whether emulations would be conscious also makes <a href=\"http://www.overcomingbias.com/tag/ems\">Robin Hanson's \"em revolution\" scenario</a> less appealing.</p>\n<p style=\"text-align: justify;\">For a long time, I've vaguely hoped that advances in neuroscience and cognitive science would lead to unraveling the problem of consciousness. Perhaps working on creating the first emulations would do the trick. But this is only a vague hope, I have no clear idea of <em>how </em>that could possibly happen. Another hope would be that if we can get all the other problems in Friendly AI right, we'll be able to trust the AI to solve consciousness for us. But with our present understanding of consciousness, can we really be sure that would be the case?</p>\n<p style=\"text-align: justify;\">That leads me to my second question for the LessWrong community: is there anything we can do <em>now </em>to to get clearer on consciousness? Any way to <a href=\"/lw/7t2/hard_problem_hack_away_at_the_edges/\">hack away</a> <a href=\"/lw/8ns/hack_away_at_the_edges/\">at the edges?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 3, "XSryTypw5Hszpa4TS": 3, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6nbuf6ZDiQe5RWwqv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 19, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "23794", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 177, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GNnHHmm8EzePmKzPk", "guRASsKLfdQ5m7LLw", "6bSHiD9TxsJwe2WqT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-26T15:40:52.755Z", "modifiedAt": null, "url": null, "title": "Two angles on Repetitive Strain Injury", "slug": "two-angles-on-repetitive-strain-injury", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:36.935Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HncaJbSKM5HFdLxp2/two-angles-on-repetitive-strain-injury", "pageUrlRelative": "/posts/HncaJbSKM5HFdLxp2/two-angles-on-repetitive-strain-injury", "linkUrl": "https://www.lesswrong.com/posts/HncaJbSKM5HFdLxp2/two-angles-on-repetitive-strain-injury", "postedAtFormatted": "Monday, August 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20angles%20on%20Repetitive%20Strain%20Injury&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20angles%20on%20Repetitive%20Strain%20Injury%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHncaJbSKM5HFdLxp2%2Ftwo-angles-on-repetitive-strain-injury%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20angles%20on%20Repetitive%20Strain%20Injury%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHncaJbSKM5HFdLxp2%2Ftwo-angles-on-repetitive-strain-injury", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHncaJbSKM5HFdLxp2%2Ftwo-angles-on-repetitive-strain-injury", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p><a href=\"http://esr.ibiblio.org/?p=4975#more-4975\">Discussion of buckling-spring keyboards</a>, which give accurate tactile feedback. You can get them for about $70, and a lot of people swear by them.</p>\n<p>Here's my RSI story: Some years ago, I was getting a lot of pain in my right elbow, presumably as + result of excessive Blockout (3D tetris), counted crosstitch, and being polite for no good reason to someone I was very angry at when I was under stress. Rest was not helping. I remember needing to sign a bunch of checks, and using my right hand for the signatures and my left for the account numbers, and it was still hard on my elbow.</p>\n<p><a href=\"http://www.amazon.com/The-Way-Energy-Mastering-Internal/dp/0671736450\">The Way of Energy</a> is an excellent introduction to Taoist standing meditation. I worked up to being able to do twenty minutes of just plain standing and twenty minutes of holding a balloon (arms circled at a little below shoulder level) a day. After mere weeks, my elbow problem went away and never came back. Subjectively, I hit a point in meditation where it became obvious to me that I was using more effort to stand than I needed to, and I could just let go of the excess tension.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HncaJbSKM5HFdLxp2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 2, "extendedScore": null, "score": 1.317040324684252e-06, "legacy": true, "legacyId": "23869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-26T21:00:41.560Z", "modifiedAt": null, "url": null, "title": "Open thread, August 26 - September 1, 2013", "slug": "open-thread-august-26-september-1-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.042Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PrguWdB85Rzg6d6kT/open-thread-august-26-september-1-2013", "pageUrlRelative": "/posts/PrguWdB85Rzg6d6kT/open-thread-august-26-september-1-2013", "linkUrl": "https://www.lesswrong.com/posts/PrguWdB85Rzg6d6kT/open-thread-august-26-september-1-2013", "postedAtFormatted": "Monday, August 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20August%2026%20-%20September%201%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20August%2026%20-%20September%201%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrguWdB85Rzg6d6kT%2Fopen-thread-august-26-september-1-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20August%2026%20-%20September%201%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrguWdB85Rzg6d6kT%2Fopen-thread-august-26-september-1-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrguWdB85Rzg6d6kT%2Fopen-thread-august-26-september-1-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_icj\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_ib0\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_i93\" class=\"content clear\">\n<div class=\"md\">\n<div>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PrguWdB85Rzg6d6kT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23870", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-27T08:59:46.986Z", "modifiedAt": null, "url": null, "title": "Meetup : Saskatoon Meetup: Value of Information", "slug": "meetup-saskatoon-meetup-value-of-information", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nicholas_Rutherford", "createdAt": "2013-08-01T02:29:11.736Z", "isAdmin": false, "displayName": "Nicholas_Rutherford"}, "userId": "nucgkHPJBwJuK8sY7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZoiLkWcdPccFXjjK6/meetup-saskatoon-meetup-value-of-information", "pageUrlRelative": "/posts/ZoiLkWcdPccFXjjK6/meetup-saskatoon-meetup-value-of-information", "linkUrl": "https://www.lesswrong.com/posts/ZoiLkWcdPccFXjjK6/meetup-saskatoon-meetup-value-of-information", "postedAtFormatted": "Tuesday, August 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saskatoon%20Meetup%3A%20Value%20of%20Information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saskatoon%20Meetup%3A%20Value%20of%20Information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZoiLkWcdPccFXjjK6%2Fmeetup-saskatoon-meetup-value-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saskatoon%20Meetup%3A%20Value%20of%20Information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZoiLkWcdPccFXjjK6%2Fmeetup-saskatoon-meetup-value-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZoiLkWcdPccFXjjK6%2Fmeetup-saskatoon-meetup-value-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qa'>Saskatoon Meetup: Value of Information</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 August 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again everyone!</p>\n\n<p>Saskatoon's 2nd meetup, at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>For this week we'll be going over Value of Information calculations, what they are, and why they are super useful!  More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qa'>Saskatoon Meetup: Value of Information</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZoiLkWcdPccFXjjK6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3179291987374935e-06, "legacy": true, "legacyId": "23888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saskatoon_Meetup__Value_of_Information\">Discussion article for the meetup : <a href=\"/meetups/qa\">Saskatoon Meetup: Value of Information</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 August 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again everyone!</p>\n\n<p>Saskatoon's 2nd meetup, at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>For this week we'll be going over Value of Information calculations, what they are, and why they are super useful!  More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saskatoon_Meetup__Value_of_Information1\">Discussion article for the meetup : <a href=\"/meetups/qa\">Saskatoon Meetup: Value of Information</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saskatoon Meetup: Value of Information", "anchor": "Discussion_article_for_the_meetup___Saskatoon_Meetup__Value_of_Information", "level": 1}, {"title": "Discussion article for the meetup : Saskatoon Meetup: Value of Information", "anchor": "Discussion_article_for_the_meetup___Saskatoon_Meetup__Value_of_Information1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-27T13:04:42.568Z", "modifiedAt": "2020-07-11T06:44:38.555Z", "url": null, "title": "How I Am Productive", "slug": "how-i-am-productive", "viewCount": null, "lastCommentedAt": "2015-09-12T13:11:03.949Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JTHe5oGvdj6T73o4o/how-i-am-productive", "pageUrlRelative": "/posts/JTHe5oGvdj6T73o4o/how-i-am-productive", "linkUrl": "https://www.lesswrong.com/posts/JTHe5oGvdj6T73o4o/how-i-am-productive", "postedAtFormatted": "Tuesday, August 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20I%20Am%20Productive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20I%20Am%20Productive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTHe5oGvdj6T73o4o%2Fhow-i-am-productive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20I%20Am%20Productive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTHe5oGvdj6T73o4o%2Fhow-i-am-productive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTHe5oGvdj6T73o4o%2Fhow-i-am-productive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4068, "htmlBody": "<p>I like to think that I get a lot of stuff done. &nbsp;Other people have noticed this and asked me how I'm so productive. &nbsp;This essay is where I try and \"share my secrets\", so to speak.</p>\n<p>The real secret is that, in the past, I wasn't nearly as productive. &nbsp;I struggled with procrastination, had issues completing assignments on time, and always felt like I never had enough time to do things. &nbsp;But, starting in January and continuing for the past eight months, I have slowly implemented several systems and habits in my life that, taken together, have made me productive. &nbsp;Productivity is not a talent I have -- I've <em>learned</em>&nbsp;to be productive over the past several months and I have habits in place where I basically <em>cannot fail</em> to be productive.</p>\n<p>Hopefully these systems will work for you. &nbsp;I've seen some people adopt them to some success, but I've never seen anyone do it <em>exactly</em> the way I do. &nbsp;And perhaps it would even be bad to do it exactly the way I do, because everyone is just a little bit different. &nbsp;I'm <a href=\"http://www.lesswrong.com/lw/9v/beware_of_otheroptimizing/\">being aware of other-optimizing</a>&nbsp;and letting you just know what's worked for me. &nbsp;I make no claims that these systems will work for you. &nbsp;Your mileage may vary.</p>\n<p>So what are the systems? &nbsp;To get you to be productive, we'll need to get you to <strong>organize</strong>, to <strong>prioritize</strong>, then to <strong>do</strong>&nbsp;and <strong>review</strong>. &nbsp;Have those four things down and you'll have everything you need to be productive.</p>\n<p>&nbsp;</p>\n<hr />\n<h2><br /></h2>\n<h2>Organize</h2>\n<p>The first step to being productive <strong>is to be organized and remember things without memorizing them.</strong>&nbsp; If we get these systems down, you won't forget your ideas, when and where events are, what tasks you need to complete, what papers you have, and what emails you have.</p>\n<p>&nbsp;</p>\n<p><strong>The Most Important Rule: Write Things Down</strong></p>\n<p>If you only take away one system from one category, I want it to be this one. &nbsp;Whole essays can be written about these systems and this one is no different -- <strong>write things down.</strong>&nbsp; Whenever you have a cool idea, an event invitation, a task, etc., write it down. &nbsp;Always. &nbsp;Constantly. &nbsp;No excuses.</p>\n<p>I've found in my life that stress has come in surprising part from trying to keep everything in my head. &nbsp;When I write down everything I think is worth remembering, whether it be a concrete thing I need to do or just a cool yet unimportant idea I want to follow up on sometime later, I write it down. &nbsp;That gets it out of my head, and I no longer feel the need to remember things (as long as I remember to look them up later), and I feel much better.</p>\n<p>I've also found in my life that I constantly think I'll remember something and it's not worth writing down. &nbsp;More than half the time, I've been wrong and forgotten the thing. &nbsp;This has meant I've forgotten cool ideas and even forgotten events or to complete key items. &nbsp;Always write things down, no matter how convinced you are that you'll remember them.</p>\n<p><strong>How do you do this?</strong> &nbsp;I suggest getting something that will always be with you that you can write things down on. &nbsp;For the vast majority of my readers, this can be a phone where you text yourself messages. &nbsp;For a long time, I would use my smartphone to email myself notes, because I knew I'd always check my email later and then could record the note to a text document. &nbsp;Later on, I moved to keeping track of ideas on <a href=\"http://www.evernote.com\">Evernote</a>&nbsp;and then later moved on to keeping track of ideas on <a href=\"http://www.workflowy.com\">Workflowy</a>. &nbsp;Workflowy costs $5 a month to use it to full potential (worth it, in my opinion), but there are free alternatives (that aren't as good, in my opinion).</p>\n<p>However, don't shy away from the good old pen and paper if it gets the job done. &nbsp;I got <a href=\"http://www.amazon.com/dp/B008SLD3ES\">this notepad</a>&nbsp;for $6 and it's been great.</p>\n<p>&nbsp;</p>\n<p><strong>Keep Track of Events: The Calendar</strong></p>\n<p>Of course, some of the things you want to write down will be particular things that need to be recorded in particularly useful places. &nbsp;One of these things is <strong>events</strong>, or places you need to be at a particular time and place. &nbsp;For this, you can use any calendar, but I like <a href=\"http://calendar.google.com\">Google Calendar</a>&nbsp;the best. &nbsp;Whenever you get invited to an event, record it on your calendar. &nbsp;(We'll include reviewing your calendar regularly in a bit, so you won't forget what's there.)</p>\n<p>A common mistake I see people make is to rely on Facebook events to keep track of their events. &nbsp;Perhaps this works for some people, but not all events are done through Facebook or can be done through Facebook, so you end up keeping track of events in multiple places, which causes confusion and missed events. &nbsp;Wherever you record events, <strong>record all your events in one place.</strong></p>\n<p>&nbsp;</p>\n<p><strong>Keep Track of Tasks: The To-Do List</strong></p>\n<p>The next thing you'll want to keep track of is <strong>tasks</strong>. &nbsp;For this, you need a to-do list. &nbsp;I spent a lot of my life just using a TextEdit document, but I recommend you use a dedicated app instead. &nbsp;I personally use&nbsp;<a href=\"http://www.workflowy.com\">Workflowy</a>&nbsp;here too, but others work great. &nbsp;In the past I've used&nbsp;<a href=\"http://www.trello.com\">Trello</a>&nbsp;to great success. &nbsp;I've seen others succeed with <a href=\"https://asana.com/\">Asana</a>&nbsp;or even just a text document on the computer.</p>\n<p>A common mistake I see people make here is using their email as their to-do list. &nbsp;This might make some sense, but often emails contain information unnecessary to your tasks which slows you down, and sometimes emails contain multiple action points. &nbsp;Worse, emails contain no easy way to prioritize tasks (which is really important and will be discussed in a bit).</p>\n<p>Bottom line: <strong>Keep all your tasks in one crisp, clear place.</strong>&nbsp; Don't spread out your to-do lists across multiple applications and don't put it in with a bunch of other stuff.</p>\n<p>&nbsp;</p>\n<p><strong>Action, Waiting, Reference: Stay Organized with Zones</strong></p>\n<p>Once you have your ideas written down, your events on your calendar, and your tasks on your to-do list, it's time to organize the materials you'll have to deal with. &nbsp;Lots of physical papers and computer documents come at you throughout your day and it's time to organize them.</p>\n<p>The trick here? &nbsp;Get a surface area you can keep clear and divide it into three zones: <strong>action</strong>, <strong>waiting</strong>, and <strong>reference</strong>.</p>\n<p>&nbsp;</p>\n<p>The <strong>action zone</strong>&nbsp;is for things that need to be done. &nbsp;Have a form you need to fill out? &nbsp;Something you need to read? &nbsp;Even more outlandish things like a necklace you need to repair or something? &nbsp;Keep everything needed for a task together in folders or with paperclips as necessary, put it in the action zone, and record the task on your to-do list.</p>\n<p>The <strong>waiting zone</strong>&nbsp;is for things that eventually need to be done, but which cannot be done yet because you're waiting on something. &nbsp;Perhaps you need feedback from someone, a package still needs to arrive, or the task only can be done on a certain day. &nbsp;For this, keep everything grouped together in the waiting zone, and record on your to-do list what the task is and what you're waiting for. &nbsp;(We'll revisit implementing zones in the to-do list in a little bit.) &nbsp;Move things to action and update your to-do list when what you're waiting for arrives.</p>\n<p>The <strong>reference zone</strong>&nbsp;is for things you might need to look at and need to be kept around, but are not associated with any task. &nbsp;For examples, things I have had in my reference zone are passwords, details about tasks from people, items that are relevant but not necessary to the work that I'm doing, etc.</p>\n<p>&nbsp;</p>\n<p><strong>Always Inbox Zero: Apply the Folders to Your Email</strong></p>\n<p>Email is really messy for most people, but it doesn't have to be. &nbsp;The solution here is to implement the zones in your email too. &nbsp;I use <a href=\"http://www.gmail.com\">Gmail</a>, but nearly every email system includes folders these days. &nbsp;Use that system to create three folders -- action, waiting, and reference -- in your email, then sort your email according to the folders and record on your to-do list.</p>\n<p><strong>There is no reason to have any email in your inbox.</strong>&nbsp; You should be at \"inbox zero\" <em>constantly</em>. &nbsp;Whenever an email comes in, process it and file it. &nbsp;Got an email from Nancy that you need to reply to? &nbsp;Put it in \"Action\" and put \"Reply to Nancy's email\" on your to-do list. &nbsp;Got a long email from your boss that you don't even have time to read yet? &nbsp;Put it in \"Action\" and put \"Read boss's email\" on your to-do list. &nbsp;Then when you go back to read it, you can determine the next action item.</p>\n<p>Emails also make sense to be put in waiting. &nbsp;If it's important I get a reply from the email, I'll put it in waiting to remind myself to follow up later if necessary (more on that later). &nbsp;I'll also put emails in waiting if I'm expecting a reply from someone else first, or if it's information for an action item I can't act on yet, or if I want to reply later on.</p>\n<p>Lastly, reference is very important for emails that you need to keep around to read, but don't need to reply to. &nbsp;Lots of notes that people send me get processed into my relevant Workflowy document and then kept in reference for as long as they're relevant.</p>\n<p align=\"center\"><img src=\"http://everydayutilitarian.com/images/posts/how-i-am-productive/zero.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<hr />\n<h2><br /></h2>\n<h2>Prioritize</h2>\n<p>Now that you're all organized, it's time to get in a position to do the things you need to do. &nbsp;But watch out, because unless you have time to complete your entire to-do list in one sitting, it's a poor use of time to just go from the top to the bottom. &nbsp;Instead, we need to <strong>go from the most important to the least important.</strong></p>\n<p>&nbsp;</p>\n<p><strong>Eisenhower Matrix: Do What's Important</strong></p>\n<p><strong></strong>How do you prioritize? &nbsp;The best tactic I've seen here is called <a href=\"https://timegt.com/2010/07/14/what-is-the-eisenhower-matrix/\">The Eisenhower Matrix</a>. &nbsp;It comes from Steven Covey's book <strong><a href=\"http://www.amazon.com/dp/0684802031\">First Things First</a></strong>&nbsp;but is credited to President Dwight D. Eisenhower.</p>\n<p>Here, you take your to-do list and organize everything into four quadrants: important and urgent, important and not urgent, unimportant and urgent, and unimportant and not urgent. &nbsp;This is very easy to do on Workflowy, and still possible on something like Trello.</p>\n<p>There's pretty universal agreement that you complete all the \"important and urgent\" tasks first and the \"unimportant and not urgent\" tasks last. &nbsp;But the real trick is that <strong>after you complete the important and urgent tasks, you should move to complete the important and not urgent tasks.</strong>&nbsp; Ignore the not important and urgent tasks until you've completed all important tasks and even be comfortable with skipping unimportant tasks if necessary. &nbsp;Why? &nbsp;Because they're not important.</p>\n<p>If you get this matrix down, you'll soon get ahead on your tasks, because you'll be completing important tasks <em>before they become urgent</em>.</p>\n<p>Also, note the inclusion of \"waiting\" here as one of the tabs in my to-do list. &nbsp;This is where I put tasks I can't complete yet with a note of what I'm waiting on. &nbsp;Something like talking to my Dad three days from now would be tagged as \"#30aug :: Talk to Dad\" (using Workflowy hashtags), but I'd also do things with unclear dates, like \"Brian responds to email :: Forward response to Seth\". &nbsp;Beware that being able to manage unclear deadlines (where you don't know what day the task will be) is something that most to-do list apps struggle with.</p>\n<p><img src=\"http://everydayutilitarian.com/images/posts/how-i-am-productive/matrix.png\" alt=\"\" width=\"292\" height=\"300\" /></p>\n<p><strong>Timeboxing: Plan Your Day in Advance</strong></p>\n<p>The next prioritization thing to master is <em>planning your day in advance</em>. &nbsp;You do this through making \"time boxes\" for things, or periods of time where you'll do something predefined. &nbsp;For example, I'll set aside some time to work through my to-do list or to work on particular projects. &nbsp;For bigger projects, I'll decide how much I want to work on them in any particular day or week and set them aside from my to-do list. &nbsp;I'll then block out time for them on my calendar and end up with <a href=\"http://www.everydayutilitarian.com/wp-content/uploads/2013/08/Screen-Shot-2013-08-26-at-1.07.28-PM.png\">days like this</a>.</p>\n<p>Since I plan my days in advance using this timebox method, I just plan every minute of the calendar in advance and have a plan so I always know what to be doing and never miss a beat. &nbsp;Of course, things come up and you'll have to change your plan for the day, but that's better than having no plan at all.</p>\n<p><img src=\"http://www.everydayutilitarian.com/images/posts/how-i-am-productive/workflowy.png\" alt=\"\" /></p>\n<p><strong>Two Minute Rule</strong></p>\n<p>It's important to be mindful of how much time it takes to record a task, put it in your to-do list, and prioritize it, however. &nbsp;For most people, including me, it's about two minutes for any given task. &nbsp;This gives rise to the \"two minute rule\": <strong>if doing somethign would take less than two minutes, just do it now.</strong>&nbsp; Likewise, if it would take over two minutes, put it in your to-do list and do it at the best time.</p>\n<p>&nbsp;</p>\n<hr />\n<h2><br /></h2>\n<h2>Do</h2>\n<p>Now that you have your to-do list set and timeboxes for when you're going to work and on what, it's time to actually <em>do the work</em>.</p>\n<p>&nbsp;</p>\n<p><strong>The Pomodoro Technique</strong></p>\n<p>The ideal timebox should be a length that is a multiple of thirty minutes so you can do the most powerful productivity thing there is: <a href=\"http://www.pomodorotechnique.com/\">The Pomodoro Technique</a>. &nbsp;Beware that it doesn't work for some, but I do urge you to give it a fair shake and a few tries, because for those whom the Pomodoro works, the Pomodoro Technique works wonders.</p>\n<p>Here's how you do it. &nbsp;Set a timer for 25 minutes. &nbsp;During those 25 minutes (a) work <strong>only</strong>&nbsp;on your task at hand; (b)&nbsp;<strong>do not&nbsp;</strong>do anything else, even for a second; (c) be completely focused; (d) be free from distractions; (e) and <strong>do not multi-task.</strong>&nbsp;There are some acceptable things to do during a Pomodoro, however: go to the bathroom, drink, listen to music. &nbsp;But there are tons more things not to do during a Pomodoro: check Facebook, read your email, etc. &nbsp;The list will go on.</p>\n<p>After the timer expires, take a five minute break. &nbsp;During these five minutes, do anything you'd like <strong>except</strong>&nbsp;the task on hand. &nbsp;Even if you feel like the break is boring and you're itching to get back on task, <strong>don't.</strong>&nbsp; You're only hurting yourself in the long-run. &nbsp;This five minute break will restore your focus, keep you grounded, provide a way to think through your ideas in a different setting, and prevent you from needing longer breaks later in the day.</p>\n<p>It should be noted, however, that the Pomodoro can be a bit difficult to get in the habit of, though. &nbsp;To solve this, I've found it useful to work my way up to the full Pomodoro by spending a month getting used to \"15 minutes of work, 5 minutes break\", then another month doing \"20 minutes of work, 5 minutes break\", and then finally \"25 minutes of work, 5 minutes break\".</p>\n<p>Different people have tried other multiples besides 25 and 5, but I'm still convinced that 25-5 is the ideal split. &nbsp;Perhaps 27-3 could work better for advanced Pomodoro users, but I wouldn't push it further. &nbsp;I've seen things like 90-30 or <a href=\"http://zenhabits.net/my-fav-procrastination-hack-30-10/\">30-10</a>, and all of these seem to involve working just a little too long (losing focus) and then taking a lot more break than is necessary. &nbsp;Of course, if it works for you, then it works.</p>\n<p>Here's the <a href=\"http://www.online-stopwatch.com/full-screen-interval-timer/?c=vxv55r3380\">25-5 stopwatch</a>&nbsp;I use and my <a href=\"http://www.online-stopwatch.com/full-screen-interval-timer/?c=sxodrlz842\">20-5 stopwatch</a>. &nbsp;I've also liked <a href=\"http://tomato-timer.com/\">Tomato Timer.com</a>, but any timer can work.</p>\n<p>&nbsp;</p>\n<p><strong>Be Comfortable with Breaks</strong></p>\n<p>The important lesson of working a lot is to be comfortable with taking a break. &nbsp;The novice productive person will think it virtuous to work clear through a break and onward, thinking that he or she is making even better use of their time, defeating all those sissy workers who need breaks! &nbsp;But really, this person is just setting up their own downfall, because they'll crash and burn.</p>\n<p>Burnout is real and one of the most dangerous things you can do is <a href=\"/lw/8gv/the_curse_of_identity/\">train yourself to feel guilty about not working</a>. &nbsp;So you need to remember to take breaks. &nbsp;The break in a Pomodoro is a good one, but I also recommend taking a larger break (like 30 minutes) after completing three or four Pomodoros.</p>\n<p>One particularly good break I'd like to give a shout-out to is to take a nap. &nbsp;Taking a nap at a fairly regular time has <a href=\"http://www.medicalnewstoday.com/releases/179882.php\">health benefits</a>&nbsp;(see also <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000341\">here</a>, <a href=\"http://www.eurekalert.org/pub_releases/2006-06/uom-wwc060106.php\">here</a>, and <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17053484\">here</a>) and <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/11763827\">doesn't harm your night sleep</a>&nbsp;if you nap for 20 minutes and don't nap too late in the afternoon or evening. &nbsp;In fact, I've actually found naps to be a <em>time saver</em>&nbsp;instead of time \"wasted\" for a break, because I can sleep less at night and still feel rested and be focused throughout the day.</p>\n<p>&nbsp;</p>\n<p><strong>Keep Your Energy Up</strong></p>\n<p>Another thing to prevent your chance of crashing and needing a long break to restore your energy is to keep your energy up. &nbsp;I recommend drinking something that is somewhat sugary but not too sugary (I drink water-diluted lemonade in a 25%-75% mix) and remembering to exercise on a regular basis. &nbsp;Also, <a href=\"http://www.nomeatathlete.com/healthy-eating/\">eating healthy</a>&nbsp;and sleeping right works wonders for keeping your attention on your work.</p>\n<p>&nbsp;</p>\n<hr />\n<h2><br /></h2>\n<h2>Review</h2>\n<p>Of course, it's not enough to do if you're not going to learn from how you're doing and improve. &nbsp;I suggest you review your life on multiple levels -- daily, weekly, monthly, and once every six months.</p>\n<p>For the <strong>daily review</strong>, I keep track of whether I've <a href=\"http://www.everydayutilitarian.com/essays/category/resolutions/\">succeeded at certain habits</a>&nbsp;like exercising and eating right, and log the amount of time I've spent on various things so I can keep track of my time usage. &nbsp;I also complete other relevant logs, and then spend a bit of time reflecting how things have gone for the day and think of ways to repeat successes and avoid mistakes. &nbsp;I then check the plan for the next day and tweak it if necessary. &nbsp;This process takes me about 15 to 20 minutes.</p>\n<p>For the <strong>weekly review</strong>, I go through my action-waiting-reference zones wherever they exist (physical piles, email, and computer folders) and process them -- make sure everything there is still relevant and still belongs in the same place. &nbsp;I'll remove whatever needs to be removed at this stage and remind myself what I'm working on. &nbsp;I'll organize and clean anything that isn't organized at this stage and get everything together. &nbsp;I'll then quickly re-read m<a href=\"http://www.everydayutilitarian.com/essays/my-strategic-plan/\">y strategic plan</a>&nbsp;and plan out the week in accordance with my goals. &nbsp;Recently, I've set amounts of time per week I want to be spending on certain projects, so it's now a matter of making a schedule that works. &nbsp;This process usually takes me 45 minutes to an hour.</p>\n<p>For the <strong>monthly review</strong>, I reflect on the habits I've been trying to build for the month and decide what habits I want to keep, what habits I want to add, and what habits I want to subtract. &nbsp;I review how the month as a whole went and think about what I can do to repeat successes and avert future failures. &nbsp;I then write up a reflection and <a href=\"http://www.everydayutilitarian.com/essays/category/resolutions/\">publish it on my blog</a>. &nbsp;This process usually takes me two hours.</p>\n<p>For the <strong>six month review</strong>, I return to my goals and think about how my life trajectory as a whole is going. &nbsp;What are my life goals? &nbsp;What am I doing to accomplish them? &nbsp;Am I closer to my goals than I was six months ago? &nbsp;Should I be working toward new goals? &nbsp;What common mistakes did I make through the past six months that I want to avoid? &nbsp;I then <a href=\"http://www.everydayutilitarian.com/essays/my-strategic-plan/\">write a document</a>with my personal mission and goals for the next six months and skim it every week to constantly remind myself of what I want to be doing. &nbsp;This process usually takes me three hours.</p>\n<p>&nbsp;</p>\n<p>Yes, there will be an unlucky day where you do all four reviews and spend like six and a half hours reviewing your life at different levels. &nbsp;Perhaps this is a bit much for people, but I've found tremendous benefit from it. &nbsp;I've found that spending this day reviewing my life has saved me from not just days, but even months, of wasted time that doesn't accomplish what I really want to do. &nbsp;<strong>Reviewing is another way of saving you time.</strong></p>\n<p><strong><br /></strong></p>\n<hr />\n<h2><br /></h2>\n<h2>Additional Tips</h2>\n<p>Now I've given you all my main advice, but I have some additional tips if you want to keep reading.</p>\n<p><strong>Carefully form these habits over time.</strong> &nbsp;This is a lot to do at once, so do it in stages. &nbsp;Build the habit of writing things down first, and then slowly get the apps you like in place for ideas, events, and tasks. &nbsp;After you have that down, spend the time necessary to get your email in order and implement the zones wherever possible. &nbsp;Then begin to move into prioritizing your tasks with the Eisenhower Matrix. &nbsp;After you have this down, begin planning your days in advance with timeboxes and start doing your reviews. &nbsp;While you're building that habit, simultaneously start building up the Pomodoro habit, slowly approaching 25-5 over a few months.</p>\n<p><strong>Find a way to reliably stay on habit.</strong> &nbsp;Don't make the common failure of sticking to something for a month or two and abandoning it. &nbsp;Spend a lot of energy thinking through how you'll stay on habit and how you'll not be like all the other people who think they'll stay on habit then fail. &nbsp;<a href=\"http://hackthesystem.com/blog/the-bet-switch-mechanism-the-one-simple-tactic-that-will-make-you-lose-weight-eat-right-and-get-in-the-best-shape-of-your-life/\">Make a bet with a friend</a>, <a href=\"https://www.beeminder.com/\">start up Beeminder</a>, or create some other kind of commitment device.</p>\n<p><strong>Form the productivity mindset.</strong> I had a lot of trouble implementing this plan until I was able to think of myself as an important person who does important things and should personally value my time. &nbsp;I had to really want to be productive before I could start being productive. &nbsp;Success at this will follow from the right mindset. &nbsp;It's time to start thinking of yourself as important. &nbsp;If you can't fool yourself, maybe it's time to look at your goals and decide what goals would make you feel important and then <em>do those goals instead</em>.</p>\n<p><strong>Behold the power of routines.</strong>&nbsp;I find it a lot easier to exercise if I have a routine of \"every other day, right after waking up\" or \"every other day, right before dinner\". &nbsp;Your routine can be built from here. &nbsp;It's a lot easier to stick to timeboxes if they're regularly occurring. &nbsp;Use a calendar and build yourself something nice.</p>\n<p><strong>Put everything in a particular place.</strong>&nbsp; People lose a lot of time just hunting around for things. &nbsp;Solve this by spending some time ahead of time organizing things in your life and getting them into particular places. &nbsp;Then always make sure things return to their places.</p>\n<p><strong>Declutter your life.</strong>&nbsp; You'll work better if you have less stuff to keep track of and less commitments to worry about. &nbsp;Get rid of everything and delegate anything you can.</p>\n<p><strong>Make a productivity place.</strong>&nbsp;This works especially well in colleges where there is a large variety of places you could be working. &nbsp;Find a place to work, set up your Pomodoros, and follow them to the letter. &nbsp;Don't mess up. &nbsp;Take your longer breaks somewhere else. &nbsp;If you do mess up, find a new productivity place and start again. &nbsp;I found this really helpful for my mindset, but others have found it silly.</p>\n<p><strong>Don't neglect friends and family.</strong>&nbsp; This is a big one. &nbsp;Remember, the goal of being more productive is to free time to do the things you want and be with the people you want. &nbsp;It's not to spend 100 hour workweeks neglecting those who are important to you. &nbsp;Make sure to take some time off to spend with friends and family. &nbsp;Schedule it in your calendar if you have to. &nbsp;This will matter most in the long-run for your life.</p>\n<p><strong>Productivity &ne; Busy and Busy &ne; Productivity.</strong>&nbsp; If you do productivity right, you shouldn't feel busy all that often. &nbsp;Being busy is a sign of having poor productivity and/or having taken on too many commitments, and is rarely ever a sign of doing things correctly.</p>\n<p>&nbsp;</p>\n<hr />\n<h2><br /></h2>\n<h2>Conclusion</h2>\n<p>These tips are really a result of me experimenting for eight months. &nbsp;I'd expect you to take a similar amount of time to go from zero to productive and end up with different systems that work for you and your environment. &nbsp;But I think there are a lot of power in these systems and I'm interested to see what other people do and how other people run with them. &nbsp;After all, <a href=\"/lw/dr/generalizing_from_one_example/\">they work for <em>me</em></a>.</p>\n<p>&nbsp;</p>\n<p><strong>Further reading:</strong></p>\n<p>* <a href=\"http://www.thesecretweapon.org/\">The Secret Weapon</a></p>\n<p>* David Allen's <a href=\"http://www.amazon.com/dp/0142000280\"><strong>Getting Things Done: The Art of Stress-Free Productivity</strong></a></p>\n<p>* Scott Young's <a href=\"http://www.stafforini.com/blog/summary-of-the-little-book-of-productivity-by-scott-young/\"><strong>The Little Book of Productivity</strong></a></p>\n<p>* <a href=\"/lw/fux/my_workflow/\">Paul Christiano's Workflow</a></p>\n<p>* <a href=\"/lw/hgd/10step_antiprocrastination_checklist/\">10 Step Anti-Procrostination Checklist</a></p>\n<p>* <a href=\"http://www.zenhabits.net\">Zenhabits</a></p>\n<p>&nbsp;</p>\n<p>-</p>\n<p><em>(Also <a href=\"http://www.everydayutilitarian.com/essays/how-i-am-productive\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JTHe5oGvdj6T73o4o", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 64, "extendedScore": null, "score": 0.000155, "legacy": true, "legacyId": "23889", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I like to think that I get a lot of stuff done. &nbsp;Other people have noticed this and asked me how I'm so productive. &nbsp;This essay is where I try and \"share my secrets\", so to speak.</p>\n<p>The real secret is that, in the past, I wasn't nearly as productive. &nbsp;I struggled with procrastination, had issues completing assignments on time, and always felt like I never had enough time to do things. &nbsp;But, starting in January and continuing for the past eight months, I have slowly implemented several systems and habits in my life that, taken together, have made me productive. &nbsp;Productivity is not a talent I have -- I've <em>learned</em>&nbsp;to be productive over the past several months and I have habits in place where I basically <em>cannot fail</em> to be productive.</p>\n<p>Hopefully these systems will work for you. &nbsp;I've seen some people adopt them to some success, but I've never seen anyone do it <em>exactly</em> the way I do. &nbsp;And perhaps it would even be bad to do it exactly the way I do, because everyone is just a little bit different. &nbsp;I'm <a href=\"http://www.lesswrong.com/lw/9v/beware_of_otheroptimizing/\">being aware of other-optimizing</a>&nbsp;and letting you just know what's worked for me. &nbsp;I make no claims that these systems will work for you. &nbsp;Your mileage may vary.</p>\n<p>So what are the systems? &nbsp;To get you to be productive, we'll need to get you to <strong>organize</strong>, to <strong>prioritize</strong>, then to <strong>do</strong>&nbsp;and <strong>review</strong>. &nbsp;Have those four things down and you'll have everything you need to be productive.</p>\n<p>&nbsp;</p>\n<hr>\n<h2><br></h2>\n<h2 id=\"Organize\">Organize</h2>\n<p>The first step to being productive <strong>is to be organized and remember things without memorizing them.</strong>&nbsp; If we get these systems down, you won't forget your ideas, when and where events are, what tasks you need to complete, what papers you have, and what emails you have.</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Most_Important_Rule__Write_Things_Down\">The Most Important Rule: Write Things Down</strong></p>\n<p>If you only take away one system from one category, I want it to be this one. &nbsp;Whole essays can be written about these systems and this one is no different -- <strong>write things down.</strong>&nbsp; Whenever you have a cool idea, an event invitation, a task, etc., write it down. &nbsp;Always. &nbsp;Constantly. &nbsp;No excuses.</p>\n<p>I've found in my life that stress has come in surprising part from trying to keep everything in my head. &nbsp;When I write down everything I think is worth remembering, whether it be a concrete thing I need to do or just a cool yet unimportant idea I want to follow up on sometime later, I write it down. &nbsp;That gets it out of my head, and I no longer feel the need to remember things (as long as I remember to look them up later), and I feel much better.</p>\n<p>I've also found in my life that I constantly think I'll remember something and it's not worth writing down. &nbsp;More than half the time, I've been wrong and forgotten the thing. &nbsp;This has meant I've forgotten cool ideas and even forgotten events or to complete key items. &nbsp;Always write things down, no matter how convinced you are that you'll remember them.</p>\n<p><strong>How do you do this?</strong> &nbsp;I suggest getting something that will always be with you that you can write things down on. &nbsp;For the vast majority of my readers, this can be a phone where you text yourself messages. &nbsp;For a long time, I would use my smartphone to email myself notes, because I knew I'd always check my email later and then could record the note to a text document. &nbsp;Later on, I moved to keeping track of ideas on <a href=\"http://www.evernote.com\">Evernote</a>&nbsp;and then later moved on to keeping track of ideas on <a href=\"http://www.workflowy.com\">Workflowy</a>. &nbsp;Workflowy costs $5 a month to use it to full potential (worth it, in my opinion), but there are free alternatives (that aren't as good, in my opinion).</p>\n<p>However, don't shy away from the good old pen and paper if it gets the job done. &nbsp;I got <a href=\"http://www.amazon.com/dp/B008SLD3ES\">this notepad</a>&nbsp;for $6 and it's been great.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Keep_Track_of_Events__The_Calendar\">Keep Track of Events: The Calendar</strong></p>\n<p>Of course, some of the things you want to write down will be particular things that need to be recorded in particularly useful places. &nbsp;One of these things is <strong>events</strong>, or places you need to be at a particular time and place. &nbsp;For this, you can use any calendar, but I like <a href=\"http://calendar.google.com\">Google Calendar</a>&nbsp;the best. &nbsp;Whenever you get invited to an event, record it on your calendar. &nbsp;(We'll include reviewing your calendar regularly in a bit, so you won't forget what's there.)</p>\n<p>A common mistake I see people make is to rely on Facebook events to keep track of their events. &nbsp;Perhaps this works for some people, but not all events are done through Facebook or can be done through Facebook, so you end up keeping track of events in multiple places, which causes confusion and missed events. &nbsp;Wherever you record events, <strong>record all your events in one place.</strong></p>\n<p>&nbsp;</p>\n<p><strong id=\"Keep_Track_of_Tasks__The_To_Do_List\">Keep Track of Tasks: The To-Do List</strong></p>\n<p>The next thing you'll want to keep track of is <strong>tasks</strong>. &nbsp;For this, you need a to-do list. &nbsp;I spent a lot of my life just using a TextEdit document, but I recommend you use a dedicated app instead. &nbsp;I personally use&nbsp;<a href=\"http://www.workflowy.com\">Workflowy</a>&nbsp;here too, but others work great. &nbsp;In the past I've used&nbsp;<a href=\"http://www.trello.com\">Trello</a>&nbsp;to great success. &nbsp;I've seen others succeed with <a href=\"https://asana.com/\">Asana</a>&nbsp;or even just a text document on the computer.</p>\n<p>A common mistake I see people make here is using their email as their to-do list. &nbsp;This might make some sense, but often emails contain information unnecessary to your tasks which slows you down, and sometimes emails contain multiple action points. &nbsp;Worse, emails contain no easy way to prioritize tasks (which is really important and will be discussed in a bit).</p>\n<p>Bottom line: <strong>Keep all your tasks in one crisp, clear place.</strong>&nbsp; Don't spread out your to-do lists across multiple applications and don't put it in with a bunch of other stuff.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Action__Waiting__Reference__Stay_Organized_with_Zones\">Action, Waiting, Reference: Stay Organized with Zones</strong></p>\n<p>Once you have your ideas written down, your events on your calendar, and your tasks on your to-do list, it's time to organize the materials you'll have to deal with. &nbsp;Lots of physical papers and computer documents come at you throughout your day and it's time to organize them.</p>\n<p>The trick here? &nbsp;Get a surface area you can keep clear and divide it into three zones: <strong>action</strong>, <strong>waiting</strong>, and <strong>reference</strong>.</p>\n<p>&nbsp;</p>\n<p>The <strong>action zone</strong>&nbsp;is for things that need to be done. &nbsp;Have a form you need to fill out? &nbsp;Something you need to read? &nbsp;Even more outlandish things like a necklace you need to repair or something? &nbsp;Keep everything needed for a task together in folders or with paperclips as necessary, put it in the action zone, and record the task on your to-do list.</p>\n<p>The <strong>waiting zone</strong>&nbsp;is for things that eventually need to be done, but which cannot be done yet because you're waiting on something. &nbsp;Perhaps you need feedback from someone, a package still needs to arrive, or the task only can be done on a certain day. &nbsp;For this, keep everything grouped together in the waiting zone, and record on your to-do list what the task is and what you're waiting for. &nbsp;(We'll revisit implementing zones in the to-do list in a little bit.) &nbsp;Move things to action and update your to-do list when what you're waiting for arrives.</p>\n<p>The <strong>reference zone</strong>&nbsp;is for things you might need to look at and need to be kept around, but are not associated with any task. &nbsp;For examples, things I have had in my reference zone are passwords, details about tasks from people, items that are relevant but not necessary to the work that I'm doing, etc.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Always_Inbox_Zero__Apply_the_Folders_to_Your_Email\">Always Inbox Zero: Apply the Folders to Your Email</strong></p>\n<p>Email is really messy for most people, but it doesn't have to be. &nbsp;The solution here is to implement the zones in your email too. &nbsp;I use <a href=\"http://www.gmail.com\">Gmail</a>, but nearly every email system includes folders these days. &nbsp;Use that system to create three folders -- action, waiting, and reference -- in your email, then sort your email according to the folders and record on your to-do list.</p>\n<p><strong>There is no reason to have any email in your inbox.</strong>&nbsp; You should be at \"inbox zero\" <em>constantly</em>. &nbsp;Whenever an email comes in, process it and file it. &nbsp;Got an email from Nancy that you need to reply to? &nbsp;Put it in \"Action\" and put \"Reply to Nancy's email\" on your to-do list. &nbsp;Got a long email from your boss that you don't even have time to read yet? &nbsp;Put it in \"Action\" and put \"Read boss's email\" on your to-do list. &nbsp;Then when you go back to read it, you can determine the next action item.</p>\n<p>Emails also make sense to be put in waiting. &nbsp;If it's important I get a reply from the email, I'll put it in waiting to remind myself to follow up later if necessary (more on that later). &nbsp;I'll also put emails in waiting if I'm expecting a reply from someone else first, or if it's information for an action item I can't act on yet, or if I want to reply later on.</p>\n<p>Lastly, reference is very important for emails that you need to keep around to read, but don't need to reply to. &nbsp;Lots of notes that people send me get processed into my relevant Workflowy document and then kept in reference for as long as they're relevant.</p>\n<p align=\"center\"><img src=\"http://everydayutilitarian.com/images/posts/how-i-am-productive/zero.png\" alt=\"\"></p>\n<p>&nbsp;</p>\n<hr>\n<h2><br></h2>\n<h2 id=\"Prioritize\">Prioritize</h2>\n<p>Now that you're all organized, it's time to get in a position to do the things you need to do. &nbsp;But watch out, because unless you have time to complete your entire to-do list in one sitting, it's a poor use of time to just go from the top to the bottom. &nbsp;Instead, we need to <strong>go from the most important to the least important.</strong></p>\n<p>&nbsp;</p>\n<p><strong id=\"Eisenhower_Matrix__Do_What_s_Important\">Eisenhower Matrix: Do What's Important</strong></p>\n<p><strong></strong>How do you prioritize? &nbsp;The best tactic I've seen here is called <a href=\"https://timegt.com/2010/07/14/what-is-the-eisenhower-matrix/\">The Eisenhower Matrix</a>. &nbsp;It comes from Steven Covey's book <strong><a href=\"http://www.amazon.com/dp/0684802031\">First Things First</a></strong>&nbsp;but is credited to President Dwight D. Eisenhower.</p>\n<p>Here, you take your to-do list and organize everything into four quadrants: important and urgent, important and not urgent, unimportant and urgent, and unimportant and not urgent. &nbsp;This is very easy to do on Workflowy, and still possible on something like Trello.</p>\n<p>There's pretty universal agreement that you complete all the \"important and urgent\" tasks first and the \"unimportant and not urgent\" tasks last. &nbsp;But the real trick is that <strong>after you complete the important and urgent tasks, you should move to complete the important and not urgent tasks.</strong>&nbsp; Ignore the not important and urgent tasks until you've completed all important tasks and even be comfortable with skipping unimportant tasks if necessary. &nbsp;Why? &nbsp;Because they're not important.</p>\n<p>If you get this matrix down, you'll soon get ahead on your tasks, because you'll be completing important tasks <em>before they become urgent</em>.</p>\n<p>Also, note the inclusion of \"waiting\" here as one of the tabs in my to-do list. &nbsp;This is where I put tasks I can't complete yet with a note of what I'm waiting on. &nbsp;Something like talking to my Dad three days from now would be tagged as \"#30aug :: Talk to Dad\" (using Workflowy hashtags), but I'd also do things with unclear dates, like \"Brian responds to email :: Forward response to Seth\". &nbsp;Beware that being able to manage unclear deadlines (where you don't know what day the task will be) is something that most to-do list apps struggle with.</p>\n<p><img src=\"http://everydayutilitarian.com/images/posts/how-i-am-productive/matrix.png\" alt=\"\" width=\"292\" height=\"300\"></p>\n<p><strong id=\"Timeboxing__Plan_Your_Day_in_Advance\">Timeboxing: Plan Your Day in Advance</strong></p>\n<p>The next prioritization thing to master is <em>planning your day in advance</em>. &nbsp;You do this through making \"time boxes\" for things, or periods of time where you'll do something predefined. &nbsp;For example, I'll set aside some time to work through my to-do list or to work on particular projects. &nbsp;For bigger projects, I'll decide how much I want to work on them in any particular day or week and set them aside from my to-do list. &nbsp;I'll then block out time for them on my calendar and end up with <a href=\"http://www.everydayutilitarian.com/wp-content/uploads/2013/08/Screen-Shot-2013-08-26-at-1.07.28-PM.png\">days like this</a>.</p>\n<p>Since I plan my days in advance using this timebox method, I just plan every minute of the calendar in advance and have a plan so I always know what to be doing and never miss a beat. &nbsp;Of course, things come up and you'll have to change your plan for the day, but that's better than having no plan at all.</p>\n<p><img src=\"http://www.everydayutilitarian.com/images/posts/how-i-am-productive/workflowy.png\" alt=\"\"></p>\n<p><strong id=\"Two_Minute_Rule\">Two Minute Rule</strong></p>\n<p>It's important to be mindful of how much time it takes to record a task, put it in your to-do list, and prioritize it, however. &nbsp;For most people, including me, it's about two minutes for any given task. &nbsp;This gives rise to the \"two minute rule\": <strong>if doing somethign would take less than two minutes, just do it now.</strong>&nbsp; Likewise, if it would take over two minutes, put it in your to-do list and do it at the best time.</p>\n<p>&nbsp;</p>\n<hr>\n<h2><br></h2>\n<h2 id=\"Do\">Do</h2>\n<p>Now that you have your to-do list set and timeboxes for when you're going to work and on what, it's time to actually <em>do the work</em>.</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Pomodoro_Technique\">The Pomodoro Technique</strong></p>\n<p>The ideal timebox should be a length that is a multiple of thirty minutes so you can do the most powerful productivity thing there is: <a href=\"http://www.pomodorotechnique.com/\">The Pomodoro Technique</a>. &nbsp;Beware that it doesn't work for some, but I do urge you to give it a fair shake and a few tries, because for those whom the Pomodoro works, the Pomodoro Technique works wonders.</p>\n<p>Here's how you do it. &nbsp;Set a timer for 25 minutes. &nbsp;During those 25 minutes (a) work <strong>only</strong>&nbsp;on your task at hand; (b)&nbsp;<strong>do not&nbsp;</strong>do anything else, even for a second; (c) be completely focused; (d) be free from distractions; (e) and <strong>do not multi-task.</strong>&nbsp;There are some acceptable things to do during a Pomodoro, however: go to the bathroom, drink, listen to music. &nbsp;But there are tons more things not to do during a Pomodoro: check Facebook, read your email, etc. &nbsp;The list will go on.</p>\n<p>After the timer expires, take a five minute break. &nbsp;During these five minutes, do anything you'd like <strong>except</strong>&nbsp;the task on hand. &nbsp;Even if you feel like the break is boring and you're itching to get back on task, <strong>don't.</strong>&nbsp; You're only hurting yourself in the long-run. &nbsp;This five minute break will restore your focus, keep you grounded, provide a way to think through your ideas in a different setting, and prevent you from needing longer breaks later in the day.</p>\n<p>It should be noted, however, that the Pomodoro can be a bit difficult to get in the habit of, though. &nbsp;To solve this, I've found it useful to work my way up to the full Pomodoro by spending a month getting used to \"15 minutes of work, 5 minutes break\", then another month doing \"20 minutes of work, 5 minutes break\", and then finally \"25 minutes of work, 5 minutes break\".</p>\n<p>Different people have tried other multiples besides 25 and 5, but I'm still convinced that 25-5 is the ideal split. &nbsp;Perhaps 27-3 could work better for advanced Pomodoro users, but I wouldn't push it further. &nbsp;I've seen things like 90-30 or <a href=\"http://zenhabits.net/my-fav-procrastination-hack-30-10/\">30-10</a>, and all of these seem to involve working just a little too long (losing focus) and then taking a lot more break than is necessary. &nbsp;Of course, if it works for you, then it works.</p>\n<p>Here's the <a href=\"http://www.online-stopwatch.com/full-screen-interval-timer/?c=vxv55r3380\">25-5 stopwatch</a>&nbsp;I use and my <a href=\"http://www.online-stopwatch.com/full-screen-interval-timer/?c=sxodrlz842\">20-5 stopwatch</a>. &nbsp;I've also liked <a href=\"http://tomato-timer.com/\">Tomato Timer.com</a>, but any timer can work.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Be_Comfortable_with_Breaks\">Be Comfortable with Breaks</strong></p>\n<p>The important lesson of working a lot is to be comfortable with taking a break. &nbsp;The novice productive person will think it virtuous to work clear through a break and onward, thinking that he or she is making even better use of their time, defeating all those sissy workers who need breaks! &nbsp;But really, this person is just setting up their own downfall, because they'll crash and burn.</p>\n<p>Burnout is real and one of the most dangerous things you can do is <a href=\"/lw/8gv/the_curse_of_identity/\">train yourself to feel guilty about not working</a>. &nbsp;So you need to remember to take breaks. &nbsp;The break in a Pomodoro is a good one, but I also recommend taking a larger break (like 30 minutes) after completing three or four Pomodoros.</p>\n<p>One particularly good break I'd like to give a shout-out to is to take a nap. &nbsp;Taking a nap at a fairly regular time has <a href=\"http://www.medicalnewstoday.com/releases/179882.php\">health benefits</a>&nbsp;(see also <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000341\">here</a>, <a href=\"http://www.eurekalert.org/pub_releases/2006-06/uom-wwc060106.php\">here</a>, and <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17053484\">here</a>) and <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/11763827\">doesn't harm your night sleep</a>&nbsp;if you nap for 20 minutes and don't nap too late in the afternoon or evening. &nbsp;In fact, I've actually found naps to be a <em>time saver</em>&nbsp;instead of time \"wasted\" for a break, because I can sleep less at night and still feel rested and be focused throughout the day.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Keep_Your_Energy_Up\">Keep Your Energy Up</strong></p>\n<p>Another thing to prevent your chance of crashing and needing a long break to restore your energy is to keep your energy up. &nbsp;I recommend drinking something that is somewhat sugary but not too sugary (I drink water-diluted lemonade in a 25%-75% mix) and remembering to exercise on a regular basis. &nbsp;Also, <a href=\"http://www.nomeatathlete.com/healthy-eating/\">eating healthy</a>&nbsp;and sleeping right works wonders for keeping your attention on your work.</p>\n<p>&nbsp;</p>\n<hr>\n<h2><br></h2>\n<h2 id=\"Review\">Review</h2>\n<p>Of course, it's not enough to do if you're not going to learn from how you're doing and improve. &nbsp;I suggest you review your life on multiple levels -- daily, weekly, monthly, and once every six months.</p>\n<p>For the <strong>daily review</strong>, I keep track of whether I've <a href=\"http://www.everydayutilitarian.com/essays/category/resolutions/\">succeeded at certain habits</a>&nbsp;like exercising and eating right, and log the amount of time I've spent on various things so I can keep track of my time usage. &nbsp;I also complete other relevant logs, and then spend a bit of time reflecting how things have gone for the day and think of ways to repeat successes and avoid mistakes. &nbsp;I then check the plan for the next day and tweak it if necessary. &nbsp;This process takes me about 15 to 20 minutes.</p>\n<p>For the <strong>weekly review</strong>, I go through my action-waiting-reference zones wherever they exist (physical piles, email, and computer folders) and process them -- make sure everything there is still relevant and still belongs in the same place. &nbsp;I'll remove whatever needs to be removed at this stage and remind myself what I'm working on. &nbsp;I'll organize and clean anything that isn't organized at this stage and get everything together. &nbsp;I'll then quickly re-read m<a href=\"http://www.everydayutilitarian.com/essays/my-strategic-plan/\">y strategic plan</a>&nbsp;and plan out the week in accordance with my goals. &nbsp;Recently, I've set amounts of time per week I want to be spending on certain projects, so it's now a matter of making a schedule that works. &nbsp;This process usually takes me 45 minutes to an hour.</p>\n<p>For the <strong>monthly review</strong>, I reflect on the habits I've been trying to build for the month and decide what habits I want to keep, what habits I want to add, and what habits I want to subtract. &nbsp;I review how the month as a whole went and think about what I can do to repeat successes and avert future failures. &nbsp;I then write up a reflection and <a href=\"http://www.everydayutilitarian.com/essays/category/resolutions/\">publish it on my blog</a>. &nbsp;This process usually takes me two hours.</p>\n<p>For the <strong>six month review</strong>, I return to my goals and think about how my life trajectory as a whole is going. &nbsp;What are my life goals? &nbsp;What am I doing to accomplish them? &nbsp;Am I closer to my goals than I was six months ago? &nbsp;Should I be working toward new goals? &nbsp;What common mistakes did I make through the past six months that I want to avoid? &nbsp;I then <a href=\"http://www.everydayutilitarian.com/essays/my-strategic-plan/\">write a document</a>with my personal mission and goals for the next six months and skim it every week to constantly remind myself of what I want to be doing. &nbsp;This process usually takes me three hours.</p>\n<p>&nbsp;</p>\n<p>Yes, there will be an unlucky day where you do all four reviews and spend like six and a half hours reviewing your life at different levels. &nbsp;Perhaps this is a bit much for people, but I've found tremendous benefit from it. &nbsp;I've found that spending this day reviewing my life has saved me from not just days, but even months, of wasted time that doesn't accomplish what I really want to do. &nbsp;<strong>Reviewing is another way of saving you time.</strong></p>\n<p><strong><br></strong></p>\n<hr>\n<h2><br></h2>\n<h2 id=\"Additional_Tips\">Additional Tips</h2>\n<p>Now I've given you all my main advice, but I have some additional tips if you want to keep reading.</p>\n<p><strong>Carefully form these habits over time.</strong> &nbsp;This is a lot to do at once, so do it in stages. &nbsp;Build the habit of writing things down first, and then slowly get the apps you like in place for ideas, events, and tasks. &nbsp;After you have that down, spend the time necessary to get your email in order and implement the zones wherever possible. &nbsp;Then begin to move into prioritizing your tasks with the Eisenhower Matrix. &nbsp;After you have this down, begin planning your days in advance with timeboxes and start doing your reviews. &nbsp;While you're building that habit, simultaneously start building up the Pomodoro habit, slowly approaching 25-5 over a few months.</p>\n<p><strong>Find a way to reliably stay on habit.</strong> &nbsp;Don't make the common failure of sticking to something for a month or two and abandoning it. &nbsp;Spend a lot of energy thinking through how you'll stay on habit and how you'll not be like all the other people who think they'll stay on habit then fail. &nbsp;<a href=\"http://hackthesystem.com/blog/the-bet-switch-mechanism-the-one-simple-tactic-that-will-make-you-lose-weight-eat-right-and-get-in-the-best-shape-of-your-life/\">Make a bet with a friend</a>, <a href=\"https://www.beeminder.com/\">start up Beeminder</a>, or create some other kind of commitment device.</p>\n<p><strong>Form the productivity mindset.</strong> I had a lot of trouble implementing this plan until I was able to think of myself as an important person who does important things and should personally value my time. &nbsp;I had to really want to be productive before I could start being productive. &nbsp;Success at this will follow from the right mindset. &nbsp;It's time to start thinking of yourself as important. &nbsp;If you can't fool yourself, maybe it's time to look at your goals and decide what goals would make you feel important and then <em>do those goals instead</em>.</p>\n<p><strong>Behold the power of routines.</strong>&nbsp;I find it a lot easier to exercise if I have a routine of \"every other day, right after waking up\" or \"every other day, right before dinner\". &nbsp;Your routine can be built from here. &nbsp;It's a lot easier to stick to timeboxes if they're regularly occurring. &nbsp;Use a calendar and build yourself something nice.</p>\n<p><strong>Put everything in a particular place.</strong>&nbsp; People lose a lot of time just hunting around for things. &nbsp;Solve this by spending some time ahead of time organizing things in your life and getting them into particular places. &nbsp;Then always make sure things return to their places.</p>\n<p><strong>Declutter your life.</strong>&nbsp; You'll work better if you have less stuff to keep track of and less commitments to worry about. &nbsp;Get rid of everything and delegate anything you can.</p>\n<p><strong>Make a productivity place.</strong>&nbsp;This works especially well in colleges where there is a large variety of places you could be working. &nbsp;Find a place to work, set up your Pomodoros, and follow them to the letter. &nbsp;Don't mess up. &nbsp;Take your longer breaks somewhere else. &nbsp;If you do mess up, find a new productivity place and start again. &nbsp;I found this really helpful for my mindset, but others have found it silly.</p>\n<p><strong>Don't neglect friends and family.</strong>&nbsp; This is a big one. &nbsp;Remember, the goal of being more productive is to free time to do the things you want and be with the people you want. &nbsp;It's not to spend 100 hour workweeks neglecting those who are important to you. &nbsp;Make sure to take some time off to spend with friends and family. &nbsp;Schedule it in your calendar if you have to. &nbsp;This will matter most in the long-run for your life.</p>\n<p><strong>Productivity \u2260 Busy and Busy \u2260 Productivity.</strong>&nbsp; If you do productivity right, you shouldn't feel busy all that often. &nbsp;Being busy is a sign of having poor productivity and/or having taken on too many commitments, and is rarely ever a sign of doing things correctly.</p>\n<p>&nbsp;</p>\n<hr>\n<h2><br></h2>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>These tips are really a result of me experimenting for eight months. &nbsp;I'd expect you to take a similar amount of time to go from zero to productive and end up with different systems that work for you and your environment. &nbsp;But I think there are a lot of power in these systems and I'm interested to see what other people do and how other people run with them. &nbsp;After all, <a href=\"/lw/dr/generalizing_from_one_example/\">they work for <em>me</em></a>.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Further_reading_\">Further reading:</strong></p>\n<p>* <a href=\"http://www.thesecretweapon.org/\">The Secret Weapon</a></p>\n<p>* David Allen's <a href=\"http://www.amazon.com/dp/0142000280\"><strong>Getting Things Done: The Art of Stress-Free Productivity</strong></a></p>\n<p>* Scott Young's <a href=\"http://www.stafforini.com/blog/summary-of-the-little-book-of-productivity-by-scott-young/\"><strong>The Little Book of Productivity</strong></a></p>\n<p>* <a href=\"/lw/fux/my_workflow/\">Paul Christiano's Workflow</a></p>\n<p>* <a href=\"/lw/hgd/10step_antiprocrastination_checklist/\">10 Step Anti-Procrostination Checklist</a></p>\n<p>* <a href=\"http://www.zenhabits.net\">Zenhabits</a></p>\n<p>&nbsp;</p>\n<p>-</p>\n<p><em>(Also <a href=\"http://www.everydayutilitarian.com/essays/how-i-am-productive\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "sections": [{"title": "Organize", "anchor": "Organize", "level": 1}, {"title": "The Most Important Rule: Write Things Down", "anchor": "The_Most_Important_Rule__Write_Things_Down", "level": 2}, {"title": "Keep Track of Events: The Calendar", "anchor": "Keep_Track_of_Events__The_Calendar", "level": 2}, {"title": "Keep Track of Tasks: The To-Do List", "anchor": "Keep_Track_of_Tasks__The_To_Do_List", "level": 2}, {"title": "Action, Waiting, Reference: Stay Organized with Zones", "anchor": "Action__Waiting__Reference__Stay_Organized_with_Zones", "level": 2}, {"title": "Always Inbox Zero: Apply the Folders to Your Email", "anchor": "Always_Inbox_Zero__Apply_the_Folders_to_Your_Email", "level": 2}, {"title": "Prioritize", "anchor": "Prioritize", "level": 1}, {"title": "Eisenhower Matrix: Do What's Important", "anchor": "Eisenhower_Matrix__Do_What_s_Important", "level": 2}, {"title": "Timeboxing: Plan Your Day in Advance", "anchor": "Timeboxing__Plan_Your_Day_in_Advance", "level": 2}, {"title": "Two Minute Rule", "anchor": "Two_Minute_Rule", "level": 2}, {"title": "Do", "anchor": "Do", "level": 1}, {"title": "The Pomodoro Technique", "anchor": "The_Pomodoro_Technique", "level": 2}, {"title": "Be Comfortable with Breaks", "anchor": "Be_Comfortable_with_Breaks", "level": 2}, {"title": "Keep Your Energy Up", "anchor": "Keep_Your_Energy_Up", "level": 2}, {"title": "Review", "anchor": "Review", "level": 1}, {"title": "Additional Tips", "anchor": "Additional_Tips", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Further reading:", "anchor": "Further_reading_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "34 comments"}], "headingsCount": 20}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6NvbSwuSAooQxxf7f", "tAXrD8Y6hcJ8dt6Nt", "baTWMegR42PAsH9qJ", "E4gud47NNqgtsEeEr", "585XAc2C4RfHvYKNq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-08-27T13:04:42.568Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-27T16:15:44.287Z", "modifiedAt": null, "url": null, "title": "Meetup : New Meetup: Boulder CO", "slug": "meetup-new-meetup-boulder-co", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.806Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hsWb3jcTRfGP5wezm/meetup-new-meetup-boulder-co", "pageUrlRelative": "/posts/hsWb3jcTRfGP5wezm/meetup-new-meetup-boulder-co", "linkUrl": "https://www.lesswrong.com/posts/hsWb3jcTRfGP5wezm/meetup-new-meetup-boulder-co", "postedAtFormatted": "Tuesday, August 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20New%20Meetup%3A%20Boulder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20New%20Meetup%3A%20Boulder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhsWb3jcTRfGP5wezm%2Fmeetup-new-meetup-boulder-co%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20New%20Meetup%3A%20Boulder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhsWb3jcTRfGP5wezm%2Fmeetup-new-meetup-boulder-co", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhsWb3jcTRfGP5wezm%2Fmeetup-new-meetup-boulder-co", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qb'>New Meetup: Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 September 2013 07:03:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Munzinger building of CU Boulder</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Alright, so I've heard that the way these things get started is that someone wants to make a meetup and says I'm going to be sitting at this cafe or whatever at so and so time and anyone who wants to talk rationality can come there.</p>\n\n<p>So, I'll be waiting on the padio inside the Munzinger Building next Tuesday from 7:00 to 8:00 PM and if anyone is intrested we can talk about rationality.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qb'>New Meetup: Boulder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hsWb3jcTRfGP5wezm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.318302513042188e-06, "legacy": true, "legacyId": "23890", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___New_Meetup__Boulder_CO\">Discussion article for the meetup : <a href=\"/meetups/qb\">New Meetup: Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 September 2013 07:03:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Munzinger building of CU Boulder</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Alright, so I've heard that the way these things get started is that someone wants to make a meetup and says I'm going to be sitting at this cafe or whatever at so and so time and anyone who wants to talk rationality can come there.</p>\n\n<p>So, I'll be waiting on the padio inside the Munzinger Building next Tuesday from 7:00 to 8:00 PM and if anyone is intrested we can talk about rationality.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___New_Meetup__Boulder_CO1\">Discussion article for the meetup : <a href=\"/meetups/qb\">New Meetup: Boulder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : New Meetup: Boulder CO", "anchor": "Discussion_article_for_the_meetup___New_Meetup__Boulder_CO", "level": 1}, {"title": "Discussion article for the meetup : New Meetup: Boulder CO", "anchor": "Discussion_article_for_the_meetup___New_Meetup__Boulder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-27T17:37:44.943Z", "modifiedAt": null, "url": null, "title": "Meetup : Arizona State Lunch Group", "slug": "meetup-arizona-state-lunch-group", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:04.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Danny_Hintze", "createdAt": "2010-12-04T23:01:40.826Z", "isAdmin": false, "displayName": "Danny_Hintze"}, "userId": "2fHm6t2WFDMPShg5b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5J4gtM8F86PEaTbXS/meetup-arizona-state-lunch-group", "pageUrlRelative": "/posts/5J4gtM8F86PEaTbXS/meetup-arizona-state-lunch-group", "linkUrl": "https://www.lesswrong.com/posts/5J4gtM8F86PEaTbXS/meetup-arizona-state-lunch-group", "postedAtFormatted": "Tuesday, August 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Arizona%20State%20Lunch%20Group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Arizona%20State%20Lunch%20Group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5J4gtM8F86PEaTbXS%2Fmeetup-arizona-state-lunch-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Arizona%20State%20Lunch%20Group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5J4gtM8F86PEaTbXS%2Fmeetup-arizona-state-lunch-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5J4gtM8F86PEaTbXS%2Fmeetup-arizona-state-lunch-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qc'>Arizona State Lunch Group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 September 2013 12:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">301 E. Orange Mall Tempe, AZ 85281</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting up at Engrained in the ASU memorial union (There are many other food options available in the same place). The goal of the meeting is to get a regular weekly lunch group going at ASU and more generally in the Phoenix metro area.</p>\n\n<p>Feel free to text me at 602-501-9420 (I am in the process of porting my number to google voice, so phone calls don't work right now)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qc'>Arizona State Lunch Group</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5J4gtM8F86PEaTbXS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.318372761033117e-06, "legacy": true, "legacyId": "23891", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Arizona_State_Lunch_Group\">Discussion article for the meetup : <a href=\"/meetups/qc\">Arizona State Lunch Group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 September 2013 12:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">301 E. Orange Mall Tempe, AZ 85281</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting up at Engrained in the ASU memorial union (There are many other food options available in the same place). The goal of the meeting is to get a regular weekly lunch group going at ASU and more generally in the Phoenix metro area.</p>\n\n<p>Feel free to text me at 602-501-9420 (I am in the process of porting my number to google voice, so phone calls don't work right now)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Arizona_State_Lunch_Group1\">Discussion article for the meetup : <a href=\"/meetups/qc\">Arizona State Lunch Group</a></h2>", "sections": [{"title": "Discussion article for the meetup : Arizona State Lunch Group", "anchor": "Discussion_article_for_the_meetup___Arizona_State_Lunch_Group", "level": 1}, {"title": "Discussion article for the meetup : Arizona State Lunch Group", "anchor": "Discussion_article_for_the_meetup___Arizona_State_Lunch_Group1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-27T19:06:55.348Z", "modifiedAt": null, "url": null, "title": "What should a college student do to maximize future earnings for effective altruism?", "slug": "what-should-a-college-student-do-to-maximize-future-earnings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:34.392Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hhj24dWKnGqEd5Gw2/what-should-a-college-student-do-to-maximize-future-earnings", "pageUrlRelative": "/posts/hhj24dWKnGqEd5Gw2/what-should-a-college-student-do-to-maximize-future-earnings", "linkUrl": "https://www.lesswrong.com/posts/hhj24dWKnGqEd5Gw2/what-should-a-college-student-do-to-maximize-future-earnings", "postedAtFormatted": "Tuesday, August 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20should%20a%20college%20student%20do%20to%20maximize%20future%20earnings%20for%20effective%20altruism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20should%20a%20college%20student%20do%20to%20maximize%20future%20earnings%20for%20effective%20altruism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhhj24dWKnGqEd5Gw2%2Fwhat-should-a-college-student-do-to-maximize-future-earnings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20should%20a%20college%20student%20do%20to%20maximize%20future%20earnings%20for%20effective%20altruism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhhj24dWKnGqEd5Gw2%2Fwhat-should-a-college-student-do-to-maximize-future-earnings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhhj24dWKnGqEd5Gw2%2Fwhat-should-a-college-student-do-to-maximize-future-earnings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 799, "htmlBody": "<p>&nbsp;</p>\n<p>I'd like to solicit advice since I'm starting at Stanford this Fall and I'm interested in optimal philanthropy.</p>\n<p>First off, what should I major in? I have experience in programming and math, so I'm thinking of majoring in CS, possibly with a second major or a minor in applied math. But switching costs are still extremely low at the moment, so I should consider other fields.</p>\n<p>Some majors that could have higher lifetime earnings than straight CS:</p>\n<ul>\n<li>Petroleum engineering. Would non-oil energy sources cause pay to drop over the next 40 years?</li>\n<li>Actuarial math. If I understand correctly, actuaries had high pay because they were basically a cartel, artificially limiting the supply of certifications to a certain number each year. And I've heard that people that used to hire actuaries now hire cheaper equivalents, so pay could be less over the next 40 years.</li>\n<li>Chemical engineering, nuclear engineering, electrical and electronics engineering, mechanical engineering, aerospace engineering.</li>\n<li>Pre-med.</li>\n<li>Quantitative finance.</li>\n</ul>\n<p>Thoughts?</p>\n<p>Stanford actually has <a href=\"http://studentaffairs.stanford.edu/cdc/jobs/salary-grads\">salary data for 2011-2012 graduates by major</a>. CS has highest earnings, by quite far. The data is incomplete because few people responded and some groups were omitted for privacy, so we don't know what e.g. petroleum engineers or double majors earned.</p>\n<p>Should I double-major? There are some earnings statistics <a href=\"/en.wikipedia.org/wiki/Double_majors_in_the_United_States.html#Effect_on_earnings_and_other_labor_market_ourcomes\">here</a>; to summarize, two majors in the same field doesn't help; a science major plus a humanities major has lower earnings than the science major alone; greatest returns are achieved by pairing a math/science major with an engineering major, which increases earnings \"up to 30%\" above the math/science major alone. I'd guess these effects are largely not causation, but correlation caused by conscientiousness/ambition causing both double majors and higher earnings.</p>\n<p>I could also get minors. I'm planning to very carefully look over the requirements for each major and minor, since there do seem to be some cheap gains. A math minor can be done in one quarter, for instance; a math major takes only a bit more than two quarters.</p>\n<p>I have a table with the unit requirements of each combination of majors and minors. Most students take 15 units a quarter. Here are some major/minor combinations I could do:</p>\n<ul>\n<li>If I take 18.8 units a quarter, I could double-major in CS and econ.</li>\n<li>If I take 15.8 units a quarter, I could major in CS and minor in math and econ.</li>\n<li>If I take 15.4 units a quarter, I could double-major in CS and math.</li>\n</ul>\n<p>Cal Newport <a href=\"http://calnewport.com/blog/2010/09/27/how-double-majors-can-ruin-your-life-two-arguments-for-doing-less/\">argues</a> that this sort of thing a bad idea because hard schedules do not actually impress employers more.</p>\n<p>Would employers care about double majors in undergrad if I also get a graduate degree? I will do a master's degree or a PhD, partly because those make it a lot easier to emigrate to the US. (I'm from South Africa, which doesn't have much of a software industry.)</p>\n<p>What other things could increase earnings?</p>\n<ul>\n<li>Doing an internship every summer.</li>\n<li>Networking. Stanford's <a href=\"http://studentaffairs.stanford.edu/cdc/jobs/how-students-find-jobs\">statistics on how 2011-2012 graduates found jobs</a> indicates that around 29% of them got jobs through networking.</li>\n<li>Better social skills? I'm planning on taking some classes on public speaking, improv, etc.; what else should I do?</li>\n<li>Some way of signalling leadership skills? Maybe I could try to get into a leadership position at a student club or something.</li>\n<li>Honors programs, or doing research. Do employers care about this?</li>\n<li>Following the advice of Stanford's Career Development Center, for instance about how to prepare for career fairs, using their internship network, making appointments with their career counselors, etc.</li>\n<li>Studying abroad. I'm already studying abroad by going to Stanford, so this is probably less valuable for me than for most students, though it still seems likely to be worthwhile. Stanford has a Washington program involving internships and classes taught by policymakers, which might be worth doing. Both these would make it harder to do multiple majors and minors.</li>\n</ul>\n<p>Many thanks for all advice given!</p>\n<p>&nbsp;</p>\n<p>EDIT: I used a scoring rule to rank all combinations of majors and minors in CS, math, economics and MS&amp;E (management science and engineering) according to practicality and estimated effect on earnings. Unit estimates include all breadth requirements etc., assuming I don't take stupid courses. Here's the top 20; the top 10 all look pretty good:</p>\n<table border=\"2\" cellpadding=\"2\">\n<tbody>\n<tr>\n<td>CS</td>\n<td>Math</td>\n<td>Econ</td>\n<td>MS&amp;E</td>\n<td>&nbsp;</td>\n<td>Total Units</td>\n<td>Units per quarter</td>\n<td>Hours/day</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>198</td>\n<td>16.5</td>\n<td>7.1</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>.</td>\n<td>minor</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>207</td>\n<td>17.3</td>\n<td>7.4</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>189</td>\n<td>15.8</td>\n<td>6.8</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>216</td>\n<td>18.0</td>\n<td>7.7</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>minor</td>\n<td>minor</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>216</td>\n<td>18.0</td>\n<td>7.7</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>183</td>\n<td>15.3</td>\n<td>6.5</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>.</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>199</td>\n<td>16.6</td>\n<td>7.1</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>210</td>\n<td>17.5</td>\n<td>7.5</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>minor</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>180</td>\n<td>15.0</td>\n<td>6.4</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>202</td>\n<td>16.8</td>\n<td>7.2</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>minor</td>\n<td>minor</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>190</td>\n<td>15.8</td>\n<td>6.8</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>minor</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>208</td>\n<td>17.3</td>\n<td>7.4</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>211</td>\n<td>17.6</td>\n<td>7.5</td>\n</tr>\n<tr>\n<td>.</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>192</td>\n<td>16.0</td>\n<td>6.9</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>225</td>\n<td>18.8</td>\n<td>8.0</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>.</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>234</td>\n<td>19.5</td>\n<td>8.4</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>.</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>171</td>\n<td>14.3</td>\n<td>6.1</td>\n</tr>\n<tr>\n<td>.</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>195</td>\n<td>16.3</td>\n<td>7.0</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>228</td>\n<td>19.0</td>\n<td>8.1</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>minor</td>\n<td>.</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>181</td>\n<td>15.1</td>\n<td>6.5</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>minor</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>220</td>\n<td>18.3</td>\n<td>7.9</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>226</td>\n<td>18.8</td>\n<td>8.1</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>.</td>\n<td>minor</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>181</td>\n<td>15.1</td>\n<td>6.5</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>175</td>\n<td>14.6</td>\n<td>6.3</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>185</td>\n<td>15.4</td>\n<td>6.6</td>\n</tr>\n<tr>\n<td>minor</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>172</td>\n<td>14.3</td>\n<td>6.1</td>\n</tr>\n<tr>\n<td>.</td>\n<td>.</td>\n<td>MAJOR</td>\n<td>MAJOR</td>\n<td>&nbsp;</td>\n<td>183</td>\n<td>15.3</td>\n<td>6.5</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>minor</td>\n<td>MAJOR</td>\n<td>.</td>\n<td>&nbsp;</td>\n<td>235</td>\n<td>19.6</td>\n<td>8.4</td>\n</tr>\n<tr>\n<td>MAJOR</td>\n<td>.</td>\n<td>.</td>\n<td>minor</td>\n<td>&nbsp;</td>\n<td>172</td>\n<td>14.3</td>\n<td>6.1</td>\n</tr>\n</tbody>\n</table>\n<p>Another option is to major or minor in M&amp;CS (mathematical and computational sciences) instead of math or CS separately.</p>\n<p>&nbsp;</p>\n<p>EDIT 2: <a href=\"http://imgur.com/H63tvTW\">Here</a>&nbsp;is a graph of graduates' salaries by major.&nbsp;Y-axis is salary of 2011-2012 Stanford graduates. X-axis is degree: 1 is BA/BS, 2 is MA/MS, 3 is PhD; intermediate values are for groups containing two degree-levels. The sample size is tiny because only 30% of students responded, and some groups were omitted for privacy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hhj24dWKnGqEd5Gw2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 21, "extendedScore": null, "score": 1.3184491517008415e-06, "legacy": true, "legacyId": "23892", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-28T07:17:24.166Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem", "slug": "meetup-melbourne-practical-rationality-group-prediction", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BraydenM", "createdAt": "2013-06-10T09:17:31.294Z", "isAdmin": false, "displayName": "BraydenM"}, "userId": "KBZHqcMC6z2rPZkZK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jD8x9xY4LDMKfS7zx/meetup-melbourne-practical-rationality-group-prediction", "pageUrlRelative": "/posts/jD8x9xY4LDMKfS7zx/meetup-melbourne-practical-rationality-group-prediction", "linkUrl": "https://www.lesswrong.com/posts/jD8x9xY4LDMKfS7zx/meetup-melbourne-practical-rationality-group-prediction", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Practical%20Rationality%3A%20Group%20Prediction%20Calibration%20and%20Aumann's%20Agreement%20Theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Practical%20Rationality%3A%20Group%20Prediction%20Calibration%20and%20Aumann's%20Agreement%20Theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjD8x9xY4LDMKfS7zx%2Fmeetup-melbourne-practical-rationality-group-prediction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Practical%20Rationality%3A%20Group%20Prediction%20Calibration%20and%20Aumann's%20Agreement%20Theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjD8x9xY4LDMKfS7zx%2Fmeetup-melbourne-practical-rationality-group-prediction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjD8x9xY4LDMKfS7zx%2Fmeetup-melbourne-practical-rationality-group-prediction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qd'>Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 September 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Kings St, West Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This event will give us a chance to improve out prediction and updating skills, and we have prepared 90 minutes of group exercises as formal activities for the night, which kick off at 7:30pm.\nThis event is open to interested members who are less familiar with the LW material, and you are invited to bring a friend who might be interested. \nAll attendees are kindly requested to RSVP via the group meetup page, where you will find more information about the event: <a href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/137079922/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/events/137079922/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qd'>Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jD8x9xY4LDMKfS7zx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3190752145876103e-06, "legacy": true, "legacyId": "23906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Practical_Rationality__Group_Prediction_Calibration_and_Aumann_s_Agreement_Theorem\">Discussion article for the meetup : <a href=\"/meetups/qd\">Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 September 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Kings St, West Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This event will give us a chance to improve out prediction and updating skills, and we have prepared 90 minutes of group exercises as formal activities for the night, which kick off at 7:30pm.\nThis event is open to interested members who are less familiar with the LW material, and you are invited to bring a friend who might be interested. \nAll attendees are kindly requested to RSVP via the group meetup page, where you will find more information about the event: <a href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/137079922/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/events/137079922/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Practical_Rationality__Group_Prediction_Calibration_and_Aumann_s_Agreement_Theorem1\">Discussion article for the meetup : <a href=\"/meetups/qd\">Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem", "anchor": "Discussion_article_for_the_meetup___Melbourne_Practical_Rationality__Group_Prediction_Calibration_and_Aumann_s_Agreement_Theorem", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem", "anchor": "Discussion_article_for_the_meetup___Melbourne_Practical_Rationality__Group_Prediction_Calibration_and_Aumann_s_Agreement_Theorem1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-28T11:01:30.329Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:04.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ErK8BixRchr4wAB6z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/66X8Jh86ThfW22LfS/", "pageUrlRelative": "/posts/66X8Jh86ThfW22LfS/", "linkUrl": "https://www.lesswrong.com/posts/66X8Jh86ThfW22LfS/", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F66X8Jh86ThfW22LfS%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F66X8Jh86ThfW22LfS%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F66X8Jh86ThfW22LfS%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "66X8Jh86ThfW22LfS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 0, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "23912", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}