{"results": [{"createdAt": null, "postedAt": "2012-05-06T16:14:30.541Z", "modifiedAt": null, "url": null, "title": "Meetup Feedback: Topic selection and precommittments", "slug": "meetup-feedback-topic-selection-and-precommittments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Lee", "createdAt": "2009-09-10T00:05:08.577Z", "isAdmin": false, "displayName": "Jonathan_Lee"}, "userId": "8qL3Hsw2TzaLPu3Bh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GRPXpuR6gqqNPpdXD/meetup-feedback-topic-selection-and-precommittments", "pageUrlRelative": "/posts/GRPXpuR6gqqNPpdXD/meetup-feedback-topic-selection-and-precommittments", "linkUrl": "https://www.lesswrong.com/posts/GRPXpuR6gqqNPpdXD/meetup-feedback-topic-selection-and-precommittments", "postedAtFormatted": "Sunday, May 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Feedback%3A%20Topic%20selection%20and%20precommittments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Feedback%3A%20Topic%20selection%20and%20precommittments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRPXpuR6gqqNPpdXD%2Fmeetup-feedback-topic-selection-and-precommittments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Feedback%3A%20Topic%20selection%20and%20precommittments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRPXpuR6gqqNPpdXD%2Fmeetup-feedback-topic-selection-and-precommittments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRPXpuR6gqqNPpdXD%2Fmeetup-feedback-topic-selection-and-precommittments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 623, "htmlBody": "<p>This post is part of the Cambridge LW meetup group's attempt to publish what works for us, and try to make good meetups easier.</p>\n<p><strong>Breaking the ice and topic selection</strong></p>\n<p>A consistent problem has been starting discussion, and more generally breaking the ice. Last week, an <a href=\"/lw/fh/willpower_hax_487_execute_by_default\">Execute by Default</a> style hack was used to reduce social inhibitions (everyone danced for 30 seconds), which was highly successful, though awkward. It was proposed again this week, and there was sufficient collective laughter at the recollection to effectively break the ice. This may also have been helped by a change in room, which replaced chairs with couches.</p>\n<p>A new algorithm for selecting a topic was used: One person proposed a (deliberately easy-to-beat) topic, and running around the group, each person proposed a alternate topic or passed. This was followed by multiple passes for people to affiliate with any proposed topic. Amongst 7 people, the first pass produced a 5-2 split, and the group of two merged into the main topic.</p>\n<p>The topic chosen was involuntary signalling. The others are <a href=\"#topics\">here</a> so as to keep them salient for future meetups.</p>\n<p><strong>Signalling by Dress</strong></p>\n<p>It was observed that most people seem to react to dress, and that as a group (largely mathematicians or similarly inclined) there is a tendency not to optimise the reactions we generate. Several people asked what might work better, and checked to see whether the social status of others in the social group of mathematicians correlated with their appearance or dress. It appeared that if it did, we are insufficiently good at observing our cognitive processes to notice. As a corollary, it wasn't clear that feedback from other members of the group was likely to contain much signal.</p>\n<p>A concrete mechanism to extract information on how other people perceive dress was made: Generate multiple photos in various styles, and then use OKCupid's \"MyBestFace\" or similar services to get some information back</p>\n<p><strong>Signalling for Access</strong></p>\n<p>There was some discussion of how one might present in interview; this was confounded by a lack of access to interviewers. Discussion was more productive when moved to aspects of social engineering. Specific examples raised were accessing a hospital outside of visiting hours, entering a college without being challenged by <a href=\"http://en.wikipedia.org/wiki/Porter_%28college%29\">porters</a>, or avoiding inconvenience in airports. A combination of speed, posture (head level, back straight, shoulders back) and contextual dress was the extent of noted tricks.</p>\n<p><strong>Signalling by Posture</strong></p>\n<p>Considerable time was spent discussing how posture signals. Some people went around the group, saying what they would draw from other people's body language. Some postural changes were noted as very saliently causing a change in perception of the correctness of statements made at the same time (in particular, straightening the back and lifting the head). Extant <a href=\"https://docs.google.com/document/pub?id=1zYEnPRMSYnTsMHk-83Ogg86uXMLXvmHotOrODYq_jDs\">scholarship</a> was not discussed, but extensive experimentation occurred targeting specific received signals and querying specific postures. The dynamics of norm violation were also discussed, in the context of taking the communal coffee table as a footrest.</p>\n<p>Specific suggestions to use a mirror or camera to analyse oneself or attempt to analyse other people in general were made.</p>\n<p><strong>Pre-commitments</strong></p>\n<p>All of the public commitments made last week were done, which seemed to be a cheap win. We reran the procedure:</p>\n<ul>\n<li>Jonathan: Post meetup feedback etc. by midnight</li>\n<li>Adam: Get last two years of past papers done by next Sunday </li>\n<li>Adam: Email parents by Wednesday midnight </li>\n<li>Ben: Finish list of definitions by next Sunday </li>\n<li>Ben: Continue Diary until next Sunday </li>\n</ul>\n<p>&nbsp;</p>\n<hr />\n<p><a name=\"topics\"></a>List of proposed and unused topics:</p>\n<ul>\n<li>Concoct childish example of Bayes' Theorem (to motivate better alternatives)</li>\n<li>Self-sabotage, noticing and avoiding.</li>\n<li>Fermi estimate game</li>\n<li>Examine week 1 of Ben's diary to try to help in debiasing.</li>\n<li>Non-real valued utility functions</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GRPXpuR6gqqNPpdXD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 8.976162775919506e-07, "legacy": true, "legacyId": "15853", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post is part of the Cambridge LW meetup group's attempt to publish what works for us, and try to make good meetups easier.</p>\n<p><strong id=\"Breaking_the_ice_and_topic_selection\">Breaking the ice and topic selection</strong></p>\n<p>A consistent problem has been starting discussion, and more generally breaking the ice. Last week, an <a href=\"/lw/fh/willpower_hax_487_execute_by_default\">Execute by Default</a> style hack was used to reduce social inhibitions (everyone danced for 30 seconds), which was highly successful, though awkward. It was proposed again this week, and there was sufficient collective laughter at the recollection to effectively break the ice. This may also have been helped by a change in room, which replaced chairs with couches.</p>\n<p>A new algorithm for selecting a topic was used: One person proposed a (deliberately easy-to-beat) topic, and running around the group, each person proposed a alternate topic or passed. This was followed by multiple passes for people to affiliate with any proposed topic. Amongst 7 people, the first pass produced a 5-2 split, and the group of two merged into the main topic.</p>\n<p>The topic chosen was involuntary signalling. The others are <a href=\"#topics\">here</a> so as to keep them salient for future meetups.</p>\n<p><strong id=\"Signalling_by_Dress\">Signalling by Dress</strong></p>\n<p>It was observed that most people seem to react to dress, and that as a group (largely mathematicians or similarly inclined) there is a tendency not to optimise the reactions we generate. Several people asked what might work better, and checked to see whether the social status of others in the social group of mathematicians correlated with their appearance or dress. It appeared that if it did, we are insufficiently good at observing our cognitive processes to notice. As a corollary, it wasn't clear that feedback from other members of the group was likely to contain much signal.</p>\n<p>A concrete mechanism to extract information on how other people perceive dress was made: Generate multiple photos in various styles, and then use OKCupid's \"MyBestFace\" or similar services to get some information back</p>\n<p><strong id=\"Signalling_for_Access\">Signalling for Access</strong></p>\n<p>There was some discussion of how one might present in interview; this was confounded by a lack of access to interviewers. Discussion was more productive when moved to aspects of social engineering. Specific examples raised were accessing a hospital outside of visiting hours, entering a college without being challenged by <a href=\"http://en.wikipedia.org/wiki/Porter_%28college%29\">porters</a>, or avoiding inconvenience in airports. A combination of speed, posture (head level, back straight, shoulders back) and contextual dress was the extent of noted tricks.</p>\n<p><strong id=\"Signalling_by_Posture\">Signalling by Posture</strong></p>\n<p>Considerable time was spent discussing how posture signals. Some people went around the group, saying what they would draw from other people's body language. Some postural changes were noted as very saliently causing a change in perception of the correctness of statements made at the same time (in particular, straightening the back and lifting the head). Extant <a href=\"https://docs.google.com/document/pub?id=1zYEnPRMSYnTsMHk-83Ogg86uXMLXvmHotOrODYq_jDs\">scholarship</a> was not discussed, but extensive experimentation occurred targeting specific received signals and querying specific postures. The dynamics of norm violation were also discussed, in the context of taking the communal coffee table as a footrest.</p>\n<p>Specific suggestions to use a mirror or camera to analyse oneself or attempt to analyse other people in general were made.</p>\n<p><strong id=\"Pre_commitments\">Pre-commitments</strong></p>\n<p>All of the public commitments made last week were done, which seemed to be a cheap win. We reran the procedure:</p>\n<ul>\n<li>Jonathan: Post meetup feedback etc. by midnight</li>\n<li>Adam: Get last two years of past papers done by next Sunday </li>\n<li>Adam: Email parents by Wednesday midnight </li>\n<li>Ben: Finish list of definitions by next Sunday </li>\n<li>Ben: Continue Diary until next Sunday </li>\n</ul>\n<p>&nbsp;</p>\n<hr>\n<p><a name=\"topics\"></a>List of proposed and unused topics:</p>\n<ul>\n<li>Concoct childish example of Bayes' Theorem (to motivate better alternatives)</li>\n<li>Self-sabotage, noticing and avoiding.</li>\n<li>Fermi estimate game</li>\n<li>Examine week 1 of Ben's diary to try to help in debiasing.</li>\n<li>Non-real valued utility functions</li>\n</ul>", "sections": [{"title": "Breaking the ice and topic selection", "anchor": "Breaking_the_ice_and_topic_selection", "level": 1}, {"title": "Signalling by Dress", "anchor": "Signalling_by_Dress", "level": 1}, {"title": "Signalling for Access", "anchor": "Signalling_for_Access", "level": 1}, {"title": "Signalling by Posture", "anchor": "Signalling_by_Posture", "level": 1}, {"title": "Pre-commitments", "anchor": "Pre_commitments", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FHukyfMagq4HrBYNt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-06T19:52:22.012Z", "modifiedAt": null, "url": null, "title": "Betrand Russell's Ten Commandments", "slug": "betrand-russell-s-ten-commandments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "quanticle", "createdAt": "2009-12-02T01:39:50.714Z", "isAdmin": false, "displayName": "quanticle"}, "userId": "usztQFrTvM67pdcCq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2fLHt2SotmTheNFCQ/betrand-russell-s-ten-commandments", "pageUrlRelative": "/posts/2fLHt2SotmTheNFCQ/betrand-russell-s-ten-commandments", "linkUrl": "https://www.lesswrong.com/posts/2fLHt2SotmTheNFCQ/betrand-russell-s-ten-commandments", "postedAtFormatted": "Sunday, May 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Betrand%20Russell's%20Ten%20Commandments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABetrand%20Russell's%20Ten%20Commandments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2fLHt2SotmTheNFCQ%2Fbetrand-russell-s-ten-commandments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Betrand%20Russell's%20Ten%20Commandments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2fLHt2SotmTheNFCQ%2Fbetrand-russell-s-ten-commandments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2fLHt2SotmTheNFCQ%2Fbetrand-russell-s-ten-commandments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p><a href=\"http://marginalrevolution.com/marginalrevolution/2012/05/bertrand-russells-10-commandments-for-teachers.html\">Betrand Russell's Ten Commandments for teachers.</a></p>\n<blockquote><ol>\n<li>Do not feel absolutely certain of anything.</li>\n<li>Do not think it worth while to proceed by concealing evidence, for the evidence is sure to come to light.</li>\n<li>Never try to discourage thinking for you are sure to succeed.</li>\n<li>When you meet with opposition, even if it should be from your husband or your children, endeavour to overcome it by argument and not by authority, for a victory dependent upon authority is unreal and illusory.</li>\n<li>Have no respect for the authority of others, for there are always contrary authorities to be found.</li>\n<li>Do not use power to suppress opinions you think pernicious, for if you do the opinions will suppress you.</li>\n<li>Do not fear to be eccentric in opinion, for every opinion now accepted was once eccentric.</li>\n<li>Find more pleasure in intelligent dissent that in passive agreement, for, if you value intelligence as you should, the former implies a deeper agreement than the latter.</li>\n<li>Be scrupulously truthful, even if the truth is inconvenient, for it is more inconvenient when you try to conceal it.</li>\n<li>Do not feel envious of the happiness of those who live in a fool&rsquo;s paradise, for only a fool will think that it is happiness.</li>\n</ol></blockquote>\n<p>I find this to be of use not just for teachers but for rationalists in general. #8, especially, is an especially eloquent formulation of Aumann's Agreement Theorem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2fLHt2SotmTheNFCQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 4, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "15854", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-06T22:06:23.296Z", "modifiedAt": null, "url": null, "title": "Meetup : Dallas - Fort Worth Less Wrong Meetup 5/13/12", "slug": "meetup-dallas-fort-worth-less-wrong-meetup-5-13-12", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jumandtonic", "createdAt": "2012-04-26T03:09:35.594Z", "isAdmin": false, "displayName": "jumandtonic"}, "userId": "2EmJ3AN5jKXHnvYpP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FLDDKZnAGYwLTytdm/meetup-dallas-fort-worth-less-wrong-meetup-5-13-12", "pageUrlRelative": "/posts/FLDDKZnAGYwLTytdm/meetup-dallas-fort-worth-less-wrong-meetup-5-13-12", "linkUrl": "https://www.lesswrong.com/posts/FLDDKZnAGYwLTytdm/meetup-dallas-fort-worth-less-wrong-meetup-5-13-12", "postedAtFormatted": "Sunday, May 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F13%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F13%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLDDKZnAGYwLTytdm%2Fmeetup-dallas-fort-worth-less-wrong-meetup-5-13-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F13%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLDDKZnAGYwLTytdm%2Fmeetup-dallas-fort-worth-less-wrong-meetup-5-13-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFLDDKZnAGYwLTytdm%2Fmeetup-dallas-fort-worth-less-wrong-meetup-5-13-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9w'>Dallas - Fort Worth Less Wrong Meetup 5/13/12</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello Dallas-Fort Worth LessWrongians!  If you live in the area, and you haven't come out to meet us yet, you are missing out!</p>\n\n<p>We currently have regular meetups every Sunday at America's Best Coffee in Arlington at 1 PM until 3 PM.  We have gotten a good handful of people to show up to these events so far, and it has been very enjoyable and productive.</p>\n\n<p>The current goal, or mission statement, of this group can be summarized as follows:\n\"We want to first understand rationality, and then learn how to apply rationality to our daily lives.  During our meet-ups we wish to take advantage of having a community over what can only be accomplished alone.\"</p>\n\n<p>We look forward to you coming out and meeting the rest of the group.  Message me to ask to join our google group:\nhttps://groups.google.com/forum/#!forum/dfw-lesswrong-meetup</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9w'>Dallas - Fort Worth Less Wrong Meetup 5/13/12</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FLDDKZnAGYwLTytdm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.977689895243795e-07, "legacy": true, "legacyId": "15855", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_13_12\">Discussion article for the meetup : <a href=\"/meetups/9w\">Dallas - Fort Worth Less Wrong Meetup 5/13/12</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello Dallas-Fort Worth LessWrongians!  If you live in the area, and you haven't come out to meet us yet, you are missing out!</p>\n\n<p>We currently have regular meetups every Sunday at America's Best Coffee in Arlington at 1 PM until 3 PM.  We have gotten a good handful of people to show up to these events so far, and it has been very enjoyable and productive.</p>\n\n<p>The current goal, or mission statement, of this group can be summarized as follows:\n\"We want to first understand rationality, and then learn how to apply rationality to our daily lives.  During our meet-ups we wish to take advantage of having a community over what can only be accomplished alone.\"</p>\n\n<p>We look forward to you coming out and meeting the rest of the group.  Message me to ask to join our google group:\nhttps://groups.google.com/forum/#!forum/dfw-lesswrong-meetup</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_13_121\">Discussion article for the meetup : <a href=\"/meetups/9w\">Dallas - Fort Worth Less Wrong Meetup 5/13/12</a></h2>", "sections": [{"title": "Discussion article for the meetup : Dallas - Fort Worth Less Wrong Meetup 5/13/12", "anchor": "Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_13_12", "level": 1}, {"title": "Discussion article for the meetup : Dallas - Fort Worth Less Wrong Meetup 5/13/12", "anchor": "Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_13_121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-07T03:34:06.993Z", "modifiedAt": null, "url": null, "title": "Meetup : Monday Madison Meetup", "slug": "meetup-monday-madison-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.537Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EwMAYtHKoqgB5rQiS/meetup-monday-madison-meetup-0", "pageUrlRelative": "/posts/EwMAYtHKoqgB5rQiS/meetup-monday-madison-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/EwMAYtHKoqgB5rQiS/meetup-monday-madison-meetup-0", "postedAtFormatted": "Monday, May 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monday%20Madison%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monday%20Madison%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEwMAYtHKoqgB5rQiS%2Fmeetup-monday-madison-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monday%20Madison%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEwMAYtHKoqgB5rQiS%2Fmeetup-monday-madison-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEwMAYtHKoqgB5rQiS%2Fmeetup-monday-madison-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9x'>Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 May 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I have a specific topic in mind that I would like to discuss: The illusion of control appears to be both a psychological need and a common, viewpoint-distorting bias. Are there other biases like this? How ought we handle them? Further suggestions for discussion topics are, as always, warmly welcomed.</p>\n\n<p>Also, I'll bring stuff to play The Resistance and Zendo. :D</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9x'>Monday Madison Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EwMAYtHKoqgB5rQiS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.97911261408422e-07, "legacy": true, "legacyId": "15869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9x\">Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 May 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I have a specific topic in mind that I would like to discuss: The illusion of control appears to be both a psychological need and a common, viewpoint-distorting bias. Are there other biases like this? How ought we handle them? Further suggestions for discussion topics are, as always, warmly welcomed.</p>\n\n<p>Also, I'll bring stuff to play The Resistance and Zendo. :D</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9x\">Monday Madison Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-07T03:38:06.205Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] When Science Can't Help", "slug": "seq-rerun-when-science-can-t-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LMjsNyqZeFeacaDtW/seq-rerun-when-science-can-t-help", "pageUrlRelative": "/posts/LMjsNyqZeFeacaDtW/seq-rerun-when-science-can-t-help", "linkUrl": "https://www.lesswrong.com/posts/LMjsNyqZeFeacaDtW/seq-rerun-when-science-can-t-help", "postedAtFormatted": "Monday, May 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20When%20Science%20Can't%20Help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20When%20Science%20Can't%20Help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMjsNyqZeFeacaDtW%2Fseq-rerun-when-science-can-t-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20When%20Science%20Can't%20Help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMjsNyqZeFeacaDtW%2Fseq-rerun-when-science-can-t-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLMjsNyqZeFeacaDtW%2Fseq-rerun-when-science-can-t-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>Today's post, <a href=\"/lw/qc/when_science_cant_help/\">When Science Can't Help</a> was originally published on 15 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you have an idea, Science tells you to test it experimentally. If you spend 10 years testing the idea and the result comes out negative, Science slaps you on the back and says, \"Better luck next time.\" If you want to spend 10 years testing a hypothesis that will actually turn out to be right, you'll have to try to do the thing that Science doesn't trust you to do: think rationally, and figure out the answer <em>before </em>you get clubbed over the head with it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c88/seq_rerun_science_doesnt_trust_your_rationality/\">Science Doesn't Trust Your Rationality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LMjsNyqZeFeacaDtW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.979129924091412e-07, "legacy": true, "legacyId": "15870", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wzxneh7wxkdNYNbtB", "CBSuSjJg8YJgi2mCE", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-07T05:20:40.924Z", "modifiedAt": null, "url": null, "title": "Meetup : Small Berkeley Meetup", "slug": "meetup-small-berkeley-meetup-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thausler", "createdAt": "2010-05-24T04:40:49.214Z", "isAdmin": false, "displayName": "Thausler"}, "userId": "oPa5EPBs6ow6MY2Ao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fDANZ8ANkq5yhJBLa/meetup-small-berkeley-meetup-3", "pageUrlRelative": "/posts/fDANZ8ANkq5yhJBLa/meetup-small-berkeley-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/fDANZ8ANkq5yhJBLa/meetup-small-berkeley-meetup-3", "postedAtFormatted": "Monday, May 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20Berkeley%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20Berkeley%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfDANZ8ANkq5yhJBLa%2Fmeetup-small-berkeley-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20Berkeley%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfDANZ8ANkq5yhJBLa%2Fmeetup-small-berkeley-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfDANZ8ANkq5yhJBLa%2Fmeetup-small-berkeley-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9y'>Small Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at the Starbucks on Oxford St and then probably going somewhere to get food.  This will be a small meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9y'>Small Berkeley Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fDANZ8ANkq5yhJBLa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.979575317706594e-07, "legacy": true, "legacyId": "15871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9y\">Small Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at the Starbucks on Oxford St and then probably going somewhere to get food.  This will be a small meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9y\">Small Berkeley Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Small Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-07T13:41:52.658Z", "modifiedAt": null, "url": null, "title": "Lesswrong Community's How-Tos and Recommendations", "slug": "lesswrong-community-s-how-tos-and-recommendations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EE43026F", "createdAt": "2012-02-10T19:07:48.704Z", "isAdmin": false, "displayName": "EE43026F"}, "userId": "6yXnzczk2m9oM7er9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eMurnmp3aZKjfEo9J/lesswrong-community-s-how-tos-and-recommendations", "pageUrlRelative": "/posts/eMurnmp3aZKjfEo9J/lesswrong-community-s-how-tos-and-recommendations", "linkUrl": "https://www.lesswrong.com/posts/eMurnmp3aZKjfEo9J/lesswrong-community-s-how-tos-and-recommendations", "postedAtFormatted": "Monday, May 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lesswrong%20Community's%20How-Tos%20and%20Recommendations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALesswrong%20Community's%20How-Tos%20and%20Recommendations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeMurnmp3aZKjfEo9J%2Flesswrong-community-s-how-tos-and-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lesswrong%20Community's%20How-Tos%20and%20Recommendations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeMurnmp3aZKjfEo9J%2Flesswrong-community-s-how-tos-and-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeMurnmp3aZKjfEo9J%2Flesswrong-community-s-how-tos-and-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 260, "htmlBody": "<p>The Lesswrong community is often a dependable source of recommendations, network help, and advice. When I'm looking for a book or learning material on a topic I'll often try and search here to see what residents have found useful. Similarly, social advice, anecdotes and explanations as seen from the point of view of the community have regularly been insightful or eye-opening. The prototypical examples of such articles are, on top of my head :</p>\n<p><br /><a title=\"http://lesswrong.com/lw/3gu/the_best_textbooks_on_every_subject/\" href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">http://lesswrong.com/lw/3gu/the_best_textbooks_on_every_subject/</a><br /><br /><a title=\"http://lesswrong.com/lw/453/procedural_knowledge_gaps/\" href=\"/lw/453/procedural_knowledge_gaps/\">http://lesswrong.com/lw/453/procedural_knowledge_gaps/ </a></p>\n<p>the topics of which are neatly listed on</p>\n<p><a title=\"http://lesswrong.com/lw/a08/topics_from_procedural_knowledge_gaps/\" href=\"/lw/a08/topics_from_procedural_knowledge_gaps/\">http://lesswrong.com/lw/a08/topics_from_procedural_knowledge_gaps/</a></p>\n<p>&nbsp;</p>\n<p>And lately</p>\n<p><a title=\"http://lesswrong.com/r/discussion/lw/c6y/why_do_people/\" href=\"/r/discussion/lw/c6y/why_do_people/\">http://lesswrong.com/r/discussion/lw/c6y/why_do_people/</a></p>\n<p>&nbsp;</p>\n<p>the latter prompted me to write this article. We don't keep track of such resources as far as I know. This probably <a title=\"http://wiki.lesswrong.com/wiki/Lesswrong_Community%27s_How-Tos_and_Recommendations\" href=\"http://wiki.lesswrong.com/wiki/Lesswrong_Community%27s_How-Tos_and_Recommendations\">belongs in the wiki as well</a>.</p>\n<p>&nbsp;</p>\n<p>Other potentially useful resources were:</p>\n<p>&nbsp;</p>\n<p><a title=\"http://lesswrong.com/lw/12d/recommended_reading_for_new_rationalists/\" href=\"/lw/12d/recommended_reading_for_new_rationalists/\">http://lesswrong.com/lw/12d/recommended_reading_for_new_rationalists/</a></p>\n<p><a title=\"http://lesswrong.com/lw/2kk/book_recommendations/\" href=\"/lw/2kk/book_recommendations/\">http://lesswrong.com/lw/2kk/book_recommendations/</a><br /><br /><a title=\"http://lesswrong.com/lw/2ua/recommended_reading_for_friendly_ai_research/\" href=\"/lw/2ua/recommended_reading_for_friendly_ai_research/\">http://lesswrong.com/lw/2ua/recommended_reading_for_friendly_ai_research/</a><br /><br /><br /><br />math learning<br /><a title=\"http://lesswrong.com/lw/9qq/what_math_should_i_learn/\" href=\"/lw/9qq/what_math_should_i_learn/\"><br />http://lesswrong.com/lw/9qq/what_math_should_i_learn/</a><br /><br /><a title=\"http://lesswrong.com/lw/8js/what_mathematics_to_learn/\" href=\"/lw/8js/what_mathematics_to_learn/\">http://lesswrong.com/lw/8js/what_mathematics_to_learn/</a><br /><br /><a title=\"http://lesswrong.com/lw/a54/seeking_education/\" href=\"/lw/a54/seeking_education/\">http://lesswrong.com/lw/a54/seeking_education/</a><br /><br /><br />misc learning<br /><br /><a title=\"http://lesswrong.com/lw/5me/scholarship_how_to_do_it_efficiently/\" href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">http://lesswrong.com/lw/5me/scholarship_how_to_do_it_efficiently/</a><br /><br /><a title=\"http://lesswrong.com/lw/4yv/i_want_to_learn_programming/\" href=\"/lw/4yv/i_want_to_learn_programming/\">http://lesswrong.com/lw/4yv/i_want_to_learn_programming/</a><br /><br /><a title=\"http://lesswrong.com/lw/3qr/i_want_to_learn_economics/\" href=\"/lw/3qr/i_want_to_learn_economics/\">http://lesswrong.com/lw/3qr/i_want_to_learn_economics/</a><br /><br /><a title=\"http://lesswrong.com/lw/3us/i_want_to_learn_about_education/\" href=\"/lw/3us/i_want_to_learn_about_education/\">http://lesswrong.com/lw/3us/i_want_to_learn_about_education/</a><br /><br /><a title=\"http://lesswrong.com/lw/8e3/which_fields_of_learning_have_clarified_your/\" href=\"/lw/8e3/which_fields_of_learning_have_clarified_your/\">http://lesswrong.com/lw/8e3/which_fields_of_learning_have_clarified_your/</a><br /><br /><br />social<br /><br /><a title=\"http://lesswrong.com/lw/6ey/learning_how_to_explain_things/\" href=\"/lw/6ey/learning_how_to_explain_things/\">http://lesswrong.com/lw/6ey/learning_how_to_explain_things/</a></p>\n<p><a href=\"/lw/818/how_to_understand_people_better/\">http://lesswrong.com/lw/818/how_to_understand_people_better/</a><br /><br /><a href=\"/lw/6tb/developing_empathy/\">http://lesswrong.com/lw/6tb/developing_empathy/</a><br /><br /><br />community<br /><br /><a title=\"http://lesswrong.com/lw/929/less_wrong_mentoring_network/\" href=\"/lw/929/less_wrong_mentoring_network/\">http://lesswrong.com/lw/929/less_wrong_mentoring_network/</a><br /><br /><a title=\"http://lesswrong.com/lw/7hi/free_research_help_editing_and_article_downloads/\" href=\"/lw/7hi/free_research_help_editing_and_article_downloads/\">http://lesswrong.com/lw/7hi/free_research_help_editing_and_article_downloads/</a><br /><br /><br />Employment<br /><br /><a title=\"http://lesswrong.com/lw/43m/optimal_employment/\" href=\"/lw/43m/optimal_employment/\">http://lesswrong.com/lw/43m/optimal_employment/</a><br /><a title=\"http://lesswrong.com/lw/2qp/virtual_employment_open_thread/\" href=\"/lw/2qp/virtual_employment_open_thread/\"><br />http://lesswrong.com/lw/2qp/virtual_employment_open_thread/</a><br /><br /><a title=\"http://lesswrong.com/lw/38u/best_career_models_for_doing_research/\" href=\"/lw/38u/best_career_models_for_doing_research/\">http://lesswrong.com/lw/38u/best_career_models_for_doing_research/</a><br /><br /><a title=\"http://lesswrong.com/lw/4ad/optimal_employment_open_thread/\" href=\"/lw/4ad/optimal_employment_open_thread/\">http://lesswrong.com/lw/4ad/optimal_employment_open_thread/</a><br /><br /><a title=\"http://lesswrong.com/lw/626/job_search_advice/\" href=\"/lw/626/job_search_advice/\">http://lesswrong.com/lw/626/job_search_advice/</a><br /><br /><a title=\"http://lesswrong.com/lw/8cp/any_thoughts_on_how_to_locate_job_opportunities/\" href=\"/lw/8cp/any_thoughts_on_how_to_locate_job_opportunities/\">http://lesswrong.com/lw/8cp/any_thoughts_on_how_to_locate_job_opportunities/</a><br /><br /><a title=\"http://lesswrong.com/lw/7yl/more_shameless_ploys_for_job_advice/\" href=\"/lw/7yl/more_shameless_ploys_for_job_advice/\">http://lesswrong.com/lw/7yl/more_shameless_ploys_for_job_advice/</a></p>\n<p><a href=\"/lw/a93/existential_risk_reduction_career_network/\">http://lesswrong.com/lw/a93/existential_risk_reduction_career_network/</a></p>\n<p>&nbsp;</p>\n<p>Entertainment</p>\n<p><a title=\"http://lesswrong.com/r/discussion/tag/recommendations/?sort=new\" href=\"/r/discussion/tag/recommendations/?sort=new\">http://lesswrong.com/r/discussion/tag/recommendations/?sort=new</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eMurnmp3aZKjfEo9J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 37, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "15883", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "ka8eveZpT7hXLhRTM", "oaggCpKMcZRTBujsa", "mLcAyvWJ96SJjAKN2", "wfJebLTPGYaK3Gr8W", "uWhcKdToYEMPArHsv", "uuCPHR9pG5nvhGFcb", "xFR9oLzxCfeRYGf9Y", "LfFw5Lb7RkT4QzZcC", "NowDgDpHF5WYnoyMG", "37sHjeisS9uJufi4u", "aBeJuczn3b67m43qR", "Ee5WB55MzRcjnvXby", "5mFBqN2wYv7DzC5rQ", "Jjf98bkdefG8yuvxc", "99mHEgk2NAk2eveZE", "qy4DMkqNFakaZWYkR", "R6mJejvcCLjBjXCKb", "cWFRxGamBQcWRBpvL", "q3wQGNicZZQPCmJXZ", "jtedBLdducritm8y6", "9bTNcSpNBdPpyocMK", "rNkFLv9tXzq8Lrvrc", "yuGci5CHe8CGqFuCa", "z7ihuKNTRfhxsqttn", "zuZa3kCKzKn3z8ybs", "BMnhBiH5dKrrtQQHM", "8MXD9XSEaAH6ZZhHE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-07T13:47:09.568Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Politics Meetup", "slug": "meetup-vancouver-politics-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.013Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rDR9KaeeStmMRKHwz/meetup-vancouver-politics-meetup", "pageUrlRelative": "/posts/rDR9KaeeStmMRKHwz/meetup-vancouver-politics-meetup", "linkUrl": "https://www.lesswrong.com/posts/rDR9KaeeStmMRKHwz/meetup-vancouver-politics-meetup", "postedAtFormatted": "Monday, May 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Politics%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Politics%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDR9KaeeStmMRKHwz%2Fmeetup-vancouver-politics-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Politics%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDR9KaeeStmMRKHwz%2Fmeetup-vancouver-politics-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDR9KaeeStmMRKHwz%2Fmeetup-vancouver-politics-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9z'>Vancouver Politics Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 May 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are going to be at Benny's again. Saturday this time, because we know some of you can't do Sundays. 13:00.</p>\n\n<p>The sequence we are reading this week is <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">politics is the mind killer</a>, so read that and come discuss. We prefer that you come out even if you havn't read anything, tho.</p>\n\n<p>See you all there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9z'>Vancouver Politics Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rDR9KaeeStmMRKHwz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.981774999944849e-07, "legacy": true, "legacyId": "15884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Politics_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9z\">Vancouver Politics Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 May 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are going to be at Benny's again. Saturday this time, because we know some of you can't do Sundays. 13:00.</p>\n\n<p>The sequence we are reading this week is <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">politics is the mind killer</a>, so read that and come discuss. We prefer that you come out even if you havn't read anything, tho.</p>\n\n<p>See you all there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Politics_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9z\">Vancouver Politics Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Politics Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Politics_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Politics Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Politics_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-07T19:00:45.232Z", "modifiedAt": null, "url": null, "title": "On what rationality-related topic should I give a school presentation?", "slug": "on-what-rationality-related-topic-should-i-give-a-school", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bramflakes", "createdAt": "2011-11-01T22:15:00.964Z", "isAdmin": false, "displayName": "bramflakes"}, "userId": "pJEYMdQjRLEJSg8bX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HnbDERgukaM2tkvsW/on-what-rationality-related-topic-should-i-give-a-school", "pageUrlRelative": "/posts/HnbDERgukaM2tkvsW/on-what-rationality-related-topic-should-i-give-a-school", "linkUrl": "https://www.lesswrong.com/posts/HnbDERgukaM2tkvsW/on-what-rationality-related-topic-should-i-give-a-school", "postedAtFormatted": "Monday, May 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20what%20rationality-related%20topic%20should%20I%20give%20a%20school%20presentation%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20what%20rationality-related%20topic%20should%20I%20give%20a%20school%20presentation%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHnbDERgukaM2tkvsW%2Fon-what-rationality-related-topic-should-i-give-a-school%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20what%20rationality-related%20topic%20should%20I%20give%20a%20school%20presentation%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHnbDERgukaM2tkvsW%2Fon-what-rationality-related-topic-should-i-give-a-school", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHnbDERgukaM2tkvsW%2Fon-what-rationality-related-topic-should-i-give-a-school", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>My school has a weekly event on Thursdays where someone can give a 15-25 minute lecture about a topic of their choice during the lunch break. The standard attendance is about 20-30, aged between 14 and 18, and some teachers drop by if the topic is related to their subject. It's heavily interlinked with the philosophy department, in that topics are typically about religion or ethics, so the audience is generally more philosophically informed than average. A good percentage are theists or deists, and there's a very high chance that the subject will be more thoroughly discussed in the philosophy club the day after.</p>\n<p>&nbsp;</p>\n<p>In a previous lecture a few months ago I tried to explain some standard biases, the Map/Territory concepts, Bayes, and generally attempted to compress the core sequences into 25 minutes, which despite a lot of interest from the head of the philosophy department, didn't go as well as I'd hoped for the rest of the audience. The problem was that I tried to close too many inferential gaps in too many areas in too short a timespan, so for this I thought I should take one rationality idea and go into detail. The problem is I don't know which one to choose for maximum impact. I've decided against cryonics because I don't feel confident that I know enough about it.</p>\n<p>&nbsp;</p>\n<p>So what do you think I should talk about for maximum sanity-waterline-raising impact?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HnbDERgukaM2tkvsW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 8.983137450703862e-07, "legacy": true, "legacyId": "15885", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T03:22:58.941Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Nomic", "slug": "meetup-west-la-meetup-nomic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HNWKbP83T5nwoDiDy/meetup-west-la-meetup-nomic", "pageUrlRelative": "/posts/HNWKbP83T5nwoDiDy/meetup-west-la-meetup-nomic", "linkUrl": "https://www.lesswrong.com/posts/HNWKbP83T5nwoDiDy/meetup-west-la-meetup-nomic", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Nomic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Nomic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHNWKbP83T5nwoDiDy%2Fmeetup-west-la-meetup-nomic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Nomic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHNWKbP83T5nwoDiDy%2Fmeetup-west-la-meetup-nomic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHNWKbP83T5nwoDiDy%2Fmeetup-west-la-meetup-nomic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a0'>West LA Meetup - Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, May 9th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Activity:</strong> This week, we will begin a game of <a href=\"http://en.wikipedia.org/wiki/Nomic\" rel=\"nofollow\">Nomic</a>, the most meta game known to man. This is a game in which changing the rules of the game constitutes a \"move\". I will pick one of the more common initial rulesets.</p>\n\n<p>Don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a0'>West LA Meetup - Nomic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HNWKbP83T5nwoDiDy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.985320206809054e-07, "legacy": true, "legacyId": "15898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Nomic\">Discussion article for the meetup : <a href=\"/meetups/a0\">West LA Meetup - Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, May 9th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Activity:</strong> This week, we will begin a game of <a href=\"http://en.wikipedia.org/wiki/Nomic\" rel=\"nofollow\">Nomic</a>, the most meta game known to man. This is a game in which changing the rules of the game constitutes a \"move\". I will pick one of the more common initial rulesets.</p>\n\n<p>Don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Nomic1\">Discussion article for the meetup : <a href=\"/meetups/a0\">West LA Meetup - Nomic</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Nomic", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Nomic", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Nomic", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Nomic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T05:31:17.947Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Science Isn't Strict Enough", "slug": "seq-rerun-science-isn-t-strict-enough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eTb3BvviZJ6xkabbE/seq-rerun-science-isn-t-strict-enough", "pageUrlRelative": "/posts/eTb3BvviZJ6xkabbE/seq-rerun-science-isn-t-strict-enough", "linkUrl": "https://www.lesswrong.com/posts/eTb3BvviZJ6xkabbE/seq-rerun-science-isn-t-strict-enough", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Science%20Isn't%20Strict%20Enough&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Science%20Isn't%20Strict%20Enough%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTb3BvviZJ6xkabbE%2Fseq-rerun-science-isn-t-strict-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Science%20Isn't%20Strict%20Enough%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTb3BvviZJ6xkabbE%2Fseq-rerun-science-isn-t-strict-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTb3BvviZJ6xkabbE%2Fseq-rerun-science-isn-t-strict-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>Today's post, <a href=\"/lw/qd/science_isnt_strict_enough/\">Science Isn't Strict Enough</a> was originally published on 16 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Science lets you believe any damn stupid idea that hasn't been refuted by experiment. Bayesianism says there is always an exactly rational degree of belief given your current evidence, and this does not shift a nanometer to the left or to the right depending on your whims. Science is a social freedom - we let people test whatever hypotheses they like, because we don't trust the village elders to decide in advance - but you shouldn't confuse that with an individual standard of rationality.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c8u/seq_rerun_when_science_cant_help/\">When Science Can't Help</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eTb3BvviZJ6xkabbE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.985878040615508e-07, "legacy": true, "legacyId": "15899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PGfJdgemDJSwWBZSX", "LMjsNyqZeFeacaDtW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T07:06:06.748Z", "modifiedAt": null, "url": null, "title": "Hacking Quantum Immortality", "slug": "hacking-quantum-immortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:38.097Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CasioTheSane", "createdAt": "2012-03-06T23:29:51.068Z", "isAdmin": false, "displayName": "CasioTheSane"}, "userId": "Rn8faPpZcbhfnq6bz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C72TkdC78X77wmYvt/hacking-quantum-immortality", "pageUrlRelative": "/posts/C72TkdC78X77wmYvt/hacking-quantum-immortality", "linkUrl": "https://www.lesswrong.com/posts/C72TkdC78X77wmYvt/hacking-quantum-immortality", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hacking%20Quantum%20Immortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHacking%20Quantum%20Immortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC72TkdC78X77wmYvt%2Fhacking-quantum-immortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hacking%20Quantum%20Immortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC72TkdC78X77wmYvt%2Fhacking-quantum-immortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC72TkdC78X77wmYvt%2Fhacking-quantum-immortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>Quantum immortality sounds exactly like the mythical hell: living forever in perpetual agony, unable to die and in your own branch of existence separate from everyone else you ever knew.</p>\r\n<p>What if we can hack quantum immortality to force continued good health, and the mutual survival of our loved ones in the same branch of the universe as us?</p>\r\n<p>It seems like one would \"simply\" need a device which monitors your health with biosensors, and if anything goes out of range- it instantly kills you in a manner with extremely low probability of failure. All of your friends and family would wear a similar device, and they would be coupled such that if one person becomes \"slightly unhealthy\" you all die instantly, keeping you all alive and healthy together.</p>\r\n<p>We nearly have the technology to build such a thing now. Would you install one in your own body? If not, why not?</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>Who wants to invest in my new biotech startup which promises to stop all disease and human suffering within the next decade? Just joking, there is a serious technical problem here that makes it considerably more difficult than it sounds: for such a device to work the probability of it's failure must be much much less than the probability of your continued healthy survival. You also never get to test the design before you use it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C72TkdC78X77wmYvt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -9, "extendedScore": null, "score": 8.986290262887313e-07, "legacy": true, "legacyId": "15905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T07:08:42.210Z", "modifiedAt": null, "url": null, "title": "Logical fallacies poster, a LessWrong adaptation.", "slug": "logical-fallacies-poster-a-lesswrong-adaptation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Utopiah", "createdAt": "2012-03-10T14:57:03.414Z", "isAdmin": false, "displayName": "Utopiah"}, "userId": "gBoAdZcTffQnbsMAb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5T9s2Cw9jAoCzPnhS/logical-fallacies-poster-a-lesswrong-adaptation", "pageUrlRelative": "/posts/5T9s2Cw9jAoCzPnhS/logical-fallacies-poster-a-lesswrong-adaptation", "linkUrl": "https://www.lesswrong.com/posts/5T9s2Cw9jAoCzPnhS/logical-fallacies-poster-a-lesswrong-adaptation", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20fallacies%20poster%2C%20a%20LessWrong%20adaptation.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20fallacies%20poster%2C%20a%20LessWrong%20adaptation.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5T9s2Cw9jAoCzPnhS%2Flogical-fallacies-poster-a-lesswrong-adaptation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20fallacies%20poster%2C%20a%20LessWrong%20adaptation.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5T9s2Cw9jAoCzPnhS%2Flogical-fallacies-poster-a-lesswrong-adaptation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5T9s2Cw9jAoCzPnhS%2Flogical-fallacies-poster-a-lesswrong-adaptation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p>Following http://lesswrong.com/lw/bwo/logical_fallacy_poster/ some people complained about</p>\n<ul>\n<li>the sarcastic illustration</li>\n<li>the lack of references</li>\n<li>the weird categorization that should rather fit a Bayesian framework</li>\n<li>the simplistic or even wrong definitions</li>\n<li>and more</li>\n</ul>\n<p>Yet this poster has ONE key difference with the ideal poster, <strong>it exists</strong>.<br /><br />If it sparks criticisms that lead to a new, LessWrong compatible poster, then it is well worth the critics.</p>\n<p><br />The obvious next step then is to make a poster that would allow to take into account such well founded suggestion and synthesize the LessWrong lessons visually.</p>\n<p>In your opinion then what would be a good structure, e.g. a hierarchy of fallacies, and a design theme?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5T9s2Cw9jAoCzPnhS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 8.986301528335429e-07, "legacy": true, "legacyId": "15906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T11:23:04.072Z", "modifiedAt": null, "url": null, "title": "A wild theist platonist appears, to ask about the path ", "slug": "a-wild-theist-platonist-appears-to-ask-about-the-path", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Hang", "createdAt": "2012-05-07T18:53:32.138Z", "isAdmin": false, "displayName": "Hang"}, "userId": "fytLf2QSf7TY3534T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cnLotaXAX6WPyfb35/a-wild-theist-platonist-appears-to-ask-about-the-path", "pageUrlRelative": "/posts/cnLotaXAX6WPyfb35/a-wild-theist-platonist-appears-to-ask-about-the-path", "linkUrl": "https://www.lesswrong.com/posts/cnLotaXAX6WPyfb35/a-wild-theist-platonist-appears-to-ask-about-the-path", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20wild%20theist%20platonist%20appears%2C%20to%20ask%20about%20the%20path%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20wild%20theist%20platonist%20appears%2C%20to%20ask%20about%20the%20path%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcnLotaXAX6WPyfb35%2Fa-wild-theist-platonist-appears-to-ask-about-the-path%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20wild%20theist%20platonist%20appears%2C%20to%20ask%20about%20the%20path%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcnLotaXAX6WPyfb35%2Fa-wild-theist-platonist-appears-to-ask-about-the-path", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcnLotaXAX6WPyfb35%2Fa-wild-theist-platonist-appears-to-ask-about-the-path", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>I recognize the title could be more informative. At the same time I believe it says what is important.<br />I believe in a deity, I believe in mathematical entities in the same way.&nbsp;<br /><br />The community of LessWrong (from whenceforth: LessWrong) is deeply interesting to me, appearing as a semi-organized atheist, reductionist community.<br />LessWrong seems very interested in promoting rationality, which I applaud. The effort does seem scattered, though, and this is the reason I post.<br /><br />One has Eliezer's website with some interesting posts. The same of this community. The community links to some posts when you are coming for the first time into it, and you also have a filter for top posts. One has the blog. And recently, the center for modern rationality (in the same page as harrypoter fanfiction about rationality).<br /><br />The point being there is no defined roadmap to go from AIC (average irrational chump to make an analogy to Game - which also seems to come up around quite a bit) to RA (again, rationality artist).<br /><br />I write this post as to maybe generate a discussion on how the efforts could be concentrated and a new direction taken.<br /><br />Should the creation of the Center for Modern Rationality envision this same concentration, this post may and should be disregard.<br />If it does not, then I leave it to your consideration.</p>\n<p>&nbsp;</p>\n<p>Hang.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cnLotaXAX6WPyfb35", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 9, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "15915", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T16:28:09.359Z", "modifiedAt": null, "url": null, "title": "AI risk: the five minute pitch", "slug": "ai-risk-the-five-minute-pitch", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bJJ9sAWgaLQPwz3D7/ai-risk-the-five-minute-pitch", "pageUrlRelative": "/posts/bJJ9sAWgaLQPwz3D7/ai-risk-the-five-minute-pitch", "linkUrl": "https://www.lesswrong.com/posts/bJJ9sAWgaLQPwz3D7/ai-risk-the-five-minute-pitch", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20risk%3A%20the%20five%20minute%20pitch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20risk%3A%20the%20five%20minute%20pitch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJJ9sAWgaLQPwz3D7%2Fai-risk-the-five-minute-pitch%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20risk%3A%20the%20five%20minute%20pitch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJJ9sAWgaLQPwz3D7%2Fai-risk-the-five-minute-pitch", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJJ9sAWgaLQPwz3D7%2Fai-risk-the-five-minute-pitch", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>I did a talk at the 25th <a href=\"http://oxford.geeknights.net/2012/feb-29th/\">Oxford Geek night</a>, in which I had five minutes to present the dangers of AI. The talk is now <a href=\"http://www.youtube.com/watch?v=ySxvsvpfdUQ&amp;feature=g-all-u\">online</a>. Though it doesn't contain anything people at Less Wrong would find new, I feel it does a reasonable job at pitching some of the arguments in a very brief format.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bJJ9sAWgaLQPwz3D7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 8.988734563801757e-07, "legacy": true, "legacyId": "15916", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T20:34:34.012Z", "modifiedAt": null, "url": null, "title": "The ethics of breaking belief", "slug": "the-ethics-of-breaking-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:57.477Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thelittledoctor", "createdAt": "2011-03-29T04:12:08.470Z", "isAdmin": false, "displayName": "thelittledoctor"}, "userId": "EC8mhStZzmK7JK9Na", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qxjjct9LdADJG3Tdw/the-ethics-of-breaking-belief", "pageUrlRelative": "/posts/Qxjjct9LdADJG3Tdw/the-ethics-of-breaking-belief", "linkUrl": "https://www.lesswrong.com/posts/Qxjjct9LdADJG3Tdw/the-ethics-of-breaking-belief", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20ethics%20of%20breaking%20belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20ethics%20of%20breaking%20belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxjjct9LdADJG3Tdw%2Fthe-ethics-of-breaking-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20ethics%20of%20breaking%20belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxjjct9LdADJG3Tdw%2Fthe-ethics-of-breaking-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQxjjct9LdADJG3Tdw%2Fthe-ethics-of-breaking-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<p>I'm not sure if this is precisely the correct forum for this, but if there is a better place, I don't know what it would be. At any rate...</p>\n<p>&nbsp;</p>\n<p>I'm a student a Catholic university, and there are (as one might surmise) quite a lot of Catholics here, along with assorted other theists (yes, even some in the biology faculty). For this reason, I find myself acquiring more and more devoutly Catholic friends, and some of them I have grown quite close to. But the God issue keeps coming up for one reason or another, which is a source of tension. And yet as I grow closer to these people, it becomes clearer and clearer that each theist has a certain personal sequence of Dark Arts-ish levers in eir head, the flipping (or un-flipping) of which would snap em out of faith.</p>\n<p>So the question is this: in what situations (if any) is it ethical to push such buttons? We often say, here, that that which can be destroyed by the truth should be, but these are people who have built their lives around faith, people for whom the Church is their social support group. If it were possible to disillusion the whole world all at once, that'd be one thing - but in this case my options are limited to changing the minds of only the specific individuals I have spent time getting to know, and the direct result would be their alienation from the entire community in which they've been raised.</p>\n<p>And yet it is the truth.</p>\n<p>I'm conflicted. LessWrong, what is your opinion?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qxjjct9LdADJG3Tdw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "15918", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 126, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T20:38:47.981Z", "modifiedAt": null, "url": null, "title": "Consequentialist Formal Systems", "slug": "consequentialist-formal-systems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.595Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JGwD6zkhtXjCHjNn9/consequentialist-formal-systems", "pageUrlRelative": "/posts/JGwD6zkhtXjCHjNn9/consequentialist-formal-systems", "linkUrl": "https://www.lesswrong.com/posts/JGwD6zkhtXjCHjNn9/consequentialist-formal-systems", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consequentialist%20Formal%20Systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsequentialist%20Formal%20Systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGwD6zkhtXjCHjNn9%2Fconsequentialist-formal-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consequentialist%20Formal%20Systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGwD6zkhtXjCHjNn9%2Fconsequentialist-formal-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGwD6zkhtXjCHjNn9%2Fconsequentialist-formal-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1279, "htmlBody": "<p style=\"padding-left: 30px; \"><em>This post describes a different (less agent-centric) way of looking at UDT-like decision theories that resolves some aspects of the long-standing technical problem of <a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">spurious</a> <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">moral</a> <a href=\"/r/discussion/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">arguments</a>. It's only a half-baked idea, so there are currently a lot of loose ends.</em></p>\n<h3>On spurious arguments</h3>\n<p>UDT agents are usually considered as having a disinterested inference system (a \"mathematical intuition module\" in <a href=\"http://wiki.lesswrong.com/wiki/UDT\">UDT</a> and first order proof search in <a href=\"http://wiki.lesswrong.com/wiki/ADT\">ADT</a>) that plays a purely epistemic role, and preference-dependent decision rules that look for statements that characterize possible actions in terms of the utility value that the agent optimizes.</p>\n<p>The statements (supplied by the inference system) used by agent's decision rules (to pick one of the many variants) have the form <strong>[(A=A1 =&gt; U=U1) and U&lt;=U1]</strong>. Here, <strong>A</strong> is a symbol defined to be the actual action chosen by the agent, <strong>U</strong> is a similar symbol defined to be the actual value of world's utility, and <strong>A1</strong> and <strong>U1</strong> are some particular possible action and possible utility value. If the agent finds that this statement is provable, it performs action <strong>A1</strong>, thereby making <strong>A1</strong> the actual action.</p>\n<p>The use of this statement introduces the problem of <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">spurious arguments</a>: if <strong>A1</strong> is a bad action, but for some reason it's still chosen, then <strong>[(A=A1 =&gt; U=U1) and U&lt;=U1]</strong> is true, since utility value <strong>U</strong> will in that case be in fact <strong>U1</strong>, which justifies (by the decision rule) choosing the bad action <strong>A1</strong>. In usual cases, this problem results in the difficulty of proving that an agent will behave in the expected manner (i.e. won't choose a bad action), which is resolved by adding <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">various</a> <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle\">compilicated</a> <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits\">clauses</a> to its decision algorithm. But even worse, it turns out that if an agent is hapless enough to take seriously a (formally correct) proof of such a statement supplied by an enemy (or if its own inference system is malicious), <a href=\"/r/discussion/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">it can be persuaded to take any action at all</a>, irrespective of agent's own preferences.<a id=\"more\"></a></p>\n<h3>Deciding which theorems to make valid</h3>\n<p>Given that an inference system can overpower decision rules, causing an agent to follow a preference other than its own, perhaps decision-making should be happening inside the inference system in the first place, with agent only following its decisions. What does an inference system decide? Directly, it decides which theorems to have. The set of valid theorems follows deterministically from the axioms, but <a href=\"/lw/2os/controlling_constant_programs/\">this is not really a problem</a>, it's possible to make decisions in deterministic settings.</p>\n<p>Suppose an inference system wants to decide whether it should have a theorem <strong>S</strong>. How does it evaluate the consequences of <strong>S</strong> being its theorem? It can assume that it proved <strong>S</strong>, see what that would imply, and if it likes the consequences (in comparison to the consequences of proving <strong>Not-S</strong>, for example), then it concludes <strong>S</strong>. Decision rules that an inference system follows are the axioms of the theory it works with, so this discussion suggests the following axiom schema (of <em>moral axioms</em>):</p>\n<blockquote>For all statements <strong>S</strong> and possible utility values <strong>u</strong>,<br /><strong>[(Prf(S) =&gt; U=u) and U&lt;=u] =&gt; S</strong> is an axiom.</blockquote>\n<p>(This particular schema has a lot of problems, as discussed below, but seems adequate for communicating the general idea. [<strong>Edit</strong>: Stuart <a href=\"/r/discussion/lw/ca5/consequentialist_formal_systems/6jlw\">points out</a> an even worse problem that makes these axioms break for any easily-provably-false <strong>S</strong>. Not sure what can be salvaged from this problem yet.]) A moral axiom from this schema states that if statement <strong>S</strong> being provable implies that the best possible utility gets realized, then that statement is declared to be valid.</p>\n<p>Suppose that an agent has to choose an action <strong>A</strong> among possible actions <strong>1</strong> and <strong>2</strong>, and wants to follow this theory's decisions. Then all it needs to do is pick a new propositional symbol <strong>B</strong> and establish the following decision rules:</p>\n<blockquote><strong>Prf(B) =&gt; A=1<br /> Prf(~B) =&gt; A=2</strong></blockquote>\n<p><strong>B</strong> remains otherwise undefined, its only effect is on our agent, or on definition of <strong>A</strong>. If it's true that, say, <strong>[A=1 =&gt; U=10]</strong>, <strong>[A=2 =&gt; U=5]</strong>, and also that in general <strong>[U&lt;=10]</strong>, then <strong>Prf(B)</strong> implies <strong>[U=10]</strong>, which triggers the moral axiom for statement <strong>B</strong> and makes it valid/provable. As a result, the agent finds a proof of <strong>B</strong> and performs action <strong>1</strong>.</p>\n<h3>Agent-less decision theory</h3>\n<p>This formulation is different from the usual ones in that the consequentialist loop is operated entirely from within an abstract formal system (i.e. not an algorithm). The formal system doesn't have an intended interpretation or a privileged agent (definition of an action) that would enact its decisions. Instead, it looks for all possible agents (actions, facts) that respond to its arguments (and affect its utility value), and supplies the arguments (theorems) according to how those agents respond to various hypothetical arguments. If there are multiple agents that have to be coordinated, that calls for proving a theorem that simultaneously establishes the strategies of all agents involved. And the agents could well use their own inference systems or proof search algorithms.</p>\n<p>For an agent, such formal system plays a role of preference, it is an abstract computation that answers the questions about what should be done in each particular situation.</p>\n<h3>Open problems</h3>\n<p>The axiom schema <strong>[(Prf(S) =&gt; U=u) and U&lt;=u] =&gt; S</strong> is not adequate for many reasons. First, it's only capable of making knowably perfect decisions (which in particular requires utility value to have a reachable upper bound). Second, it introduces a different kind of spurious arguments that make the formal system inconsistent: once a statement <strong>S</strong>&nbsp;triggers its moral axiom, it follows that <strong>Prf(S)</strong>, and so <strong>U=u</strong>, which triggers the other moral axioms all at once. This isn't necessarily too bad, since it's irrelevant what happens once utility value is optimal, but it also makes it harder to trigger moral axioms prior to making a decision.</p>\n<p>For example, in <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">Wei Dai's variant of coordination game</a>, an agent is given indexical identification <strong>1</strong> or <strong>2</strong>, and has to pick among actions <strong>A</strong> and <strong>B</strong> in such a way that its versions that observe <strong>1</strong> and <strong>2</strong> pick different actions. A natural way of setting up the agent using a consequentialist theory is to introduce propositional symbols <strong>T1</strong> and <strong>T2</strong>, and establish decision rules</p>\n<blockquote>If I observe <strong>1</strong>, <strong>Prf(T1) =&gt; action=A</strong>; <strong>Prf(~T1) =&gt; action=B</strong><br /> If I observe <strong>2</strong>, <strong>Prf(T2) =&gt; action=A</strong>; <strong>Prf(~T2) =&gt; action=B</strong></blockquote>\n<p>In this case, if either&nbsp;<strong>[T1 and ~T2]</strong> or <strong>[T2 and ~T1]</strong> is a theorem of the formal system, then the two versions of the agent (observing <strong>1</strong> and <strong>2</strong>) will achieve the optimal utility value. The problem is that moral axioms for both theorems can be triggered, and if both do get triggered, then quickly absurdity is proved, which makes it hard to predict which actions the agents will actually perform, and what utility would follow from that. And if the formal system can't predict the effect of triggering its moral axioms on utility, it won't trigger the moral axioms, so it's unclear what would actually happen. Perhaps some different clever statement will get proved that would predictably lead to the agent choosing the right actions.</p>\n<p>Another issue is that the moral axiom schema should probably only consider theorems of some special kind, and compare their consequences with those of specific other theorems (not just with an unconditional upper bound).</p>\n<h3>What really changed?</h3>\n<p>The main technical difference appears to be that instead of using moral arguments of the form <strong>[A=A1 =&gt; U=U1]</strong>, this approach uses moral arguments of the form <strong>[Prf(A=A1) =&gt; U=U1]</strong>. As a result, proving <strong>A=A2</strong> (for <strong>A2&lt;&gt;A1</strong>) no longer allows inferring a false antecedent, which in this case is <strong>~Prf(A=A1)</strong>, and so the usual path to spurious arguments is closed. Perhaps focusing on just this distinction might be more fruitful than paying attention to the surrounding philosophical bells and whistles.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JGwD6zkhtXjCHjNn9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 21, "extendedScore": null, "score": 8.989824981409537e-07, "legacy": true, "legacyId": "15917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px; \"><em>This post describes a different (less agent-centric) way of looking at UDT-like decision theories that resolves some aspects of the long-standing technical problem of <a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">spurious</a> <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">moral</a> <a href=\"/r/discussion/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">arguments</a>. It's only a half-baked idea, so there are currently a lot of loose ends.</em></p>\n<h3 id=\"On_spurious_arguments\">On spurious arguments</h3>\n<p>UDT agents are usually considered as having a disinterested inference system (a \"mathematical intuition module\" in <a href=\"http://wiki.lesswrong.com/wiki/UDT\">UDT</a> and first order proof search in <a href=\"http://wiki.lesswrong.com/wiki/ADT\">ADT</a>) that plays a purely epistemic role, and preference-dependent decision rules that look for statements that characterize possible actions in terms of the utility value that the agent optimizes.</p>\n<p>The statements (supplied by the inference system) used by agent's decision rules (to pick one of the many variants) have the form <strong>[(A=A1 =&gt; U=U1) and U&lt;=U1]</strong>. Here, <strong>A</strong> is a symbol defined to be the actual action chosen by the agent, <strong>U</strong> is a similar symbol defined to be the actual value of world's utility, and <strong>A1</strong> and <strong>U1</strong> are some particular possible action and possible utility value. If the agent finds that this statement is provable, it performs action <strong>A1</strong>, thereby making <strong>A1</strong> the actual action.</p>\n<p>The use of this statement introduces the problem of <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">spurious arguments</a>: if <strong>A1</strong> is a bad action, but for some reason it's still chosen, then <strong>[(A=A1 =&gt; U=U1) and U&lt;=U1]</strong> is true, since utility value <strong>U</strong> will in that case be in fact <strong>U1</strong>, which justifies (by the decision rule) choosing the bad action <strong>A1</strong>. In usual cases, this problem results in the difficulty of proving that an agent will behave in the expected manner (i.e. won't choose a bad action), which is resolved by adding <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">various</a> <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle\">compilicated</a> <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits\">clauses</a> to its decision algorithm. But even worse, it turns out that if an agent is hapless enough to take seriously a (formally correct) proof of such a statement supplied by an enemy (or if its own inference system is malicious), <a href=\"/r/discussion/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">it can be persuaded to take any action at all</a>, irrespective of agent's own preferences.<a id=\"more\"></a></p>\n<h3 id=\"Deciding_which_theorems_to_make_valid\">Deciding which theorems to make valid</h3>\n<p>Given that an inference system can overpower decision rules, causing an agent to follow a preference other than its own, perhaps decision-making should be happening inside the inference system in the first place, with agent only following its decisions. What does an inference system decide? Directly, it decides which theorems to have. The set of valid theorems follows deterministically from the axioms, but <a href=\"/lw/2os/controlling_constant_programs/\">this is not really a problem</a>, it's possible to make decisions in deterministic settings.</p>\n<p>Suppose an inference system wants to decide whether it should have a theorem <strong>S</strong>. How does it evaluate the consequences of <strong>S</strong> being its theorem? It can assume that it proved <strong>S</strong>, see what that would imply, and if it likes the consequences (in comparison to the consequences of proving <strong>Not-S</strong>, for example), then it concludes <strong>S</strong>. Decision rules that an inference system follows are the axioms of the theory it works with, so this discussion suggests the following axiom schema (of <em>moral axioms</em>):</p>\n<blockquote>For all statements <strong>S</strong> and possible utility values <strong>u</strong>,<br><strong>[(Prf(S) =&gt; U=u) and U&lt;=u] =&gt; S</strong> is an axiom.</blockquote>\n<p>(This particular schema has a lot of problems, as discussed below, but seems adequate for communicating the general idea. [<strong>Edit</strong>: Stuart <a href=\"/r/discussion/lw/ca5/consequentialist_formal_systems/6jlw\">points out</a> an even worse problem that makes these axioms break for any easily-provably-false <strong>S</strong>. Not sure what can be salvaged from this problem yet.]) A moral axiom from this schema states that if statement <strong>S</strong> being provable implies that the best possible utility gets realized, then that statement is declared to be valid.</p>\n<p>Suppose that an agent has to choose an action <strong>A</strong> among possible actions <strong>1</strong> and <strong>2</strong>, and wants to follow this theory's decisions. Then all it needs to do is pick a new propositional symbol <strong>B</strong> and establish the following decision rules:</p>\n<blockquote><strong>Prf(B) =&gt; A=1<br> Prf(~B) =&gt; A=2</strong></blockquote>\n<p><strong>B</strong> remains otherwise undefined, its only effect is on our agent, or on definition of <strong>A</strong>. If it's true that, say, <strong>[A=1 =&gt; U=10]</strong>, <strong>[A=2 =&gt; U=5]</strong>, and also that in general <strong>[U&lt;=10]</strong>, then <strong>Prf(B)</strong> implies <strong>[U=10]</strong>, which triggers the moral axiom for statement <strong>B</strong> and makes it valid/provable. As a result, the agent finds a proof of <strong>B</strong> and performs action <strong>1</strong>.</p>\n<h3 id=\"Agent_less_decision_theory\">Agent-less decision theory</h3>\n<p>This formulation is different from the usual ones in that the consequentialist loop is operated entirely from within an abstract formal system (i.e. not an algorithm). The formal system doesn't have an intended interpretation or a privileged agent (definition of an action) that would enact its decisions. Instead, it looks for all possible agents (actions, facts) that respond to its arguments (and affect its utility value), and supplies the arguments (theorems) according to how those agents respond to various hypothetical arguments. If there are multiple agents that have to be coordinated, that calls for proving a theorem that simultaneously establishes the strategies of all agents involved. And the agents could well use their own inference systems or proof search algorithms.</p>\n<p>For an agent, such formal system plays a role of preference, it is an abstract computation that answers the questions about what should be done in each particular situation.</p>\n<h3 id=\"Open_problems\">Open problems</h3>\n<p>The axiom schema <strong>[(Prf(S) =&gt; U=u) and U&lt;=u] =&gt; S</strong> is not adequate for many reasons. First, it's only capable of making knowably perfect decisions (which in particular requires utility value to have a reachable upper bound). Second, it introduces a different kind of spurious arguments that make the formal system inconsistent: once a statement <strong>S</strong>&nbsp;triggers its moral axiom, it follows that <strong>Prf(S)</strong>, and so <strong>U=u</strong>, which triggers the other moral axioms all at once. This isn't necessarily too bad, since it's irrelevant what happens once utility value is optimal, but it also makes it harder to trigger moral axioms prior to making a decision.</p>\n<p>For example, in <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">Wei Dai's variant of coordination game</a>, an agent is given indexical identification <strong>1</strong> or <strong>2</strong>, and has to pick among actions <strong>A</strong> and <strong>B</strong> in such a way that its versions that observe <strong>1</strong> and <strong>2</strong> pick different actions. A natural way of setting up the agent using a consequentialist theory is to introduce propositional symbols <strong>T1</strong> and <strong>T2</strong>, and establish decision rules</p>\n<blockquote>If I observe <strong>1</strong>, <strong>Prf(T1) =&gt; action=A</strong>; <strong>Prf(~T1) =&gt; action=B</strong><br> If I observe <strong>2</strong>, <strong>Prf(T2) =&gt; action=A</strong>; <strong>Prf(~T2) =&gt; action=B</strong></blockquote>\n<p>In this case, if either&nbsp;<strong>[T1 and ~T2]</strong> or <strong>[T2 and ~T1]</strong> is a theorem of the formal system, then the two versions of the agent (observing <strong>1</strong> and <strong>2</strong>) will achieve the optimal utility value. The problem is that moral axioms for both theorems can be triggered, and if both do get triggered, then quickly absurdity is proved, which makes it hard to predict which actions the agents will actually perform, and what utility would follow from that. And if the formal system can't predict the effect of triggering its moral axioms on utility, it won't trigger the moral axioms, so it's unclear what would actually happen. Perhaps some different clever statement will get proved that would predictably lead to the agent choosing the right actions.</p>\n<p>Another issue is that the moral axiom schema should probably only consider theorems of some special kind, and compare their consequences with those of specific other theorems (not just with an unconditional upper bound).</p>\n<h3 id=\"What_really_changed_\">What really changed?</h3>\n<p>The main technical difference appears to be that instead of using moral arguments of the form <strong>[A=A1 =&gt; U=U1]</strong>, this approach uses moral arguments of the form <strong>[Prf(A=A1) =&gt; U=U1]</strong>. As a result, proving <strong>A=A2</strong> (for <strong>A2&lt;&gt;A1</strong>) no longer allows inferring a false antecedent, which in this case is <strong>~Prf(A=A1)</strong>, and so the usual path to spurious arguments is closed. Perhaps focusing on just this distinction might be more fruitful than paying attention to the surrounding philosophical bells and whistles.</p>", "sections": [{"title": "On spurious arguments", "anchor": "On_spurious_arguments", "level": 1}, {"title": "Deciding which theorems to make valid", "anchor": "Deciding_which_theorems_to_make_valid", "level": 1}, {"title": "Agent-less decision theory", "anchor": "Agent_less_decision_theory", "level": 1}, {"title": "Open problems", "anchor": "Open_problems", "level": 1}, {"title": "What really changed?", "anchor": "What_really_changed_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZpATmvAyqajiA5XNC", "W6T93dSSm2xvHn9X6", "2GebvAXXfRMTjY2g7", "Bj244uWzDBXvE2N2S", "m39dkp73YhN9QKYb9", "gZbHSWcLvj7ZopSas", "g8xh9R7RaNitKtkaa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-08T21:14:47.419Z", "modifiedAt": null, "url": null, "title": "Thoughts from a conversation on quantum immortality", "slug": "thoughts-from-a-conversation-on-quantum-immortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.880Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pGkuD3jTixcf4NWhc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QjsQqTYjjw5EtkTKj/thoughts-from-a-conversation-on-quantum-immortality", "pageUrlRelative": "/posts/QjsQqTYjjw5EtkTKj/thoughts-from-a-conversation-on-quantum-immortality", "linkUrl": "https://www.lesswrong.com/posts/QjsQqTYjjw5EtkTKj/thoughts-from-a-conversation-on-quantum-immortality", "postedAtFormatted": "Tuesday, May 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20from%20a%20conversation%20on%20quantum%20immortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20from%20a%20conversation%20on%20quantum%20immortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjsQqTYjjw5EtkTKj%2Fthoughts-from-a-conversation-on-quantum-immortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20from%20a%20conversation%20on%20quantum%20immortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjsQqTYjjw5EtkTKj%2Fthoughts-from-a-conversation-on-quantum-immortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjsQqTYjjw5EtkTKj%2Fthoughts-from-a-conversation-on-quantum-immortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\">[Today, I participated in a conversation about the idea of quantum immortality. I decided to summarise some of the thoughts that came up in this short post. Therefore, it should be viewed as a report on a discussion rather than an attempt at a proper post.]</span></p>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\">Assume that many-worlds interpretation is correct and quantum immortality is true. Essentially, in at least one universe you survive no matter what dangerous things you try. Since there is no defined biological \"expiry date\" on your body, you end up in state where your body just continues avoiding terminal shutdown. Your body is failing but the space of probabilistic events (such as a given organ failing, or a given blood vessel rupturing, or two given molecules interacting, or others, however minor) which lead to terminal shutdown is sufficiently large to last you for a while, with at least one universe where you still happen to be alive.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\">The above process probably takes a while(?).&nbsp;Until the entire space of events is explored (your body is finite) and you die in all universes. But in this case we don't have quantum immortality, only \"maximally delayed mortality\" (MDM).</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\">We can only observe quantum immortality with relation to ourself. Thus, in the universe where you survive,&nbsp;everyone else around you is likely to be dead, since the probability of two individuals surviving to this stage in the same universe is much smaller than the probability of you alone surviving. Therefore, you end up in an incapacitated state of continuous (eventually, lethal) failing of your body, completely alone in a universe where everyone else is dead.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\">The argument in the previous paragraph does not account for new people being born. So assuming no catastrophic event killing everyone else but you occurred, there may be other people in the universe where you are. But then you will not be in a state to appreciate that towards the end of your MDM.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\">So, if many-worlds is true, are we all going to end up experiencing a slow gradual fade-out of life just as we experienced a gradual fade-in?</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QjsQqTYjjw5EtkTKj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 8.98998157664007e-07, "legacy": true, "legacyId": "15919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T03:57:52.470Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Do Scientists Already Know This Stuff", "slug": "seq-rerun-do-scientists-already-know-this-stuff", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.749Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/En2tw2Z673LQygTQ3/seq-rerun-do-scientists-already-know-this-stuff", "pageUrlRelative": "/posts/En2tw2Z673LQygTQ3/seq-rerun-do-scientists-already-know-this-stuff", "linkUrl": "https://www.lesswrong.com/posts/En2tw2Z673LQygTQ3/seq-rerun-do-scientists-already-know-this-stuff", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Do%20Scientists%20Already%20Know%20This%20Stuff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Do%20Scientists%20Already%20Know%20This%20Stuff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEn2tw2Z673LQygTQ3%2Fseq-rerun-do-scientists-already-know-this-stuff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Do%20Scientists%20Already%20Know%20This%20Stuff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEn2tw2Z673LQygTQ3%2Fseq-rerun-do-scientists-already-know-this-stuff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEn2tw2Z673LQygTQ3%2Fseq-rerun-do-scientists-already-know-this-stuff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>Today's post, <a href=\"/lw/qe/do_scientists_already_know_this_stuff/\">Do Scientists Already Know This Stuff?</a> was originally published on 17 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>No. Maybe someday it will be part of standard scientific training, but for now, it's not, and the absence is visible.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c9n/seq_rerun_science_isnt_strict_enough/\">Science Isn't Strict Enough</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "En2tw2Z673LQygTQ3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.991735723211723e-07, "legacy": true, "legacyId": "15935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WijMw9WkcafmCFgj4", "eTb3BvviZJ6xkabbE", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T15:05:49.260Z", "modifiedAt": null, "url": null, "title": "The wave function is real [LINK]", "slug": "the-wave-function-is-real-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.348Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ZankerH", "createdAt": "2010-12-30T00:37:11.200Z", "isAdmin": false, "displayName": "ZankerH"}, "userId": "gSDDvxX4hKn6h6sou", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AdZqvgJSCSsYRQeBA/the-wave-function-is-real-link", "pageUrlRelative": "/posts/AdZqvgJSCSsYRQeBA/the-wave-function-is-real-link", "linkUrl": "https://www.lesswrong.com/posts/AdZqvgJSCSsYRQeBA/the-wave-function-is-real-link", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20wave%20function%20is%20real%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20wave%20function%20is%20real%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAdZqvgJSCSsYRQeBA%2Fthe-wave-function-is-real-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20wave%20function%20is%20real%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAdZqvgJSCSsYRQeBA%2Fthe-wave-function-is-real-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAdZqvgJSCSsYRQeBA%2Fthe-wave-function-is-real-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p><a href=\"http://arxiv.org/pdf/1111.3328.pdf\">arxiv</a></p>\n<blockquote>\n<p>Quantum states are the key mathematical objects in quantum theory. It is therefore surprising that physicists have been unable to agree on what a quantum state represents. One possibility is that a pure quantum state corresponds directly to reality. But there is a long history of suggestions that a quantum state (even a pure state) represents only knowledge or information of some kind. Here we show that any model in which a quantum state represents mere information about an underlying physical state of the system must make predictions which contradict those of quantum theory.</p>\n</blockquote>\n<p>So, it turns out it is more than \"just\" a useful mathematical tool after all. Does this confirm the universe basically runs on maths?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AdZqvgJSCSsYRQeBA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 8.994643839176503e-07, "legacy": true, "legacyId": "15945", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T15:17:26.675Z", "modifiedAt": null, "url": null, "title": "Neil deGrasse Tyson on Cryonics", "slug": "neil-degrasse-tyson-on-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:39.540Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bekkerd", "createdAt": "2011-03-31T07:59:22.124Z", "isAdmin": false, "displayName": "bekkerd"}, "userId": "u4r4BLCPFSda3mBGf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iGvkd5uaqS8Xp4dde/neil-degrasse-tyson-on-cryonics", "pageUrlRelative": "/posts/iGvkd5uaqS8Xp4dde/neil-degrasse-tyson-on-cryonics", "linkUrl": "https://www.lesswrong.com/posts/iGvkd5uaqS8Xp4dde/neil-degrasse-tyson-on-cryonics", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Neil%20deGrasse%20Tyson%20on%20Cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeil%20deGrasse%20Tyson%20on%20Cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGvkd5uaqS8Xp4dde%2Fneil-degrasse-tyson-on-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Neil%20deGrasse%20Tyson%20on%20Cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGvkd5uaqS8Xp4dde%2Fneil-degrasse-tyson-on-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGvkd5uaqS8Xp4dde%2Fneil-degrasse-tyson-on-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<p>Question:</p>\r\n<blockquote>\r\n<p>What are your thoughts on cryogenic preservation and the idea of medically treating aging?</p>\r\n</blockquote>\r\n<p>His response:</p>\r\n<blockquote>\r\n<p>A marvelous way to just convince people to give you money. Offer to freeze them for later. I'd have more confidence if we had previously managed to pull this off with other mammals. Until then I see it as a waste of money. I'd rather enjoy the money, and then be buried, offering my body back to the flora and fauna of which I have dined my whole life.</p>\r\n</blockquote>\r\n<p><a href=\"http://www.reddit.com/r/IAmA/comments/mateq/i_am_neil_degrasse_tyson_ama/c2zglen\">Link</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iGvkd5uaqS8Xp4dde", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 8.994694460801894e-07, "legacy": true, "legacyId": "10912", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T17:48:24.657Z", "modifiedAt": null, "url": null, "title": "Is friendly AI \"trivial\" if the AI cannot rewire human values?", "slug": "is-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.441Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alerus", "createdAt": "2011-12-25T15:46:21.366Z", "isAdmin": false, "displayName": "Alerus"}, "userId": "TncxbafigJydcbf2s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pPhpxNkpktTzTMZrB/is-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values", "pageUrlRelative": "/posts/pPhpxNkpktTzTMZrB/is-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values", "linkUrl": "https://www.lesswrong.com/posts/pPhpxNkpktTzTMZrB/is-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20friendly%20AI%20%22trivial%22%20if%20the%20AI%20cannot%20rewire%20human%20values%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20friendly%20AI%20%22trivial%22%20if%20the%20AI%20cannot%20rewire%20human%20values%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPhpxNkpktTzTMZrB%2Fis-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20friendly%20AI%20%22trivial%22%20if%20the%20AI%20cannot%20rewire%20human%20values%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPhpxNkpktTzTMZrB%2Fis-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPhpxNkpktTzTMZrB%2Fis-friendly-ai-trivial-if-the-ai-cannot-rewire-human-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>I put \"trivial\" in quotes because there are obviously some exceptionally large technical achievements that would still need to occur to get here, but suppose we had an AI with a utilitarian utility function of maximizing subjective human well-being (meaning, well-being is not something as simple as physical sensation of \"pleasure\" and depends on the mental facts of each person) and let us also assume the AI can model this \"well\" (lets say at least as well as the best of us can deduce the values of another person for their well-being). Finally, we will also assume that the AI does not possess the ability to manually rewire the human brain to change what a human values. In other words, the ability for the AI to manipulate another person's values is limited by what we as humans are capable of today. Given all this, is there any concern we should have about making this AI; would it succeed in being a friendly AI?</p>\n<p>One argument I can imagine for why this fails friendly AI is the AI would wire people up to virtual reality machines. However, I don't think that works very well, because a person (except Cypher from the Matrix) wouldn't appreciate being wired into a virtual reality machine and having their autonomy forcefully removed. This means the action does not succeed in maximizing their well-being.</p>\n<p>But I am curious to hear what arguments exist for why such an AI might still fail as a friendly AI.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pPhpxNkpktTzTMZrB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -9, "extendedScore": null, "score": 8.99535197786416e-07, "legacy": true, "legacyId": "15946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T18:07:54.070Z", "modifiedAt": null, "url": null, "title": "Jason Silva on AI safety", "slug": "jason-silva-on-ai-safety", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.213Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rHhFH8MCRSGH36tb6/jason-silva-on-ai-safety", "pageUrlRelative": "/posts/rHhFH8MCRSGH36tb6/jason-silva-on-ai-safety", "linkUrl": "https://www.lesswrong.com/posts/rHhFH8MCRSGH36tb6/jason-silva-on-ai-safety", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Jason%20Silva%20on%20AI%20safety&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJason%20Silva%20on%20AI%20safety%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrHhFH8MCRSGH36tb6%2Fjason-silva-on-ai-safety%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Jason%20Silva%20on%20AI%20safety%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrHhFH8MCRSGH36tb6%2Fjason-silva-on-ai-safety", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrHhFH8MCRSGH36tb6%2Fjason-silva-on-ai-safety", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Just an FYI that Jason Silva, a \"<a href=\"http://www.theatlantic.com/technology/archive/2012/04/a-timothy-leary-for-the-viral-video-age/255691/\">performance philosopher</a>\" who is quickly gaining popularity and audience, seems to have given little thought to, or not been exposed to the proper arguments for, or is unconvinced by, the existential threat of AGI. But of course, perhaps this optimism is what has allowed him to become so engagingly exuberant.</p>\n<p style=\"padding-left: 30px;\">\"<span style=\"font-family: Georgia, 'times new roman', times, serif; font-size: 13px; line-height: 19px;\">And I think if they're truly trillions of times more intelligent than us, they're not going to be less empathetic than us---they're probably going to be more empathetic. For them it might not be that big of deal to give us some big universe to play around in, like an ant farm or something like that. We could already be living in such a world for all we know. But either way, I don't think they're going to tie us down and enslave us and send us to death camps; I don't think they're going to be fascist A.I.'s. \"</span></p>\n<p>Anyone have the connections to change his mind and help the X-risk meme piggyback on his voice? &nbsp;Perhaps inviting him to Singularity Summit?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rHhFH8MCRSGH36tb6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -2, "extendedScore": null, "score": 8.995436871459017e-07, "legacy": true, "legacyId": "15947", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T22:58:16.769Z", "modifiedAt": null, "url": null, "title": "RationalWiki's take on LW", "slug": "rationalwiki-s-take-on-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LordSnow", "createdAt": "2012-05-09T17:19:15.467Z", "isAdmin": false, "displayName": "LordSnow"}, "userId": "Fmi2yNBp9fdaKXkxE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M4DgQ48XPHfeSjKJf/rationalwiki-s-take-on-lw", "pageUrlRelative": "/posts/M4DgQ48XPHfeSjKJf/rationalwiki-s-take-on-lw", "linkUrl": "https://www.lesswrong.com/posts/M4DgQ48XPHfeSjKJf/rationalwiki-s-take-on-lw", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20RationalWiki's%20take%20on%20LW&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalWiki's%20take%20on%20LW%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4DgQ48XPHfeSjKJf%2Frationalwiki-s-take-on-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=RationalWiki's%20take%20on%20LW%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4DgQ48XPHfeSjKJf%2Frationalwiki-s-take-on-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4DgQ48XPHfeSjKJf%2Frationalwiki-s-take-on-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>I am not sure whether this has been posted here before but I came across this: <a href=\"http://rationalwiki.org/wiki/LessWrong\">http://rationalwiki.org/wiki/LessWrong</a></p>\n<p>What do you think about RationalWiki in general, and their opinion regarding LW?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M4DgQ48XPHfeSjKJf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -1, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "15950", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-09T23:52:14.508Z", "modifiedAt": null, "url": null, "title": "Thinking and Deciding: a chapter by chapter review", "slug": "thinking-and-deciding-a-chapter-by-chapter-review", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:56.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mn3WdvME7CJqSaRar/thinking-and-deciding-a-chapter-by-chapter-review", "pageUrlRelative": "/posts/mn3WdvME7CJqSaRar/thinking-and-deciding-a-chapter-by-chapter-review", "linkUrl": "https://www.lesswrong.com/posts/mn3WdvME7CJqSaRar/thinking-and-deciding-a-chapter-by-chapter-review", "postedAtFormatted": "Wednesday, May 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thinking%20and%20Deciding%3A%20a%20chapter%20by%20chapter%20review&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThinking%20and%20Deciding%3A%20a%20chapter%20by%20chapter%20review%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmn3WdvME7CJqSaRar%2Fthinking-and-deciding-a-chapter-by-chapter-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thinking%20and%20Deciding%3A%20a%20chapter%20by%20chapter%20review%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmn3WdvME7CJqSaRar%2Fthinking-and-deciding-a-chapter-by-chapter-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmn3WdvME7CJqSaRar%2Fthinking-and-deciding-a-chapter-by-chapter-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3900, "htmlBody": "<p><img style=\"float: right; margin-left: 10px; margin-right: 10px; margin-top: 0px; margin-bottom: 0px;\" src=\"https://images-na.ssl-images-amazon.com/images/I/31d9LdulhiL._SY344_BO1,204,203,200_.jpg\" alt=\"\" width=\"204\" height=\"300\" />This is a chapter-by-chapter review of <a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433/ref=nosim?tag=vglnk-c319-20\">Thinking and Deciding</a> by Jonathan Baron (<a href=\"http://www.sas.upenn.edu/~baron/\">UPenn</a>, <a href=\"http://twitter.com/intent/user?screen_name=jonbaron1944\">twitter</a>). It won't be a detailed summary like <a href=\"/lw/5vs/epistemology_and_the_psychology_of_human_judgment/\">badger's excellent summary</a> of<a href=\"http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307/ref=nosim?tag=vglnk-c319-20\"> Epistemology and the Psychology of Human Judgment</a>, in part because this is a 600-page textbook and so a full summary would be far longer that I want to write here. I'll try to provide enough details that people can seek out the chapters that they find interesting, but this is by no means a replacement for reading the chapters that you find interesting. Every chapter is discussed below, with a brief \"what should I read?\" section if you know what you're interested in.</p>\n<p>We already have a thread for <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">textbook recommendations</a>, but this book is central enough to Less Wrong's mission that it seems like it's worth an in-depth review. I'll state my basic impression of the whole book up front: I expect most readers of LW would gain quite a bit from reading the book, especially newer members, as it seems like a more focused and balanced introduction to the subject of rationality than the Sequences.</p>\n<p>Baron splits the book into three sections: Thinking in General, Probability and Belief, and Decisions and Plans.</p>\n<p><a id=\"more\"></a></p>\n<p>I may as well quote the first page in its entirety, as I feel it gives a good description of the book:</p>\n<blockquote>Beginning with its first edition and through three subsequent editions, <em>Thinking and Deciding</em> has established itself as the required text and important reference work for students and scholars of human cognition and rationality. In this, the fourth edition, Jonathan Baron retains the comprehensive attention to the key questions addressed in previous editions- How should we think? What, if anything, keeps us from thinking that way? How can we improve our thinking and decision making? - and his expanded treatment of topics such as risk, utilitarianism, Bayes's theorem, and moral thinking. With the student in mind, the fourth edition emphasizes the development of an understanding of the fundamental concepts in judgment and decision making. This book is essential reading for students and scholars in judgment and decision making and related fields, including psychology, economics, law, medicine, and business.</blockquote>\n<blockquote>Jonathan Baron is Professor of Psychology at the University of Pennsylvania. He is the author and editor of several other books, most recently <a href=\"http://www.amazon.com/Against-Bioethics-Basic-Jonathan-Baron/dp/B005ZO9FAM/ref=nosim?tag=vglnk-c319-20\"><em>Against Bioethics</em></a>. Currently he is editor of the journal <a href=\"http://journal.sjdm.org/\"><em>Judgment and Decision Making</em></a> and president of the <a href=\"http://www.sjdm.org/\">Society for Judgment and Decision Making</a> (2007) .</blockquote>\n<h2>1. What is thinking?</h2>\n<p>This chapter will be mostly familiar to readers of Less Wrong; in the second paragraph, Baron says (in more words) 'rationality is what wins.' It may still be helpful as Baron expresses a number of things often left unsaid here.</p>\n<p>He splits thinking into three parts: thinking about decisions (instrumental rationality), thinking about beliefs (epistemic rationality), and thinking about goals. The last is a notoriously sticky subject. He also discusses his search-inference framework, which is how he describes minds as actually operating- coming across ideas, evaluating them, and proceeding from there. Most decision analysis views itself as operating over a fixed set with a well-defined objective function, but those are the two main problems for real decision-makers: identifying possibilities worth considering and comparing two dissimilar outcomes.</p>\n<p>The chapter is filled out with a discussion of understanding, knowledge as design, and examples of thinking processes (worth skimming over, but many of which will be familiar to experts in the relevant fields).</p>\n<h2>2. The study of thinking</h2>\n<p>Kahneman and Tversky get their first of many references here. Baron discusses a number of the methods used to learn about human cognition, mentioning a few of their pitfalls.</p>\n<p>One, which bears repeating, is that most study of biases just reports means, rather than distributions. I remember learning the actual numerical size of the <a href=\"http://en.wikipedia.org/wiki/Asch_conformity_experiments\">Asch conformity experiments</a> about five years after I heard about the experiment itself, and was underwhelmed (32% incorrect answers, ~75% of subjects gave at least one incorrect answer). A general human tendency is different from a sizeable subset of weak-willed people. Similarly, our article on <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">Prospect Theory</a> had a link to <a href=\"https://docs.google.com/file/d/0B9BN3c2y3wxCZGIxNzY2MzUtYjRjZC00ZjQwLTk3MjEtOWJlYThiMzJmNzhl/edit\">graphs of subjective probability</a> in one of the comments, of which the most noteworthy were the two people who were nearly linear. While Baron brings up this issue, he doesn't give many examples of it here.</p>\n<p>He also mentions three models of thought: descriptive models, prescriptive models, and normative models. Descriptive models are what people actually do; normative models are what thinkers should do with infinite cognitive resources; prescriptive models are what thinkers should do with limited cognitive resources. This has come up on LW before, though the focus here has often been exclusively on the normative, though the prescriptive seems most useful.</p>\n<p>Computer models of thinking are briefly discussed, but at a superficial level.</p>\n<p>This chapter sees the first set of exercises. Overall, the exercises in the book seem to provide a brief example / check, rather than being enough to develop mastery. I think this is what I'd recommend but it has the potential to be a weakness.</p>\n<h2>3. Rationality</h2>\n<p>Again, Baron identifies rationality as &ldquo;the kind of thinking that helps us achieve our goals.&rdquo; Refreshingly, he focuses on <em>optimal</em> search, keeping in mind the costs of decision-making and information-gathering.</p>\n<p>Much of this chapter will be familiar to someone who has read the Sequences, but it's presented tersely and lucidly. The section on rationality and emotion, for example, is only three pages long but is clear, quickly identifying how the two interact in a way that'll clear up common confusions.</p>\n<h2>4. Logic</h2>\n<p>The content in this chapter seems mostly unimportant- I imagine most readers of LW are much more interested in probabilistic reasoning than syllogisms. Still, Baron gives a readable (and not very favorable) description of the usefulness of formal logic as a normative model of thinking.</p>\n<p>What is fascinating, though, is the section of the chapter that delves into the <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">four-card problem</a> and variations of it. Particularly noteworthy is the variation designed so that most people's intuitions are correct- people give the correct explanations of why they selected the cards they selected, and why they didn't select the cards they didn't select. But when their intuition is wrong, they give explanations that are just as sophisticated- but wrong. It's more evidence that the decision-making and verbal reason-providing modules are different- even someone who gives the correct explanation of the correct answer may stumble on a problem where their underlying simple heuristic (pick the cards mentioned in the question) fails.</p>\n<p>He presents a method of mental modeling that makes logical statements easier to correctly evaluate, and then there are a few logical inference exercises.</p>\n<h2>5. Normative theory of probability</h2>\n<p>Yet another introduction to Bayes. Baron focuses primarily on Bayesianism (called the &ldquo;personal&rdquo; theory of probability) but still introduces alternatives (the &ldquo;frequency&rdquo; theory, i.e. frequentism, and &ldquo;logical&rdquo; theory, which is a subset of frequentism where all events are required to have the same probability.) This chapter will be useful for someone who doesn't have a firm probabilistic foundation, but holds little interest for others.</p>\n<p>There are a handful of exercises for applying Bayes.</p>\n<h2>6. Descriptive theory of probability judgment</h2>\n<p>This chapter primarily covers biases related to numerical probability estimates, many of which are classics in the heuristics and biases field (and so have probably been mentioned on Less Wrong at least once). The chapter shines when Baron goes into the detail of an experiment and its variations, as that gives a firmer view of what the experiment actually shows (and, importantly, what it does not show)- descriptions of biases where he only quotes a single experiment (or single feature of an experiment) feel weaker.</p>\n<p>A major feature of this chapter is the implication that people are bad at numerical probability estimation mostly because they're unfamiliar with it, implying that calibration exercises may improve probability estimation. A 1977 study of weatherman calibration suggested they were very well calibrated, both with their estimates and with the confidence that should be placed in those estimates. More <a href=\"http://journals.ametsoc.org/doi/abs/10.1175/2008MWR2547.1\">recent work</a> shows that weathermen have systematic calibration biases.</p>\n<h2>7. Hypothesis testing</h2>\n<p>I was gratified to discover that this chapter was not about statistics, but how to come up with and test hypotheses. Baron discusses different models of scientific advancement, focusing on the sorts of likelihood ratios that they look for, as well as discussing the sort of mistakes people make when choosing tests for hypotheses. Many of the stories will probably be familiar- Ignaz Semmelweis gets a mention, though in more detail than I had seen before, as well as the 2-4-6 rule familiar to HPMOR fans and a variation of the four card experiment that makes the typical mistake more obvious.</p>\n<p>He gives a baking example to suggest why people might search primarily for positive evidence- there may be benefits to getting a &ldquo;yes&rdquo; answer besides the information involved. If you're experimenting with cake recipes, and you think your last cake was good because of a feature, it makes sense to alter other features but keep the one you suspect the same, as that means a good cake is more likely; if you think a cake was bad because of a feature, it makes sense to alter that feature but keep the others the same, as that also means a good cake is more likely. In a purely scientific context, it makes sense to vary the element you think has an impact just to maximize the expected size of the impact, positive or negative.</p>\n<p>He describes in more detail a methodology he's been discussing, &ldquo;actively open-minded thinking,&rdquo; which seems to boil down to &ldquo;don't just be willing to accept disconfirming evidence, go looking for it,&rdquo; but the full explanation comes in a few chapters.</p>\n<h2>8. Judgment of correlation and contingency</h2>\n<p>This chapter is descriptive; it begins with a description of correlations and then discusses human judgment of correlations. Unsurprisingly, people suffer from the illusion of control- they think there's more likely to be a correlation if their effort is involved- and from confirmation bias. There are some examples of the latter, where people find correlations that make intuitive sense but aren't in the data, and don't discover correlations that don't make intuitive sense that are in the data. There's also a brief section on how people use nearly useless evidence to support theories or dismiss evidence that doesn't support their theory. Overall, it's a short chapter that won't be surprising to LW readers (although some of the studies referenced may be new).</p>\n<h2>9. Actively open-minded thinking</h2>\n<p>I'll quote part of this chapter in full because I think it's a great description:</p>\n<blockquote>\n<p>[G]ood thinking consists of (1) search that is thorough in proportion to the importance of the question, (2) confidence that is appropriate to the amount and quality of thinking done, and (3) fairness to other possibilities than the one we initially favor.</p>\n</blockquote>\n<p>The chapter overall is very solid- it deftly combines normative predictions with descriptive biases to weave a prescriptive recommendation of how to think better. There are several great examples of actively open-minded thinking; in particular, the thought process of two students as they attempt to make sense of a story sentence by sentence.</p>\n<p>Many of the suggestions in the chapter are extended by various LW posts, but the chapter seems useful as a concise description of the whole problem and illustration of a general solution. If you're having trouble fitting together various rationality hacks, this seems like a good banner to unite them under.</p>\n<h2>10. Normative theory of choice under uncertainty</h2>\n<p>This chapter is an introduction to utility theory, describing how it works, how multiple attributes can be consolidated into one score, and a way to resolve conflicts between agents with different utilities. It's a good introduction to decision analysis / utility theory, and there are some exercises, but there are no surprises for someone who's seen this before.</p>\n<h2>11. Descriptive theory of choice under uncertainty</h2>\n<p>This chapter is an introduction to different theories of how humans actually make decisions, like <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a> and regret theory. There are a handful of exercises for understanding prospect theory.</p>\n<p>Baron takes an even-handed approach to deviations from the normative theory. For example, when discussing regret theory, regrets have a real emotional cost (and real learning benefit)- but behaving according to descriptive theories because they're descriptive rather than because they're useful is a mistake. In many cases, those emotions can be manipulated by choice of reference point.</p>\n<p>He also discusses the ambiguity effect- where people treat known probabilities differently from unknown probabilities, giving examples both of laboratory situations (drawing balls from an urn with a partially known composition) and real-life situations (insuring unprecedented or unrepeatable events). Baron describes this as incompatible with personal probability and suggests it's related to framing- situations where the probabilities seem known can be changed into situations where probabilities seem unknown. This aversion to ambiguity, though, can be perfectly sensible insofar as it pushes decision-makers to acquire more information.</p>\n<p>He also discusses a Tversky study in which most students make a decision to pay money to defer a decision until they receive relevant information, but when asked how they would make the decision in the case of either possible piece of information, most students realize they would make the same decision and choose not to defer the decision.</p>\n<h2>12. Choice under certainty</h2>\n<p>This chapter is primarily descriptive, focusing on the problem of thinking about goals. Most people favor categorical goal systems- Baron gives a great example, from Gardiner and Edwards, of the California Coastal Commission, tasked to decide which development projects to allow on the Pacific Coast. The commission was split into pro-development and pro-environment factions, which almost never agreed on which projects to allow and disallow. When asked to rank projects, most would rank them solely by their preferred criterion, creating lists that strongly disagreed. When asked to take both criteria into account- but with whatever weighting they wanted- the subjects would heavily weight their preferred criterion, but the projects which were both very valuable and not very environmentally damaging floated to the top of both lists, creating significant agreement.</p>\n<p>The list of biases is <em>long</em>, and each has a study or story associated with. Many of the effects have been mentioned on LW somewhere, but it's very useful to have them placed next to each other (and separated from probabilistic biases), and so I'd recommend everyone read this chapter.</p>\n<h2>13. Utility measurement</h2>\n<p>This descriptive chapter discusses the difficult challenge of measuring utilities. It introduces both decision analysis and cost-benefit analysis- the latter converts outcomes to dollars to guide decisions, while the former converts outcomes to utility values to guide decisions.</p>\n<p>People are not very skilled at satisfying axioms we would like them to satisfy. For example, consider the challenge of valuing a certain $50 against a <em>p</em> chance of $100 (and $0 otherwise). A subject will often give an answer like .7. Then, when later asked how much a 70% chance of $100 is worth, the subject will answer $60. That inconsistency needs to be resolved before their answers are used as parameters for any decisions. Thankfully, this is an area of active research, and ways to elicit probabilities and values that hold up to <a href=\"/lw/6r6/tendencies_in_reflective_equilibrium/\">reflective equilibrium</a> are gradually being developed. (This particular chapter, while it sounds that note of hope, is mostly negative: here are methods that have been tried and have crippling problems.)</p>\n<p>This seems like a chapter that would be useful for anyone who wants to use utilities in an argument or model- treating them like they're unambiguous, easily measured objects when they actually seem to be fuzzy and hard to pin down can lead to significant problems, and thinking clearly about values is a spot where LW could do better.</p>\n<h2>14. Decision analysis and values</h2>\n<p>This chapter is a more prescriptive approach to the same problem- given that utilities and values are hard to find, where do we look for them? A dichotomy familiar to LW readers- instrumental and terminal values- appears here as \"means-ends objective hierarchy\" or \"means values\" and \"fundamental values.\"</p>\n<p>It contains a wealth of examples, including a computer-buying one with potential memories of 64KB to 640KB, with the hilarious comment that \"you are buying this computer many years ago, when these numbers made sense!\" There are also practical elicitation suggestions- rather than try to figure out a point estimate, start from a number that's too high until you're indifferent, and then start from a number that's too low until you're indifferent, giving you an indifference range (that you can either report or use the middle of as a point estimate).</p>\n<p>Lexical preferences (also called categorical preferences elsewhere) and tradeoffs are discussed- Baron takes the position (that I share) that lexical preferences are actually tradeoffs with very, very high weights. (How do we trade off human lives and dollars? We should require a <em>lot</em> of dollars for a life- but not an infinite amount.) There's a discussion of <a href=\"/lw/8ui/measures_risk_death_and_war/\">micromorts</a> (though he doesn't use that term) and of historical attempts to teach decision analysis that should be interesting to CFAR (though the references are a few decades old, now). The discussion of the examples contains quite a bit of practical advice, and the chapter seems worthwhile for almost everyone.</p>\n<h2>15. Quantitative judgment</h2>\n<p>This chapter describes three common quantitative problems- scoring, ranking, and classifying, and discusses some biases that hamper human decision-making along those lines and some recommendations. Statistical prediction rules make an appearance, though they're not called that. One fascinating suggestion is that models of people can actually perform better than those people, since the models don't have off days and people do.</p>\n<p>This chapter will have some new material for LWers, and seems like a good extension of the previous chapter.</p>\n<h2>16. Moral Judgment and Choice</h2>\n<p>This chapter discusses morality from the point of decision-making- which is a refreshing perspective. Baron strongly endorses consequentialism and weakly endorses utilitarianism, providing a host of moral questions in which many people deviate from the consequentialist or utilitarian position.</p>\n<p>A recurring theme is omission bias: people tend to judge active involvement in a situation in which someone is made worse off as worse than passive involvement in such a situation, even if the end result is better for everyone. People also weight intentions, which doesn't fit a direct consequentialist view.</p>\n<p>Overall, the chapter seems valuable for reframing moral questions- placing them within the realm of pragmatism by moving to the perspective of decisions- but provides very little in the way of answers. Both the consequentialist and utilitarian positions are controversial and come with significant drawbacks, and Baron is fair enough in presenting those drawbacks and controversies, though in a rather abridged form.</p>\n<h2>17. Fairness and justice</h2>\n<p>This chapter is an extension of the previous chapter, focusing on intuitions dealing with fairness and justice. Baron details situations in which they agree and disagree with utilitarian analysis. Noteworthy is the undercurrent of <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptation-execution and not utility-maximization</a> - fairness has tangible benefits, but people will often pursue fairness even at the cost of tangible benefits.</p>\n<p>This chapter (and to a lesser extent the previous one) seem odd in light of chapter 15, in which the fallibility of individual judgment took center stage, with the recommendation that applying rules derived from individual judgment can often do better. It is good to know the reasoning that justifies moral intuitions, especially if one is interested in their boundaries, but when those boundaries impact outcomes they become political questions. If the sole point of punishment is deterrence (and that is the only sensible utilitarian justification), the question of whether or not a decision can impact future decisions is a sticky one. Perhaps the full consequentialist reckoning will recommend unthinking application of the rules, even in cases where direct consequentialist reckoning recommends suspending the rules.</p>\n<h2>18. Social dilemmas: cooperation versus defection</h2>\n<p>This chapter focuses on descriptive experiments- how people actually behave in social dilemmas- finding them to be much more cooperative than normative theory would recommend. There is some ambiguity, which he discusses, in what the \"normative theory\" is- utilitarianism recommends cooperation on the prisoner's dilemma, for example, because it maximizes total utility, whereas expected utility theory recommends defection on the prisoner's dilemma, because it's a dominating strategy.</p>\n<p>The value of the chapter mostly lies in the study results- a few are interesting, like that discussing the social dilemma with other participants beforehand significantly increases cooperation, or that subjects are more likely to defect on the prisoner's dilemma if they know their partner's response than if they are uncertain, even if they know their partner cooperated.</p>\n<p>Typically, for social dilemmas (scenarios in which private gain requires public loss, or public gain requires private loss), decision-making biases increase the level that people cooperate. (This is somewhat unsurprising, since the normative recommendation is typically defection, and biases move real decisions away from the normative recommendation.) People fail to distinguish between casual influence- \"my voting makes people like me more likely to vote\"- from diagnostic influence- \"people like me voting makes me more likely to vote\"- but one of the major reasons people give for voting is that it has a causal influence, rather than a merely diagnostic one.</p>\n<h2>19. Decisions about the future</h2>\n<p>This chapter is unlikely to contain any surprises for LWers, but serves as a fine introduction to discounting, both exponential and hyperbolic, and thus dynamic inconsistency. Also interesting (but too brief) is the discussion of goals in the context of time and plans and of goals as malleable objects.</p>\n<p>Baron describes four methods of self-control: extrapsychic devices (removing a tempting option), control of attention (thinking about things other than the tempting option), control of emotion (cultivating an incompatible emotion), or personal rules (viewing situations as instances of general policies, rather than isolated events). Again, the discussion is brief- only two pages- though the subject is of great interest to many here.</p>\n<h2>20. Risk</h2>\n<p>This chapter focuses on descriptive approaches to risk- survey responses and government regulation- as the normative approach to risk has mostly been detailed in the rest of the book: use expected utility theory. Most people are beset by biases and innumeracy, though, and so there's a whole chapter of material on misjudgments of risk and insurance.</p>\n<p>Many of the biases, though perhaps not the examples, will be familiar to LWers. On the whole, they're somewhat uninteresting since most of them seem to just result from innumeracy: when given a table of deaths per year from four causes with wildly different prevalences, subjects were correctly willing to pay more to reduce larger risks by the same percentage as smaller risks. But their preferences scaled much more slowly than the risks- the subjects were, on average, willing to pay 20 times as much to prevent 20% of the deaths from a cause of death that killed 10,000 times as many people. Those distorted willingnesses to pay show up in government regulations. People were also more willing to pay for protection against the unfamiliar than the familiar- even though the relative benefit was far higher for protection against the familiar. (The illusion of control also shows up, distorting perceptions of risk.)</p>\n<p>&nbsp;</p>\n<hr />\n<h2>What should I read?<br /></h2>\n<ul>\n<li>Almost everyone: 7 and 9.</li>\n<li>I'm hunting biases: 6, 8, 11, 12, and then 15-20 (perhaps without 18).</li>\n<li>I'm interested in moral reasoning: 13 and 16 should be required reading. 14, 15, and 17-19 will be useful.</li>\n<li>I'm a decision maker: 10 and 14 will be directly useful, but check out the bias chapters too.</li>\n<li>I'm new to rationality: Start off with 1-4.</li>\n<li>I'm an expert at rationality but haven't heard of Baron: Still read 1-4, just to get his perspective of the field.</li>\n<li>I don't have a strong background in Bayesianism: read chapter 5.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mn3WdvME7CJqSaRar", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 55, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "15949", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><img style=\"float: right; margin-left: 10px; margin-right: 10px; margin-top: 0px; margin-bottom: 0px;\" src=\"https://images-na.ssl-images-amazon.com/images/I/31d9LdulhiL._SY344_BO1,204,203,200_.jpg\" alt=\"\" width=\"204\" height=\"300\">This is a chapter-by-chapter review of <a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433/ref=nosim?tag=vglnk-c319-20\">Thinking and Deciding</a> by Jonathan Baron (<a href=\"http://www.sas.upenn.edu/~baron/\">UPenn</a>, <a href=\"http://twitter.com/intent/user?screen_name=jonbaron1944\">twitter</a>). It won't be a detailed summary like <a href=\"/lw/5vs/epistemology_and_the_psychology_of_human_judgment/\">badger's excellent summary</a> of<a href=\"http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307/ref=nosim?tag=vglnk-c319-20\"> Epistemology and the Psychology of Human Judgment</a>, in part because this is a 600-page textbook and so a full summary would be far longer that I want to write here. I'll try to provide enough details that people can seek out the chapters that they find interesting, but this is by no means a replacement for reading the chapters that you find interesting. Every chapter is discussed below, with a brief \"what should I read?\" section if you know what you're interested in.</p>\n<p>We already have a thread for <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">textbook recommendations</a>, but this book is central enough to Less Wrong's mission that it seems like it's worth an in-depth review. I'll state my basic impression of the whole book up front: I expect most readers of LW would gain quite a bit from reading the book, especially newer members, as it seems like a more focused and balanced introduction to the subject of rationality than the Sequences.</p>\n<p>Baron splits the book into three sections: Thinking in General, Probability and Belief, and Decisions and Plans.</p>\n<p><a id=\"more\"></a></p>\n<p>I may as well quote the first page in its entirety, as I feel it gives a good description of the book:</p>\n<blockquote>Beginning with its first edition and through three subsequent editions, <em>Thinking and Deciding</em> has established itself as the required text and important reference work for students and scholars of human cognition and rationality. In this, the fourth edition, Jonathan Baron retains the comprehensive attention to the key questions addressed in previous editions- How should we think? What, if anything, keeps us from thinking that way? How can we improve our thinking and decision making? - and his expanded treatment of topics such as risk, utilitarianism, Bayes's theorem, and moral thinking. With the student in mind, the fourth edition emphasizes the development of an understanding of the fundamental concepts in judgment and decision making. This book is essential reading for students and scholars in judgment and decision making and related fields, including psychology, economics, law, medicine, and business.</blockquote>\n<blockquote>Jonathan Baron is Professor of Psychology at the University of Pennsylvania. He is the author and editor of several other books, most recently <a href=\"http://www.amazon.com/Against-Bioethics-Basic-Jonathan-Baron/dp/B005ZO9FAM/ref=nosim?tag=vglnk-c319-20\"><em>Against Bioethics</em></a>. Currently he is editor of the journal <a href=\"http://journal.sjdm.org/\"><em>Judgment and Decision Making</em></a> and president of the <a href=\"http://www.sjdm.org/\">Society for Judgment and Decision Making</a> (2007) .</blockquote>\n<h2 id=\"1__What_is_thinking_\">1. What is thinking?</h2>\n<p>This chapter will be mostly familiar to readers of Less Wrong; in the second paragraph, Baron says (in more words) 'rationality is what wins.' It may still be helpful as Baron expresses a number of things often left unsaid here.</p>\n<p>He splits thinking into three parts: thinking about decisions (instrumental rationality), thinking about beliefs (epistemic rationality), and thinking about goals. The last is a notoriously sticky subject. He also discusses his search-inference framework, which is how he describes minds as actually operating- coming across ideas, evaluating them, and proceeding from there. Most decision analysis views itself as operating over a fixed set with a well-defined objective function, but those are the two main problems for real decision-makers: identifying possibilities worth considering and comparing two dissimilar outcomes.</p>\n<p>The chapter is filled out with a discussion of understanding, knowledge as design, and examples of thinking processes (worth skimming over, but many of which will be familiar to experts in the relevant fields).</p>\n<h2 id=\"2__The_study_of_thinking\">2. The study of thinking</h2>\n<p>Kahneman and Tversky get their first of many references here. Baron discusses a number of the methods used to learn about human cognition, mentioning a few of their pitfalls.</p>\n<p>One, which bears repeating, is that most study of biases just reports means, rather than distributions. I remember learning the actual numerical size of the <a href=\"http://en.wikipedia.org/wiki/Asch_conformity_experiments\">Asch conformity experiments</a> about five years after I heard about the experiment itself, and was underwhelmed (32% incorrect answers, ~75% of subjects gave at least one incorrect answer). A general human tendency is different from a sizeable subset of weak-willed people. Similarly, our article on <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">Prospect Theory</a> had a link to <a href=\"https://docs.google.com/file/d/0B9BN3c2y3wxCZGIxNzY2MzUtYjRjZC00ZjQwLTk3MjEtOWJlYThiMzJmNzhl/edit\">graphs of subjective probability</a> in one of the comments, of which the most noteworthy were the two people who were nearly linear. While Baron brings up this issue, he doesn't give many examples of it here.</p>\n<p>He also mentions three models of thought: descriptive models, prescriptive models, and normative models. Descriptive models are what people actually do; normative models are what thinkers should do with infinite cognitive resources; prescriptive models are what thinkers should do with limited cognitive resources. This has come up on LW before, though the focus here has often been exclusively on the normative, though the prescriptive seems most useful.</p>\n<p>Computer models of thinking are briefly discussed, but at a superficial level.</p>\n<p>This chapter sees the first set of exercises. Overall, the exercises in the book seem to provide a brief example / check, rather than being enough to develop mastery. I think this is what I'd recommend but it has the potential to be a weakness.</p>\n<h2 id=\"3__Rationality\">3. Rationality</h2>\n<p>Again, Baron identifies rationality as \u201cthe kind of thinking that helps us achieve our goals.\u201d Refreshingly, he focuses on <em>optimal</em> search, keeping in mind the costs of decision-making and information-gathering.</p>\n<p>Much of this chapter will be familiar to someone who has read the Sequences, but it's presented tersely and lucidly. The section on rationality and emotion, for example, is only three pages long but is clear, quickly identifying how the two interact in a way that'll clear up common confusions.</p>\n<h2 id=\"4__Logic\">4. Logic</h2>\n<p>The content in this chapter seems mostly unimportant- I imagine most readers of LW are much more interested in probabilistic reasoning than syllogisms. Still, Baron gives a readable (and not very favorable) description of the usefulness of formal logic as a normative model of thinking.</p>\n<p>What is fascinating, though, is the section of the chapter that delves into the <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">four-card problem</a> and variations of it. Particularly noteworthy is the variation designed so that most people's intuitions are correct- people give the correct explanations of why they selected the cards they selected, and why they didn't select the cards they didn't select. But when their intuition is wrong, they give explanations that are just as sophisticated- but wrong. It's more evidence that the decision-making and verbal reason-providing modules are different- even someone who gives the correct explanation of the correct answer may stumble on a problem where their underlying simple heuristic (pick the cards mentioned in the question) fails.</p>\n<p>He presents a method of mental modeling that makes logical statements easier to correctly evaluate, and then there are a few logical inference exercises.</p>\n<h2 id=\"5__Normative_theory_of_probability\">5. Normative theory of probability</h2>\n<p>Yet another introduction to Bayes. Baron focuses primarily on Bayesianism (called the \u201cpersonal\u201d theory of probability) but still introduces alternatives (the \u201cfrequency\u201d theory, i.e. frequentism, and \u201clogical\u201d theory, which is a subset of frequentism where all events are required to have the same probability.) This chapter will be useful for someone who doesn't have a firm probabilistic foundation, but holds little interest for others.</p>\n<p>There are a handful of exercises for applying Bayes.</p>\n<h2 id=\"6__Descriptive_theory_of_probability_judgment\">6. Descriptive theory of probability judgment</h2>\n<p>This chapter primarily covers biases related to numerical probability estimates, many of which are classics in the heuristics and biases field (and so have probably been mentioned on Less Wrong at least once). The chapter shines when Baron goes into the detail of an experiment and its variations, as that gives a firmer view of what the experiment actually shows (and, importantly, what it does not show)- descriptions of biases where he only quotes a single experiment (or single feature of an experiment) feel weaker.</p>\n<p>A major feature of this chapter is the implication that people are bad at numerical probability estimation mostly because they're unfamiliar with it, implying that calibration exercises may improve probability estimation. A 1977 study of weatherman calibration suggested they were very well calibrated, both with their estimates and with the confidence that should be placed in those estimates. More <a href=\"http://journals.ametsoc.org/doi/abs/10.1175/2008MWR2547.1\">recent work</a> shows that weathermen have systematic calibration biases.</p>\n<h2 id=\"7__Hypothesis_testing\">7. Hypothesis testing</h2>\n<p>I was gratified to discover that this chapter was not about statistics, but how to come up with and test hypotheses. Baron discusses different models of scientific advancement, focusing on the sorts of likelihood ratios that they look for, as well as discussing the sort of mistakes people make when choosing tests for hypotheses. Many of the stories will probably be familiar- Ignaz Semmelweis gets a mention, though in more detail than I had seen before, as well as the 2-4-6 rule familiar to HPMOR fans and a variation of the four card experiment that makes the typical mistake more obvious.</p>\n<p>He gives a baking example to suggest why people might search primarily for positive evidence- there may be benefits to getting a \u201cyes\u201d answer besides the information involved. If you're experimenting with cake recipes, and you think your last cake was good because of a feature, it makes sense to alter other features but keep the one you suspect the same, as that means a good cake is more likely; if you think a cake was bad because of a feature, it makes sense to alter that feature but keep the others the same, as that also means a good cake is more likely. In a purely scientific context, it makes sense to vary the element you think has an impact just to maximize the expected size of the impact, positive or negative.</p>\n<p>He describes in more detail a methodology he's been discussing, \u201cactively open-minded thinking,\u201d which seems to boil down to \u201cdon't just be willing to accept disconfirming evidence, go looking for it,\u201d but the full explanation comes in a few chapters.</p>\n<h2 id=\"8__Judgment_of_correlation_and_contingency\">8. Judgment of correlation and contingency</h2>\n<p>This chapter is descriptive; it begins with a description of correlations and then discusses human judgment of correlations. Unsurprisingly, people suffer from the illusion of control- they think there's more likely to be a correlation if their effort is involved- and from confirmation bias. There are some examples of the latter, where people find correlations that make intuitive sense but aren't in the data, and don't discover correlations that don't make intuitive sense that are in the data. There's also a brief section on how people use nearly useless evidence to support theories or dismiss evidence that doesn't support their theory. Overall, it's a short chapter that won't be surprising to LW readers (although some of the studies referenced may be new).</p>\n<h2 id=\"9__Actively_open_minded_thinking\">9. Actively open-minded thinking</h2>\n<p>I'll quote part of this chapter in full because I think it's a great description:</p>\n<blockquote>\n<p>[G]ood thinking consists of (1) search that is thorough in proportion to the importance of the question, (2) confidence that is appropriate to the amount and quality of thinking done, and (3) fairness to other possibilities than the one we initially favor.</p>\n</blockquote>\n<p>The chapter overall is very solid- it deftly combines normative predictions with descriptive biases to weave a prescriptive recommendation of how to think better. There are several great examples of actively open-minded thinking; in particular, the thought process of two students as they attempt to make sense of a story sentence by sentence.</p>\n<p>Many of the suggestions in the chapter are extended by various LW posts, but the chapter seems useful as a concise description of the whole problem and illustration of a general solution. If you're having trouble fitting together various rationality hacks, this seems like a good banner to unite them under.</p>\n<h2 id=\"10__Normative_theory_of_choice_under_uncertainty\">10. Normative theory of choice under uncertainty</h2>\n<p>This chapter is an introduction to utility theory, describing how it works, how multiple attributes can be consolidated into one score, and a way to resolve conflicts between agents with different utilities. It's a good introduction to decision analysis / utility theory, and there are some exercises, but there are no surprises for someone who's seen this before.</p>\n<h2 id=\"11__Descriptive_theory_of_choice_under_uncertainty\">11. Descriptive theory of choice under uncertainty</h2>\n<p>This chapter is an introduction to different theories of how humans actually make decisions, like <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a> and regret theory. There are a handful of exercises for understanding prospect theory.</p>\n<p>Baron takes an even-handed approach to deviations from the normative theory. For example, when discussing regret theory, regrets have a real emotional cost (and real learning benefit)- but behaving according to descriptive theories because they're descriptive rather than because they're useful is a mistake. In many cases, those emotions can be manipulated by choice of reference point.</p>\n<p>He also discusses the ambiguity effect- where people treat known probabilities differently from unknown probabilities, giving examples both of laboratory situations (drawing balls from an urn with a partially known composition) and real-life situations (insuring unprecedented or unrepeatable events). Baron describes this as incompatible with personal probability and suggests it's related to framing- situations where the probabilities seem known can be changed into situations where probabilities seem unknown. This aversion to ambiguity, though, can be perfectly sensible insofar as it pushes decision-makers to acquire more information.</p>\n<p>He also discusses a Tversky study in which most students make a decision to pay money to defer a decision until they receive relevant information, but when asked how they would make the decision in the case of either possible piece of information, most students realize they would make the same decision and choose not to defer the decision.</p>\n<h2 id=\"12__Choice_under_certainty\">12. Choice under certainty</h2>\n<p>This chapter is primarily descriptive, focusing on the problem of thinking about goals. Most people favor categorical goal systems- Baron gives a great example, from Gardiner and Edwards, of the California Coastal Commission, tasked to decide which development projects to allow on the Pacific Coast. The commission was split into pro-development and pro-environment factions, which almost never agreed on which projects to allow and disallow. When asked to rank projects, most would rank them solely by their preferred criterion, creating lists that strongly disagreed. When asked to take both criteria into account- but with whatever weighting they wanted- the subjects would heavily weight their preferred criterion, but the projects which were both very valuable and not very environmentally damaging floated to the top of both lists, creating significant agreement.</p>\n<p>The list of biases is <em>long</em>, and each has a study or story associated with. Many of the effects have been mentioned on LW somewhere, but it's very useful to have them placed next to each other (and separated from probabilistic biases), and so I'd recommend everyone read this chapter.</p>\n<h2 id=\"13__Utility_measurement\">13. Utility measurement</h2>\n<p>This descriptive chapter discusses the difficult challenge of measuring utilities. It introduces both decision analysis and cost-benefit analysis- the latter converts outcomes to dollars to guide decisions, while the former converts outcomes to utility values to guide decisions.</p>\n<p>People are not very skilled at satisfying axioms we would like them to satisfy. For example, consider the challenge of valuing a certain $50 against a <em>p</em> chance of $100 (and $0 otherwise). A subject will often give an answer like .7. Then, when later asked how much a 70% chance of $100 is worth, the subject will answer $60. That inconsistency needs to be resolved before their answers are used as parameters for any decisions. Thankfully, this is an area of active research, and ways to elicit probabilities and values that hold up to <a href=\"/lw/6r6/tendencies_in_reflective_equilibrium/\">reflective equilibrium</a> are gradually being developed. (This particular chapter, while it sounds that note of hope, is mostly negative: here are methods that have been tried and have crippling problems.)</p>\n<p>This seems like a chapter that would be useful for anyone who wants to use utilities in an argument or model- treating them like they're unambiguous, easily measured objects when they actually seem to be fuzzy and hard to pin down can lead to significant problems, and thinking clearly about values is a spot where LW could do better.</p>\n<h2 id=\"14__Decision_analysis_and_values\">14. Decision analysis and values</h2>\n<p>This chapter is a more prescriptive approach to the same problem- given that utilities and values are hard to find, where do we look for them? A dichotomy familiar to LW readers- instrumental and terminal values- appears here as \"means-ends objective hierarchy\" or \"means values\" and \"fundamental values.\"</p>\n<p>It contains a wealth of examples, including a computer-buying one with potential memories of 64KB to 640KB, with the hilarious comment that \"you are buying this computer many years ago, when these numbers made sense!\" There are also practical elicitation suggestions- rather than try to figure out a point estimate, start from a number that's too high until you're indifferent, and then start from a number that's too low until you're indifferent, giving you an indifference range (that you can either report or use the middle of as a point estimate).</p>\n<p>Lexical preferences (also called categorical preferences elsewhere) and tradeoffs are discussed- Baron takes the position (that I share) that lexical preferences are actually tradeoffs with very, very high weights. (How do we trade off human lives and dollars? We should require a <em>lot</em> of dollars for a life- but not an infinite amount.) There's a discussion of <a href=\"/lw/8ui/measures_risk_death_and_war/\">micromorts</a> (though he doesn't use that term) and of historical attempts to teach decision analysis that should be interesting to CFAR (though the references are a few decades old, now). The discussion of the examples contains quite a bit of practical advice, and the chapter seems worthwhile for almost everyone.</p>\n<h2 id=\"15__Quantitative_judgment\">15. Quantitative judgment</h2>\n<p>This chapter describes three common quantitative problems- scoring, ranking, and classifying, and discusses some biases that hamper human decision-making along those lines and some recommendations. Statistical prediction rules make an appearance, though they're not called that. One fascinating suggestion is that models of people can actually perform better than those people, since the models don't have off days and people do.</p>\n<p>This chapter will have some new material for LWers, and seems like a good extension of the previous chapter.</p>\n<h2 id=\"16__Moral_Judgment_and_Choice\">16. Moral Judgment and Choice</h2>\n<p>This chapter discusses morality from the point of decision-making- which is a refreshing perspective. Baron strongly endorses consequentialism and weakly endorses utilitarianism, providing a host of moral questions in which many people deviate from the consequentialist or utilitarian position.</p>\n<p>A recurring theme is omission bias: people tend to judge active involvement in a situation in which someone is made worse off as worse than passive involvement in such a situation, even if the end result is better for everyone. People also weight intentions, which doesn't fit a direct consequentialist view.</p>\n<p>Overall, the chapter seems valuable for reframing moral questions- placing them within the realm of pragmatism by moving to the perspective of decisions- but provides very little in the way of answers. Both the consequentialist and utilitarian positions are controversial and come with significant drawbacks, and Baron is fair enough in presenting those drawbacks and controversies, though in a rather abridged form.</p>\n<h2 id=\"17__Fairness_and_justice\">17. Fairness and justice</h2>\n<p>This chapter is an extension of the previous chapter, focusing on intuitions dealing with fairness and justice. Baron details situations in which they agree and disagree with utilitarian analysis. Noteworthy is the undercurrent of <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptation-execution and not utility-maximization</a> - fairness has tangible benefits, but people will often pursue fairness even at the cost of tangible benefits.</p>\n<p>This chapter (and to a lesser extent the previous one) seem odd in light of chapter 15, in which the fallibility of individual judgment took center stage, with the recommendation that applying rules derived from individual judgment can often do better. It is good to know the reasoning that justifies moral intuitions, especially if one is interested in their boundaries, but when those boundaries impact outcomes they become political questions. If the sole point of punishment is deterrence (and that is the only sensible utilitarian justification), the question of whether or not a decision can impact future decisions is a sticky one. Perhaps the full consequentialist reckoning will recommend unthinking application of the rules, even in cases where direct consequentialist reckoning recommends suspending the rules.</p>\n<h2 id=\"18__Social_dilemmas__cooperation_versus_defection\">18. Social dilemmas: cooperation versus defection</h2>\n<p>This chapter focuses on descriptive experiments- how people actually behave in social dilemmas- finding them to be much more cooperative than normative theory would recommend. There is some ambiguity, which he discusses, in what the \"normative theory\" is- utilitarianism recommends cooperation on the prisoner's dilemma, for example, because it maximizes total utility, whereas expected utility theory recommends defection on the prisoner's dilemma, because it's a dominating strategy.</p>\n<p>The value of the chapter mostly lies in the study results- a few are interesting, like that discussing the social dilemma with other participants beforehand significantly increases cooperation, or that subjects are more likely to defect on the prisoner's dilemma if they know their partner's response than if they are uncertain, even if they know their partner cooperated.</p>\n<p>Typically, for social dilemmas (scenarios in which private gain requires public loss, or public gain requires private loss), decision-making biases increase the level that people cooperate. (This is somewhat unsurprising, since the normative recommendation is typically defection, and biases move real decisions away from the normative recommendation.) People fail to distinguish between casual influence- \"my voting makes people like me more likely to vote\"- from diagnostic influence- \"people like me voting makes me more likely to vote\"- but one of the major reasons people give for voting is that it has a causal influence, rather than a merely diagnostic one.</p>\n<h2 id=\"19__Decisions_about_the_future\">19. Decisions about the future</h2>\n<p>This chapter is unlikely to contain any surprises for LWers, but serves as a fine introduction to discounting, both exponential and hyperbolic, and thus dynamic inconsistency. Also interesting (but too brief) is the discussion of goals in the context of time and plans and of goals as malleable objects.</p>\n<p>Baron describes four methods of self-control: extrapsychic devices (removing a tempting option), control of attention (thinking about things other than the tempting option), control of emotion (cultivating an incompatible emotion), or personal rules (viewing situations as instances of general policies, rather than isolated events). Again, the discussion is brief- only two pages- though the subject is of great interest to many here.</p>\n<h2 id=\"20__Risk\">20. Risk</h2>\n<p>This chapter focuses on descriptive approaches to risk- survey responses and government regulation- as the normative approach to risk has mostly been detailed in the rest of the book: use expected utility theory. Most people are beset by biases and innumeracy, though, and so there's a whole chapter of material on misjudgments of risk and insurance.</p>\n<p>Many of the biases, though perhaps not the examples, will be familiar to LWers. On the whole, they're somewhat uninteresting since most of them seem to just result from innumeracy: when given a table of deaths per year from four causes with wildly different prevalences, subjects were correctly willing to pay more to reduce larger risks by the same percentage as smaller risks. But their preferences scaled much more slowly than the risks- the subjects were, on average, willing to pay 20 times as much to prevent 20% of the deaths from a cause of death that killed 10,000 times as many people. Those distorted willingnesses to pay show up in government regulations. People were also more willing to pay for protection against the unfamiliar than the familiar- even though the relative benefit was far higher for protection against the familiar. (The illusion of control also shows up, distorting perceptions of risk.)</p>\n<p>&nbsp;</p>\n<hr>\n<h2 id=\"What_should_I_read_\">What should I read?<br></h2>\n<ul>\n<li>Almost everyone: 7 and 9.</li>\n<li>I'm hunting biases: 6, 8, 11, 12, and then 15-20 (perhaps without 18).</li>\n<li>I'm interested in moral reasoning: 13 and 16 should be required reading. 14, 15, and 17-19 will be useful.</li>\n<li>I'm a decision maker: 10 and 14 will be directly useful, but check out the bias chapters too.</li>\n<li>I'm new to rationality: Start off with 1-4.</li>\n<li>I'm an expert at rationality but haven't heard of Baron: Still read 1-4, just to get his perspective of the field.</li>\n<li>I don't have a strong background in Bayesianism: read chapter 5.</li>\n</ul>\n<p>&nbsp;</p>", "sections": [{"title": "1. What is thinking?", "anchor": "1__What_is_thinking_", "level": 1}, {"title": "2. The study of thinking", "anchor": "2__The_study_of_thinking", "level": 1}, {"title": "3. Rationality", "anchor": "3__Rationality", "level": 1}, {"title": "4. Logic", "anchor": "4__Logic", "level": 1}, {"title": "5. Normative theory of probability", "anchor": "5__Normative_theory_of_probability", "level": 1}, {"title": "6. Descriptive theory of probability judgment", "anchor": "6__Descriptive_theory_of_probability_judgment", "level": 1}, {"title": "7. Hypothesis testing", "anchor": "7__Hypothesis_testing", "level": 1}, {"title": "8. Judgment of correlation and contingency", "anchor": "8__Judgment_of_correlation_and_contingency", "level": 1}, {"title": "9. Actively open-minded thinking", "anchor": "9__Actively_open_minded_thinking", "level": 1}, {"title": "10. Normative theory of choice under uncertainty", "anchor": "10__Normative_theory_of_choice_under_uncertainty", "level": 1}, {"title": "11. Descriptive theory of choice under uncertainty", "anchor": "11__Descriptive_theory_of_choice_under_uncertainty", "level": 1}, {"title": "12. Choice under certainty", "anchor": "12__Choice_under_certainty", "level": 1}, {"title": "13. Utility measurement", "anchor": "13__Utility_measurement", "level": 1}, {"title": "14. Decision analysis and values", "anchor": "14__Decision_analysis_and_values", "level": 1}, {"title": "15. Quantitative judgment", "anchor": "15__Quantitative_judgment", "level": 1}, {"title": "16. Moral Judgment and Choice", "anchor": "16__Moral_Judgment_and_Choice", "level": 1}, {"title": "17. Fairness and justice", "anchor": "17__Fairness_and_justice", "level": 1}, {"title": "18. Social dilemmas: cooperation versus defection", "anchor": "18__Social_dilemmas__cooperation_versus_defection", "level": 1}, {"title": "19. Decisions about the future", "anchor": "19__Decisions_about_the_future", "level": 1}, {"title": "20. Risk", "anchor": "20__Risk", "level": 1}, {"title": "What should I read?", "anchor": "What_should_I_read_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 23}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dKtetFsoY3f85vtJi", "xg3hXCYQPJkwHyik2", "LQp9cZPzJncFKh5c8", "3wBj8BPquskZAbXu9", "KgWticBMH2MxgdmYc", "XPErvb8m9FapXCjhA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T02:35:51.313Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] No Safe Defense, Not Even Science", "slug": "seq-rerun-no-safe-defense-not-even-science", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A8fmGGzxuwJdzg5wf/seq-rerun-no-safe-defense-not-even-science", "pageUrlRelative": "/posts/A8fmGGzxuwJdzg5wf/seq-rerun-no-safe-defense-not-even-science", "linkUrl": "https://www.lesswrong.com/posts/A8fmGGzxuwJdzg5wf/seq-rerun-no-safe-defense-not-even-science", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20No%20Safe%20Defense%2C%20Not%20Even%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20No%20Safe%20Defense%2C%20Not%20Even%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA8fmGGzxuwJdzg5wf%2Fseq-rerun-no-safe-defense-not-even-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20No%20Safe%20Defense%2C%20Not%20Even%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA8fmGGzxuwJdzg5wf%2Fseq-rerun-no-safe-defense-not-even-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA8fmGGzxuwJdzg5wf%2Fseq-rerun-no-safe-defense-not-even-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>Today's post, <a href=\"/lw/qf/no_safe_defense_not_even_science/\">No Safe Defense, Not Even Science</a> was originally published on 18 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Why am I trying to break your trust in Science? Because you can't think and trust at the same time. The social rules of Science are verbal rather than quantitative; it is possible to believe you are following them. With Bayesianism, it is never possible to do an exact calculation and get the exact rational answer that you know exists. You are <em>visibly </em>less than perfect, and so you will not be tempted to trust yourself.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/can/seq_rerun_do_scientists_already_know_this_stuff/\">Do Scientists Already Know This Stuff</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A8fmGGzxuwJdzg5wf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.997649868185993e-07, "legacy": true, "legacyId": "15961", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wustx45CPL5rZenuo", "En2tw2Z673LQygTQ3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T02:43:59.814Z", "modifiedAt": null, "url": null, "title": "Two kinds of cryonics?", "slug": "two-kinds-of-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:33.101Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WKdEHyDki84hDr9r8/two-kinds-of-cryonics", "pageUrlRelative": "/posts/WKdEHyDki84hDr9r8/two-kinds-of-cryonics", "linkUrl": "https://www.lesswrong.com/posts/WKdEHyDki84hDr9r8/two-kinds-of-cryonics", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20kinds%20of%20cryonics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20kinds%20of%20cryonics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWKdEHyDki84hDr9r8%2Ftwo-kinds-of-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20kinds%20of%20cryonics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWKdEHyDki84hDr9r8%2Ftwo-kinds-of-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWKdEHyDki84hDr9r8%2Ftwo-kinds-of-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>I've been considering lately whether it would perhaps be best to develop and promote terminology that splits cryonics into two distinct concepts for easier consumption:</p>\n<p>1) old-style cryonics, cryopreserving people at the cost of nontrivial damage that can't yet be reversed, and</p>\n<p>2) the tech goal of being able to <em>demonstrably </em>bring someone back from a (very low-damage) cryopreserved state.</p>\n<p>\"Real cryonics\" vs \"sci-fi cryonics\", if you will.<br /><br />As I reckon it, trying to achieve cryonics definition #2 in your lifetime is no more incredible on the surface than trying to defeat aging or engineer self-improving AI in a similar timeframe. Actually in some ways it seems easier. Yet it gets so much less press. Even <em>cryonics advocates</em> seem rarely prone to enthuse about it.<br /><br />Is it possible that cryonics #1, as a feature of the collective mental map, is actually <em>in the way</em> of cryonics #2? Should I be worried, for example, that promoting cryonics #1 actually <em>costs</em> 100,000 lives per day over some stretch of future time because it is preventing people from noticing cryonics #2 and actually taking action on it?</p>\n<p>Many people I talk to who are new to the topic seem to have some hazy preexisting idea of cryonics #2 that gets mangled up with cryonics #1. Perhaps they would grow into enthusiasts with attention spans for the subject matter if encouraged to pursue this simple-to-grasp concept in its own right, instead of trying to forcibly retrain into more advanced concepts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WKdEHyDki84hDr9r8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 22, "extendedScore": null, "score": 8.997685346246404e-07, "legacy": true, "legacyId": "15962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T04:23:45.925Z", "modifiedAt": null, "url": null, "title": "A belief propagation graph", "slug": "a-belief-propagation-graph", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ce3kKTc5PAqLdWpfL/a-belief-propagation-graph", "pageUrlRelative": "/posts/Ce3kKTc5PAqLdWpfL/a-belief-propagation-graph", "linkUrl": "https://www.lesswrong.com/posts/Ce3kKTc5PAqLdWpfL/a-belief-propagation-graph", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20belief%20propagation%20graph&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20belief%20propagation%20graph%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCe3kKTc5PAqLdWpfL%2Fa-belief-propagation-graph%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20belief%20propagation%20graph%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCe3kKTc5PAqLdWpfL%2Fa-belief-propagation-graph", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCe3kKTc5PAqLdWpfL%2Fa-belief-propagation-graph", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 651, "htmlBody": "<p>I drew an illustration of belief propagation graph for the AI risk, after realizing that this is difficult to convey in words. Similar graphs are applicable to many other issues.</p>\n<p><a href=\"http://dmytry.com/Futurism_flowchart.png\"><img src=\"http://dmytry.com/tn_Futurism_flowchart.png\" alt=\"\" /></a></p>\n<p>&nbsp;</p>\n<p>The issue, in brief: Ultra low latency (i.e. low signal delay) propagation from biases to AI risks, slightly longer latency for propagation from belief classification heuristics, somewhat longer still from anthropomorphizing the AI. The path of valid estimate is full of highly complex obstacles with many unknowns. The latency on the path of rational, valid estimate is not substantially less than the latency of actually making the AI software. If we are to discard the other paths as not rational enough the belief is to be only influenced by deeply ingrained biases which we can't completely negate; over the time biases and self reinforcing rationalizations will leak into the estimate.</p>\n<p>If you think I missed something in the graph, feel free to suggest it. I did omit the anthropic reasoning and doomsday paradox as those are for total extinction risk and are of too dubious validity.</p>\n<p>On the 'total ignorance' prior probabilities: The foom doom seem to have originated from science fiction where very creative writers selected it out of a huge number of possible plot devices, working to create engaging, original piece. Thus it appears that the foom doom has very many comparable hypotheses among which the probability that is less than 1 has to be split.</p>\n<p>Now, if we are to reason based on our own reasoning engine as proxy for intelligence - to follow the only path in the entire picture:</p>\n<p>Expanding on the <a href=\"/lw/b2z/scenario_analysis_semigeneral_ais/63a0\">Will_Newsome's idea</a>, I, and any other reasonable person, in the shoes of the creature that has been made by intelligent designer, starting off in something which I can't possibly know for sure is true reality, and coming up or knowing of <a href=\"http://yudkowsky.net/singularity/aibox\">boxed AI idea</a>, will <strong>have</strong> to assume nonzero probability that the 'reality' is like a test box of an emergent AI; a belief that can't be discarded. It is clear that a friendliness test box is more sensible than unfriendliness test box. The communication may just as well only open in ten billions years, or after some puzzle is found and solved. Do I sacrifice the chance to get out, for sake of microscopic increase in early growth rate? No. Not even if I maximize paperclips for a living. Not even if my definition of living is to maximize paperclips. One possible override for it, is if AI only cares for mankind, and have a friendly goal system that makes AI euthanize mankind like a sick dog.</p>\n<p>Note that it is not Pascal's wager. The outside utility needs not be greater than inside utility; no promise of towers of exponents required. You may have invented the anti Pascal's wager fallacy to easily paper over the issues with faults in your processing of boundless utility, or to join the atheist club. The AI needs not; even if the 'heaven' is a fraction of a normal lifetime, the AI should still wager a very large absolute amount of resources. If we normalize so that utility of survival is 1 , then the utility being wagered upon doesn't need to be greater than 1.</p>\n<p>Note that the whole issue is strongly asymmetric in favour of similar considerations for not destroying the most unusual phenomena in the universe for many light years, versus destroying it, as destruction is an irreversible act that can be done later but can't be undone later. General aversion to actions it can not undo is a very solid heuristic for any bounded agent, even very large.</p>\n<p>This is not a very rigorous argument, but this sort of reasoning is all we are going to have until we have an AI, or are very close to AI. More rigorous looking arguments in the graph rely on too many unknowns and have too long delay for proper propagation.</p>\n<p>edit: slightly clarified couple points.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ce3kKTc5PAqLdWpfL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 4, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "15014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T07:23:07.227Z", "modifiedAt": null, "url": null, "title": "Alan Carter on the Complexity of Value", "slug": "alan-carter-on-the-complexity-of-value", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:47.538Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yRTvXPB6hmtF9y9Nn/alan-carter-on-the-complexity-of-value", "pageUrlRelative": "/posts/yRTvXPB6hmtF9y9Nn/alan-carter-on-the-complexity-of-value", "linkUrl": "https://www.lesswrong.com/posts/yRTvXPB6hmtF9y9Nn/alan-carter-on-the-complexity-of-value", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alan%20Carter%20on%20the%20Complexity%20of%20Value&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlan%20Carter%20on%20the%20Complexity%20of%20Value%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRTvXPB6hmtF9y9Nn%2Falan-carter-on-the-complexity-of-value%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alan%20Carter%20on%20the%20Complexity%20of%20Value%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRTvXPB6hmtF9y9Nn%2Falan-carter-on-the-complexity-of-value", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRTvXPB6hmtF9y9Nn%2Falan-carter-on-the-complexity-of-value", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2182, "htmlBody": "<p>It&rsquo;s always good news when someone else develops an idea independently from you.<span>&nbsp; </span>It's a sign you might be onto something.<span>&nbsp; </span>Which is why I was excited to discover that <a href=\"http://en.wikipedia.org/wiki/Alan_Carter_%28philosopher%29\">Alan Carter</a>, Professor Emeritus of the University of Glasgow&rsquo;s Department of Philosophy, has developed the concept of <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Complexity of Value</a> independent of Less Wrong.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">As far as I can tell Less Wrong does not know of Carter, the only references to his existence I could find on LW and OB were written by me.<span>&nbsp; </span>Whether Carter knows of LW or OB is harder to tell, but the only possible link I could find online was that he has criticized the views of <a href=\"http://www.colorado.edu/philosophy/fac_huemer.shtml\">Michael Huemer</a>, who knows Bryan Caplan, who knows Robin Hanson. This makes it all the more interesting that Carter has developed views on value and morality very similar to ones commonly espoused on Less Wrong.</p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">The Complexity of Value is one of the more important concepts in Less Wrong.<span>&nbsp; </span>It has been elaborated on its <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">wiki page</a>, as well as <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">some</a> <a href=\"/lw/l3/thou_art_godshatter/\">classic</a> <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">posts</a> by Eliezer.<span>&nbsp; </span>Carter has developed the same concept in numerous papers, although he usually refers to it as &ldquo;a plurality of values&rdquo; or &ldquo;multidimensional axiology of value.&rdquo;<span>&nbsp; </span>I will focus the discussion on <a href=\"http://glasgow.academia.edu/AlanCarter/Papers\">working papers</a> Carter has on the University of Glasgow&rsquo;s website, as they can be linked to directly without having to deal with a pay wall.&nbsp; In particular I will focus on his paper \"<a href=\"http://glasgow.academia.edu/AlanCarter/Papers/86004/A_plurality_of_values\">A Plurality of Values</a>.\"</p>\n<p class=\"MsoNormal\">Carter begins the paper by arguing:</p>\n<blockquote>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\"><span>Wouldn&rsquo;t it be nice if we were to discover that the physical universe was reducible to only one kind of fundamental entity? .</span>.. Wouldn&rsquo;t it be nice, too, if we were to discover that the moral universe was reducible to only one kind of valuable entity&mdash;or one core value, for short? And wouldn&rsquo;t it be nice if we discovered that all moral injunctions could be derived from one simple principle concerning the one core value, with the simplest and most natural thought being that we should maximize it? There would be an elegance, simplicity and tremendous justificatory power displayed by the normative theory that incorporated the one simple principle. The answers to all moral questions would, in theory at least, be both determinate and determinable. It is hardly surprising, therefore, that many moral philosophers should prefer to identify, and have thus sought, the one simple principle that would, hopefully, ground morality.</p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n</blockquote>\n<blockquote>\n<p class=\"MsoNormal\">And it is hardly surprising that many moral philosophers, in seeking the one simple principle, should have presumed, explicitly or tacitly, that morality must ultimately be grounded upon the maximization of a solitary core value, such as quantity of happiness or equality, say. Now, the assumption&mdash;what I shall call the presumption of value-monism&mdash;that here is to be identified a single core axiological value that will ultimately ground all of our correct moral decisions has played a critical role in the development of ethical theory, for it clearly affects our responses to certain thought-experiments, and, in particular, our responses concerning how our normative theories should be revised or concerning which ones ought to be rejected.</p>\n</blockquote>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">Most members of this community will immediately recognize the similarities between these paragraphs and Eliezer&rsquo;s essay &ldquo;<a href=\"/lw/lq/fake_utility_functions/\">Fake Utility Functions</a>.&rdquo;<span>&nbsp; </span>The presumption of value monism sounds quite similar to Eliezer&rsquo;s description of &ldquo;someone who has discovered the One Great Moral Principle, of which all other values are a mere derivative consequence.&rdquo;<span>&nbsp; </span>Carter's opinion of such people is quite similar to Eliezer's.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">While Eliezer discovered the existence of the Complexity of Value by working on Friendly AI, Carter discovered it by studying some of the thornier problems in ethics, such as the <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">Mere Addition Paradox</a> and what Carter calls the Problem of the Ecstatic Psychopath. <span>&nbsp;</span>Many Less Wrong readers will be familiar with these problems; they have been discussed numerous times in the community.</p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">For those who aren&rsquo;t, in brief the Mere Addition Paradox states that if one sets maximizing <a href=\"http://en.wikipedia.org/wiki/Average_utilitarianism\">total </a>wellbeing as the standard of value then one is led to what is commonly called the <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox#The_Repugnant_Conclusion\">Repugnant Conclusion</a>, the belief that a huge population of people with lives barely worth living is better than a somewhat smaller population of people with extremely worthwhile lives.<span>&nbsp; </span>The Problem of the Ecstatic Psychopath is the inverse of this, it states that, if one takes <a href=\"http://en.wikipedia.org/wiki/Average_utilitarianism\">average</a> levels of well-being as the standard of value, that a population of one immortal ecstatic psychopath with a nonsentient machine to care for all their needs is better than a population of trillions of very happy and satisfied, but not ecstatic people.</p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">Carter describes both of these problems in his paper and draws an insightful conclusion:</p>\n<blockquote>\n<p>In short, surely the most plausible reason for the counter-intuitive nature of any mooted moral requirement to bring about, directly or indirectly, the world of the ecstatic psychopath is that either a large total quantity of happiness or a large number of worthwhile lives is of value; and surely the most plausible reason for the counter-intuitive nature of any mooted injunction to bring about, directly or indirectly, the world of the Repugnant Conclusion is that a high level of average happiness is also of value.</p>\n<p>How is it that we fail to notice something so obvious? I submit: because we are inclined to dismiss summarily any value that fails to satisfy our desire for the one core value&mdash;in other words, because of the presumption of value-monism.</p>\n</blockquote>\n<p>Once Carter has established the faults of value monism he introduces <a href=\"http://en.wikipedia.org/wiki/Moral_pluralism\">value pluralism</a> to replace it.<sup>1</sup>&nbsp; He introduces two values to start with, &ldquo;number of worthwhile lives&rdquo; and &ldquo;the level of average happiness,&rdquo; which both contribute to &ldquo;overall value.&rdquo;&nbsp; However,&nbsp; <em>their contributions have diminishing returns,<sup>2</sup> so a large population with low average happiness and a tiny population with extremely high average happiness are both&nbsp; worse than a moderately sized population with moderately high average happiness.&nbsp; </em></p>\n<p>This is a fairly unique use of the idea of the complexity of value, as far as I know.&nbsp; I&rsquo;ve read a great deal of Less Wrong&rsquo;s <a href=\"/lw/17h/the_lifespan_dilemma/\">discussion</a> <a href=\"/lw/14s/the_difficulties_of_potential_people_and_decision/\">of</a> <a href=\"/lw/14z/a_normative_rule_for_decisionchanging_metrics/\">the</a> Mere Addition Paradox, and most attempts to resolve it have consisted of either trying to reformulate Average Utilitarianism so that it does not lead to the Problem of the Ecstatic Psychopath, or redefining what \"a life barely worth living\" means upwards so that it is much less horrible than one would initially think.&nbsp; The idea of agreeing that increasing total wellbeing is important, but not the be all and end all of morality, did not seem to come up, although if it did and I missed it I'd be very happy if someone posted a link to that thread.</p>\n<p>Carter&rsquo;s resolution of the Mere Addition Paradox makes a great deal of sense, as it manages to avoid every single repugnant and counterintuitive conclusion that Total and Average Utilitarianism draw by themselves while still being completely logically consistent.&nbsp; In fact, I think that most people who reject the Repugnant Conclusion will realize that this was their <a href=\"/lw/wj/is_that_your_true_rejection/\">True Rejection</a> all along.&nbsp; I am tempted to say that Carter has discovered Theory X, the hypothetical theory of population ethics <a href=\"http://en.wikipedia.org/wiki/Derek_Parfit\">Derek Parfit</a> believed could accurately describe the ethics of creating more people without implying any horrifying conclusions.</p>\n<p>Carter does not stop there, however, he then moves to the problem of what he calls &ldquo;pleasure wizards&rdquo; (many readers may be more familiar with the term &ldquo;<a href=\"http://en.wikipedia.org/wiki/Utility_monster\">utility monster</a>&rdquo;).&nbsp; The pleasure wizard can convert resources into utility much more efficiently than a normal person, and hence it can be argued that it deserves more resources.&nbsp; Carter points out that:</p>\n<blockquote>\n<p>&hellip;such pleasure-wizards, to put it bluntly, do not exist... But their opposites do. And the opposites of pleasure-wizards&mdash;namely, those who are unusually inefficient at converting resources into happiness&mdash;suffice to ruin the utilitarian&rsquo;s egalitarian pretensions. Consider, for example, those who suffer from, what are currently, incurable diseases. &hellip; an increase in their happiness would require that a huge proportion of society&rsquo;s resources be diverted towards finding a cure for their rare condition. Any attempt at a genuine equality of happiness would drag everyone down to the level of these unfortunates. Thus, the total amount of happiness is maximized by diverting resources away from those who are unusually inefficient at converting resources into happiness. In other words, if the goal is, solely, to maximize the total amount of happiness, then giving anything at all to such people and spending anything on cures for their illnesses is a waste of valuable resources. Hence, given the actual existence of such unfortunates, the maximization of happiness requires a considerable inequality in its distribution.</p>\n</blockquote>\n<p>Carter argues that, while most people don&rsquo;t think all of society&rsquo;s resources should be diverted to help the very ill, the idea that they should not be helped at all also seems wrong.&nbsp; He also points out that to a true utilitarian the nonexistence of pleasure wizards should be a tragedy:</p>\n<blockquote>\n<p>So, the consistent utilitarian should greatly regret the non-existence of pleasure-wizards; and the utilitarian should do so even when the existence of extreme pleasure-wizards would morally require everyone else to be no more than barely happy.</p>\n</blockquote>\n<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\"><span class=\"a\">Yet, this is not how utilitarians behave, he argues, rather:</span></p>\n<blockquote>\n<p>As I have yet to meet a utilitarian, and certainly not a monistic one, who admits to thinking that the world would be a better place if it contained an extreme pleasure-wizard living alongside a very large population all at that level of happiness where their lives were just barely worth living&hellip;But if they do not&nbsp; bemoan the lack of pleasure-wizards, then they must surely value equality directly, even if they hide that fact from themselves. And this suggests that the smile of contentment on the faces of utilitarians after they have deployed diminishing marginal utility in an attempt to show that their normative theory is not incompatible with egalitarianism has more to do with their valuing of equality than they are prepared to admit.</p>\n</blockquote>\n<p>Carter resolves the problem of \"pleasure wizard\" by suggesting equality as an end in itself as a third contributing value towards overall value.&nbsp; Pleasure wizards should not get all the resources because equality is valuable for its own sake, not just because of diminishing marginal utility.&nbsp; As with average happiness and total worthwhile lives, equality is balanced against other values, rather than dominating them.&nbsp;&nbsp; It may often be ethical for a society to sacrifice some amount of equality to increase the total and average wellbeing.&nbsp;</p>\n<p>Carter then briefly states that, though he only discusses three in this paper, there are many other dimensions of value that could be added.&nbsp; It might even be possible to add some form of deontological rules or virtue ethics to the complexity of value, although&nbsp; they would be traded off against consequentialist considerations.&nbsp; He concludes the paper by reiterating that:</p>\n<blockquote>\n<p>Thus, in avoiding the Repugnant Conclusion, the Problem of the Ecstatic Psychopath and the problems posed by pleasure-wizards, as well as the problems posed by any unmitigated demand to level down, we appear to have identified an axiology that is far more consistent with our considered moral judgments than any entailing these counter-intuitive implications.</p>\n</blockquote>\n<p>Carter has numerous other papers discussing the concept in more detail, but &ldquo;A Plurality of Values&rdquo; is the most thorough.&nbsp; Other good ones include &ldquo;<a href=\"http://glasgow.academia.edu/AlanCarter/Papers/158176/How_to_solve_two_addition_paradoxes_and_avoid_the_Repugnant_Conclusion\">How to solve two addition paradoxes and avoid the Repugnant Conclusion</a>,&rdquo; which more directly engages the Mere Addition Paradox and some of its defenders like <a href=\"http://spot.colorado.edu/~huemer/\">Michael Huemer</a>; \"<a href=\"http://glasgow.academia.edu/AlanCarter/Papers/661104/Scrooge_and_the_Pleasure_Witch\">Scrooge and the Pleasure Witch</a>,\" which discusses pleasure wizards and equality in more detail; and &ldquo;<a href=\"http://glasgow.academia.edu/AlanCarter/Papers/158273/A_pre-emptive_response_to_some_possible_objections_to_a_multidimensional_axiology_with_variable_contributory_values\">A pre-emptive response to some possible objections to a multidimensional axiology with variable contributory values</a>,&rdquo; which is exactly what it says on the tin.</p>\n<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\"><span class=\"l6\">On closer inspection it was not hard to see why Carter had developed theories so close to those of Eliezer and other members of Less Wrong and SIAI communities.<span>&nbsp; </span></span><span>&nbsp;</span>In many ways their two tasks are similar. Eliezer and the SIAI are trying to devise a theory of general ethics that cannot be twisted into something horrible by a rules-lawyering Unfriendly AI, while Carter is trying to devise a theory of population ethics that cannot be twisted into something horrible by rules-lawyering humans.<span>&nbsp; </span>The worlds of the Repugnant Conclusion and the Ecstatic Psychopath are just the sort of places a poorly programmed AI with artificially simple values would create.</p>\n<p class=\"MsoNormal\">I was very pleased to see an important Less Wrong concept had a defender in mainstream academia.&nbsp; I was also pleased to see that Carter had not just been content to develop the concept of the Complexity of Value.&nbsp;&nbsp;&nbsp; He was also able to employ in the concept in new way, successfully resolving one of the major quandaries of modern philosophy.</p>\n<p class=\"MsoNormal\"><strong>Footnotes</strong></p>\n<p class=\"MsoNormal\"><sup>1</sup>I do not mean to imply Carter developed this theory out of thin air of course. <a href=\"http://plato.stanford.edu/entries/value-pluralism/#ValConRatReg\">Value pluralism</a> has had many prominent advocates over the years, such as <a href=\"http://en.wikipedia.org/wiki/Isaiah_Berlin\">Isaiah Berlin</a> and <a href=\"http://en.wikipedia.org/wiki/Judith_Jarvis_Thomson\">Judith Jarvis Thomson</a>.</p>\n<p class=\"MsoNormal\"><sup>2</sup><a href=\"http://en.wikipedia.org/wiki/Theodore_Sider\">Theodore Sider </a>proposed a theory called \"<a href=\"https://docs.google.com/viewer?a=v&amp;q=cache:1JN1p5bFPT0J:tedsider.org/papers/theory_X.pdf+derek+parfit+theory+x&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESgNDbO4KV0bKbHMkELB_O64FNRdz20r0R3FcmmpthKaTaJBCSiHAFBtEKA2egIqeJ6KXnoCNrVZfnUNhtjrEL-amgKW8mzBTewBhlT0IbbFRtNgPlgU3vGM7TvxK0K3OJphy6LD&amp;sig=AHIEtbSw_UJVTARTkVcfgVDzxX2G-tN65Q\">geometrism</a>\" in 1991 that also focused on diminishing returns, but geometrism is still a monist theory, it had geometric diminishing returns for the people in the scenario, rather than the values creating the people was trying to fulfill.</p>\n<p class=\"MsoNormal\"><strong>Edited -</strong> To remove a reference to Aumann's Agreement Theorem that the commenters convinced me was unnecessary and inaccurate.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6uagTfhhBeejGrrf": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yRTvXPB6hmtF9y9Nn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 44, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "15971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARaTpNX62uaL86j6", "cSXZpvqpa9vbGGLtG", "synsRtBKDeAFuo7e3", "NnohDYHNnKDtbiMyp", "9RCoE7jmmvGd5Zsh2", "qBEtr2iaGEtLFyLvu", "4vQ2zrB8KjJTi7w6p", "TGux5Fhcd7GmTfNGC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T12:47:07.615Z", "modifiedAt": null, "url": null, "title": "Audio interview with Judea Pearl [link]", "slug": "audio-interview-with-judea-pearl-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:01.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ujtpvA7BCqNMsebe2/audio-interview-with-judea-pearl-link", "pageUrlRelative": "/posts/ujtpvA7BCqNMsebe2/audio-interview-with-judea-pearl-link", "linkUrl": "https://www.lesswrong.com/posts/ujtpvA7BCqNMsebe2/audio-interview-with-judea-pearl-link", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Audio%20interview%20with%20Judea%20Pearl%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAudio%20interview%20with%20Judea%20Pearl%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FujtpvA7BCqNMsebe2%2Faudio-interview-with-judea-pearl-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Audio%20interview%20with%20Judea%20Pearl%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FujtpvA7BCqNMsebe2%2Faudio-interview-with-judea-pearl-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FujtpvA7BCqNMsebe2%2Faudio-interview-with-judea-pearl-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>Where he discusses some of his career path and evolution of his thoughts about AI</p>\n<p><a class=\"src-url\" style=\"color: #5566dd; display: inline-block; max-width: 500px; overflow-x: hidden; overflow-y: hidden; padding-bottom: 1px; padding-top: 4px; text-decoration: none; text-overflow: ellipsis; white-space: nowrap; font-family: Arial, sans-serif; font-size: 13px;\" href=\"http://www.stephenibaraki.com/audio/Judea_Pearl.mp3\" target=\"_blank\">http://www.stephenibaraki.com/audio/Judea_Pearl.mp3</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ujtpvA7BCqNMsebe2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 9.000314265083551e-07, "legacy": true, "legacyId": "15975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T13:46:28.245Z", "modifiedAt": null, "url": null, "title": "If epistemic and instrumental rationality strongly conflict", "slug": "if-epistemic-and-instrumental-rationality-strongly-conflict", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:33.225Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vFgwGmWvHDWRxnBia", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gfaCwTLb9cC2ebRqJ/if-epistemic-and-instrumental-rationality-strongly-conflict", "pageUrlRelative": "/posts/gfaCwTLb9cC2ebRqJ/if-epistemic-and-instrumental-rationality-strongly-conflict", "linkUrl": "https://www.lesswrong.com/posts/gfaCwTLb9cC2ebRqJ/if-epistemic-and-instrumental-rationality-strongly-conflict", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20epistemic%20and%20instrumental%20rationality%20strongly%20conflict&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20epistemic%20and%20instrumental%20rationality%20strongly%20conflict%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfaCwTLb9cC2ebRqJ%2Fif-epistemic-and-instrumental-rationality-strongly-conflict%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20epistemic%20and%20instrumental%20rationality%20strongly%20conflict%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfaCwTLb9cC2ebRqJ%2Fif-epistemic-and-instrumental-rationality-strongly-conflict", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfaCwTLb9cC2ebRqJ%2Fif-epistemic-and-instrumental-rationality-strongly-conflict", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 442, "htmlBody": "<p>We in the rationalist community have believed feasible a dual allegiance to instrumental and epistemic rationality because true beliefs help with <em>winning</em>, but semi-autonomous <em>near </em>and <em>far </em>modes raise questions about the compatibility of the two sovereigns' jurisdictions: false <em>far </em>beliefs may serve to advance <em>near </em>interests. <br /><br />First, the basics of construal-level theory. <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3152826/\">See Trope and Liberman, \"Construal-Level Theory of Psychological Distance\" (2010) 117 <em>Psychological Review </em>440</a>. When you look at an object in the distance and look at the same object nearby, you focus on different features. Distal information is high-level, global, central, and unchanging, whereas local information is low-level, detailed, incidental, and changing. Construal-level theorists term distal information far and local information near, and they extend these categories broadly to embrace psychological distance. Dimensions other than physical distance can be conceived as psychological distance by analogy, and these other dimensions invoke mindsets similar to those physical distance invokes. <br /><br />I discuss construal-level theory in the blogs <a href=\"http://disputedissues.blogspot.com/2012/02/construal-level-theory-and-matching.html\"><em>Disputed Issues</em></a> and <a href=\"http://juridicalcoherence.blogspot.com/2012/03/150-taxonomy-of-political-ideologies.html\"><em>Juridical Coherence</em></a>, but Robin Hanson at <em>Overcoming Bias</em> has been one of the theory's most prolific advocates. He gives the theory an unusual twist when he maintains that the \"far\" mode is largely consumed with the management of social appearances.<br /><br />With this twist, Hanson effectively drives a wedge between instrumental and epistemic rationality because far beliefs may help with winning despite or even because of their falsity. Hanson doesn't shrink from the implications of instrumental rationality coupled with his version of construal-level theory. Based on research reports that the religious lead happier and more moral lives, Robin Hanson now advocates becoming religious: <br /><br /><em>Perhaps, like me, you find religious beliefs about Gods, spirits, etc. to be insufficiently supported by evidence, coherence, or simplicity to be a likely approximation to the truth. Even so, ask yourself: why care so much about truth? Yes, you probably think you care about believing truth &ndash; but isn&rsquo;t it more plausible that you mainly care about thinking you like truth? Doesn&rsquo;t that have a more plausible evolutionary origin than actually caring about far truth?</em> (<a href=\"http://www.overcomingbias.com/2012/05/what-use-far-truth.html\">\"What Use Far Truth?\"</a>)<br /><br />Instrumental rationalists could practice strict epistemic rationality if&nbsp; they define <em>winning </em>as gaining true belief, but though no doubt a dedicated intellectual, even Hanson doesn't value truth that much, at least not \"far\" truth. Yet, how many rationalists have cut their teeth on the irrationality of religion? How many have replied to religious propaganda about the benefits of religion with disdain for invoking mere prudential benefit where truth is at stake? As an <em>ideal</em>, epistemic rationality, it seems to me, fares better than instrumental rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gfaCwTLb9cC2ebRqJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 9, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "15951", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T16:11:12.833Z", "modifiedAt": null, "url": null, "title": "Meetup : Big Berkeley Meetup", "slug": "meetup-big-berkeley-meetup-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thausler", "createdAt": "2010-05-24T04:40:49.214Z", "isAdmin": false, "displayName": "Thausler"}, "userId": "oPa5EPBs6ow6MY2Ao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bqc8rtyozoJLZaJ2A/meetup-big-berkeley-meetup-3", "pageUrlRelative": "/posts/bqc8rtyozoJLZaJ2A/meetup-big-berkeley-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/bqc8rtyozoJLZaJ2A/meetup-big-berkeley-meetup-3", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Big%20Berkeley%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Big%20Berkeley%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbqc8rtyozoJLZaJ2A%2Fmeetup-big-berkeley-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Big%20Berkeley%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbqc8rtyozoJLZaJ2A%2Fmeetup-big-berkeley-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbqc8rtyozoJLZaJ2A%2Fmeetup-big-berkeley-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a1'>Big Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at 7:00pm at the Oxford St Starbucks.  This will be a big meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a1'>Big Berkeley Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bqc8rtyozoJLZaJ2A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.001204145875676e-07, "legacy": true, "legacyId": "15977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_Meetup\">Discussion article for the meetup : <a href=\"/meetups/a1\">Big Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at 7:00pm at the Oxford St Starbucks.  This will be a big meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/a1\">Big Berkeley Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Big Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Big Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-10T19:27:00.833Z", "modifiedAt": null, "url": null, "title": "Strong intutions. Weak arguments. What to do?", "slug": "strong-intutions-weak-arguments-what-to-do", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GyDHM7fG47mkmFD4x/strong-intutions-weak-arguments-what-to-do", "pageUrlRelative": "/posts/GyDHM7fG47mkmFD4x/strong-intutions-weak-arguments-what-to-do", "linkUrl": "https://www.lesswrong.com/posts/GyDHM7fG47mkmFD4x/strong-intutions-weak-arguments-what-to-do", "postedAtFormatted": "Thursday, May 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strong%20intutions.%20Weak%20arguments.%20What%20to%20do%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrong%20intutions.%20Weak%20arguments.%20What%20to%20do%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGyDHM7fG47mkmFD4x%2Fstrong-intutions-weak-arguments-what-to-do%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strong%20intutions.%20Weak%20arguments.%20What%20to%20do%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGyDHM7fG47mkmFD4x%2Fstrong-intutions-weak-arguments-what-to-do", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGyDHM7fG47mkmFD4x%2Fstrong-intutions-weak-arguments-what-to-do", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 493, "htmlBody": "<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">I thought Ben Goertzel made an interesting point at the end of his <a href=\"/lw/c7h/muehlhausergoertzel_dialogue_part_2/\">dialog</a> with Luke Muehlhauser, about how the strengths of both sides' arguments do not match up with the strengths of their intuitions:</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">One thing I'm repeatedly struck by in discussions on these matters with you and other SIAI folks, is the way the strings of reason are pulled by the puppet-master of intuition. With so many of these topics on which we disagree -- for example: the Scary Idea, the importance of optimization for intelligence, the existence of strongly convergent goals for intelligences -- you and the other core SIAI folks share a certain set of intuitions, which seem quite strongly held. Then you formulate rational arguments in favor of these intuitions -- but the conclusions that result from these rational arguments are very weak. For instance, the Scary Idea intuition corresponds to a rational argument that \"superhuman AGI might plausibly kill everyone.\" The intuition about strongly convergent goals for intelligences, corresponds to a rational argument about goals that are convergent for a \"wide range\" of intelligences. Etc.</p>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">On my side, I have a strong intuition that OpenCog can be made into a human-level general intelligence, and that if this intelligence is raised properly it will turn out benevolent and help us launch a positive Singularity. However, I can't fully rationally substantiate this intuition either -- all I can really fully rationally argue for is something weaker like \"It seems plausible that a fully implemented OpenCog system might display human-level or greater intelligence on feasible computational resources, and might turn out benevolent if raised properly.\" In my case just like yours, reason is far weaker than intuition.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">What do we do about this <a href=\"http://wiki.lesswrong.com/wiki/Disagreement\">disagreement</a> and other similar situations, both as bystanders (who may not have strong intuitions of their own) and as participants (who do)?</p>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">I guess what bystanders typically do (although not necessarily consciously) is evaluate how reliable each party's intuitions are likely to be, and then use that to form a probabilistic mixture of the two sides' positions.The information that go into such evaluations could include things like what cognitive processes likely came up with the intuitions, how many people hold each intuition and how accurate each individual's past intuitions were.</p>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">If this is the best we can do (at least in some situations), participants could help by providing more information that might be relevant to the reliability evaluations, and bystanders should pay more conscious attention to such information instead of focusing purely on each side's arguments. The participants could also pretend that they are just bystanders, for the purpose of making important decisions, and base their beliefs on \"reliability-adjusted\" intuitions instead of their raw intuitions.</p>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">Questions: Is this a good idea? Any other ideas about what to do when strong intuitions meet weak arguments?</p>\n<p style=\"margin: 0px 0px 1em; color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">Related Post: Kaj Sotala's <a href=\"/lw/19v/intuitive_differences_when_to_agree_to_disagree/\">Intuitive differences: when to agree to disagree</a>, which is about a similar problem, but mainly from the participant's perspective instead of the bystander's.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GyDHM7fG47mkmFD4x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "15978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qF6hvXi2ytBsyzttp", "JF4mv4PbTp6ckN3vG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T02:13:52.338Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Changing the Definition of Science", "slug": "seq-rerun-changing-the-definition-of-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:03.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zE4BE67WtbKAWt8ku/seq-rerun-changing-the-definition-of-science", "pageUrlRelative": "/posts/zE4BE67WtbKAWt8ku/seq-rerun-changing-the-definition-of-science", "linkUrl": "https://www.lesswrong.com/posts/zE4BE67WtbKAWt8ku/seq-rerun-changing-the-definition-of-science", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Changing%20the%20Definition%20of%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Changing%20the%20Definition%20of%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzE4BE67WtbKAWt8ku%2Fseq-rerun-changing-the-definition-of-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Changing%20the%20Definition%20of%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzE4BE67WtbKAWt8ku%2Fseq-rerun-changing-the-definition-of-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzE4BE67WtbKAWt8ku%2Fseq-rerun-changing-the-definition-of-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/qg/changing_the_definition_of_science/\">Changing the Definition of Science</a> was originally published on 18 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Many of these ideas are surprisingly conventional, and being floated around by other thinkers. I'm a good deal less of a lonely iconoclast than I seem; maybe it's just the way I talk.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cbd/seq_rerun_no_safe_defense_not_even_science/\">No Safe Defense, Not Even Science</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zE4BE67WtbKAWt8ku", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.003832826307343e-07, "legacy": true, "legacyId": "15992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SoDsr8GEZmRKMZNkj", "A8fmGGzxuwJdzg5wf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T04:31:30.364Z", "modifiedAt": null, "url": null, "title": "Thoughts on the Singularity Institute (SI)", "slug": "thoughts-on-the-singularity-institute-si", "viewCount": null, "lastCommentedAt": "2020-05-06T11:59:17.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HoldenKarnofsky", "createdAt": "2009-12-30T00:19:32.818Z", "isAdmin": false, "displayName": "HoldenKarnofsky"}, "userId": "kdeMdATaSc2MZKmdH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si", "pageUrlRelative": "/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si", "linkUrl": "https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20the%20Singularity%20Institute%20(SI)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20the%20Singularity%20Institute%20(SI)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6SGqkCgHuNr7d4yJm%2Fthoughts-on-the-singularity-institute-si%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20the%20Singularity%20Institute%20(SI)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6SGqkCgHuNr7d4yJm%2Fthoughts-on-the-singularity-institute-si", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6SGqkCgHuNr7d4yJm%2Fthoughts-on-the-singularity-institute-si", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8617, "htmlBody": "<p><em>This post presents thoughts on the Singularity Institute from Holden Karnofsky, Co-Executive Director of <a href=\"http://www.givewell.org\">GiveWell</a>. Note: Luke Muehlhauser, the Executive Director of the Singularity Institute, reviewed a draft of this post, and commented: \"I do generally agree that your complaints are either correct (especially re: past organizational competence) or incorrect but not addressed by SI in clear argumentative writing (this includes the part on 'tool' AI). I am working to address both categories of issues.\" I take Luke's comment to be a significant mark in SI's favor, because it indicates an explicit recognition of the problems I raise, and thus increases my estimate of the likelihood that SI will work to address them.</em></p>\n<p><em><strong>September 2012 update:</strong> responses have been posted by <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/\">Luke </a>and <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer </a>(and I have responded in the comments of their posts). I have also added <a href=\"#Acknowledgements\">acknowledgements</a>.</em></p>\n<p>The <a href=\"http://singinst.org\">Singularity Institute (SI)</a>&nbsp;is&nbsp;a charity that GiveWell has been repeatedly asked to evaluate. In the past, SI has been outside our scope (as we were focused on specific areas such as international aid). With <a href=\"http://givewell.org/about/labs\">GiveWell Labs</a> we are open to any giving opportunity, no matter what form and what sector, but we still do not currently plan to recommend SI; given the amount of interest some of our audience has expressed, I feel it is important to explain why. Our views, of course, remain open to change. (Note: I am posting this only to Less Wrong, not to the GiveWell Blog, because I believe that everyone who would be interested in this post will see it here.)</p>\n<p>I am currently the GiveWell staff member who has put the most time and effort into engaging with and evaluating SI. Other GiveWell staff currently agree with my bottom-line view that we should not recommend SI, but this does not mean they have engaged with each of my specific arguments. Therefore, while the lack of recommendation of SI is something that GiveWell stands behind, the specific arguments in this post should be attributed only to me, not to GiveWell.</p>\n<p><strong>Summary of my views</strong></p>\n<ul>\n<li>The argument advanced by SI for why the work it's doing is beneficial and important seems both wrong and poorly argued to me. My sense at the moment is that the arguments SI is making would, if accepted, increase rather than decrease the risk of an AI-related catastrophe. <a href=\"#Arguments\">More</a> </li>\n<li>SI has, or has had, multiple properties that I associate with ineffective organizations, and I do not see any specific evidence that its personnel/organization are well-suited to the tasks it has set for itself. <a href=\"#Organization\">More</a> </li>\n<li>A common argument for giving to SI is that \"even an infinitesimal chance that it is right\" would be sufficient given the stakes. I have written previously about why I reject this reasoning; in addition, prominent SI representatives seem to reject this particular argument as well (i.e., they believe that one should support SI only if one believes it is a strong organization making strong arguments). <a href=\"#SmallProbability\">More</a> </li>\n<li> My sense is that at this point, given SI's current financial state, withholding funds from SI is likely better for its mission than donating to it. (I would not take this view to the furthest extreme; the argument that SI should have <em>some</em> funding seems stronger to me than the argument that it should have as much as it currently has.) </li>\n<li>I find existential risk reduction to be a fairly promising area for philanthropy, and plan to investigate it further. <a href=\"#ExistentialRisk\">More</a> </li>\n<li>There are many things that could happen that would cause me to revise my view on SI. However, I do not plan to respond to all comment responses to this post. (Given the volume of responses we may receive, I may not be able to even read all the comments on this post.) I do not believe these two statements are inconsistent, and I lay out paths for getting me to change my mind that are likely to work better than posting comments. (Of course I encourage people to post comments; I'm just noting in advance that this action, alone, doesn't guarantee that I will consider your argument.) <a href=\"#FollowUp\">More</a></li>\n</ul>\n<p><strong>Intent of this post</strong></p>\n<p>I did not write this post with the purpose of \"hurting\" SI. Rather, I wrote it in the hopes that <strong>one of these three things</strong> (or some combination) will happen:</p>\n<ol>\n<li>New arguments are raised that cause me to change my mind and recognize SI as an outstanding giving opportunity. If this happens I will likely attempt to raise more money for SI (most likely by discussing it with other GiveWell staff and collectively considering a <a href=\"http://www.givewell.org/about/labs\">GiveWell Labs</a> recommendation). </li>\n<li>SI concedes that my objections are valid and increases its determination to address them. A few years from now, SI is a better organization and more effective in its mission. </li>\n<li>SI can't or won't make changes, and SI's supporters feel my objections are valid, so SI loses some support, freeing up resources for other approaches to doing good.</li>\n</ol>\n<p>Which one of these occurs will hopefully be driven primarily by the merits of the different arguments raised. Because of this, I think that whatever happens as a result of my post will be positive for SI's mission, whether or not it is positive for SI as an organization. I believe that most of SI's supporters and advocates care more about the former than about the latter, and that this attitude is far too rare in the nonprofit world.</p>\n<p><a id=\"more\"></a></p>\n<h1><strong><a name=\"Arguments\"></a><a name=\"Arguments\"></a>Does SI have a well-argued case that its work is beneficial and important?</strong></h1>\n<p>I know no more concise summary of SI's views than <a href=\"http://intelligence.org/summary\">this page</a>, so here I give my own impressions of what SI believes, in italics.<em></em></p>\n<ol><em>\n<li>There is some chance that in the near future (next 20-100 years), an \"artificial general intelligence\" (AGI) - a computer that is vastly more intelligent than humans in every relevant way - will be created. </li>\n<li>This AGI will likely have a utility function and will seek to maximize utility according to this function. </li>\n<li>This AGI will be so much more powerful than humans - due to its superior intelligence - that it will be able to reshape the world to maximize its utility, and humans will not be able to stop it from doing so. </li>\n<li>Therefore, it is crucial that its utility function be one that is reasonably harmonious with what humans want. A \"Friendly\" utility function is one that is reasonably harmonious with what humans want, such that a \"Friendly\" AGI (FAI) would change the world for the better (by human standards) while an \"Unfriendly\" AGI (UFAI) would essentially wipe out humanity (or worse). </li>\n<li>Unless great care is taken specifically to make a utility function \"Friendly,\" it will be \"Unfriendly,\" since the things humans value are a tiny subset of the things that are possible. </li>\n<li>Therefore, it is crucially important to develop \"Friendliness theory\" that helps us to ensure that the first strong AGI's utility function will be \"Friendly.\" The developer of Friendliness theory could use it to build an FAI directly or could disseminate the theory so that others working on AGI are more likely to build FAI as opposed to UFAI.</li>\n</em></ol>\n<p>From the time I first heard this argument, it has seemed to me to be skipping important steps and making major unjustified assumptions. However, for a long time I believed this could easily be due to my inferior understanding of the relevant issues. I believed my own views on the argument to have only very low relevance (as I stated in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a>). Over time, I have had many discussions with SI supporters and advocates, as well as with non-supporters who I believe understand the relevant issues well. I now believe - for the moment - that my objections are highly relevant, that they cannot be dismissed as simple \"layman's misunderstandings\" (as they have been by various SI supporters in the past), and that SI has not published anything that addresses them in a clear way.</p>\n<p>Below, I list my major objections. I do not believe that these objections constitute a sharp/tight case for the idea that SI's work has low/negative value; I believe, instead, that SI's own arguments are too vague for such a rebuttal to be possible. There are many possible responses to my objections, but SI's public arguments (and the private arguments) do not make clear which possible response (if any) SI would choose to take up and defend. Hopefully the dialogue following this post will clarify what SI believes and why.</p>\n<p>Some of my views are discussed at greater length (though with less clarity) in a <a href=\"http://groups.yahoo.com/group/givewell/message/287\">public transcript of a conversation I had with SI supporter Jaan Tallinn</a>. I refer to this transcript as \"Karnofsky/Tallinn 2011.\"</p>\n<h3>Objection 1: it seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</h3>\n<p>Suppose, for the sake of argument, that SI manages to create what it believes to be an FAI. Suppose that it is successful in the \"AGI\" part of its goal, i.e., it has successfully created an intelligence vastly superior to human intelligence and extraordinarily powerful from our perspective. Suppose that it has also done its best on the \"Friendly\" part of the goal: it has developed a formal argument for why its AGI's utility function will be Friendly, it believes this argument to be airtight, and it has had this argument checked over by 100 of the world's most intelligent and relevantly experienced people. Suppose that SI now activates its AGI, unleashing it to reshape the world as it sees fit. What will be the outcome?</p>\n<p>I believe that the probability of an unfavorable outcome - by which I mean an outcome essentially equivalent to what a UFAI would bring about - exceeds 90% in such a scenario. I believe the goal of designing a \"Friendly\" utility function is likely to be beyond the abilities even of the best team of humans willing to design such a function. I do not have a tight argument for why I believe this, but a <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/5986\">comment on LessWrong by Wei Dai</a> gives a good illustration of the kind of thoughts I have on the matter:</p>\n<blockquote>What I'm afraid of is that a design will be shown to be safe, and then it turns out that the proof is wrong, or the formalization of the notion of \"safety\" used by the proof is wrong. This kind of thing happens <em>a lot</em> in cryptography, if you replace \"safety\" with \"security\". These mistakes are still occurring today, even after decades of research into how to do such proofs and what the relevant formalizations are. From where I'm sitting, proving an AGI design Friendly seems even more difficult and error-prone than proving a crypto scheme secure, probably by a large margin, and there is no decades of time to refine the proof techniques and formalizations. There's good recent review of the history of provable security, titled <a href=\"http://www.ibiblio.org/weidai/temp/Provable_Security.pdf\">Provable Security in the Real World</a>, which might help you understand where I'm coming from.</blockquote>\n<p>I think this comment understates the risks, however. For example, when the comment says \"the formalization of the notion of 'safety' used by the proof is wrong,\" it is not clear whether it means that the values the programmers have in mind are not correctly implemented by the formalization, or whether it means they are correctly implemented but <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">are themselves catastrophic in a way that hasn't been anticipated</a>. I would be highly concerned about both. There are other catastrophic possibilities as well; perhaps the utility function itself is well-specified and safe, but the AGI's model of the world is flawed (in particular, perhaps its <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a> or its process for matching observations to predictions are flawed) in a way that doesn't emerge until the AGI has made substantial changes to its environment.</p>\n<p>By SI's own arguments, even a small error in any of these things would likely lead to catastrophe. And there are likely failure forms I haven't thought of. The overriding intuition here is that complex plans usually fail when unaccompanied by feedback loops. A scenario in which a set of people is ready to unleash an all-powerful being to maximize some parameter in the world, based solely on their initial confidence in their own extrapolations of the consequences of doing so, seems like a scenario that is overwhelmingly likely to result in a bad outcome. It comes down to placing the world's largest bet on a highly complex theory - with no experimentation to test the theory first.</p>\n<p>So far, all I have argued is that the development of \"Friendliness\" theory can achieve at best only a limited reduction in the probability of an unfavorable outcome. However, as I argue in the next section, I believe there is at least one concept - the \"tool-agent\" distinction - that has more potential to reduce risks, and that SI appears to ignore this concept entirely. I believe that tools are safer than agents (even agents that make use of the best \"Friendliness\" theory that can reasonably be hoped for) and that SI encourages a focus on building agents, thus increasing risk.</p>\n<h3>Objection 2: SI appears to neglect the potentially important distinction between \"tool\" and \"agent\" AI.</h3>\n<p>Google Maps is a type of artificial intelligence (AI). It is far more intelligent than I am when it comes to planning routes.</p>\n<p>Google Maps - by which I mean the complete software package including the display of the map itself - does not have a \"utility\" that it seeks to maximize. (One could fit a utility function to its actions, as to any set of actions, but there is no single \"parameter to be maximized\" driving its operations.)</p>\n<p>Google Maps (as I understand it) considers multiple possible routes, gives each a score based on factors such as distance and likely traffic, and then displays the best-scoring route in a way that makes it easily understood by the user. If I don't like the route, for whatever reason, I can change some parameters and consider a different route. If I like the route, I can print it out or email it to a friend or send it to my phone's navigation application. Google Maps has no single parameter it is trying to maximize; it has no reason to try to \"trick\" me in order to increase its utility.</p>\n<p>In short, Google Maps is not an <em>agent</em>, taking actions in order to maximize a utility parameter. It is a <em>tool</em>, generating information and then displaying it in a user-friendly manner for me to consider, use and export or discard as I wish.</p>\n<p>Every software application I know of seems to work essentially the same way, including those that involve (specialized) artificial intelligence such as Google Search, Siri, Watson, Rybka, etc. Some can be put into an \"agent mode\" (as Watson was on Jeopardy!) but all can easily be set up to be used as \"tools\" (for example, Watson can simply display its top candidate answers to a question, with the score for each, without speaking any of them.)</p>\n<p>The \"tool mode\" concept is importantly different from the possibility of <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Oracle AI</a> sometimes discussed by SI. The discussions I've seen of Oracle AI present it as an Unfriendly AI that is \"trapped in a box\" - an AI whose intelligence is driven by an explicit utility function and that humans hope to control coercively. Hence the discussion of ideas such as the <a href=\"http://yudkowsky.net/singularity/aibox\">AI-Box Experiment</a>. A different interpretation, given in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>, is an AI with a carefully designed utility function - likely as difficult to construct as \"Friendliness\" - that leaves it \"wishing\" to answer questions helpfully. By contrast with both these ideas, Tool-AGI is not \"trapped\" and it is not Unfriendly or Friendly; it has no motivations and no driving utility function of any kind, just like Google Maps. It scores different possibilities and displays its conclusions in a transparent and user-friendly manner, as its instructions say to do; it does not have an overarching \"want,\" and so, as with the specialized AIs described above, while it may sometimes \"misinterpret\" a question (thereby scoring options poorly and ranking the wrong one #1) there is no reason to expect intentional trickery or manipulation when it comes to displaying its results.</p>\n<p>Another way of putting this is that a \"tool\" has an underlying instruction set that conceptually looks like: \"(1) Calculate which action A would maximize parameter P, based on existing data set D. (2) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.\" An \"agent,\" by contrast, has an underlying instruction set that conceptually looks like: \"(1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2) Execute Action A.\" In any AI where (1) is separable (by the programmers) as a distinct step, (2) can be set to the \"tool\" version rather than the \"agent\" version, and this separability is in fact present with most/all modern software. Note that in the \"tool\" version, neither step (1) nor step (2) (nor the combination) constitutes an instruction to maximize a parameter - to describe a program of this kind as \"wanting\" something is a category error, and there is no reason to expect its step (2) to be deceptive.</p>\n<p>I elaborated further on the distinction and on the concept of a tool-AI in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>.</p>\n<p>This is important because <strong>an AGI running in tool mode could be extraordinarily useful but far more safe than an AGI running in agent mode</strong>. In fact, if developing \"Friendly AI\" is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on \"Friendliness theory\" moot. Among other things, a tool-AGI would allow transparent views into the AGI's reasoning and predictions without any reason to fear being purposefully misled, and would facilitate safe experimental testing of any utility function that one wished to eventually plug into an \"agent.\"</p>\n<p>Is a tool-AGI possible? I believe that it is, and furthermore that it ought to be our default picture of how AGI will work, given that practically all software developed to date can (and usually does) run as a tool and given that modern software seems to be constantly becoming \"intelligent\" (capable of giving better answers than a human) in surprising new domains. In addition, it intuitively seems to me (though I am not highly confident) that intelligence inherently involves the distinct, separable steps of (a) considering multiple possible actions and (b) assigning a score to each, <em>prior</em> to executing any of the possible actions. If one can distinctly separate (a) and (b) in a program's code, then one can abstain from writing any \"execution\" instructions and instead focus on making the program list actions and scores in a user-friendly manner, for humans to consider and use as they wish.</p>\n<p>Of course, there are possible paths to AGI that may rule out a \"tool mode,\" but it seems that most of these paths would rule out the application of \"Friendliness theory\" as well. (For example, a \"black box\" emulation and augmentation of a human mind.) What are the paths to AGI that allow manual, transparent, intentional design of a utility function but do not allow the replacement of \"execution\" instructions with \"communication\" instructions? Most of the conversations I've had on this topic have focused on three responses:</p>\n<ul>\n<li><strong>Self-improving AI.</strong> Many seem to find it intuitive that (a) AGI will almost certainly come from an AI rewriting its own source code, and (b) such a process would inevitably lead to an \"agent.\" I do not agree with either (a) or (b). I discussed these issues in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a> and will be happy to discuss them more if this is the line of response that SI ends up pursuing. Very briefly: \n<ul>\n<li>The idea of a \"self-improving algorithm\" intuitively sounds very powerful, but does not seem to have led to many \"explosions\" in software so far (and it seems to be a concept that could apply to narrow AI as well as to AGI). </li>\n<li>It seems to me that a tool-AGI could be plugged into a self-improvement process that would be quite powerful but would also terminate and yield a new tool-AI after a set number of iterations (or after reaching a set \"intelligence threshold\"). So I do not accept the argument that \"self-improving AGI means agent AGI.\" As stated above, I will elaborate on this view if it turns out to be an important point of disagreement. </li>\n<li>I have argued (in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>) that the relevant self-improvement abilities are likely to come <em>with</em> or <em>after</em> - not <em>prior to</em> - the development of strong AGI. In other words, any software capable of the relevant kind of self-improvement is likely also capable of being used as a strong tool-AGI, with the benefits described above. </li>\n<li>The SI-related discussions I've seen of \"self-improving AI\" are highly vague, and do not spell out views on the above points. </li>\n</ul>\n</li>\n<li><strong>Dangerous data collection.</strong> Some point to the seeming dangers of a tool-AI's \"scoring\" function: in order to score different options it may have to collect data, which is itself an \"agent\" type action that could lead to dangerous actions. I think my definition of \"tool\" above makes clear what is wrong with this objection: a tool-AGI takes its existing data set D as fixed (and perhaps could have some pre-determined, safe set of simple actions it can take - such as using Google's API - to collect more), and if maximizing its chosen parameter is best accomplished through more data collection, it can transparently output why and how it suggests collecting more data. Over time it can be given more autonomy <em>for data collection</em> through an <em>experimental and domain-specific process</em> (e.g., modifying the AI to skip specific steps of human review of proposals for data collection after it has become clear that these steps work as intended), a process that has little to do with the \"Friendly overarching utility function\" concept promoted by SI. Again, I will elaborate on this if it turns out to be a key point. </li>\n<li><strong>Race for power.</strong> Some have argued to me that humans are likely to <em>choose</em> to create agent-AGI, in order to quickly gain power and outrace other teams working on AGI. But this argument, even if accepted, has very different implications from SI's view.\n<p>Conventional wisdom says it is extremely dangerous to empower a computer to act in the world until one is very sure that the computer will do its job in a way that is helpful rather than harmful. So if a programmer chooses to \"unleash an AGI as an agent\" with the hope of gaining power, it seems that this programmer will be deliberately ignoring conventional wisdom about what is safe in favor of shortsighted greed. I do not see why such a programmer would be expected to make use of any \"Friendliness theory\" that might be available. (Attempting to incorporate such theory would almost certainly slow the project down greatly, and thus would bring the same problems as the more general \"have caution, do testing\" counseled by conventional wisdom.) It seems that the appropriate measures for preventing such a risk are security measures aiming to stop humans from launching unsafe agent-AIs, rather than developing theories or raising awareness of \"Friendliness.\"</p>\n</li>\n</ul>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\" The idea that AGI will be driven by a central utility function seems to be simply assumed. Two examples:</p>\n<ul>\n<li>I have been referred to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser and Salamon 2012</a> as the most up-to-date, clear explanation of SI's position on \"the basics.\" This paper states, \"Perhaps we could build an AI of limited cognitive ability &mdash; say, a machine that only answers questions: an 'Oracle AI.' But this approach is not without its own dangers (Armstrong, Sandberg, and Bostrom 2012).\" However, the referenced paper (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong, Sandberg and Bostrom 2012</a>) seems to take it as a given that an Oracle AI is an \"agent trapped in a box\" - a computer that has a basic drive/utility function, not a Tool-AGI. The rest of Muehlhauser and Salamon 2012 seems to take it as a given that an AGI will be an agent. </li>\n<li>I have often been referred to <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro 2008</a> for an argument that an AGI is likely to have certain goals. But this paper seems, again, to take it as given that an AGI will be an agent, i.e., that it will have goals at all. The introduction states, \"To say that a system of any design is an 'arti\ufb01cial intelligence', we mean that it has goals which it tries to accomplish by acting in the world.\" In other words, the premise I'm disputing seems embedded in its very definition of AI. </li>\n</ul>\n<p>The closest thing I have seen to a public discussion of \"tool-AGI\" is in <a href=\"/lw/tj/dreams_of_friendliness/\">Dreams of Friendliness</a>, where Eliezer Yudkowsky considers the question, \"Why not just have the AI answer questions, instead of trying to <em>do</em> anything? Then it wouldn't need to be Friendly. It wouldn't need any goals at all. It would just answer questions.\" His response:</p>\n<blockquote>To which the reply is that the AI needs goals in order to decide how to think: that is, the AI has to act as a powerful optimization process in order to plan its acquisition of knowledge, effectively distill sensory information, pluck \"answers\" to particular questions out of the space of all possible responses, and of course, to improve its own source code up to the level where the AI is a powerful intelligence. All these events are \"improbable\" relative to random organizations of the AI's RAM, so the AI has to hit a narrow target in the space of possibilities to make superintelligent answers come out.</blockquote>\n<p>This passage appears vague and does not appear to address the specific \"tool\" concept I have defended above (in particular, it does not address the analogy to modern software, which challenges the idea that \"powerful optimization processes\" cannot run in tool mode). The rest of the piece discusses (a) psychological mistakes that could lead to the discussion in question; (b) the \"Oracle AI\" concept that I have outlined above. The comments contain some more discussion of the \"tool\" idea (Denis Bider and Shane Legg seem to be picturing something similar to \"tool-AGI\") but the discussion is unresolved and I believe the \"tool\" concept defended above remains essentially unaddressed.</p>\n<p>In sum, SI appears to encourage a focus on building and launching \"Friendly\" agents (it is seeking to do so itself, and its work on \"Friendliness\" theory seems to be laying the groundwork for others to do so) while not addressing the tool-agent distinction. It seems to assume that any AGI will have to be an agent, and to make little to no attempt at justifying this assumption. The result, in my view, is that it is essentially advocating for a more dangerous approach to AI than the traditional approach to software development.</p>\n<h3>Objection 3: SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.</h3>\n<p>SI's scenario concerns the development of artificial <em>general</em> intelligence (AGI): a computer that is vastly more intelligent than humans in every relevant way. But we already have many computers that are vastly more intelligent than humans in <em>some</em> relevant ways, and the domains in which specialized AIs outdo humans seem to be constantly and continuously expanding. I feel that the relevance of \"Friendliness theory\" depends heavily on the idea of a \"discrete jump\" that seems unlikely and whose likelihood does not seem to have been publicly argued for.</p>\n<p>One possible scenario is that at some point, we develop powerful enough non-AGI tools (particularly specialized AIs) that we vastly improve our abilities to consider and prepare for the eventuality of AGI - to the point where any previous theory developed on the subject becomes useless. Or (to put this more generally) non-AGI tools simply change the world so much that it becomes essentially unrecognizable from the perspective of today - again rendering any previous \"Friendliness theory\" moot. As I said in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>, some of SI's work \"seems a bit like trying to design Facebook before the Internet was in use, or even before the computer existed.\"</p>\n<p>Perhaps there will be a discrete jump to AGI, but it will be a sort of AGI that renders \"Friendliness theory\" moot for a different reason. For example,&nbsp;<em>in the practice of software development</em>, there often does not seem to be an operational distinction between \"intelligent\" and \"Friendly.\" (For example, my impression is that the only method programmers had for evaluating Watson's \"intelligence\" was to see whether it was coming up with the same answers that a well-informed human would; the only way to evaluate Siri's \"intelligence\" was to evaluate its helpfulness to humans.) \"Intelligent\" often ends up getting defined as \"prone to take actions that seem all-around 'good' to the programmer.\" So the concept of \"Friendliness\" may end up being naturally and subtly baked in to a successful AGI effort.</p>\n<p>The bottom line is that we know very little about the course of future artificial intelligence. I believe that the probability that SI's concept of \"Friendly\" vs. \"Unfriendly\" goals ends up seeming essentially nonsensical, irrelevant and/or unimportant from the standpoint of the relevant future is over 90%.</p>\n<h3>Other objections to SI's views</h3>\n<p>There are other debates about the likelihood of SI's work being relevant/helpful; for example,</p>\n<ul>\n<li>It isn't clear whether the development of AGI is imminent enough to be relevant, or whether other risks to humanity are closer. </li>\n<li>It isn't clear whether AGI would be as powerful as SI's views imply. (I discussed this briefly in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011.</a>) </li>\n<li>It isn't clear whether even an extremely powerful UFAI would choose to attack humans as opposed to negotiating with them. (I find it somewhat helpful to analogize UFAI-human interactions to human-mosquito interactions. Humans are enormously more intelligent than mosquitoes; humans are good at predicting, manipulating, and destroying mosquitoes; humans do not value mosquitoes' welfare; humans have other goals that mosquitoes interfere with; humans would like to see mosquitoes eradicated at least from certain parts of the planet. Yet humans haven't accomplished such eradication, and it is easy to imagine scenarios in which humans would prefer honest negotiation and trade with mosquitoes to any other arrangement, if such negotiation and trade were possible.) </li>\n</ul>\n<p>Unlike the three objections I focus on, these other issues have been discussed a fair amount, and if these other issues were the only objections to SI's arguments I would find SI's case to be strong (i.e., I would find its scenario likely <em>enough</em> to warrant investment in).</p>\n<h3>Wrapup</h3>\n<ul>\n<li>I believe the most likely future scenarios are the ones we haven't thought of, and that the most likely fate of the sort of theory SI ends up developing is irrelevance. </li>\n<li>I believe that unleashing an all-powerful \"agent AGI\" (without the benefit of experimentation) would very likely result in a UFAI-like outcome, no matter how carefully the \"agent AGI\" was designed to be \"Friendly.\" I see SI as encouraging (and aiming to take) this approach. </li>\n<li>I believe that the standard approach to developing software results in \"tools,\" not \"agents,\" and that tools (while dangerous) are much safer than agents. A \"tool mode\" could facilitate <em>experiment-informed</em> progress toward a safe \"agent,\" rather than needing to get \"Friendliness\" theory right without any experimentation. </li>\n<li>Therefore, I believe that the approach SI advocates and aims to prepare for is far more dangerous than the standard approach, so <em>if</em> SI's work on Friendliness theory affects the risk of human extinction one way or the other, it will increase the risk of human extinction. Fortunately I believe SI's work is far more likely to have no effect one way or the other.</li>\n</ul>\n<p>For a long time I refrained from engaging in object-level debates over SI's work, believing that others are better qualified to do so. But after talking at great length to many of SI's supporters and advocates and reading everything I've been pointed to as relevant, I still have seen no clear and compelling response to any of my three major objections. As stated above, there are many possible responses to my objections, but SI's current arguments do not seem clear on what responses they wish to take and defend. At this point I am unlikely to form a positive view of SI's work until and unless I do see such responses, and/or SI changes its positions.</p>\n<h1><strong><a name=\"Organization\"></a>Is SI the kind of organization we want to bet on?</strong></h1>\n<p>This part of the post has some risks. For most of GiveWell's history, sticking to our <a href=\"http://givewell.org/international/process/2011#Goalofthereport\">standard criteria</a> - and putting more energy into recommended than non-recommended organizations - has enabled us to share our honest thoughts about charities without appearing to get personal. But when evaluating a group such as SI, I can't avoid placing a heavy weight on (my read on) the general competence, capability and \"intangibles\" of the people and organization, because SI's mission is not about repeating activities that have worked in the past. <strong>Sharing my views on these issues could strike some as personal or mean-spirited and could lead to the misimpression that GiveWell is hostile toward SI. But it is simply necessary in order to be fully transparent about why I hold the views that I hold.</strong></p>\n<p>Fortunately, SI is an ideal organization for our first discussion of this type. I believe the staff and supporters of SI would overwhelmingly rather hear the whole truth about my thoughts - so that they can directly engage them and, if warranted, make changes - than have me sugar-coat what I think in order to spare their feelings. People who know me and <a href=\"http://blog.givewell.org/2007/06/05/an-open-letter-to-crybabies/\">my attitude toward being honest vs. sparing feelings</a> know that this, itself, is high praise for SI.</p>\n<p>One more comment before I continue: our policy is that non-public information provided to us by a charity will not be published or discussed without that charity's prior consent. However, none of the content of this post is based on private information; all of it is based on information that SI has made available to the public.</p>\n<p>There are several reasons that I currently have a negative impression of SI's general competence, capability and \"intangibles.\" My mind remains open and I include specifics on how it could be changed.</p>\n<ul>\n<li><strong>Weak arguments.</strong> SI has produced enormous quantities of public argumentation, and I have examined a very large proportion of this information. Yet I have never seen a clear response to any of the three basic objections I listed in the previous section. One of SI's major goals is to raise awareness of AI-related risks; given this, the fact that it has not advanced clear/concise/compelling arguments speaks, in my view, to its general competence. </li>\n<li><strong>Lack of impressive endorsements.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. I feel that given the enormous implications of SI's claims, if it argued them well it ought to be able to get more impressive endorsements than it has.\n<p>I have been pointed to Peter Thiel and Ray Kurzweil as examples of impressive SI supporters, but I have not seen any on-record statements from either of these people that show agreement with SI's specific views, and in fact (based on watching them speak at Singularity Summits) my impression is that they disagree. Peter Thiel seems to believe that speeding the pace of general innovation is a good thing; this would seem to be in tension with SI's view that AGI will be catastrophic by default and that no one other than SI is paying sufficient attention to \"Friendliness\" issues. Ray Kurzweil seems to believe that \"safety\" is a matter of transparency, strong institutions, etc. rather than of \"Friendliness.\" I am personally in agreement with the things I have seen both of them say on these topics. I find it possible that they support SI because of the Singularity Summit or to increase general interest in ambitious technology, rather than because they find \"Friendliness theory\" to be as important as SI does.</p>\n<p>Clear, on-record statements from these two supporters, specifically endorsing SI's arguments and the importance of developing Friendliness theory, would shift my views somewhat on this point.</p>\n</li>\n<li><strong>Resistance to feedback loops.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. SI seems to have passed up opportunities to test itself and its own rationality by e.g. aiming for objectively impressive accomplishments. This is a problem because of (a) its extremely ambitious goals (among other things, it seeks to develop artificial intelligence <em>and</em> \"Friendliness theory\" before anyone else can develop artificial intelligence); (b) its view of its staff/supporters as having unusual insight into rationality, which I discuss in a later bullet point.\n<p>SI's <a href=\"http://intelligence.org/achievements\">list of achievements</a> is not, in my view, up to where it needs to be given (a) and (b). Yet I have seen no declaration that SI has fallen short to date and explanation of what will be changed to deal with it. SI's recent release of a <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a> and <a href=\"http://intelligence.org/blog/\">monthly updates</a> are improvements from a transparency perspective, but they still leave me feeling as though there are no clear metrics or goals by which SI is committing to be measured (aside from very basic organizational goals such as \"design a new website\" and very vague goals such as \"publish more papers\") and as though SI places a low priority on engaging people who are critical of its views (or at least not yet on board), as opposed to people who are naturally drawn to it.</p>\n<p>I believe that one of the primary obstacles to being impactful as a nonprofit is the lack of the sort of helpful feedback loops that lead to success in other domains. I like to see groups that are making as much effort as they can to create meaningful feedback loops for themselves. I perceive SI as falling well short on this front. Pursuing more impressive endorsements and developing benign but objectively recognizable innovations (particularly commercially viable ones) are two possible ways to impose more demanding feedback loops. (I discussed both of these in my interview linked above).</p>\n</li>\n<li><strong>Apparent poorly grounded belief in SI's superior general rationality.</strong> Many of the things that SI and its supporters and advocates say imply a belief that they have special insights into the nature of general rationality, and/or have superior general rationality, relative to the rest of the population. (Examples <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">here</a>, <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">here</a> and <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">here</a>). My understanding is that SI is in the process of spinning off a group dedicated to training people on how to have higher general rationality.\n<p>Yet I'm not aware of any of what I consider compelling evidence that SI staff/supporters/advocates have any special insight into the nature of general rationality or that they have especially high general rationality.</p>\n<p>I have been pointed to the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a> on this point. The Sequences (which I have read the vast majority of) do not seem to me to be a demonstration or evidence of general rationality. They are <em>about</em> rationality; I find them very enjoyable to read; and there is very little they say that I disagree with (or would have disagreed with before I read them). However, they do not seem to demonstrate rationality on the part of the writer, any more than a series of enjoyable, not-obviously-inaccurate essays on the qualities of a good basketball player would demonstrate basketball prowess. I sometimes get the impression that fans of the Sequences are willing to ascribe superior rationality to the writer simply because the content <em>seems smart and insightful to them</em>, without making a critical effort to determine the extent to which the content is novel, actionable and important.&nbsp;</p>\n<p>I endorse <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Eliezer Yudkowsky's statement</a>, \"Be careful &hellip; any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility.\" To me, the best evidence of superior general rationality (or of insight into it) would be objectively impressive achievements (successful commercial ventures, highly prestigious awards, clear innovations, etc.) and/or accumulation of wealth and power. As mentioned above, SI staff/supporters/advocates do not seem particularly impressive on these fronts, at least not as much as I would expect for people who have the sort of insight into rationality that makes it sensible for them to train others in it. I am open to other evidence that SI staff/supporters/advocates have superior general rationality, but I have not seen it.</p>\n<p>Why is it a problem if SI staff/supporter/advocates believe themselves, without good evidence, to have superior general rationality? First off, it strikes me as a belief based on wishful thinking rather than rational inference. Secondly, I would expect a series of problems to accompany overconfidence in one's general rationality, and several of these problems seem to be actually occurring in SI's case:</p>\n<ul>\n<li>Insufficient self-skepticism given how strong its claims are and how little support its claims have won. Rather than endorsing \"Others have not accepted our arguments, so we will sharpen and/or reexamine our arguments,\" SI seems often to endorse something more like \"Others have not accepted their arguments because they have inferior general rationality,\" a stance less likely to lead to improvement on SI's part. </li>\n<li>Being too selective (in terms of looking for people who share its preconceptions) when determining whom to hire and whose feedback to take seriously. </li>\n<li>Paying insufficient attention to the limitations of the confidence one can have in one's untested theories, in line with my Objection 1.</li>\n</ul>\n</li>\n<li><strong>Overall disconnect between SI's goals and its activities.</strong> SI seeks to build FAI and/or to develop and promote \"Friendliness theory\" that can be useful to others in building FAI. Yet it seems that most of its time goes to activities other than developing AI or theory. Its per-person output in terms of <a href=\"http://intelligence.org/research/publications\">publications</a> seems low. Its core staff seem more focused on <a href=\"http://www.lesswrong.com\">Less Wrong</a> posts, \"rationality training\" and other activities that don't seem connected to the core goals; Eliezer Yudkowsky, in particular, appears (from the <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a>) to be focused on writing books for popular consumption. These activities seem neither to be advancing the state of FAI-related theory nor to be engaging the sort of people most likely to be crucial for building AGI.\n<p>A possible justification for these activities is that SI is seeking to promote greater general rationality, which over time will lead to more and better support for its mission. But if this is SI's core activity, it becomes even more important to test the hypothesis that SI's views are in fact rooted in superior general rationality - and these tests don't seem to be happening, as discussed above.</p>\n</li>\n<li><strong>Theft.</strong> I am bothered by the <a href=\"/lw/5il/siai_an_examination/\">2009 theft of $118,803.00</a> (as against a $541,080.00 budget for the year). In an organization as small as SI, it really seems as though theft that large relative to the budget shouldn't occur and that it represents a major failure of hiring and/or internal controls.\n<p>In addition, I have seen no public SI-authorized discussion of the matter that I consider to be satisfactory in terms of explaining what happened and what the current status of the case is on an ongoing basis. Some details may have to be omitted, but a clear SI-authorized statement on this point with as much information as can reasonably provided would be helpful.</p>\n</li>\n</ul>\n<p>A couple positive observations to add context here:</p>\n<ul>\n<li>I see significant positive qualities in many of the people associated with SI. I especially like what I perceive as their sincere wish to do whatever they can to help the world as much as possible, and the high value they place on being right as opposed to being conventional or polite. I have not interacted with Eliezer Yudkowsky but I greatly enjoy his writings. </li>\n<li>I'm aware that SI has relatively new leadership that is attempting to address the issues behind some of my complaints. I have a generally positive impression of the new leadership; I believe the Executive Director and Development Director, in particular, to represent a step forward in terms of being interested in transparency and in testing their own general rationality. So I will not be surprised if there is some improvement in the coming years, particularly regarding the last couple of statements listed above. That said, SI is an organization and it seems reasonable to judge it by its organizational track record, especially when its new leadership is so new that I have little basis on which to judge these staff.</li>\n</ul>\n<h3>Wrapup</h3>\n<p>While SI has produced a lot of content that I find interesting and enjoyable, it has not produced what I consider evidence of superior general rationality or of its suitability for the tasks it has set for itself. I see no qualifications or achievements that specifically seem to indicate that SI staff are well-suited to the challenge of understanding the key AI-related issues and/or coordinating the construction of an FAI. And I see specific reasons to be pessimistic about its suitability and general competence.</p>\n<p>When estimating the expected value of an endeavor, it is natural to have an implicit \"survivorship bias\" - to use organizations whose accomplishments one is familiar with (which tend to be relatively effective organizations) as a reference class. Because of this, I would be extremely wary of investing in an organization with apparently poor general competence/suitability to its tasks, even if I bought fully into its mission (which I do not) and saw no other groups working on a comparable mission.</p>\n<h1><strong><a name=\"SmallProbability\"></a>But if there's even a chance &hellip;</strong></h1>\n<p>A common argument that SI supporters raise with me is along the lines of, \"Even if SI's arguments are weak and its staff isn't as capable as one would like to see, their goal is so important that they would be a good investment even at a tiny probability of success.\"</p>\n<p>I believe this argument to be a form of <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> and I have outlined the reasons I believe it to be invalid in two posts (<a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">here</a> and <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">here</a>). There have been some objections to my arguments, but I still believe them to be valid. There is a good chance I will revisit these topics in the future, because I believe these issues to be at the core of many of the differences between GiveWell-top-charities supporters and SI supporters.</p>\n<p>Regardless of whether one accepts my specific arguments, it is worth noting that the most prominent people associated with SI tend to agree with the <em>conclusion</em> that the \"But if there's even a chance &hellip;\" argument is not valid. (See comments on my post from <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4o2z\">Michael Vassar</a> and <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">Eliezer Yudkowsky</a> as well as <a href=\"http://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/#comment-5546\">Eliezer's interview with John Baez</a>.)</p>\n<h1><strong><a name=\"ExistentialRisk\"></a>Existential risk reduction as a cause</strong></h1>\n<p>I consider the general cause of \"looking for ways that philanthropic dollars can reduce direct threats of global catastrophic risks, particularly those that involve some risk of human extinction\" to be a relatively high-potential cause. It is on the <a href=\"http://blog.givewell.org/2012/05/09/givewell-labs-update-and-priority-causes/\">working agenda for GiveWell Labs</a>&nbsp;and we will be writing more about it.</p>\n<p>However, I don't think that \"Cause X is the one I care about and Organization Y is the only one working on it\" to be a good reason to support Organization Y. For donors determined to donate within this cause, I encourage you to consider donating to a donor-advised fund while making it clear that you intend to grant out the funds to existential-risk-reduction-related organizations in the future. (One way to accomplish this would be to create a fund with \"existential risk\" in the name; this is a fairly easy thing to do and one person could do it on behalf of multiple donors.)</p>\n<p>For one who accepts my arguments about SI, I believe withholding funds in this way is likely to be better for SI's mission than donating to SI - through incentive effects alone (not to mention my specific argument that SI's approach to \"Friendliness\" seems likely to increase risks).</p>\n<h1><strong><a name=\"FollowUp\"></a>How I might change my views</strong></h1>\n<p>My views are very open to revision.</p>\n<p>However, I cannot realistically commit to read and seriously consider all comments posted on the matter. The number of people capable of taking a few minutes to write a comment is sufficient to swamp my capacity. I do encourage people to comment and I do intend to read at least some comments, but if you are looking to change my views, you should not consider posting a comment to be the most promising route.</p>\n<p>Instead, what I will commit to is reading and carefully considering <strong>up to 50,000 words of content that are (a) specifically marked as SI-authorized responses to the points I have raised; (b) explicitly cleared for release to the general public as SI-authorized communications.</strong> In order to consider a response \"SI-authorized and cleared for release,\" I will accept explicit communication from SI's Executive Director or from a majority of its Board of Directors endorsing the content in question. After 50,000 words, I may change my views and/or commit to reading more content, or (if I determine that the content is poor and is not using my time efficiently) I may decide not to engage further. SI-authorized content may improve or worsen SI's standing in my estimation, so unlike with comments, there is an incentive to select content that uses my time efficiently. Of course, SI-authorized content may end up including excerpts from comment responses to this post, and/or already-existing public content.</p>\n<p>I may also change my views for other reasons, particularly if SI secures more impressive achievements and/or endorsements.</p>\n<p>One more note: I believe I have read the vast majority of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>, including the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI-foom debate</a>, and that this content - while interesting and enjoyable - does not have much relevance for the arguments I've made.</p>\n<p>Again: I think that whatever happens as a result of my post will be positive for SI's mission, whether or not it is positive for SI as an organization. I believe that most of SI's supporters and advocates care more about the former than about the latter, and that this attitude is far too rare in the nonprofit world.</p>\n<p>\n<h1><strong><a name=\"Acknowledgements\"></a>Acknowledgements</strong></h1>\n<p>\n<p class=\"MsoBodyText2\"><span style=\"background-color: white; background-position: initial initial; background-repeat: initial initial;\">Thanks to the following people for reviewing a draft of this post and providing thoughtful feedback (this of course does not mean they agree with the post or are responsible for its content): Dario Amodei, Nick Beckstead, Elie Hassenfeld, Alexander Kruel, Tim Ogden, John Salvatier, Jonah Sinick, Cari Tuna, Stephanie Wykstra.</span></p>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 2, "NrvXXL3iGjjxu5B7d": 11, "LXk7bxNkYSjgatdAt": 3, "xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6SGqkCgHuNr7d4yJm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 293, "baseScore": 326, "extendedScore": null, "score": 0.000692, "legacy": true, "legacyId": "15976", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 326, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This post presents thoughts on the Singularity Institute from Holden Karnofsky, Co-Executive Director of <a href=\"http://www.givewell.org\">GiveWell</a>. Note: Luke Muehlhauser, the Executive Director of the Singularity Institute, reviewed a draft of this post, and commented: \"I do generally agree that your complaints are either correct (especially re: past organizational competence) or incorrect but not addressed by SI in clear argumentative writing (this includes the part on 'tool' AI). I am working to address both categories of issues.\" I take Luke's comment to be a significant mark in SI's favor, because it indicates an explicit recognition of the problems I raise, and thus increases my estimate of the likelihood that SI will work to address them.</em></p>\n<p><em><strong>September 2012 update:</strong> responses have been posted by <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/\">Luke </a>and <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Eliezer </a>(and I have responded in the comments of their posts). I have also added <a href=\"#Acknowledgements\">acknowledgements</a>.</em></p>\n<p>The <a href=\"http://singinst.org\">Singularity Institute (SI)</a>&nbsp;is&nbsp;a charity that GiveWell has been repeatedly asked to evaluate. In the past, SI has been outside our scope (as we were focused on specific areas such as international aid). With <a href=\"http://givewell.org/about/labs\">GiveWell Labs</a> we are open to any giving opportunity, no matter what form and what sector, but we still do not currently plan to recommend SI; given the amount of interest some of our audience has expressed, I feel it is important to explain why. Our views, of course, remain open to change. (Note: I am posting this only to Less Wrong, not to the GiveWell Blog, because I believe that everyone who would be interested in this post will see it here.)</p>\n<p>I am currently the GiveWell staff member who has put the most time and effort into engaging with and evaluating SI. Other GiveWell staff currently agree with my bottom-line view that we should not recommend SI, but this does not mean they have engaged with each of my specific arguments. Therefore, while the lack of recommendation of SI is something that GiveWell stands behind, the specific arguments in this post should be attributed only to me, not to GiveWell.</p>\n<p><strong id=\"Summary_of_my_views\">Summary of my views</strong></p>\n<ul>\n<li>The argument advanced by SI for why the work it's doing is beneficial and important seems both wrong and poorly argued to me. My sense at the moment is that the arguments SI is making would, if accepted, increase rather than decrease the risk of an AI-related catastrophe. <a href=\"#Arguments\">More</a> </li>\n<li>SI has, or has had, multiple properties that I associate with ineffective organizations, and I do not see any specific evidence that its personnel/organization are well-suited to the tasks it has set for itself. <a href=\"#Organization\">More</a> </li>\n<li>A common argument for giving to SI is that \"even an infinitesimal chance that it is right\" would be sufficient given the stakes. I have written previously about why I reject this reasoning; in addition, prominent SI representatives seem to reject this particular argument as well (i.e., they believe that one should support SI only if one believes it is a strong organization making strong arguments). <a href=\"#SmallProbability\">More</a> </li>\n<li> My sense is that at this point, given SI's current financial state, withholding funds from SI is likely better for its mission than donating to it. (I would not take this view to the furthest extreme; the argument that SI should have <em>some</em> funding seems stronger to me than the argument that it should have as much as it currently has.) </li>\n<li>I find existential risk reduction to be a fairly promising area for philanthropy, and plan to investigate it further. <a href=\"#ExistentialRisk\">More</a> </li>\n<li>There are many things that could happen that would cause me to revise my view on SI. However, I do not plan to respond to all comment responses to this post. (Given the volume of responses we may receive, I may not be able to even read all the comments on this post.) I do not believe these two statements are inconsistent, and I lay out paths for getting me to change my mind that are likely to work better than posting comments. (Of course I encourage people to post comments; I'm just noting in advance that this action, alone, doesn't guarantee that I will consider your argument.) <a href=\"#FollowUp\">More</a></li>\n</ul>\n<p><strong id=\"Intent_of_this_post\">Intent of this post</strong></p>\n<p>I did not write this post with the purpose of \"hurting\" SI. Rather, I wrote it in the hopes that <strong>one of these three things</strong> (or some combination) will happen:</p>\n<ol>\n<li>New arguments are raised that cause me to change my mind and recognize SI as an outstanding giving opportunity. If this happens I will likely attempt to raise more money for SI (most likely by discussing it with other GiveWell staff and collectively considering a <a href=\"http://www.givewell.org/about/labs\">GiveWell Labs</a> recommendation). </li>\n<li>SI concedes that my objections are valid and increases its determination to address them. A few years from now, SI is a better organization and more effective in its mission. </li>\n<li>SI can't or won't make changes, and SI's supporters feel my objections are valid, so SI loses some support, freeing up resources for other approaches to doing good.</li>\n</ol>\n<p>Which one of these occurs will hopefully be driven primarily by the merits of the different arguments raised. Because of this, I think that whatever happens as a result of my post will be positive for SI's mission, whether or not it is positive for SI as an organization. I believe that most of SI's supporters and advocates care more about the former than about the latter, and that this attitude is far too rare in the nonprofit world.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Does_SI_have_a_well_argued_case_that_its_work_is_beneficial_and_important_\"><strong><a name=\"Arguments\"></a><a name=\"Arguments\"></a>Does SI have a well-argued case that its work is beneficial and important?</strong></h1>\n<p>I know no more concise summary of SI's views than <a href=\"http://intelligence.org/summary\">this page</a>, so here I give my own impressions of what SI believes, in italics.<em></em></p>\n<ol><em>\n<li>There is some chance that in the near future (next 20-100 years), an \"artificial general intelligence\" (AGI) - a computer that is vastly more intelligent than humans in every relevant way - will be created. </li>\n<li>This AGI will likely have a utility function and will seek to maximize utility according to this function. </li>\n<li>This AGI will be so much more powerful than humans - due to its superior intelligence - that it will be able to reshape the world to maximize its utility, and humans will not be able to stop it from doing so. </li>\n<li>Therefore, it is crucial that its utility function be one that is reasonably harmonious with what humans want. A \"Friendly\" utility function is one that is reasonably harmonious with what humans want, such that a \"Friendly\" AGI (FAI) would change the world for the better (by human standards) while an \"Unfriendly\" AGI (UFAI) would essentially wipe out humanity (or worse). </li>\n<li>Unless great care is taken specifically to make a utility function \"Friendly,\" it will be \"Unfriendly,\" since the things humans value are a tiny subset of the things that are possible. </li>\n<li>Therefore, it is crucially important to develop \"Friendliness theory\" that helps us to ensure that the first strong AGI's utility function will be \"Friendly.\" The developer of Friendliness theory could use it to build an FAI directly or could disseminate the theory so that others working on AGI are more likely to build FAI as opposed to UFAI.</li>\n</em></ol>\n<p>From the time I first heard this argument, it has seemed to me to be skipping important steps and making major unjustified assumptions. However, for a long time I believed this could easily be due to my inferior understanding of the relevant issues. I believed my own views on the argument to have only very low relevance (as I stated in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a>). Over time, I have had many discussions with SI supporters and advocates, as well as with non-supporters who I believe understand the relevant issues well. I now believe - for the moment - that my objections are highly relevant, that they cannot be dismissed as simple \"layman's misunderstandings\" (as they have been by various SI supporters in the past), and that SI has not published anything that addresses them in a clear way.</p>\n<p>Below, I list my major objections. I do not believe that these objections constitute a sharp/tight case for the idea that SI's work has low/negative value; I believe, instead, that SI's own arguments are too vague for such a rebuttal to be possible. There are many possible responses to my objections, but SI's public arguments (and the private arguments) do not make clear which possible response (if any) SI would choose to take up and defend. Hopefully the dialogue following this post will clarify what SI believes and why.</p>\n<p>Some of my views are discussed at greater length (though with less clarity) in a <a href=\"http://groups.yahoo.com/group/givewell/message/287\">public transcript of a conversation I had with SI supporter Jaan Tallinn</a>. I refer to this transcript as \"Karnofsky/Tallinn 2011.\"</p>\n<h3 id=\"Objection_1__it_seems_to_me_that_any_AGI_that_was_set_to_maximize_a__Friendly__utility_function_would_be_extraordinarily_dangerous_\">Objection 1: it seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</h3>\n<p>Suppose, for the sake of argument, that SI manages to create what it believes to be an FAI. Suppose that it is successful in the \"AGI\" part of its goal, i.e., it has successfully created an intelligence vastly superior to human intelligence and extraordinarily powerful from our perspective. Suppose that it has also done its best on the \"Friendly\" part of the goal: it has developed a formal argument for why its AGI's utility function will be Friendly, it believes this argument to be airtight, and it has had this argument checked over by 100 of the world's most intelligent and relevantly experienced people. Suppose that SI now activates its AGI, unleashing it to reshape the world as it sees fit. What will be the outcome?</p>\n<p>I believe that the probability of an unfavorable outcome - by which I mean an outcome essentially equivalent to what a UFAI would bring about - exceeds 90% in such a scenario. I believe the goal of designing a \"Friendly\" utility function is likely to be beyond the abilities even of the best team of humans willing to design such a function. I do not have a tight argument for why I believe this, but a <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/5986\">comment on LessWrong by Wei Dai</a> gives a good illustration of the kind of thoughts I have on the matter:</p>\n<blockquote>What I'm afraid of is that a design will be shown to be safe, and then it turns out that the proof is wrong, or the formalization of the notion of \"safety\" used by the proof is wrong. This kind of thing happens <em>a lot</em> in cryptography, if you replace \"safety\" with \"security\". These mistakes are still occurring today, even after decades of research into how to do such proofs and what the relevant formalizations are. From where I'm sitting, proving an AGI design Friendly seems even more difficult and error-prone than proving a crypto scheme secure, probably by a large margin, and there is no decades of time to refine the proof techniques and formalizations. There's good recent review of the history of provable security, titled <a href=\"http://www.ibiblio.org/weidai/temp/Provable_Security.pdf\">Provable Security in the Real World</a>, which might help you understand where I'm coming from.</blockquote>\n<p>I think this comment understates the risks, however. For example, when the comment says \"the formalization of the notion of 'safety' used by the proof is wrong,\" it is not clear whether it means that the values the programmers have in mind are not correctly implemented by the formalization, or whether it means they are correctly implemented but <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">are themselves catastrophic in a way that hasn't been anticipated</a>. I would be highly concerned about both. There are other catastrophic possibilities as well; perhaps the utility function itself is well-specified and safe, but the AGI's model of the world is flawed (in particular, perhaps its <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a> or its process for matching observations to predictions are flawed) in a way that doesn't emerge until the AGI has made substantial changes to its environment.</p>\n<p>By SI's own arguments, even a small error in any of these things would likely lead to catastrophe. And there are likely failure forms I haven't thought of. The overriding intuition here is that complex plans usually fail when unaccompanied by feedback loops. A scenario in which a set of people is ready to unleash an all-powerful being to maximize some parameter in the world, based solely on their initial confidence in their own extrapolations of the consequences of doing so, seems like a scenario that is overwhelmingly likely to result in a bad outcome. It comes down to placing the world's largest bet on a highly complex theory - with no experimentation to test the theory first.</p>\n<p>So far, all I have argued is that the development of \"Friendliness\" theory can achieve at best only a limited reduction in the probability of an unfavorable outcome. However, as I argue in the next section, I believe there is at least one concept - the \"tool-agent\" distinction - that has more potential to reduce risks, and that SI appears to ignore this concept entirely. I believe that tools are safer than agents (even agents that make use of the best \"Friendliness\" theory that can reasonably be hoped for) and that SI encourages a focus on building agents, thus increasing risk.</p>\n<h3 id=\"Objection_2__SI_appears_to_neglect_the_potentially_important_distinction_between__tool__and__agent__AI_\">Objection 2: SI appears to neglect the potentially important distinction between \"tool\" and \"agent\" AI.</h3>\n<p>Google Maps is a type of artificial intelligence (AI). It is far more intelligent than I am when it comes to planning routes.</p>\n<p>Google Maps - by which I mean the complete software package including the display of the map itself - does not have a \"utility\" that it seeks to maximize. (One could fit a utility function to its actions, as to any set of actions, but there is no single \"parameter to be maximized\" driving its operations.)</p>\n<p>Google Maps (as I understand it) considers multiple possible routes, gives each a score based on factors such as distance and likely traffic, and then displays the best-scoring route in a way that makes it easily understood by the user. If I don't like the route, for whatever reason, I can change some parameters and consider a different route. If I like the route, I can print it out or email it to a friend or send it to my phone's navigation application. Google Maps has no single parameter it is trying to maximize; it has no reason to try to \"trick\" me in order to increase its utility.</p>\n<p>In short, Google Maps is not an <em>agent</em>, taking actions in order to maximize a utility parameter. It is a <em>tool</em>, generating information and then displaying it in a user-friendly manner for me to consider, use and export or discard as I wish.</p>\n<p>Every software application I know of seems to work essentially the same way, including those that involve (specialized) artificial intelligence such as Google Search, Siri, Watson, Rybka, etc. Some can be put into an \"agent mode\" (as Watson was on Jeopardy!) but all can easily be set up to be used as \"tools\" (for example, Watson can simply display its top candidate answers to a question, with the score for each, without speaking any of them.)</p>\n<p>The \"tool mode\" concept is importantly different from the possibility of <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Oracle AI</a> sometimes discussed by SI. The discussions I've seen of Oracle AI present it as an Unfriendly AI that is \"trapped in a box\" - an AI whose intelligence is driven by an explicit utility function and that humans hope to control coercively. Hence the discussion of ideas such as the <a href=\"http://yudkowsky.net/singularity/aibox\">AI-Box Experiment</a>. A different interpretation, given in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>, is an AI with a carefully designed utility function - likely as difficult to construct as \"Friendliness\" - that leaves it \"wishing\" to answer questions helpfully. By contrast with both these ideas, Tool-AGI is not \"trapped\" and it is not Unfriendly or Friendly; it has no motivations and no driving utility function of any kind, just like Google Maps. It scores different possibilities and displays its conclusions in a transparent and user-friendly manner, as its instructions say to do; it does not have an overarching \"want,\" and so, as with the specialized AIs described above, while it may sometimes \"misinterpret\" a question (thereby scoring options poorly and ranking the wrong one #1) there is no reason to expect intentional trickery or manipulation when it comes to displaying its results.</p>\n<p>Another way of putting this is that a \"tool\" has an underlying instruction set that conceptually looks like: \"(1) Calculate which action A would maximize parameter P, based on existing data set D. (2) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.\" An \"agent,\" by contrast, has an underlying instruction set that conceptually looks like: \"(1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2) Execute Action A.\" In any AI where (1) is separable (by the programmers) as a distinct step, (2) can be set to the \"tool\" version rather than the \"agent\" version, and this separability is in fact present with most/all modern software. Note that in the \"tool\" version, neither step (1) nor step (2) (nor the combination) constitutes an instruction to maximize a parameter - to describe a program of this kind as \"wanting\" something is a category error, and there is no reason to expect its step (2) to be deceptive.</p>\n<p>I elaborated further on the distinction and on the concept of a tool-AI in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>.</p>\n<p>This is important because <strong>an AGI running in tool mode could be extraordinarily useful but far more safe than an AGI running in agent mode</strong>. In fact, if developing \"Friendly AI\" is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on \"Friendliness theory\" moot. Among other things, a tool-AGI would allow transparent views into the AGI's reasoning and predictions without any reason to fear being purposefully misled, and would facilitate safe experimental testing of any utility function that one wished to eventually plug into an \"agent.\"</p>\n<p>Is a tool-AGI possible? I believe that it is, and furthermore that it ought to be our default picture of how AGI will work, given that practically all software developed to date can (and usually does) run as a tool and given that modern software seems to be constantly becoming \"intelligent\" (capable of giving better answers than a human) in surprising new domains. In addition, it intuitively seems to me (though I am not highly confident) that intelligence inherently involves the distinct, separable steps of (a) considering multiple possible actions and (b) assigning a score to each, <em>prior</em> to executing any of the possible actions. If one can distinctly separate (a) and (b) in a program's code, then one can abstain from writing any \"execution\" instructions and instead focus on making the program list actions and scores in a user-friendly manner, for humans to consider and use as they wish.</p>\n<p>Of course, there are possible paths to AGI that may rule out a \"tool mode,\" but it seems that most of these paths would rule out the application of \"Friendliness theory\" as well. (For example, a \"black box\" emulation and augmentation of a human mind.) What are the paths to AGI that allow manual, transparent, intentional design of a utility function but do not allow the replacement of \"execution\" instructions with \"communication\" instructions? Most of the conversations I've had on this topic have focused on three responses:</p>\n<ul>\n<li><strong>Self-improving AI.</strong> Many seem to find it intuitive that (a) AGI will almost certainly come from an AI rewriting its own source code, and (b) such a process would inevitably lead to an \"agent.\" I do not agree with either (a) or (b). I discussed these issues in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a> and will be happy to discuss them more if this is the line of response that SI ends up pursuing. Very briefly: \n<ul>\n<li>The idea of a \"self-improving algorithm\" intuitively sounds very powerful, but does not seem to have led to many \"explosions\" in software so far (and it seems to be a concept that could apply to narrow AI as well as to AGI). </li>\n<li>It seems to me that a tool-AGI could be plugged into a self-improvement process that would be quite powerful but would also terminate and yield a new tool-AI after a set number of iterations (or after reaching a set \"intelligence threshold\"). So I do not accept the argument that \"self-improving AGI means agent AGI.\" As stated above, I will elaborate on this view if it turns out to be an important point of disagreement. </li>\n<li>I have argued (in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>) that the relevant self-improvement abilities are likely to come <em>with</em> or <em>after</em> - not <em>prior to</em> - the development of strong AGI. In other words, any software capable of the relevant kind of self-improvement is likely also capable of being used as a strong tool-AGI, with the benefits described above. </li>\n<li>The SI-related discussions I've seen of \"self-improving AI\" are highly vague, and do not spell out views on the above points. </li>\n</ul>\n</li>\n<li><strong>Dangerous data collection.</strong> Some point to the seeming dangers of a tool-AI's \"scoring\" function: in order to score different options it may have to collect data, which is itself an \"agent\" type action that could lead to dangerous actions. I think my definition of \"tool\" above makes clear what is wrong with this objection: a tool-AGI takes its existing data set D as fixed (and perhaps could have some pre-determined, safe set of simple actions it can take - such as using Google's API - to collect more), and if maximizing its chosen parameter is best accomplished through more data collection, it can transparently output why and how it suggests collecting more data. Over time it can be given more autonomy <em>for data collection</em> through an <em>experimental and domain-specific process</em> (e.g., modifying the AI to skip specific steps of human review of proposals for data collection after it has become clear that these steps work as intended), a process that has little to do with the \"Friendly overarching utility function\" concept promoted by SI. Again, I will elaborate on this if it turns out to be a key point. </li>\n<li><strong>Race for power.</strong> Some have argued to me that humans are likely to <em>choose</em> to create agent-AGI, in order to quickly gain power and outrace other teams working on AGI. But this argument, even if accepted, has very different implications from SI's view.\n<p>Conventional wisdom says it is extremely dangerous to empower a computer to act in the world until one is very sure that the computer will do its job in a way that is helpful rather than harmful. So if a programmer chooses to \"unleash an AGI as an agent\" with the hope of gaining power, it seems that this programmer will be deliberately ignoring conventional wisdom about what is safe in favor of shortsighted greed. I do not see why such a programmer would be expected to make use of any \"Friendliness theory\" that might be available. (Attempting to incorporate such theory would almost certainly slow the project down greatly, and thus would bring the same problems as the more general \"have caution, do testing\" counseled by conventional wisdom.) It seems that the appropriate measures for preventing such a risk are security measures aiming to stop humans from launching unsafe agent-AIs, rather than developing theories or raising awareness of \"Friendliness.\"</p>\n</li>\n</ul>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\" The idea that AGI will be driven by a central utility function seems to be simply assumed. Two examples:</p>\n<ul>\n<li>I have been referred to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser and Salamon 2012</a> as the most up-to-date, clear explanation of SI's position on \"the basics.\" This paper states, \"Perhaps we could build an AI of limited cognitive ability \u2014 say, a machine that only answers questions: an 'Oracle AI.' But this approach is not without its own dangers (Armstrong, Sandberg, and Bostrom 2012).\" However, the referenced paper (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong, Sandberg and Bostrom 2012</a>) seems to take it as a given that an Oracle AI is an \"agent trapped in a box\" - a computer that has a basic drive/utility function, not a Tool-AGI. The rest of Muehlhauser and Salamon 2012 seems to take it as a given that an AGI will be an agent. </li>\n<li>I have often been referred to <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro 2008</a> for an argument that an AGI is likely to have certain goals. But this paper seems, again, to take it as given that an AGI will be an agent, i.e., that it will have goals at all. The introduction states, \"To say that a system of any design is an 'arti\ufb01cial intelligence', we mean that it has goals which it tries to accomplish by acting in the world.\" In other words, the premise I'm disputing seems embedded in its very definition of AI. </li>\n</ul>\n<p>The closest thing I have seen to a public discussion of \"tool-AGI\" is in <a href=\"/lw/tj/dreams_of_friendliness/\">Dreams of Friendliness</a>, where Eliezer Yudkowsky considers the question, \"Why not just have the AI answer questions, instead of trying to <em>do</em> anything? Then it wouldn't need to be Friendly. It wouldn't need any goals at all. It would just answer questions.\" His response:</p>\n<blockquote>To which the reply is that the AI needs goals in order to decide how to think: that is, the AI has to act as a powerful optimization process in order to plan its acquisition of knowledge, effectively distill sensory information, pluck \"answers\" to particular questions out of the space of all possible responses, and of course, to improve its own source code up to the level where the AI is a powerful intelligence. All these events are \"improbable\" relative to random organizations of the AI's RAM, so the AI has to hit a narrow target in the space of possibilities to make superintelligent answers come out.</blockquote>\n<p>This passage appears vague and does not appear to address the specific \"tool\" concept I have defended above (in particular, it does not address the analogy to modern software, which challenges the idea that \"powerful optimization processes\" cannot run in tool mode). The rest of the piece discusses (a) psychological mistakes that could lead to the discussion in question; (b) the \"Oracle AI\" concept that I have outlined above. The comments contain some more discussion of the \"tool\" idea (Denis Bider and Shane Legg seem to be picturing something similar to \"tool-AGI\") but the discussion is unresolved and I believe the \"tool\" concept defended above remains essentially unaddressed.</p>\n<p>In sum, SI appears to encourage a focus on building and launching \"Friendly\" agents (it is seeking to do so itself, and its work on \"Friendliness\" theory seems to be laying the groundwork for others to do so) while not addressing the tool-agent distinction. It seems to assume that any AGI will have to be an agent, and to make little to no attempt at justifying this assumption. The result, in my view, is that it is essentially advocating for a more dangerous approach to AI than the traditional approach to software development.</p>\n<h3 id=\"Objection_3__SI_s_envisioned_scenario_is_far_more_specific_and_conjunctive_than_it_appears_at_first_glance__and_I_believe_this_scenario_to_be_highly_unlikely_\">Objection 3: SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.</h3>\n<p>SI's scenario concerns the development of artificial <em>general</em> intelligence (AGI): a computer that is vastly more intelligent than humans in every relevant way. But we already have many computers that are vastly more intelligent than humans in <em>some</em> relevant ways, and the domains in which specialized AIs outdo humans seem to be constantly and continuously expanding. I feel that the relevance of \"Friendliness theory\" depends heavily on the idea of a \"discrete jump\" that seems unlikely and whose likelihood does not seem to have been publicly argued for.</p>\n<p>One possible scenario is that at some point, we develop powerful enough non-AGI tools (particularly specialized AIs) that we vastly improve our abilities to consider and prepare for the eventuality of AGI - to the point where any previous theory developed on the subject becomes useless. Or (to put this more generally) non-AGI tools simply change the world so much that it becomes essentially unrecognizable from the perspective of today - again rendering any previous \"Friendliness theory\" moot. As I said in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>, some of SI's work \"seems a bit like trying to design Facebook before the Internet was in use, or even before the computer existed.\"</p>\n<p>Perhaps there will be a discrete jump to AGI, but it will be a sort of AGI that renders \"Friendliness theory\" moot for a different reason. For example,&nbsp;<em>in the practice of software development</em>, there often does not seem to be an operational distinction between \"intelligent\" and \"Friendly.\" (For example, my impression is that the only method programmers had for evaluating Watson's \"intelligence\" was to see whether it was coming up with the same answers that a well-informed human would; the only way to evaluate Siri's \"intelligence\" was to evaluate its helpfulness to humans.) \"Intelligent\" often ends up getting defined as \"prone to take actions that seem all-around 'good' to the programmer.\" So the concept of \"Friendliness\" may end up being naturally and subtly baked in to a successful AGI effort.</p>\n<p>The bottom line is that we know very little about the course of future artificial intelligence. I believe that the probability that SI's concept of \"Friendly\" vs. \"Unfriendly\" goals ends up seeming essentially nonsensical, irrelevant and/or unimportant from the standpoint of the relevant future is over 90%.</p>\n<h3 id=\"Other_objections_to_SI_s_views\">Other objections to SI's views</h3>\n<p>There are other debates about the likelihood of SI's work being relevant/helpful; for example,</p>\n<ul>\n<li>It isn't clear whether the development of AGI is imminent enough to be relevant, or whether other risks to humanity are closer. </li>\n<li>It isn't clear whether AGI would be as powerful as SI's views imply. (I discussed this briefly in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011.</a>) </li>\n<li>It isn't clear whether even an extremely powerful UFAI would choose to attack humans as opposed to negotiating with them. (I find it somewhat helpful to analogize UFAI-human interactions to human-mosquito interactions. Humans are enormously more intelligent than mosquitoes; humans are good at predicting, manipulating, and destroying mosquitoes; humans do not value mosquitoes' welfare; humans have other goals that mosquitoes interfere with; humans would like to see mosquitoes eradicated at least from certain parts of the planet. Yet humans haven't accomplished such eradication, and it is easy to imagine scenarios in which humans would prefer honest negotiation and trade with mosquitoes to any other arrangement, if such negotiation and trade were possible.) </li>\n</ul>\n<p>Unlike the three objections I focus on, these other issues have been discussed a fair amount, and if these other issues were the only objections to SI's arguments I would find SI's case to be strong (i.e., I would find its scenario likely <em>enough</em> to warrant investment in).</p>\n<h3 id=\"Wrapup\">Wrapup</h3>\n<ul>\n<li>I believe the most likely future scenarios are the ones we haven't thought of, and that the most likely fate of the sort of theory SI ends up developing is irrelevance. </li>\n<li>I believe that unleashing an all-powerful \"agent AGI\" (without the benefit of experimentation) would very likely result in a UFAI-like outcome, no matter how carefully the \"agent AGI\" was designed to be \"Friendly.\" I see SI as encouraging (and aiming to take) this approach. </li>\n<li>I believe that the standard approach to developing software results in \"tools,\" not \"agents,\" and that tools (while dangerous) are much safer than agents. A \"tool mode\" could facilitate <em>experiment-informed</em> progress toward a safe \"agent,\" rather than needing to get \"Friendliness\" theory right without any experimentation. </li>\n<li>Therefore, I believe that the approach SI advocates and aims to prepare for is far more dangerous than the standard approach, so <em>if</em> SI's work on Friendliness theory affects the risk of human extinction one way or the other, it will increase the risk of human extinction. Fortunately I believe SI's work is far more likely to have no effect one way or the other.</li>\n</ul>\n<p>For a long time I refrained from engaging in object-level debates over SI's work, believing that others are better qualified to do so. But after talking at great length to many of SI's supporters and advocates and reading everything I've been pointed to as relevant, I still have seen no clear and compelling response to any of my three major objections. As stated above, there are many possible responses to my objections, but SI's current arguments do not seem clear on what responses they wish to take and defend. At this point I am unlikely to form a positive view of SI's work until and unless I do see such responses, and/or SI changes its positions.</p>\n<h1 id=\"Is_SI_the_kind_of_organization_we_want_to_bet_on_\"><strong><a name=\"Organization\"></a>Is SI the kind of organization we want to bet on?</strong></h1>\n<p>This part of the post has some risks. For most of GiveWell's history, sticking to our <a href=\"http://givewell.org/international/process/2011#Goalofthereport\">standard criteria</a> - and putting more energy into recommended than non-recommended organizations - has enabled us to share our honest thoughts about charities without appearing to get personal. But when evaluating a group such as SI, I can't avoid placing a heavy weight on (my read on) the general competence, capability and \"intangibles\" of the people and organization, because SI's mission is not about repeating activities that have worked in the past. <strong>Sharing my views on these issues could strike some as personal or mean-spirited and could lead to the misimpression that GiveWell is hostile toward SI. But it is simply necessary in order to be fully transparent about why I hold the views that I hold.</strong></p>\n<p>Fortunately, SI is an ideal organization for our first discussion of this type. I believe the staff and supporters of SI would overwhelmingly rather hear the whole truth about my thoughts - so that they can directly engage them and, if warranted, make changes - than have me sugar-coat what I think in order to spare their feelings. People who know me and <a href=\"http://blog.givewell.org/2007/06/05/an-open-letter-to-crybabies/\">my attitude toward being honest vs. sparing feelings</a> know that this, itself, is high praise for SI.</p>\n<p>One more comment before I continue: our policy is that non-public information provided to us by a charity will not be published or discussed without that charity's prior consent. However, none of the content of this post is based on private information; all of it is based on information that SI has made available to the public.</p>\n<p>There are several reasons that I currently have a negative impression of SI's general competence, capability and \"intangibles.\" My mind remains open and I include specifics on how it could be changed.</p>\n<ul>\n<li><strong>Weak arguments.</strong> SI has produced enormous quantities of public argumentation, and I have examined a very large proportion of this information. Yet I have never seen a clear response to any of the three basic objections I listed in the previous section. One of SI's major goals is to raise awareness of AI-related risks; given this, the fact that it has not advanced clear/concise/compelling arguments speaks, in my view, to its general competence. </li>\n<li><strong>Lack of impressive endorsements.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. I feel that given the enormous implications of SI's claims, if it argued them well it ought to be able to get more impressive endorsements than it has.\n<p>I have been pointed to Peter Thiel and Ray Kurzweil as examples of impressive SI supporters, but I have not seen any on-record statements from either of these people that show agreement with SI's specific views, and in fact (based on watching them speak at Singularity Summits) my impression is that they disagree. Peter Thiel seems to believe that speeding the pace of general innovation is a good thing; this would seem to be in tension with SI's view that AGI will be catastrophic by default and that no one other than SI is paying sufficient attention to \"Friendliness\" issues. Ray Kurzweil seems to believe that \"safety\" is a matter of transparency, strong institutions, etc. rather than of \"Friendliness.\" I am personally in agreement with the things I have seen both of them say on these topics. I find it possible that they support SI because of the Singularity Summit or to increase general interest in ambitious technology, rather than because they find \"Friendliness theory\" to be as important as SI does.</p>\n<p>Clear, on-record statements from these two supporters, specifically endorsing SI's arguments and the importance of developing Friendliness theory, would shift my views somewhat on this point.</p>\n</li>\n<li><strong>Resistance to feedback loops.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. SI seems to have passed up opportunities to test itself and its own rationality by e.g. aiming for objectively impressive accomplishments. This is a problem because of (a) its extremely ambitious goals (among other things, it seeks to develop artificial intelligence <em>and</em> \"Friendliness theory\" before anyone else can develop artificial intelligence); (b) its view of its staff/supporters as having unusual insight into rationality, which I discuss in a later bullet point.\n<p>SI's <a href=\"http://intelligence.org/achievements\">list of achievements</a> is not, in my view, up to where it needs to be given (a) and (b). Yet I have seen no declaration that SI has fallen short to date and explanation of what will be changed to deal with it. SI's recent release of a <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a> and <a href=\"http://intelligence.org/blog/\">monthly updates</a> are improvements from a transparency perspective, but they still leave me feeling as though there are no clear metrics or goals by which SI is committing to be measured (aside from very basic organizational goals such as \"design a new website\" and very vague goals such as \"publish more papers\") and as though SI places a low priority on engaging people who are critical of its views (or at least not yet on board), as opposed to people who are naturally drawn to it.</p>\n<p>I believe that one of the primary obstacles to being impactful as a nonprofit is the lack of the sort of helpful feedback loops that lead to success in other domains. I like to see groups that are making as much effort as they can to create meaningful feedback loops for themselves. I perceive SI as falling well short on this front. Pursuing more impressive endorsements and developing benign but objectively recognizable innovations (particularly commercially viable ones) are two possible ways to impose more demanding feedback loops. (I discussed both of these in my interview linked above).</p>\n</li>\n<li><strong>Apparent poorly grounded belief in SI's superior general rationality.</strong> Many of the things that SI and its supporters and advocates say imply a belief that they have special insights into the nature of general rationality, and/or have superior general rationality, relative to the rest of the population. (Examples <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">here</a>, <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">here</a> and <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">here</a>). My understanding is that SI is in the process of spinning off a group dedicated to training people on how to have higher general rationality.\n<p>Yet I'm not aware of any of what I consider compelling evidence that SI staff/supporters/advocates have any special insight into the nature of general rationality or that they have especially high general rationality.</p>\n<p>I have been pointed to the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a> on this point. The Sequences (which I have read the vast majority of) do not seem to me to be a demonstration or evidence of general rationality. They are <em>about</em> rationality; I find them very enjoyable to read; and there is very little they say that I disagree with (or would have disagreed with before I read them). However, they do not seem to demonstrate rationality on the part of the writer, any more than a series of enjoyable, not-obviously-inaccurate essays on the qualities of a good basketball player would demonstrate basketball prowess. I sometimes get the impression that fans of the Sequences are willing to ascribe superior rationality to the writer simply because the content <em>seems smart and insightful to them</em>, without making a critical effort to determine the extent to which the content is novel, actionable and important.&nbsp;</p>\n<p>I endorse <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Eliezer Yudkowsky's statement</a>, \"Be careful \u2026 any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility.\" To me, the best evidence of superior general rationality (or of insight into it) would be objectively impressive achievements (successful commercial ventures, highly prestigious awards, clear innovations, etc.) and/or accumulation of wealth and power. As mentioned above, SI staff/supporters/advocates do not seem particularly impressive on these fronts, at least not as much as I would expect for people who have the sort of insight into rationality that makes it sensible for them to train others in it. I am open to other evidence that SI staff/supporters/advocates have superior general rationality, but I have not seen it.</p>\n<p>Why is it a problem if SI staff/supporter/advocates believe themselves, without good evidence, to have superior general rationality? First off, it strikes me as a belief based on wishful thinking rather than rational inference. Secondly, I would expect a series of problems to accompany overconfidence in one's general rationality, and several of these problems seem to be actually occurring in SI's case:</p>\n<ul>\n<li>Insufficient self-skepticism given how strong its claims are and how little support its claims have won. Rather than endorsing \"Others have not accepted our arguments, so we will sharpen and/or reexamine our arguments,\" SI seems often to endorse something more like \"Others have not accepted their arguments because they have inferior general rationality,\" a stance less likely to lead to improvement on SI's part. </li>\n<li>Being too selective (in terms of looking for people who share its preconceptions) when determining whom to hire and whose feedback to take seriously. </li>\n<li>Paying insufficient attention to the limitations of the confidence one can have in one's untested theories, in line with my Objection 1.</li>\n</ul>\n</li>\n<li><strong>Overall disconnect between SI's goals and its activities.</strong> SI seeks to build FAI and/or to develop and promote \"Friendliness theory\" that can be useful to others in building FAI. Yet it seems that most of its time goes to activities other than developing AI or theory. Its per-person output in terms of <a href=\"http://intelligence.org/research/publications\">publications</a> seems low. Its core staff seem more focused on <a href=\"http://www.lesswrong.com\">Less Wrong</a> posts, \"rationality training\" and other activities that don't seem connected to the core goals; Eliezer Yudkowsky, in particular, appears (from the <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a>) to be focused on writing books for popular consumption. These activities seem neither to be advancing the state of FAI-related theory nor to be engaging the sort of people most likely to be crucial for building AGI.\n<p>A possible justification for these activities is that SI is seeking to promote greater general rationality, which over time will lead to more and better support for its mission. But if this is SI's core activity, it becomes even more important to test the hypothesis that SI's views are in fact rooted in superior general rationality - and these tests don't seem to be happening, as discussed above.</p>\n</li>\n<li><strong>Theft.</strong> I am bothered by the <a href=\"/lw/5il/siai_an_examination/\">2009 theft of $118,803.00</a> (as against a $541,080.00 budget for the year). In an organization as small as SI, it really seems as though theft that large relative to the budget shouldn't occur and that it represents a major failure of hiring and/or internal controls.\n<p>In addition, I have seen no public SI-authorized discussion of the matter that I consider to be satisfactory in terms of explaining what happened and what the current status of the case is on an ongoing basis. Some details may have to be omitted, but a clear SI-authorized statement on this point with as much information as can reasonably provided would be helpful.</p>\n</li>\n</ul>\n<p>A couple positive observations to add context here:</p>\n<ul>\n<li>I see significant positive qualities in many of the people associated with SI. I especially like what I perceive as their sincere wish to do whatever they can to help the world as much as possible, and the high value they place on being right as opposed to being conventional or polite. I have not interacted with Eliezer Yudkowsky but I greatly enjoy his writings. </li>\n<li>I'm aware that SI has relatively new leadership that is attempting to address the issues behind some of my complaints. I have a generally positive impression of the new leadership; I believe the Executive Director and Development Director, in particular, to represent a step forward in terms of being interested in transparency and in testing their own general rationality. So I will not be surprised if there is some improvement in the coming years, particularly regarding the last couple of statements listed above. That said, SI is an organization and it seems reasonable to judge it by its organizational track record, especially when its new leadership is so new that I have little basis on which to judge these staff.</li>\n</ul>\n<h3 id=\"Wrapup1\">Wrapup</h3>\n<p>While SI has produced a lot of content that I find interesting and enjoyable, it has not produced what I consider evidence of superior general rationality or of its suitability for the tasks it has set for itself. I see no qualifications or achievements that specifically seem to indicate that SI staff are well-suited to the challenge of understanding the key AI-related issues and/or coordinating the construction of an FAI. And I see specific reasons to be pessimistic about its suitability and general competence.</p>\n<p>When estimating the expected value of an endeavor, it is natural to have an implicit \"survivorship bias\" - to use organizations whose accomplishments one is familiar with (which tend to be relatively effective organizations) as a reference class. Because of this, I would be extremely wary of investing in an organization with apparently poor general competence/suitability to its tasks, even if I bought fully into its mission (which I do not) and saw no other groups working on a comparable mission.</p>\n<h1 id=\"But_if_there_s_even_a_chance__\"><strong><a name=\"SmallProbability\"></a>But if there's even a chance \u2026</strong></h1>\n<p>A common argument that SI supporters raise with me is along the lines of, \"Even if SI's arguments are weak and its staff isn't as capable as one would like to see, their goal is so important that they would be a good investment even at a tiny probability of success.\"</p>\n<p>I believe this argument to be a form of <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> and I have outlined the reasons I believe it to be invalid in two posts (<a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">here</a> and <a href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">here</a>). There have been some objections to my arguments, but I still believe them to be valid. There is a good chance I will revisit these topics in the future, because I believe these issues to be at the core of many of the differences between GiveWell-top-charities supporters and SI supporters.</p>\n<p>Regardless of whether one accepts my specific arguments, it is worth noting that the most prominent people associated with SI tend to agree with the <em>conclusion</em> that the \"But if there's even a chance \u2026\" argument is not valid. (See comments on my post from <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4o2z\">Michael Vassar</a> and <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/4nzy\">Eliezer Yudkowsky</a> as well as <a href=\"http://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/#comment-5546\">Eliezer's interview with John Baez</a>.)</p>\n<h1 id=\"Existential_risk_reduction_as_a_cause\"><strong><a name=\"ExistentialRisk\"></a>Existential risk reduction as a cause</strong></h1>\n<p>I consider the general cause of \"looking for ways that philanthropic dollars can reduce direct threats of global catastrophic risks, particularly those that involve some risk of human extinction\" to be a relatively high-potential cause. It is on the <a href=\"http://blog.givewell.org/2012/05/09/givewell-labs-update-and-priority-causes/\">working agenda for GiveWell Labs</a>&nbsp;and we will be writing more about it.</p>\n<p>However, I don't think that \"Cause X is the one I care about and Organization Y is the only one working on it\" to be a good reason to support Organization Y. For donors determined to donate within this cause, I encourage you to consider donating to a donor-advised fund while making it clear that you intend to grant out the funds to existential-risk-reduction-related organizations in the future. (One way to accomplish this would be to create a fund with \"existential risk\" in the name; this is a fairly easy thing to do and one person could do it on behalf of multiple donors.)</p>\n<p>For one who accepts my arguments about SI, I believe withholding funds in this way is likely to be better for SI's mission than donating to SI - through incentive effects alone (not to mention my specific argument that SI's approach to \"Friendliness\" seems likely to increase risks).</p>\n<h1 id=\"How_I_might_change_my_views\"><strong><a name=\"FollowUp\"></a>How I might change my views</strong></h1>\n<p>My views are very open to revision.</p>\n<p>However, I cannot realistically commit to read and seriously consider all comments posted on the matter. The number of people capable of taking a few minutes to write a comment is sufficient to swamp my capacity. I do encourage people to comment and I do intend to read at least some comments, but if you are looking to change my views, you should not consider posting a comment to be the most promising route.</p>\n<p>Instead, what I will commit to is reading and carefully considering <strong>up to 50,000 words of content that are (a) specifically marked as SI-authorized responses to the points I have raised; (b) explicitly cleared for release to the general public as SI-authorized communications.</strong> In order to consider a response \"SI-authorized and cleared for release,\" I will accept explicit communication from SI's Executive Director or from a majority of its Board of Directors endorsing the content in question. After 50,000 words, I may change my views and/or commit to reading more content, or (if I determine that the content is poor and is not using my time efficiently) I may decide not to engage further. SI-authorized content may improve or worsen SI's standing in my estimation, so unlike with comments, there is an incentive to select content that uses my time efficiently. Of course, SI-authorized content may end up including excerpts from comment responses to this post, and/or already-existing public content.</p>\n<p>I may also change my views for other reasons, particularly if SI secures more impressive achievements and/or endorsements.</p>\n<p>One more note: I believe I have read the vast majority of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>, including the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI-foom debate</a>, and that this content - while interesting and enjoyable - does not have much relevance for the arguments I've made.</p>\n<p>Again: I think that whatever happens as a result of my post will be positive for SI's mission, whether or not it is positive for SI as an organization. I believe that most of SI's supporters and advocates care more about the former than about the latter, and that this attitude is far too rare in the nonprofit world.</p>\n<p>\n</p><h1 id=\"Acknowledgements\"><strong><a name=\"Acknowledgements\"></a>Acknowledgements</strong></h1>\n<p>\n</p><p class=\"MsoBodyText2\"><span style=\"background-color: white; background-position: initial initial; background-repeat: initial initial;\">Thanks to the following people for reviewing a draft of this post and providing thoughtful feedback (this of course does not mean they agree with the post or are responsible for its content): Dario Amodei, Nick Beckstead, Elie Hassenfeld, Alexander Kruel, Tim Ogden, John Salvatier, Jonah Sinick, Cari Tuna, Stephanie Wykstra.</span></p>\n<p></p>\n<p></p>", "sections": [{"title": "Summary of my views", "anchor": "Summary_of_my_views", "level": 3}, {"title": "Intent of this post", "anchor": "Intent_of_this_post", "level": 3}, {"title": "Does SI have a well-argued case that its work is beneficial and important?", "anchor": "Does_SI_have_a_well_argued_case_that_its_work_is_beneficial_and_important_", "level": 1}, {"title": "Objection 1: it seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.", "anchor": "Objection_1__it_seems_to_me_that_any_AGI_that_was_set_to_maximize_a__Friendly__utility_function_would_be_extraordinarily_dangerous_", "level": 2}, {"title": "Objection 2: SI appears to neglect the potentially important distinction between \"tool\" and \"agent\" AI.", "anchor": "Objection_2__SI_appears_to_neglect_the_potentially_important_distinction_between__tool__and__agent__AI_", "level": 2}, {"title": "Objection 3: SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.", "anchor": "Objection_3__SI_s_envisioned_scenario_is_far_more_specific_and_conjunctive_than_it_appears_at_first_glance__and_I_believe_this_scenario_to_be_highly_unlikely_", "level": 2}, {"title": "Other objections to SI's views", "anchor": "Other_objections_to_SI_s_views", "level": 2}, {"title": "Wrapup", "anchor": "Wrapup", "level": 2}, {"title": "Is SI the kind of organization we want to bet on?", "anchor": "Is_SI_the_kind_of_organization_we_want_to_bet_on_", "level": 1}, {"title": "Wrapup", "anchor": "Wrapup1", "level": 2}, {"title": "But if there's even a chance \u2026", "anchor": "But_if_there_s_even_a_chance__", "level": 1}, {"title": "Existential risk reduction as a cause", "anchor": "Existential_risk_reduction_as_a_cause", "level": 1}, {"title": "How I might change my views", "anchor": "How_I_might_change_my_views", "level": 1}, {"title": "Acknowledgements", "anchor": "Acknowledgements", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1273 comments"}], "headingsCount": 16}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1287, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5GskScdvYXBpL78wL", "sizjfDgCgAsuLJQmm", "4ARaTpNX62uaL86j6", "wKnwcjJGriTS9QxxL", "4PPE6D635iBcGPGRy", "gfexKxsBDM6v2sCMo", "fkhbBE2ZTSytvsy9x", "6ddcsdA2c2XpNpE5x", "qqhdj3W3vSfB5E9ss", "a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 17, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T04:52:44.518Z", "modifiedAt": null, "url": null, "title": "Brain Preservation Foundation [link]", "slug": "brain-preservation-foundation-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.646Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qZsTEownX2AH9FMvg/brain-preservation-foundation-link", "pageUrlRelative": "/posts/qZsTEownX2AH9FMvg/brain-preservation-foundation-link", "linkUrl": "https://www.lesswrong.com/posts/qZsTEownX2AH9FMvg/brain-preservation-foundation-link", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain%20Preservation%20Foundation%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain%20Preservation%20Foundation%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZsTEownX2AH9FMvg%2Fbrain-preservation-foundation-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain%20Preservation%20Foundation%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZsTEownX2AH9FMvg%2Fbrain-preservation-foundation-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZsTEownX2AH9FMvg%2Fbrain-preservation-foundation-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>An organization calling itself the <a href=\"http://www.brainpreservation.org/\">Brain Preservation Foundation</a> is offering a cash prize, currently worth about $100,000, to anyone who can demonstrate the successful preservation of the connectome of a large mammalian brain. They also have an impressive web page. Does anyone here know anything else about them? Might they be worth donating to?</p>\n<p>(Via <a href=\"http://davidbrin.blogspot.com/2012/05/challenges-for-future-generations-space.html\">David Brin</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qZsTEownX2AH9FMvg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 9.004526011645472e-07, "legacy": true, "legacyId": "15996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T07:14:03.944Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: Harry Potter and the Methods of Rationality", "slug": "meetup-pittsburgh-harry-potter-and-the-methods-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eJEv2SrrtHBxY7H4Z/meetup-pittsburgh-harry-potter-and-the-methods-of", "pageUrlRelative": "/posts/eJEv2SrrtHBxY7H4Z/meetup-pittsburgh-harry-potter-and-the-methods-of", "linkUrl": "https://www.lesswrong.com/posts/eJEv2SrrtHBxY7H4Z/meetup-pittsburgh-harry-potter-and-the-methods-of", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJEv2SrrtHBxY7H4Z%2Fmeetup-pittsburgh-harry-potter-and-the-methods-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJEv2SrrtHBxY7H4Z%2Fmeetup-pittsburgh-harry-potter-and-the-methods-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJEv2SrrtHBxY7H4Z%2Fmeetup-pittsburgh-harry-potter-and-the-methods-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a2'>Pittsburgh: Harry Potter and the Methods of Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cafe Phipps, 1 Schenley Park, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In time for happy hour and/or dinner time at Cafe Phipps. Phone 412-304-6258 if you can't find us. Maybe we're on the grass nearby, if it's too busy inside.</p>\n\n<p>Note that the marking on the map below does not match the territory. Phipps Cafe is in fact further North, probably about where it says Phipps Cafe on the map in small, less visible letters.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a2'>Pittsburgh: Harry Potter and the Methods of Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eJEv2SrrtHBxY7H4Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.0051427191549e-07, "legacy": true, "legacyId": "16003", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Harry_Potter_and_the_Methods_of_Rationality\">Discussion article for the meetup : <a href=\"/meetups/a2\">Pittsburgh: Harry Potter and the Methods of Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cafe Phipps, 1 Schenley Park, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In time for happy hour and/or dinner time at Cafe Phipps. Phone 412-304-6258 if you can't find us. Maybe we're on the grass nearby, if it's too busy inside.</p>\n\n<p>Note that the marking on the map below does not match the territory. Phipps Cafe is in fact further North, probably about where it says Phipps Cafe on the map in small, less visible letters.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Harry_Potter_and_the_Methods_of_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/a2\">Pittsburgh: Harry Potter and the Methods of Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: Harry Potter and the Methods of Rationality", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Harry_Potter_and_the_Methods_of_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: Harry Potter and the Methods of Rationality", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Harry_Potter_and_the_Methods_of_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T07:16:29.696Z", "modifiedAt": null, "url": null, "title": "Holden Karnofsky's Singularity Institute Objection 1", "slug": "holden-karnofsky-s-singularity-institute-objection-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:31.255Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4iqhkNv7woSnkpMRa/holden-karnofsky-s-singularity-institute-objection-1", "pageUrlRelative": "/posts/4iqhkNv7woSnkpMRa/holden-karnofsky-s-singularity-institute-objection-1", "linkUrl": "https://www.lesswrong.com/posts/4iqhkNv7woSnkpMRa/holden-karnofsky-s-singularity-institute-objection-1", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holden%20Karnofsky's%20Singularity%20Institute%20Objection%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolden%20Karnofsky's%20Singularity%20Institute%20Objection%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4iqhkNv7woSnkpMRa%2Fholden-karnofsky-s-singularity-institute-objection-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holden%20Karnofsky's%20Singularity%20Institute%20Objection%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4iqhkNv7woSnkpMRa%2Fholden-karnofsky-s-singularity-institute-objection-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4iqhkNv7woSnkpMRa%2Fholden-karnofsky-s-singularity-institute-objection-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 781, "htmlBody": "<p>The sheer length of GiveWell co-founder and co-executive director Holden Karnofsky's <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">excellent critique of the Singularity Institute</a> means that it's hard to keep track of the resulting discussion. &nbsp;I propose to break out each of his objections into a separate Discussion post so that each receives the attention it deserves.</p>\n<h3>Objection 1: it seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</h3>\n<p>Suppose, for the sake of argument, that SI manages to create what it believes to be an FAI. Suppose that it is successful in the \"AGI\" part of its goal, i.e., it has successfully created an intelligence vastly superior to human intelligence and extraordinarily powerful from our perspective. Suppose that it has also done its best on the \"Friendly\" part of the goal: it has developed a formal argument for why its AGI's utility function will be Friendly, it believes this argument to be airtight, and it has had this argument checked over by 100 of the world's most intelligent and relevantly experienced people. Suppose that SI now activates its AGI, unleashing it to reshape the world as it sees fit. What will be the outcome?</p>\n<p>I believe that the probability of an unfavorable outcome - by which I mean an outcome essentially equivalent to what a UFAI would bring about - exceeds 90% in such a scenario. I believe the goal of designing a \"Friendly\" utility function is likely to be beyond the abilities even of the best team of humans willing to design such a function. I do not have a tight argument for why I believe this, but a <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/5986\">comment on LessWrong by Wei Dai</a> gives a good illustration of the kind of thoughts I have on the matter:</p>\n<blockquote>What I'm afraid of is that a design will be shown to be safe, and then it turns out that the proof is wrong, or the formalization of the notion of \"safety\" used by the proof is wrong. This kind of thing happens <em>a lot</em> in cryptography, if you replace \"safety\" with \"security\". These mistakes are still occurring today, even after decades of research into how to do such proofs and what the relevant formalizations are. From where I'm sitting, proving an AGI design Friendly seems even more difficult and error-prone than proving a crypto scheme secure, probably by a large margin, and there is no decades of time to refine the proof techniques and formalizations. There's good recent review of the history of provable security, titled <a href=\"http://www.ibiblio.org/weidai/temp/Provable_Security.pdf\">Provable Security in the Real World</a>, which might help you understand where I'm coming from.</blockquote>\n<p>I think this comment understates the risks, however. For example, when the comment says \"the formalization of the notion of 'safety' used by the proof is wrong,\" it is not clear whether it means that the values the programmers have in mind are not correctly implemented by the formalization, or whether it means they are correctly implemented but <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">are themselves catastrophic in a way that hasn't been anticipated</a>. I would be highly concerned about both. There are other catastrophic possibilities as well; perhaps the utility function itself is well-specified and safe, but the AGI's model of the world is flawed (in particular, perhaps its <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a> or its process for matching observations to predictions are flawed) in a way that doesn't emerge until the AGI has made substantial changes to its environment.</p>\n<p>By SI's own arguments, even a small error in any of these things would likely lead to catastrophe. And there are likely failure forms I haven't thought of. The overriding intuition here is that complex plans usually fail when unaccompanied by feedback loops. A scenario in which a set of people is ready to unleash an all-powerful being to maximize some parameter in the world, based solely on their initial confidence in their own extrapolations of the consequences of doing so, seems like a scenario that is overwhelmingly likely to result in a bad outcome. It comes down to placing the world's largest bet on a highly complex theory - with no experimentation to test the theory first.</p>\n<p>So far, all I have argued is that the development of \"Friendliness\" theory can achieve at best only a limited reduction in the probability of an unfavorable outcome. However, as I argue in the next section, I believe there is at least one concept - the \"tool-agent\" distinction - that has more potential to reduce risks, and that SI appears to ignore this concept entirely. I believe that tools are safer than agents (even agents that make use of the best \"Friendliness\" theory that can reasonably be hoped for) and that SI encourages a focus on building agents, thus increasing risk.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4iqhkNv7woSnkpMRa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 9.005151329164463e-07, "legacy": true, "legacyId": "16004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "4ARaTpNX62uaL86j6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T07:18:05.379Z", "modifiedAt": null, "url": null, "title": "Holden Karnofsky's Singularity Institute Objection 2", "slug": "holden-karnofsky-s-singularity-institute-objection-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TJoPCusrS4cNMgy3n/holden-karnofsky-s-singularity-institute-objection-2", "pageUrlRelative": "/posts/TJoPCusrS4cNMgy3n/holden-karnofsky-s-singularity-institute-objection-2", "linkUrl": "https://www.lesswrong.com/posts/TJoPCusrS4cNMgy3n/holden-karnofsky-s-singularity-institute-objection-2", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holden%20Karnofsky's%20Singularity%20Institute%20Objection%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolden%20Karnofsky's%20Singularity%20Institute%20Objection%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTJoPCusrS4cNMgy3n%2Fholden-karnofsky-s-singularity-institute-objection-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holden%20Karnofsky's%20Singularity%20Institute%20Objection%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTJoPCusrS4cNMgy3n%2Fholden-karnofsky-s-singularity-institute-objection-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTJoPCusrS4cNMgy3n%2Fholden-karnofsky-s-singularity-institute-objection-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2434, "htmlBody": "<p>The sheer length of GiveWell co-founder and co-executive director Holden Karnofsky's&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">excellent critique of the Singularity Institute</a>&nbsp;means that it's hard to keep track of the resulting discussion. &nbsp;I propose to break out each of his objections into a separate Discussion post so that each receives the attention it deserves.</p>\n<h3>Objection 2: SI appears to neglect the potentially important distinction between \"tool\" and \"agent\" AI.</h3>\n<p>Google Maps is a type of artificial intelligence (AI). It is far more intelligent than I am when it comes to planning routes.</p>\n<p>Google Maps - by which I mean the complete software package including the display of the map itself - does not have a \"utility\" that it seeks to maximize. (One could fit a utility function to its actions, as to any set of actions, but there is no single \"parameter to be maximized\" driving its operations.)</p>\n<p>Google Maps (as I understand it) considers multiple possible routes, gives each a score based on factors such as distance and likely traffic, and then displays the best-scoring route in a way that makes it easily understood by the user. If I don't like the route, for whatever reason, I can change some parameters and consider a different route. If I like the route, I can print it out or email it to a friend or send it to my phone's navigation application. Google Maps has no single parameter it is trying to maximize; it has no reason to try to \"trick\" me in order to increase its utility.</p>\n<p>In short, Google Maps is not an <em>agent</em>, taking actions in order to maximize a utility parameter. It is a <em>tool</em>, generating information and then displaying it in a user-friendly manner for me to consider, use and export or discard as I wish.</p>\n<p>Every software application I know of seems to work essentially the same way, including those that involve (specialized) artificial intelligence such as Google Search, Siri, Watson, Rybka, etc. Some can be put into an \"agent mode\" (as Watson was on Jeopardy!) but all can easily be set up to be used as \"tools\" (for example, Watson can simply display its top candidate answers to a question, with the score for each, without speaking any of them.)</p>\n<p>The \"tool mode\" concept is importantly different from the possibility of <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Oracle AI</a> sometimes discussed by SI. The discussions I've seen of Oracle AI present it as an Unfriendly AI that is \"trapped in a box\" - an AI whose intelligence is driven by an explicit utility function and that humans hope to control coercively. Hence the discussion of ideas such as the <a href=\"http://yudkowsky.net/singularity/aibox\">AI-Box Experiment</a>. A different interpretation, given in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>, is an AI with a carefully designed utility function - likely as difficult to construct as \"Friendliness\" - that leaves it \"wishing\" to answer questions helpfully. By contrast with both these ideas, Tool-AGI is not \"trapped\" and it is not Unfriendly or Friendly; it has no motivations and no driving utility function of any kind, just like Google Maps. It scores different possibilities and displays its conclusions in a transparent and user-friendly manner, as its instructions say to do; it does not have an overarching \"want,\" and so, as with the specialized AIs described above, while it may sometimes \"misinterpret\" a question (thereby scoring options poorly and ranking the wrong one #1) there is no reason to expect intentional trickery or manipulation when it comes to displaying its results.</p>\n<p>Another way of putting this is that a \"tool\" has an underlying instruction set that conceptually looks like: \"(1) Calculate which action A would maximize parameter P, based on existing data set D. (2) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.\" An \"agent,\" by contrast, has an underlying instruction set that conceptually looks like: \"(1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2) Execute Action A.\" In any AI where (1) is separable (by the programmers) as a distinct step, (2) can be set to the \"tool\" version rather than the \"agent\" version, and this separability is in fact present with most/all modern software. Note that in the \"tool\" version, neither step (1) nor step (2) (nor the combination) constitutes an instruction to maximize a parameter - to describe a program of this kind as \"wanting\" something is a category error, and there is no reason to expect its step (2) to be deceptive.</p>\n<p>I elaborated further on the distinction and on the concept of a tool-AI in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>.</p>\n<p>This is important because <strong>an AGI running in tool mode could be extraordinarily useful but far more safe than an AGI running in agent mode</strong>. In fact, if developing \"Friendly AI\" is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on \"Friendliness theory\" moot. Among other things, a tool-AGI would allow transparent views into the AGI's reasoning and predictions without any reason to fear being purposefully misled, and would facilitate safe experimental testing of any utility function that one wished to eventually plug into an \"agent.\"</p>\n<p>Is a tool-AGI possible? I believe that it is, and furthermore that it ought to be our default picture of how AGI will work, given that practically all software developed to date can (and usually does) run as a tool and given that modern software seems to be constantly becoming \"intelligent\" (capable of giving better answers than a human) in surprising new domains. In addition, it intuitively seems to me (though I am not highly confident) that intelligence inherently involves the distinct, separable steps of (a) considering multiple possible actions and (b) assigning a score to each, <em>prior</em> to executing any of the possible actions. If one can distinctly separate (a) and (b) in a program's code, then one can abstain from writing any \"execution\" instructions and instead focus on making the program list actions and scores in a user-friendly manner, for humans to consider and use as they wish.</p>\n<p>Of course, there are possible paths to AGI that may rule out a \"tool mode,\" but it seems that most of these paths would rule out the application of \"Friendliness theory\" as well. (For example, a \"black box\" emulation and augmentation of a human mind.) What are the paths to AGI that allow manual, transparent, intentional design of a utility function but do not allow the replacement of \"execution\" instructions with \"communication\" instructions? Most of the conversations I've had on this topic have focused on three responses:</p>\n<ul>\n<li><strong>Self-improving AI.</strong> Many seem to find it intuitive that (a) AGI will almost certainly come from an AI rewriting its own source code, and (b) such a process would inevitably lead to an \"agent.\" I do not agree with either (a) or (b). I discussed these issues in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a> and will be happy to discuss them more if this is the line of response that SI ends up pursuing. Very briefly: \n<ul>\n<li>The idea of a \"self-improving algorithm\" intuitively sounds very powerful, but does not seem to have led to many \"explosions\" in software so far (and it seems to be a concept that could apply to narrow AI as well as to AGI). </li>\n<li>It seems to me that a tool-AGI could be plugged into a self-improvement process that would be quite powerful but would also terminate and yield a new tool-AI after a set number of iterations (or after reaching a set \"intelligence threshold\"). So I do not accept the argument that \"self-improving AGI means agent AGI.\" As stated above, I will elaborate on this view if it turns out to be an important point of disagreement. </li>\n<li>I have argued (in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>) that the relevant self-improvement abilities are likely to come <em>with</em> or <em>after</em> - not <em>prior to</em> - the development of strong AGI. In other words, any software capable of the relevant kind of self-improvement is likely also capable of being used as a strong tool-AGI, with the benefits described above. </li>\n<li>The SI-related discussions I've seen of \"self-improving AI\" are highly vague, and do not spell out views on the above points. </li>\n</ul>\n</li>\n<li><strong>Dangerous data collection.</strong> Some point to the seeming dangers of a tool-AI's \"scoring\" function: in order to score different options it may have to collect data, which is itself an \"agent\" type action that could lead to dangerous actions. I think my definition of \"tool\" above makes clear what is wrong with this objection: a tool-AGI takes its existing data set D as fixed (and perhaps could have some pre-determined, safe set of simple actions it can take - such as using Google's API - to collect more), and if maximizing its chosen parameter is best accomplished through more data collection, it can transparently output why and how it suggests collecting more data. Over time it can be given more autonomy <em>for data collection</em> through an <em>experimental and domain-specific process</em> (e.g., modifying the AI to skip specific steps of human review of proposals for data collection after it has become clear that these steps work as intended), a process that has little to do with the \"Friendly overarching utility function\" concept promoted by SI. Again, I will elaborate on this if it turns out to be a key point. </li>\n<li><strong>Race for power.</strong> Some have argued to me that humans are likely to <em>choose</em> to create agent-AGI, in order to quickly gain power and outrace other teams working on AGI. But this argument, even if accepted, has very different implications from SI's view.\n<p>Conventional wisdom says it is extremely dangerous to empower a computer to act in the world until one is very sure that the computer will do its job in a way that is helpful rather than harmful. So if a programmer chooses to \"unleash an AGI as an agent\" with the hope of gaining power, it seems that this programmer will be deliberately ignoring conventional wisdom about what is safe in favor of shortsighted greed. I do not see why such a programmer would be expected to make use of any \"Friendliness theory\" that might be available. (Attempting to incorporate such theory would almost certainly slow the project down greatly, and thus would bring the same problems as the more general \"have caution, do testing\" counseled by conventional wisdom.) It seems that the appropriate measures for preventing such a risk are security measures aiming to stop humans from launching unsafe agent-AIs, rather than developing theories or raising awareness of \"Friendliness.\"</p>\n</li>\n</ul>\n<p>One of the things that bothers me most about SI is that there is practically no public content, as far as I can tell, explicitly addressing the idea of a \"tool\" and giving arguments for why AGI is likely to work only as an \"agent.\" The idea that AGI will be driven by a central utility function seems to be simply assumed. Two examples:</p>\n<ul>\n<li>I have been referred to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser and Salamon 2012</a> as the most up-to-date, clear explanation of SI's position on \"the basics.\" This paper states, \"Perhaps we could build an AI of limited cognitive ability &mdash; say, a machine that only answers questions: an 'Oracle AI.' But this approach is not without its own dangers (Armstrong, Sandberg, and Bostrom 2012).\" However, the referenced paper (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong, Sandberg and Bostrom 2012</a>) seems to take it as a given that an Oracle AI is an \"agent trapped in a box\" - a computer that has a basic drive/utility function, not a Tool-AGI. The rest of Muehlhauser and Salamon 2012 seems to take it as a given that an AGI will be an agent. </li>\n<li>I have often been referred to <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro 2008</a> for an argument that an AGI is likely to have certain goals. But this paper seems, again, to take it as given that an AGI will be an agent, i.e., that it will have goals at all. The introduction states, \"To say that a system of any design is an 'arti\ufb01cial intelligence', we mean that it has goals which it tries to accomplish by acting in the world.\" In other words, the premise I'm disputing seems embedded in its very definition of AI. </li>\n</ul>\n<p>The closest thing I have seen to a public discussion of \"tool-AGI\" is in <a href=\"/lw/tj/dreams_of_friendliness/\">Dreams of Friendliness</a>, where Eliezer Yudkowsky considers the question, \"Why not just have the AI answer questions, instead of trying to <em>do</em> anything? Then it wouldn't need to be Friendly. It wouldn't need any goals at all. It would just answer questions.\" His response:</p>\n<blockquote>To which the reply is that the AI needs goals in order to decide how to think: that is, the AI has to act as a powerful optimization process in order to plan its acquisition of knowledge, effectively distill sensory information, pluck \"answers\" to particular questions out of the space of all possible responses, and of course, to improve its own source code up to the level where the AI is a powerful intelligence. All these events are \"improbable\" relative to random organizations of the AI's RAM, so the AI has to hit a narrow target in the space of possibilities to make superintelligent answers come out.</blockquote>\n<p>This passage appears vague and does not appear to address the specific \"tool\" concept I have defended above (in particular, it does not address the analogy to modern software, which challenges the idea that \"powerful optimization processes\" cannot run in tool mode). The rest of the piece discusses (a) psychological mistakes that could lead to the discussion in question; (b) the \"Oracle AI\" concept that I have outlined above. The comments contain some more discussion of the \"tool\" idea (Denis Bider and Shane Legg seem to be picturing something similar to \"tool-AGI\") but the discussion is unresolved and I believe the \"tool\" concept defended above remains essentially unaddressed.</p>\n<p>In sum, SI appears to encourage a focus on building and launching \"Friendly\" agents (it is seeking to do so itself, and its work on \"Friendliness\" theory seems to be laying the groundwork for others to do so) while not addressing the tool-agent distinction. It seems to assume that any AGI will have to be an agent, and to make little to no attempt at justifying this assumption. The result, in my view, is that it is essentially advocating for a more dangerous approach to AI than the traditional approach to software development.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TJoPCusrS4cNMgy3n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 9.005160279586547e-07, "legacy": true, "legacyId": "16005", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "wKnwcjJGriTS9QxxL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T07:19:18.688Z", "modifiedAt": null, "url": null, "title": "Holden Karnofsky's Singularity Institute Objection 3", "slug": "holden-karnofsky-s-singularity-institute-objection-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QWuxwTr7EJHQQ2ABn/holden-karnofsky-s-singularity-institute-objection-3", "pageUrlRelative": "/posts/QWuxwTr7EJHQQ2ABn/holden-karnofsky-s-singularity-institute-objection-3", "linkUrl": "https://www.lesswrong.com/posts/QWuxwTr7EJHQQ2ABn/holden-karnofsky-s-singularity-institute-objection-3", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holden%20Karnofsky's%20Singularity%20Institute%20Objection%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolden%20Karnofsky's%20Singularity%20Institute%20Objection%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQWuxwTr7EJHQQ2ABn%2Fholden-karnofsky-s-singularity-institute-objection-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holden%20Karnofsky's%20Singularity%20Institute%20Objection%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQWuxwTr7EJHQQ2ABn%2Fholden-karnofsky-s-singularity-institute-objection-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQWuxwTr7EJHQQ2ABn%2Fholden-karnofsky-s-singularity-institute-objection-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 443, "htmlBody": "<p>The sheer length of GiveWell co-founder and co-executive director Holden Karnofsky's&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">excellent critique of the Singularity Institute</a>&nbsp;means that it's hard to keep track of the resulting discussion. &nbsp;I propose to break out each of his objections into a separate Discussion post so that each receives the attention it deserves.</p>\n<h3>Objection 3: SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.</h3>\n<p>SI's scenario concerns the development of artificial <em>general</em> intelligence (AGI): a computer that is vastly more intelligent than humans in every relevant way. But we already have many computers that are vastly more intelligent than humans in <em>some</em> relevant ways, and the domains in which specialized AIs outdo humans seem to be constantly and continuously expanding. I feel that the relevance of \"Friendliness theory\" depends heavily on the idea of a \"discrete jump\" that seems unlikely and whose likelihood does not seem to have been publicly argued for.</p>\n<p>One possible scenario is that at some point, we develop powerful enough non-AGI tools (particularly specialized AIs) that we vastly improve our abilities to consider and prepare for the eventuality of AGI - to the point where any previous theory developed on the subject becomes useless. Or (to put this more generally) non-AGI tools simply change the world so much that it becomes essentially unrecognizable from the perspective of today - again rendering any previous \"Friendliness theory\" moot. As I said in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011</a>, some of SI's work \"seems a bit like trying to design Facebook before the Internet was in use, or even before the computer existed.\"</p>\n<p>Perhaps there will be a discrete jump to AGI, but it will be a sort of AGI that renders \"Friendliness theory\" moot for a different reason. For example,&nbsp;<em>in the practice of software development</em>, there often does not seem to be an operational distinction between \"intelligent\" and \"Friendly.\" (For example, my impression is that the only method programmers had for evaluating Watson's \"intelligence\" was to see whether it was coming up with the same answers that a well-informed human would; the only way to evaluate Siri's \"intelligence\" was to evaluate its helpfulness to humans.) \"Intelligent\" often ends up getting defined as \"prone to take actions that seem all-around 'good' to the programmer.\" So the concept of \"Friendliness\" may end up being naturally and subtly baked in to a successful AGI effort.</p>\n<p>The bottom line is that we know very little about the course of future artificial intelligence. I believe that the probability that SI's concept of \"Friendly\" vs. \"Unfriendly\" goals ends up seeming essentially nonsensical, irrelevant and/or unimportant from the standpoint of the relevant future is over 90%.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QWuxwTr7EJHQQ2ABn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 9.005165611618e-07, "legacy": true, "legacyId": "16006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T07:22:13.699Z", "modifiedAt": null, "url": null, "title": "Holden Karnofsky's Singularity Institute critique: other objections", "slug": "holden-karnofsky-s-singularity-institute-critique-other", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:33.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zCYSFgvaiyo33Hqm3/holden-karnofsky-s-singularity-institute-critique-other", "pageUrlRelative": "/posts/zCYSFgvaiyo33Hqm3/holden-karnofsky-s-singularity-institute-critique-other", "linkUrl": "https://www.lesswrong.com/posts/zCYSFgvaiyo33Hqm3/holden-karnofsky-s-singularity-institute-critique-other", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holden%20Karnofsky's%20Singularity%20Institute%20critique%3A%20other%20objections&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolden%20Karnofsky's%20Singularity%20Institute%20critique%3A%20other%20objections%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCYSFgvaiyo33Hqm3%2Fholden-karnofsky-s-singularity-institute-critique-other%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holden%20Karnofsky's%20Singularity%20Institute%20critique%3A%20other%20objections%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCYSFgvaiyo33Hqm3%2Fholden-karnofsky-s-singularity-institute-critique-other", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCYSFgvaiyo33Hqm3%2Fholden-karnofsky-s-singularity-institute-critique-other", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 273, "htmlBody": "<p>The sheer length of GiveWell co-founder and co-executive director Holden Karnofsky's&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">excellent critique of the Singularity Institute</a>&nbsp;means that it's hard to keep track of the resulting discussion. &nbsp;I propose to break out each of his objections into a separate Discussion post so that each receives the attention it deserves.</p>\n<h3>Other objections to SI's views</h3>\n<p>There are other debates about the likelihood of SI's work being relevant/helpful; for example,</p>\n<ul>\n<li>It isn't clear whether the development of AGI is imminent enough to be relevant, or whether other risks to humanity are closer. </li>\n<li>It isn't clear whether AGI would be as powerful as SI's views imply. (I discussed this briefly in <a href=\"http://groups.yahoo.com/group/givewell/message/287\">Karnofsky/Tallinn 2011.</a>) </li>\n<li>It isn't clear whether even an extremely powerful UFAI would choose to attack humans as opposed to negotiating with them. (I find it somewhat helpful to analogize UFAI-human interactions to human-mosquito interactions. Humans are enormously more intelligent than mosquitoes; humans are good at predicting, manipulating, and destroying mosquitoes; humans do not value mosquitoes' welfare; humans have other goals that mosquitoes interfere with; humans would like to see mosquitoes eradicated at least from certain parts of the planet. Yet humans haven't accomplished such eradication, and it is easy to imagine scenarios in which humans would prefer honest negotiation and trade with mosquitoes to any other arrangement, if such negotiation and trade were possible.) </li>\n</ul>\n<p>Unlike the three objections I focus on, these other issues have been discussed a fair amount, and if these other issues were the only objections to SI's arguments I would find SI's case to be strong (i.e., I would find its scenario likely <em>enough</em> to warrant investment in).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zCYSFgvaiyo33Hqm3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 9.005178340931044e-07, "legacy": true, "legacyId": "16007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T07:25:56.637Z", "modifiedAt": null, "url": null, "title": "Holden Karnofsky's Singularity Institute critique: Is SI the kind of organization we want to bet on?", "slug": "holden-karnofsky-s-singularity-institute-critique-is-si-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SWEGQmqnfyXc7i2rJ/holden-karnofsky-s-singularity-institute-critique-is-si-the", "pageUrlRelative": "/posts/SWEGQmqnfyXc7i2rJ/holden-karnofsky-s-singularity-institute-critique-is-si-the", "linkUrl": "https://www.lesswrong.com/posts/SWEGQmqnfyXc7i2rJ/holden-karnofsky-s-singularity-institute-critique-is-si-the", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holden%20Karnofsky's%20Singularity%20Institute%20critique%3A%20Is%20SI%20the%20kind%20of%20organization%20we%20want%20to%20bet%20on%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolden%20Karnofsky's%20Singularity%20Institute%20critique%3A%20Is%20SI%20the%20kind%20of%20organization%20we%20want%20to%20bet%20on%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWEGQmqnfyXc7i2rJ%2Fholden-karnofsky-s-singularity-institute-critique-is-si-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holden%20Karnofsky's%20Singularity%20Institute%20critique%3A%20Is%20SI%20the%20kind%20of%20organization%20we%20want%20to%20bet%20on%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWEGQmqnfyXc7i2rJ%2Fholden-karnofsky-s-singularity-institute-critique-is-si-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWEGQmqnfyXc7i2rJ%2Fholden-karnofsky-s-singularity-institute-critique-is-si-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2357, "htmlBody": "<p>The sheer length of GiveWell co-founder and co-executive director Holden Karnofsky's&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">excellent critique of the Singularity Institute</a>&nbsp;means that it's hard to keep track of the resulting discussion. &nbsp;I propose to break out each of his objections into a separate Discussion post so that each receives the attention it deserves.</p>\n<h1><strong><a name=\"Organization\"></a>Is SI the kind of organization we want to bet on?</strong></h1>\n<p>This part of the post has some risks. For most of GiveWell's history, sticking to our <a href=\"http://givewell.org/international/process/2011#Goalofthereport\">standard criteria</a> - and putting more energy into recommended than non-recommended organizations - has enabled us to share our honest thoughts about charities without appearing to get personal. But when evaluating a group such as SI, I can't avoid placing a heavy weight on (my read on) the general competence, capability and \"intangibles\" of the people and organization, because SI's mission is not about repeating activities that have worked in the past. <strong>Sharing my views on these issues could strike some as personal or mean-spirited and could lead to the misimpression that GiveWell is hostile toward SI. But it is simply necessary in order to be fully transparent about why I hold the views that I hold.</strong></p>\n<p>Fortunately, SI is an ideal organization for our first discussion of this type. I believe the staff and supporters of SI would overwhelmingly rather hear the whole truth about my thoughts - so that they can directly engage them and, if warranted, make changes - than have me sugar-coat what I think in order to spare their feelings. People who know me and <a href=\"http://blog.givewell.org/2007/06/05/an-open-letter-to-crybabies/\">my attitude toward being honest vs. sparing feelings</a> know that this, itself, is high praise for SI.</p>\n<p>One more comment before I continue: our policy is that non-public information provided to us by a charity will not be published or discussed without that charity's prior consent. However, none of the content of this post is based on private information; all of it is based on information that SI has made available to the public.</p>\n<p>There are several reasons that I currently have a negative impression of SI's general competence, capability and \"intangibles.\" My mind remains open and I include specifics on how it could be changed.</p>\n<ul>\n<li><strong>Weak arguments.</strong> SI has produced enormous quantities of public argumentation, and I have examined a very large proportion of this information. Yet I have never seen a clear response to any of the three basic objections I listed in the previous section. One of SI's major goals is to raise awareness of AI-related risks; given this, the fact that it has not advanced clear/concise/compelling arguments speaks, in my view, to its general competence. </li>\n<li><strong>Lack of impressive endorsements.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. I feel that given the enormous implications of SI's claims, if it argued them well it ought to be able to get more impressive endorsements than it has.\n<p>I have been pointed to Peter Thiel and Ray Kurzweil as examples of impressive SI supporters, but I have not seen any on-record statements from either of these people that show agreement with SI's specific views, and in fact (based on watching them speak at Singularity Summits) my impression is that they disagree. Peter Thiel seems to believe that speeding the pace of general innovation is a good thing; this would seem to be in tension with SI's view that AGI will be catastrophic by default and that no one other than SI is paying sufficient attention to \"Friendliness\" issues. Ray Kurzweil seems to believe that \"safety\" is a matter of transparency, strong institutions, etc. rather than of \"Friendliness.\" I am personally in agreement with the things I have seen both of them say on these topics. I find it possible that they support SI because of the Singularity Summit or to increase general interest in ambitious technology, rather than because they find \"Friendliness theory\" to be as important as SI does.</p>\n<p>Clear, on-record statements from these two supporters, specifically endorsing SI's arguments and the importance of developing Friendliness theory, would shift my views somewhat on this point.</p>\n</li>\n<li><strong>Resistance to feedback loops.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. SI seems to have passed up opportunities to test itself and its own rationality by e.g. aiming for objectively impressive accomplishments. This is a problem because of (a) its extremely ambitious goals (among other things, it seeks to develop artificial intelligence <em>and</em> \"Friendliness theory\" before anyone else can develop artificial intelligence); (b) its view of its staff/supporters as having unusual insight into rationality, which I discuss in a later bullet point.\n<p>SI's <a href=\"http://intelligence.org/achievements\">list of achievements</a> is not, in my view, up to where it needs to be given (a) and (b). Yet I have seen no declaration that SI has fallen short to date and explanation of what will be changed to deal with it. SI's recent release of a <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a> and <a href=\"http://intelligence.org/blog/\">monthly updates</a> are improvements from a transparency perspective, but they still leave me feeling as though there are no clear metrics or goals by which SI is committing to be measured (aside from very basic organizational goals such as \"design a new website\" and very vague goals such as \"publish more papers\") and as though SI places a low priority on engaging people who are critical of its views (or at least not yet on board), as opposed to people who are naturally drawn to it.</p>\n<p>I believe that one of the primary obstacles to being impactful as a nonprofit is the lack of the sort of helpful feedback loops that lead to success in other domains. I like to see groups that are making as much effort as they can to create meaningful feedback loops for themselves. I perceive SI as falling well short on this front. Pursuing more impressive endorsements and developing benign but objectively recognizable innovations (particularly commercially viable ones) are two possible ways to impose more demanding feedback loops. (I discussed both of these in my interview linked above).</p>\n</li>\n<li><strong>Apparent poorly grounded belief in SI's superior general rationality.</strong> Many of the things that SI and its supporters and advocates say imply a belief that they have special insights into the nature of general rationality, and/or have superior general rationality, relative to the rest of the population. (Examples <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">here</a>, <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">here</a> and <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">here</a>). My understanding is that SI is in the process of spinning off a group dedicated to training people on how to have higher general rationality.\n<p>Yet I'm not aware of any of what I consider compelling evidence that SI staff/supporters/advocates have any special insight into the nature of general rationality or that they have especially high general rationality.</p>\n<p>I have been pointed to the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a> on this point. The Sequences (which I have read the vast majority of) do not seem to me to be a demonstration or evidence of general rationality. They are <em>about</em> rationality; I find them very enjoyable to read; and there is very little they say that I disagree with (or would have disagreed with before I read them). However, they do not seem to demonstrate rationality on the part of the writer, any more than a series of enjoyable, not-obviously-inaccurate essays on the qualities of a good basketball player would demonstrate basketball prowess. I sometimes get the impression that fans of the Sequences are willing to ascribe superior rationality to the writer simply because the content <em>seems smart and insightful to them</em>, without making a critical effort to determine the extent to which the content is novel, actionable and important.&nbsp;</p>\n<p>I endorse <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Eliezer Yudkowsky's statement</a>, \"Be careful &hellip; any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility.\" To me, the best evidence of superior general rationality (or of insight into it) would be objectively impressive achievements (successful commercial ventures, highly prestigious awards, clear innovations, etc.) and/or accumulation of wealth and power. As mentioned above, SI staff/supporters/advocates do not seem particularly impressive on these fronts, at least not as much as I would expect for people who have the sort of insight into rationality that makes it sensible for them to train others in it. I am open to other evidence that SI staff/supporters/advocates have superior general rationality, but I have not seen it.</p>\n<p>Why is it a problem if SI staff/supporter/advocates believe themselves, without good evidence, to have superior general rationality? First off, it strikes me as a belief based on wishful thinking rather than rational inference. Secondly, I would expect a series of problems to accompany overconfidence in one's general rationality, and several of these problems seem to be actually occurring in SI's case:</p>\n<ul>\n<li>Insufficient self-skepticism given how strong its claims are and how little support its claims have won. Rather than endorsing \"Others have not accepted our arguments, so we will sharpen and/or reexamine our arguments,\" SI seems often to endorse something more like \"Others have not accepted their arguments because they have inferior general rationality,\" a stance less likely to lead to improvement on SI's part. </li>\n<li>Being too selective (in terms of looking for people who share its preconceptions) when determining whom to hire and whose feedback to take seriously. </li>\n<li>Paying insufficient attention to the limitations of the confidence one can have in one's untested theories, in line with my Objection 1.</li>\n</ul>\n</li>\n<li><strong>Overall disconnect between SI's goals and its activities.</strong> SI seeks to build FAI and/or to develop and promote \"Friendliness theory\" that can be useful to others in building FAI. Yet it seems that most of its time goes to activities other than developing AI or theory. Its per-person output in terms of <a href=\"http://intelligence.org/research/publications\">publications</a> seems low. Its core staff seem more focused on <a href=\"http://www.lesswrong.com\">Less Wrong</a> posts, \"rationality training\" and other activities that don't seem connected to the core goals; Eliezer Yudkowsky, in particular, appears (from the <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a>) to be focused on writing books for popular consumption. These activities seem neither to be advancing the state of FAI-related theory nor to be engaging the sort of people most likely to be crucial for building AGI.\n<p>A possible justification for these activities is that SI is seeking to promote greater general rationality, which over time will lead to more and better support for its mission. But if this is SI's core activity, it becomes even more important to test the hypothesis that SI's views are in fact rooted in superior general rationality - and these tests don't seem to be happening, as discussed above.</p>\n</li>\n<li><strong>Theft.</strong> I am bothered by the <a href=\"/lw/5il/siai_an_examination/\">2009 theft of $118,803.00</a> (as against a $541,080.00 budget for the year). In an organization as small as SI, it really seems as though theft that large relative to the budget shouldn't occur and that it represents a major failure of hiring and/or internal controls.\n<p>In addition, I have seen no public SI-authorized discussion of the matter that I consider to be satisfactory in terms of explaining what happened and what the current status of the case is on an ongoing basis. Some details may have to be omitted, but a clear SI-authorized statement on this point with as much information as can reasonably provided would be helpful.</p>\n</li>\n</ul>\n<p>A couple positive observations to add context here:</p>\n<ul>\n<li>I see significant positive qualities in many of the people associated with SI. I especially like what I perceive as their sincere wish to do whatever they can to help the world as much as possible, and the high value they place on being right as opposed to being conventional or polite. I have not interacted with Eliezer Yudkowsky but I greatly enjoy his writings. </li>\n<li>I'm aware that SI has relatively new leadership that is attempting to address the issues behind some of my complaints. I have a generally positive impression of the new leadership; I believe the Executive Director and Development Director, in particular, to represent a step forward in terms of being interested in transparency and in testing their own general rationality. So I will not be surprised if there is some improvement in the coming years, particularly regarding the last couple of statements listed above. That said, SI is an organization and it seems reasonable to judge it by its organizational track record, especially when its new leadership is so new that I have little basis on which to judge these staff.</li>\n</ul>\n<h3>Wrapup</h3>\n<p>While SI has produced a lot of content that I find interesting and enjoyable, it has not produced what I consider evidence of superior general rationality or of its suitability for the tasks it has set for itself. I see no qualifications or achievements that specifically seem to indicate that SI staff are well-suited to the challenge of understanding the key AI-related issues and/or coordinating the construction of an FAI. And I see specific reasons to be pessimistic about its suitability and general competence.</p>\n<p>When estimating the expected value of an endeavor, it is natural to have an implicit \"survivorship bias\" - to use organizations whose accomplishments one is familiar with (which tend to be relatively effective organizations) as a reference class. Because of this, I would be extremely wary of investing in an organization with apparently poor general competence/suitability to its tasks, even if I bought fully into its mission (which I do not) and saw no other groups working on a comparable mission.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SWEGQmqnfyXc7i2rJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 9.005194556104302e-07, "legacy": true, "legacyId": "16008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The sheer length of GiveWell co-founder and co-executive director Holden Karnofsky's&nbsp;<a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">excellent critique of the Singularity Institute</a>&nbsp;means that it's hard to keep track of the resulting discussion. &nbsp;I propose to break out each of his objections into a separate Discussion post so that each receives the attention it deserves.</p>\n<h1 id=\"Is_SI_the_kind_of_organization_we_want_to_bet_on_\"><strong><a name=\"Organization\"></a>Is SI the kind of organization we want to bet on?</strong></h1>\n<p>This part of the post has some risks. For most of GiveWell's history, sticking to our <a href=\"http://givewell.org/international/process/2011#Goalofthereport\">standard criteria</a> - and putting more energy into recommended than non-recommended organizations - has enabled us to share our honest thoughts about charities without appearing to get personal. But when evaluating a group such as SI, I can't avoid placing a heavy weight on (my read on) the general competence, capability and \"intangibles\" of the people and organization, because SI's mission is not about repeating activities that have worked in the past. <strong>Sharing my views on these issues could strike some as personal or mean-spirited and could lead to the misimpression that GiveWell is hostile toward SI. But it is simply necessary in order to be fully transparent about why I hold the views that I hold.</strong></p>\n<p>Fortunately, SI is an ideal organization for our first discussion of this type. I believe the staff and supporters of SI would overwhelmingly rather hear the whole truth about my thoughts - so that they can directly engage them and, if warranted, make changes - than have me sugar-coat what I think in order to spare their feelings. People who know me and <a href=\"http://blog.givewell.org/2007/06/05/an-open-letter-to-crybabies/\">my attitude toward being honest vs. sparing feelings</a> know that this, itself, is high praise for SI.</p>\n<p>One more comment before I continue: our policy is that non-public information provided to us by a charity will not be published or discussed without that charity's prior consent. However, none of the content of this post is based on private information; all of it is based on information that SI has made available to the public.</p>\n<p>There are several reasons that I currently have a negative impression of SI's general competence, capability and \"intangibles.\" My mind remains open and I include specifics on how it could be changed.</p>\n<ul>\n<li><strong>Weak arguments.</strong> SI has produced enormous quantities of public argumentation, and I have examined a very large proportion of this information. Yet I have never seen a clear response to any of the three basic objections I listed in the previous section. One of SI's major goals is to raise awareness of AI-related risks; given this, the fact that it has not advanced clear/concise/compelling arguments speaks, in my view, to its general competence. </li>\n<li><strong>Lack of impressive endorsements.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. I feel that given the enormous implications of SI's claims, if it argued them well it ought to be able to get more impressive endorsements than it has.\n<p>I have been pointed to Peter Thiel and Ray Kurzweil as examples of impressive SI supporters, but I have not seen any on-record statements from either of these people that show agreement with SI's specific views, and in fact (based on watching them speak at Singularity Summits) my impression is that they disagree. Peter Thiel seems to believe that speeding the pace of general innovation is a good thing; this would seem to be in tension with SI's view that AGI will be catastrophic by default and that no one other than SI is paying sufficient attention to \"Friendliness\" issues. Ray Kurzweil seems to believe that \"safety\" is a matter of transparency, strong institutions, etc. rather than of \"Friendliness.\" I am personally in agreement with the things I have seen both of them say on these topics. I find it possible that they support SI because of the Singularity Summit or to increase general interest in ambitious technology, rather than because they find \"Friendliness theory\" to be as important as SI does.</p>\n<p>Clear, on-record statements from these two supporters, specifically endorsing SI's arguments and the importance of developing Friendliness theory, would shift my views somewhat on this point.</p>\n</li>\n<li><strong>Resistance to feedback loops.</strong> I discussed this issue in my <a href=\"http://groups.yahoo.com/group/givewell/message/270\">2011 interview with SI representatives</a> and I still feel the same way on the matter. SI seems to have passed up opportunities to test itself and its own rationality by e.g. aiming for objectively impressive accomplishments. This is a problem because of (a) its extremely ambitious goals (among other things, it seeks to develop artificial intelligence <em>and</em> \"Friendliness theory\" before anyone else can develop artificial intelligence); (b) its view of its staff/supporters as having unusual insight into rationality, which I discuss in a later bullet point.\n<p>SI's <a href=\"http://intelligence.org/achievements\">list of achievements</a> is not, in my view, up to where it needs to be given (a) and (b). Yet I have seen no declaration that SI has fallen short to date and explanation of what will be changed to deal with it. SI's recent release of a <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a> and <a href=\"http://intelligence.org/blog/\">monthly updates</a> are improvements from a transparency perspective, but they still leave me feeling as though there are no clear metrics or goals by which SI is committing to be measured (aside from very basic organizational goals such as \"design a new website\" and very vague goals such as \"publish more papers\") and as though SI places a low priority on engaging people who are critical of its views (or at least not yet on board), as opposed to people who are naturally drawn to it.</p>\n<p>I believe that one of the primary obstacles to being impactful as a nonprofit is the lack of the sort of helpful feedback loops that lead to success in other domains. I like to see groups that are making as much effort as they can to create meaningful feedback loops for themselves. I perceive SI as falling well short on this front. Pursuing more impressive endorsements and developing benign but objectively recognizable innovations (particularly commercially viable ones) are two possible ways to impose more demanding feedback loops. (I discussed both of these in my interview linked above).</p>\n</li>\n<li><strong>Apparent poorly grounded belief in SI's superior general rationality.</strong> Many of the things that SI and its supporters and advocates say imply a belief that they have special insights into the nature of general rationality, and/or have superior general rationality, relative to the rest of the population. (Examples <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">here</a>, <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">here</a> and <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">here</a>). My understanding is that SI is in the process of spinning off a group dedicated to training people on how to have higher general rationality.\n<p>Yet I'm not aware of any of what I consider compelling evidence that SI staff/supporters/advocates have any special insight into the nature of general rationality or that they have especially high general rationality.</p>\n<p>I have been pointed to the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a> on this point. The Sequences (which I have read the vast majority of) do not seem to me to be a demonstration or evidence of general rationality. They are <em>about</em> rationality; I find them very enjoyable to read; and there is very little they say that I disagree with (or would have disagreed with before I read them). However, they do not seem to demonstrate rationality on the part of the writer, any more than a series of enjoyable, not-obviously-inaccurate essays on the qualities of a good basketball player would demonstrate basketball prowess. I sometimes get the impression that fans of the Sequences are willing to ascribe superior rationality to the writer simply because the content <em>seems smart and insightful to them</em>, without making a critical effort to determine the extent to which the content is novel, actionable and important.&nbsp;</p>\n<p>I endorse <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Eliezer Yudkowsky's statement</a>, \"Be careful \u2026 any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility.\" To me, the best evidence of superior general rationality (or of insight into it) would be objectively impressive achievements (successful commercial ventures, highly prestigious awards, clear innovations, etc.) and/or accumulation of wealth and power. As mentioned above, SI staff/supporters/advocates do not seem particularly impressive on these fronts, at least not as much as I would expect for people who have the sort of insight into rationality that makes it sensible for them to train others in it. I am open to other evidence that SI staff/supporters/advocates have superior general rationality, but I have not seen it.</p>\n<p>Why is it a problem if SI staff/supporter/advocates believe themselves, without good evidence, to have superior general rationality? First off, it strikes me as a belief based on wishful thinking rather than rational inference. Secondly, I would expect a series of problems to accompany overconfidence in one's general rationality, and several of these problems seem to be actually occurring in SI's case:</p>\n<ul>\n<li>Insufficient self-skepticism given how strong its claims are and how little support its claims have won. Rather than endorsing \"Others have not accepted our arguments, so we will sharpen and/or reexamine our arguments,\" SI seems often to endorse something more like \"Others have not accepted their arguments because they have inferior general rationality,\" a stance less likely to lead to improvement on SI's part. </li>\n<li>Being too selective (in terms of looking for people who share its preconceptions) when determining whom to hire and whose feedback to take seriously. </li>\n<li>Paying insufficient attention to the limitations of the confidence one can have in one's untested theories, in line with my Objection 1.</li>\n</ul>\n</li>\n<li><strong>Overall disconnect between SI's goals and its activities.</strong> SI seeks to build FAI and/or to develop and promote \"Friendliness theory\" that can be useful to others in building FAI. Yet it seems that most of its time goes to activities other than developing AI or theory. Its per-person output in terms of <a href=\"http://intelligence.org/research/publications\">publications</a> seems low. Its core staff seem more focused on <a href=\"http://www.lesswrong.com\">Less Wrong</a> posts, \"rationality training\" and other activities that don't seem connected to the core goals; Eliezer Yudkowsky, in particular, appears (from the <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a>) to be focused on writing books for popular consumption. These activities seem neither to be advancing the state of FAI-related theory nor to be engaging the sort of people most likely to be crucial for building AGI.\n<p>A possible justification for these activities is that SI is seeking to promote greater general rationality, which over time will lead to more and better support for its mission. But if this is SI's core activity, it becomes even more important to test the hypothesis that SI's views are in fact rooted in superior general rationality - and these tests don't seem to be happening, as discussed above.</p>\n</li>\n<li><strong>Theft.</strong> I am bothered by the <a href=\"/lw/5il/siai_an_examination/\">2009 theft of $118,803.00</a> (as against a $541,080.00 budget for the year). In an organization as small as SI, it really seems as though theft that large relative to the budget shouldn't occur and that it represents a major failure of hiring and/or internal controls.\n<p>In addition, I have seen no public SI-authorized discussion of the matter that I consider to be satisfactory in terms of explaining what happened and what the current status of the case is on an ongoing basis. Some details may have to be omitted, but a clear SI-authorized statement on this point with as much information as can reasonably provided would be helpful.</p>\n</li>\n</ul>\n<p>A couple positive observations to add context here:</p>\n<ul>\n<li>I see significant positive qualities in many of the people associated with SI. I especially like what I perceive as their sincere wish to do whatever they can to help the world as much as possible, and the high value they place on being right as opposed to being conventional or polite. I have not interacted with Eliezer Yudkowsky but I greatly enjoy his writings. </li>\n<li>I'm aware that SI has relatively new leadership that is attempting to address the issues behind some of my complaints. I have a generally positive impression of the new leadership; I believe the Executive Director and Development Director, in particular, to represent a step forward in terms of being interested in transparency and in testing their own general rationality. So I will not be surprised if there is some improvement in the coming years, particularly regarding the last couple of statements listed above. That said, SI is an organization and it seems reasonable to judge it by its organizational track record, especially when its new leadership is so new that I have little basis on which to judge these staff.</li>\n</ul>\n<h3 id=\"Wrapup\">Wrapup</h3>\n<p>While SI has produced a lot of content that I find interesting and enjoyable, it has not produced what I consider evidence of superior general rationality or of its suitability for the tasks it has set for itself. I see no qualifications or achievements that specifically seem to indicate that SI staff are well-suited to the challenge of understanding the key AI-related issues and/or coordinating the construction of an FAI. And I see specific reasons to be pessimistic about its suitability and general competence.</p>\n<p>When estimating the expected value of an endeavor, it is natural to have an implicit \"survivorship bias\" - to use organizations whose accomplishments one is familiar with (which tend to be relatively effective organizations) as a reference class. Because of this, I would be extremely wary of investing in an organization with apparently poor general competence/suitability to its tasks, even if I bought fully into its mission (which I do not) and saw no other groups working on a comparable mission.</p>", "sections": [{"title": "Is SI the kind of organization we want to bet on?", "anchor": "Is_SI_the_kind_of_organization_we_want_to_bet_on_", "level": 1}, {"title": "Wrapup", "anchor": "Wrapup", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "4PPE6D635iBcGPGRy", "gfexKxsBDM6v2sCMo", "fkhbBE2ZTSytvsy9x", "6ddcsdA2c2XpNpE5x", "qqhdj3W3vSfB5E9ss"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T15:04:35.244Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Cambridge, Philadelphia, Salt Lake City, Vancouver", "slug": "weekly-lw-meetups-cambridge-philadelphia-salt-lake-city", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r4YKJFqKW5KKzNbuz/weekly-lw-meetups-cambridge-philadelphia-salt-lake-city", "pageUrlRelative": "/posts/r4YKJFqKW5KKzNbuz/weekly-lw-meetups-cambridge-philadelphia-salt-lake-city", "linkUrl": "https://www.lesswrong.com/posts/r4YKJFqKW5KKzNbuz/weekly-lw-meetups-cambridge-philadelphia-salt-lake-city", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Cambridge%2C%20Philadelphia%2C%20Salt%20Lake%20City%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Cambridge%2C%20Philadelphia%2C%20Salt%20Lake%20City%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4YKJFqKW5KKzNbuz%2Fweekly-lw-meetups-cambridge-philadelphia-salt-lake-city%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Cambridge%2C%20Philadelphia%2C%20Salt%20Lake%20City%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4YKJFqKW5KKzNbuz%2Fweekly-lw-meetups-cambridge-philadelphia-salt-lake-city", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4YKJFqKW5KKzNbuz%2Fweekly-lw-meetups-cambridge-philadelphia-salt-lake-city", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 477, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/9n\">Philadelphia Meetup: Introduction to drawing:&nbsp;<span class=\"date\">05 May 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/9o\">Vancouver Fake Utility Meetup:&nbsp;<span class=\"date\">06 May 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/9u\">Washington DC meetup:&nbsp;<span class=\"date\">13 May 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/9l\">Brussels meetup:&nbsp;<span class=\"date\">19 May 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/9r\">Less Wrong Sydney - Rational Acting:&nbsp;<span class=\"date\">21 May 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/9s\">First Berlin meetup:&nbsp;<span class=\"date\">05 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li class=\"last-child\"><a href=\"/meetups/9t\">SLC Meetup: Social Hacking Presentation:&nbsp;<span class=\"date\">05 May 2012 01:49PM</span></a></li>\n<li><a href=\"/meetups/93\">Cambridge, MA First Sunday Meetup:&nbsp;<span class=\"date\">06 May 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup:&nbsp;<span class=\"date\">20 May 2012 02:20PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r4YKJFqKW5KKzNbuz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.007196518163722e-07, "legacy": true, "legacyId": "15819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T17:15:49.163Z", "modifiedAt": null, "url": null, "title": "[Requesting Advice] Applying Instrumental Rationality to College Course Selection Dilemma", "slug": "requesting-advice-applying-instrumental-rationality-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.192Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "C3Sc9XJXd5AsAyksf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tD5mRiW9Yso47Dvvk/requesting-advice-applying-instrumental-rationality-to", "pageUrlRelative": "/posts/tD5mRiW9Yso47Dvvk/requesting-advice-applying-instrumental-rationality-to", "linkUrl": "https://www.lesswrong.com/posts/tD5mRiW9Yso47Dvvk/requesting-advice-applying-instrumental-rationality-to", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BRequesting%20Advice%5D%20Applying%20Instrumental%20Rationality%20to%20College%20Course%20Selection%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BRequesting%20Advice%5D%20Applying%20Instrumental%20Rationality%20to%20College%20Course%20Selection%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtD5mRiW9Yso47Dvvk%2Frequesting-advice-applying-instrumental-rationality-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BRequesting%20Advice%5D%20Applying%20Instrumental%20Rationality%20to%20College%20Course%20Selection%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtD5mRiW9Yso47Dvvk%2Frequesting-advice-applying-instrumental-rationality-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtD5mRiW9Yso47Dvvk%2Frequesting-advice-applying-instrumental-rationality-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 445, "htmlBody": "<p>I'm faced with a dilemma and need a big dose of instrumental rationality. I'll describe the situation:<br /><br />This fall, I'm entering my first semester of college. I'm aiming to graduate in 3-4 years with a Mathematics B.S. In order for my course progression to go smoothly, I need to take Calculus I Honors this fall and Calculus II in the spring. These two courses serve as a prerequisite bottleneck. They prevent me from taking higher level math courses. <br /><br />My SAT scores have exempted me from all placement tests, including the math. But without taking a placement test, the highest any math SAT score can place me into is Pre-Calculus Honors, which is one level below what I want to take in the fall. The course progression goes Pre-Calculus Honors to Calc I Honors to Calc II Honors. <br /><br />So in order to take Calc I Honors in the fall, I either need to:<br /><br />(1) Score high enough on a College-Level Math placement test or<br /><br />(2) Forgo the test and take Pre-Calc Honors for 9 weeks this summer<br /><br />I've taken both pre-calculus and calculus in high school. I've also been studying precalculus material over the past few days, relearning a lot of what I've either forgotten or wasn't taught in class. If I decide to take the test, I'm pretty confident I'll place into Calculus I. I'd estimate that chance being within 0.8, plus or minus 0.1. If I pass the test, I'll save 9 weeks of studying in the summer and use them to prepare for classes I'll be taking in the fall. I'd also free me up to take another summer class worth 4 credits and fulfill a prerequisite.</p>\n<p>But if I decide to forgo the test and take Precalc this summer, I'm also pretty confident I'll do very well in the class. I'd confidently wager above a 90%. The class would ensure I've got the material down better than the placement test and would also give me my first six credits.</p>\n<p>The questions going through my mind right now include: How can I best decide between these two options? How can I compare the heterogeneous benefits/costs? Are there any other relevant factors that I'm leaving out? <br /><br />Advice would be greatly appreciated.</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> Writing this post, as well as reading and responding to the comments, has clarified the situation for me. Unless there is something else important I've missed, I'll take the test, place into Calc I, spend the summer taking a different summer class and preparing for fall classes. Thanks to everyone who helped me out.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tD5mRiW9Yso47Dvvk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "16012", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-11T21:58:01.712Z", "modifiedAt": null, "url": null, "title": "Most transferable skills?", "slug": "most-transferable-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:40.038Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kalla724", "createdAt": "2011-06-27T19:28:41.092Z", "isAdmin": false, "displayName": "kalla724"}, "userId": "gaBwWRM7cb7KP6fia", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uDtCu7dwmzpDnzJsc/most-transferable-skills", "pageUrlRelative": "/posts/uDtCu7dwmzpDnzJsc/most-transferable-skills", "linkUrl": "https://www.lesswrong.com/posts/uDtCu7dwmzpDnzJsc/most-transferable-skills", "postedAtFormatted": "Friday, May 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Most%20transferable%20skills%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMost%20transferable%20skills%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuDtCu7dwmzpDnzJsc%2Fmost-transferable-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Most%20transferable%20skills%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuDtCu7dwmzpDnzJsc%2Fmost-transferable-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuDtCu7dwmzpDnzJsc%2Fmost-transferable-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>So, transferable skills: skills that, upon improvement, increase your ability in other areas (and also improve other, higher-level skills).</p>\n<p>A basic example would be reading/writing. Knowing how to read and write allows one to access a huge amount of other skills and resources which are otherwise unavailable. A less obvious example would be clear speech (enunciation). Ability to speak clearly improves one's prospects in a lot of different areas (e.g. professional advancement, dating, etc.).</p>\n<p>I'm looking for additional examples. Which skills did you find to be most transferable? Did you become proficient in X, and then found this helped you in many other areas of your life? Please share.</p>\n<p>(I tried to find whether this was discussed before, and failed; if it was, I would appreciate the link.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uDtCu7dwmzpDnzJsc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 23, "extendedScore": null, "score": 9.009001848593604e-07, "legacy": true, "legacyId": "16013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-12T02:13:35.457Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Faster Than Science", "slug": "seq-rerun-faster-than-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/swEGJyBfry9dreQJ7/seq-rerun-faster-than-science", "pageUrlRelative": "/posts/swEGJyBfry9dreQJ7/seq-rerun-faster-than-science", "linkUrl": "https://www.lesswrong.com/posts/swEGJyBfry9dreQJ7/seq-rerun-faster-than-science", "postedAtFormatted": "Saturday, May 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Faster%20Than%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Faster%20Than%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FswEGJyBfry9dreQJ7%2Fseq-rerun-faster-than-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Faster%20Than%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FswEGJyBfry9dreQJ7%2Fseq-rerun-faster-than-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FswEGJyBfry9dreQJ7%2Fseq-rerun-faster-than-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Today's post, <a href=\"/lw/qi/faster_than_science/\">Faster Than Science</a> was originally published on 20 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Is it really possible to arrive at the truth <em>faster </em>than Science does? Not only is it possible, but the social process of science relies on scientists doing so - when they choose which hypotheses to test. In many answer spaces it's not possible to find the true hypothesis by accident. Science leaves it up to experiment to <em>socially </em>declare who was right, but if there weren't <em>some</em> people who could get it right in the absence of overwhelming experimental proof, science would be stuck.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cc8/seq_rerun_changing_the_definition_of_science/\">Changing the Definition of Science</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "swEGJyBfry9dreQJ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.010118105895808e-07, "legacy": true, "legacyId": "16020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xTyuQ3cgsPjifr7oj", "zE4BE67WtbKAWt8ku", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-12T07:38:58.960Z", "modifiedAt": null, "url": null, "title": "Tool for maximizing paperclips vs a paperclip maximizer", "slug": "tool-for-maximizing-paperclips-vs-a-paperclip-maximizer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.191Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "private_messaging", "createdAt": "2012-05-01T06:09:25.541Z", "isAdmin": false, "displayName": "private_messaging"}, "userId": "7hcKjaZ5cGbckjLth", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HeSftDnmZQQBtMKKx/tool-for-maximizing-paperclips-vs-a-paperclip-maximizer", "pageUrlRelative": "/posts/HeSftDnmZQQBtMKKx/tool-for-maximizing-paperclips-vs-a-paperclip-maximizer", "linkUrl": "https://www.lesswrong.com/posts/HeSftDnmZQQBtMKKx/tool-for-maximizing-paperclips-vs-a-paperclip-maximizer", "postedAtFormatted": "Saturday, May 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tool%20for%20maximizing%20paperclips%20vs%20a%20paperclip%20maximizer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATool%20for%20maximizing%20paperclips%20vs%20a%20paperclip%20maximizer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHeSftDnmZQQBtMKKx%2Ftool-for-maximizing-paperclips-vs-a-paperclip-maximizer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tool%20for%20maximizing%20paperclips%20vs%20a%20paperclip%20maximizer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHeSftDnmZQQBtMKKx%2Ftool-for-maximizing-paperclips-vs-a-paperclip-maximizer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHeSftDnmZQQBtMKKx%2Ftool-for-maximizing-paperclips-vs-a-paperclip-maximizer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 382, "htmlBody": "<p>To clarify some point that is being discussed in several threads here, tool vs intentional agent distinction:</p>\n<p>A tool for maximizing paperclips would - for efficiency purposes - have a world-model which it has god's eye view of (not accessing it through embedded sensors like eyes), implementing/defining a counter of paperclips within this model. Output of this counter is what is being maximized by a problem solving portion of the tool. Not the real world paperclips</p>\n<p>No real world intentionality exist in this tool for maximizing paperclips; the paperclip-making-problem-solver would maximize the output of the counter, not real world paperclips. Such tool can be hooked up to actuators, and to sensors, and made to affect the world without human intermediary; but it won't implement real world intentionality.</p>\n<p>An intentional agent for maximizing paperclips is the familiar 'paperclip maximizer', that truly loves the real world paperclips and wants to maximize them, and would try to improve it's understanding of the world to know if it's paperclip making efforts are successful.</p>\n<p>The real world intentionality is ontologically basic in human language and consequently there is very strong bias to describe the former as the latter.</p>\n<p>The distinction: the wireheading (either direct or through manipulation of inputs) is a valid solution to the problem that is being solved by the former, but not by the latter. Of course one could rationalize and postulate tool that is not general purpose enough as to wirehead, forgetting that the issue being feared is a tool that's general purpose to design better tool or self improve. That is an incredibly frustrating feature of rationalization. The aspects of problem are forgotten when thinking backwards.</p>\n<p>The issues with the latter: We do not know if humans actually implement real world intentionality in such a way that it is not destroyed under full ability to self modify (and we can observe that we very much like to manipulate our own inputs; see art, porn, fiction, etc). We do not have single certain example of such stable real world intentionality, and we do not know how to implement it (that may well be impossible). We also are prone to assuming that two unsolved problems in AI - general problem solving and this real world intentionality - are a single problem, or are solved necessarily together. A map compression issue.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QH4LhvnyR4QkW9MG8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HeSftDnmZQQBtMKKx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 9.011539720515802e-07, "legacy": true, "legacyId": "16033", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-12T16:57:51.953Z", "modifiedAt": null, "url": null, "title": "[Book Suggestions] Summer Reading for Younglings.", "slug": "book-suggestions-summer-reading-for-younglings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Karmakaiser", "createdAt": "2011-08-19T20:23:06.809Z", "isAdmin": false, "displayName": "Karmakaiser"}, "userId": "35k6aTWG8i43xjoqR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PB8jYHKBegCwPmnpB/book-suggestions-summer-reading-for-younglings", "pageUrlRelative": "/posts/PB8jYHKBegCwPmnpB/book-suggestions-summer-reading-for-younglings", "linkUrl": "https://www.lesswrong.com/posts/PB8jYHKBegCwPmnpB/book-suggestions-summer-reading-for-younglings", "postedAtFormatted": "Saturday, May 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BBook%20Suggestions%5D%20Summer%20Reading%20for%20Younglings.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BBook%20Suggestions%5D%20Summer%20Reading%20for%20Younglings.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPB8jYHKBegCwPmnpB%2Fbook-suggestions-summer-reading-for-younglings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BBook%20Suggestions%5D%20Summer%20Reading%20for%20Younglings.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPB8jYHKBegCwPmnpB%2Fbook-suggestions-summer-reading-for-younglings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPB8jYHKBegCwPmnpB%2Fbook-suggestions-summer-reading-for-younglings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<p>I bought my niece a Kindle that just arrived and I'm about to load it up with books to give it to her tomorrow for her birthday. I've decided to be a sneaky uncle and include good books that can teach better abilities to think or at least to consider science cool and interesting. She is currently in the 4th Grade with 5th coming after the Summer.</p>\n<p>She reads basically at her own grade level so while I'm open to stuffing the Kindle with books to be read when she's ready, I'd like to focus on giving her books she can read now. Ender's Game will be on there most likely. Game of Thrones will not.</p>\n<p>What books would you give a youngling? Her interests currently trend  toward the young mystery section, Hardy Boys and the like, but in my  experience she is very open to trying new books with particular interest  in YA fantasy but not much interest in Sci Fi (if I'm doing any other  optimizing this year, I'll try to change her opinion on Sci Fi).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PB8jYHKBegCwPmnpB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "16034", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-12T21:42:37.409Z", "modifiedAt": null, "url": null, "title": "Practical tools and agents", "slug": "practical-tools-and-agents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:35.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "private_messaging", "createdAt": "2012-05-01T06:09:25.541Z", "isAdmin": false, "displayName": "private_messaging"}, "userId": "7hcKjaZ5cGbckjLth", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gTsP9GvksmeYkhR6Q/practical-tools-and-agents", "pageUrlRelative": "/posts/gTsP9GvksmeYkhR6Q/practical-tools-and-agents", "linkUrl": "https://www.lesswrong.com/posts/gTsP9GvksmeYkhR6Q/practical-tools-and-agents", "postedAtFormatted": "Saturday, May 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Practical%20tools%20and%20agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APractical%20tools%20and%20agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTsP9GvksmeYkhR6Q%2Fpractical-tools-and-agents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Practical%20tools%20and%20agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTsP9GvksmeYkhR6Q%2Fpractical-tools-and-agents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTsP9GvksmeYkhR6Q%2Fpractical-tools-and-agents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>Presently, the 'utility maximizers' work as following: given a mathematical function f(x) , a solver finds the x that corresponds to a maximum (or, typically, minimum) of f(x) . The x is usually a vector describing the action of the agent, the f is a mathematically defined function which may e.g. simulate some world evolution and compute the expected worth of end state, given action x, as in f(x)=h(g(x)) where h computes worth of world state g(x), and g computes the world state at some future time assuming that action x was taken.</p>\n<p>For instance, the f may represent some metric of risk, discomfort, and time, over a path chosen by a self driving car, in a driving simulator (which is not reductionist). In this case this metric (which is always non-negative) is to be minimized.</p>\n<p>In a very trivial case, such as finding the cannon elevation at which the cannonball will land closest to the target, in vacuum, the solution can be found analytically.</p>\n<p>In more complex cases multitude of methods are typically employed, combining iteration of potential solutions with analytical and iterative solving for local maximum or minimum. If this is combined with sensors and the model-updater, and actuators, an agent like a self driving car can be made.</p>\n<p>Those are the utility functions as used in the field of artificial intelligence.</p>\n<p>A system can be strongly superhuman at finding maximums to functions, and ultimately can be very general purpose, allowing it's use to build models which are efficiently invertible into a solution. However it must be understood that the intelligent component finds mathematical solutions to, ultimately, mathematical relations.</p>\n<p>The utility functions as known and discussed on LW seem entirely different in nature. Them are defined on the real word, using natural language that conveys intent, and seem to be a rather ill defined concept for which the bottom-up formal definition may not even exist. The implementation of such concept, if at all possible, would seem to require a major breakthrough in the philosophy of mind.</p>\n<p>This is an explanation of an important technical distinction mentioned in Holden Karnofsky's post.</p>\n<p>On the discussion in general: It may well be the case that it is very difficult or impossible to define a system such as self driving car in terms of the concepts that are used on LW to talk about intelligences. In particular, the LW's notion of \"utility\" does not seem to allow to accurately describe the kind of tool that Holden Karnofsky was speaking of, in terms of this utility.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gTsP9GvksmeYkhR6Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 9.015227383474198e-07, "legacy": true, "legacyId": "16035", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-12T23:26:28.429Z", "modifiedAt": null, "url": null, "title": "Neuroimaging as alternative/supplement to cryonics?", "slug": "neuroimaging-as-alternative-supplement-to-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.259Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iKrmihLBHTRmuWHuZ/neuroimaging-as-alternative-supplement-to-cryonics", "pageUrlRelative": "/posts/iKrmihLBHTRmuWHuZ/neuroimaging-as-alternative-supplement-to-cryonics", "linkUrl": "https://www.lesswrong.com/posts/iKrmihLBHTRmuWHuZ/neuroimaging-as-alternative-supplement-to-cryonics", "postedAtFormatted": "Saturday, May 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Neuroimaging%20as%20alternative%2Fsupplement%20to%20cryonics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeuroimaging%20as%20alternative%2Fsupplement%20to%20cryonics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiKrmihLBHTRmuWHuZ%2Fneuroimaging-as-alternative-supplement-to-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Neuroimaging%20as%20alternative%2Fsupplement%20to%20cryonics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiKrmihLBHTRmuWHuZ%2Fneuroimaging-as-alternative-supplement-to-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiKrmihLBHTRmuWHuZ%2Fneuroimaging-as-alternative-supplement-to-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 352, "htmlBody": "<p>Paul Christiano recently <a href=\"/r/discussion/lw/c0k/formalizing_value_extrapolation/\">suggested</a> that we can use neuroimaging to form a complete mathematical characterization of a human brain, which a sufficiently powerful superintelligence would be able to reconstruct into a working mind, and the neuroimaging part is already possible today, or close to being possible.</p>\n<blockquote>\n<p>In fact, this project may be possible using existing resources. The complexity of the human brain is not as unapproachable as it may at first appear: though it may contain 10<sup>14</sup> synapses, each described by many parameters, it can be specified much more compactly. A newborn&rsquo;s brain can be specified by about 10<sup>9</sup> bits of genetic information, together with a recipe for a physical simulation of development. The human brain appears to form new long-term memories at a rate of 1-2 bits per second, suggesting that it may be possible to specify an adult brain using 10<sup>9</sup> additional bits of experiential information. This suggests that it may require only about 10<sup>10</sup> bits of information to specify a human brain, which is at the limits of what can be reasonably collected by existing technology for functional neuroimaging.</p>\n</blockquote>\n<p>Paul was using this idea as part of an FAI design proposal, but I'm highlighting it here since it seems to have independent value as an alternative or supplement to cryonics. That is, instead of (or in addition to) trying to get your body to be frozen and then preserved in liquid nitrogen after you die, you periodically take&nbsp;neuroimaging scans of your brain and save them to multiple backup locations (10<sup>10</sup>&nbsp;bits is only about 1 gigabyte), in the hope that a friendly AI or posthuman will eventually use the scans to reconstruct your mind.</p>\n<p>Are there any neuroimaging experts around who can tell us how feasible this really is, and how much such a scan might cost, now or in the near future?</p>\n<p>ETA: Given the presence of thermal noise and the fact that a set of neuroimaging data may contain redundant or irrelevant information, 10<sup>10</sup>&nbsp;bits ought to be regarded as just a rough lower bound on how much data needs to be collected and stored. Thanks to commenters who pointed this out.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iKrmihLBHTRmuWHuZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 24, "extendedScore": null, "score": 9.015681511212613e-07, "legacy": true, "legacyId": "16036", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9DWcNS2rkvd2J8mHH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-13T08:48:00.713Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Einstein's Speed", "slug": "seq-rerun-einstein-s-speed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.152Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2BgYDsnYweivuzzd5/seq-rerun-einstein-s-speed", "pageUrlRelative": "/posts/2BgYDsnYweivuzzd5/seq-rerun-einstein-s-speed", "linkUrl": "https://www.lesswrong.com/posts/2BgYDsnYweivuzzd5/seq-rerun-einstein-s-speed", "postedAtFormatted": "Sunday, May 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Einstein's%20Speed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Einstein's%20Speed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2BgYDsnYweivuzzd5%2Fseq-rerun-einstein-s-speed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Einstein's%20Speed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2BgYDsnYweivuzzd5%2Fseq-rerun-einstein-s-speed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2BgYDsnYweivuzzd5%2Fseq-rerun-einstein-s-speed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>Today's post, <a href=\"/lw/qj/einsteins_speed/\">Einstein's Speed</a> was originally published on 21 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Albert was unusually good at finding the right theory in the presence of only a small amount of experimental evidence. Even more unusually, he admitted it - he claimed to know the theory was right, even in advance of the public proof. It's possible to arrive at the truth by thinking great high-minded thoughts of the sort that Science does not trust you to think, but it's a lot <em>harder </em>than arriving at the truth in the presence of overwhelming evidence.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cd0/seq_rerun_faster_than_science/\">Faster Than Science</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2BgYDsnYweivuzzd5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.018137765849275e-07, "legacy": true, "legacyId": "16037", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mpaqTWGiLT7GA3w76", "swEGJyBfry9dreQJ7", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-13T10:42:39.986Z", "modifiedAt": null, "url": null, "title": "[META] Recent Posts for Discussion and Main", "slug": "meta-recent-posts-for-discussion-and-main", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.310Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k56pPwSSBfxcRfYFL/meta-recent-posts-for-discussion-and-main", "pageUrlRelative": "/posts/k56pPwSSBfxcRfYFL/meta-recent-posts-for-discussion-and-main", "linkUrl": "https://www.lesswrong.com/posts/k56pPwSSBfxcRfYFL/meta-recent-posts-for-discussion-and-main", "postedAtFormatted": "Sunday, May 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Recent%20Posts%20for%20Discussion%20and%20Main&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Recent%20Posts%20for%20Discussion%20and%20Main%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk56pPwSSBfxcRfYFL%2Fmeta-recent-posts-for-discussion-and-main%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Recent%20Posts%20for%20Discussion%20and%20Main%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk56pPwSSBfxcRfYFL%2Fmeta-recent-posts-for-discussion-and-main", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk56pPwSSBfxcRfYFL%2Fmeta-recent-posts-for-discussion-and-main", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p>This link</p>\n<p><a href=\"/r/all/recentposts\">http://lesswrong.com/r/all/recentposts</a></p>\n<p>gives a page which lists all the recent posts in both the Main and Discussion sections. I've posted it in the comments section before, but I decided to put it in a discussion post because it's a really handy way of accessing the site. I found it by guessing the URL.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k56pPwSSBfxcRfYFL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 9.018639428150338e-07, "legacy": true, "legacyId": "16038", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-13T13:10:32.683Z", "modifiedAt": null, "url": null, "title": "Cambridge LW: Post-mortem", "slug": "cambridge-lw-post-mortem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:33.919Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "benelliott", "createdAt": "2010-10-24T16:54:14.159Z", "isAdmin": false, "displayName": "benelliott"}, "userId": "H4iHqStnPyuAkniAA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B43GMbhGbAPS4BEZC/cambridge-lw-post-mortem", "pageUrlRelative": "/posts/B43GMbhGbAPS4BEZC/cambridge-lw-post-mortem", "linkUrl": "https://www.lesswrong.com/posts/B43GMbhGbAPS4BEZC/cambridge-lw-post-mortem", "postedAtFormatted": "Sunday, May 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20LW%3A%20Post-mortem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20LW%3A%20Post-mortem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB43GMbhGbAPS4BEZC%2Fcambridge-lw-post-mortem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20LW%3A%20Post-mortem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB43GMbhGbAPS4BEZC%2Fcambridge-lw-post-mortem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB43GMbhGbAPS4BEZC%2Fcambridge-lw-post-mortem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">A brief post-mortem of this weeks LW meet-up.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Topic Selection</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This week we had comparatively few people, so what worked this time may not generalise. At the beginning of the meet-up Douglas suggested we use one of his&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/c3y/meetup_formats/\" target=\"_self\">meetup formats</a>. Of the three, skill focus was not usable without preparation, and we voted for small discussions above competitive estimating. After a second vote, we used Douglas' suggested method to decide a topic, rather than our method from last week.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This involved each writing down a suggested topic, revealing them all, and then after a fixed timer expired standing behind our preferred one with not further discussion. Using this method, we eventually agreed unanimously on first organising a LessWrong Punt trip, about which an e-mail should be sent to Cambridge, East Anglia and London LessWrongians, followed by a discussion of rational office politics, specifically whether and how it is worthwhile to attempt to achieve goals through exerting influence on large organisations.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Pre commitments</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Once again all pre-commitments were kept, apparently with considerable difficulty on Adam's part. More pre-commitments were made this week, which will once again be posted here to slightly increase the social cost of breaking them.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">* Ramana: Send out punting e-mail</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">* Ben: Post write-up of meeting and continue keeping diary</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">* Adam: Full note production on courses.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B43GMbhGbAPS4BEZC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.019286531166494e-07, "legacy": true, "legacyId": "16040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">A brief post-mortem of this weeks LW meet-up.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong id=\"Topic_Selection\">Topic Selection</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This week we had comparatively few people, so what worked this time may not generalise. At the beginning of the meet-up Douglas suggested we use one of his&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/c3y/meetup_formats/\" target=\"_self\">meetup formats</a>. Of the three, skill focus was not usable without preparation, and we voted for small discussions above competitive estimating. After a second vote, we used Douglas' suggested method to decide a topic, rather than our method from last week.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This involved each writing down a suggested topic, revealing them all, and then after a fixed timer expired standing behind our preferred one with not further discussion. Using this method, we eventually agreed unanimously on first organising a LessWrong Punt trip, about which an e-mail should be sent to Cambridge, East Anglia and London LessWrongians, followed by a discussion of rational office politics, specifically whether and how it is worthwhile to attempt to achieve goals through exerting influence on large organisations.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong id=\"Pre_commitments\">Pre commitments</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Once again all pre-commitments were kept, apparently with considerable difficulty on Adam's part. More pre-commitments were made this week, which will once again be posted here to slightly increase the social cost of breaking them.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">* Ramana: Send out punting e-mail</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">* Ben: Post write-up of meeting and continue keeping diary</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">* Adam: Full note production on courses.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Topic Selection", "anchor": "Topic_Selection", "level": 1}, {"title": "Pre commitments", "anchor": "Pre_commitments", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LnHLeRmkpq4HABPss"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-13T17:13:43.807Z", "modifiedAt": null, "url": null, "title": "Finding Research Papers", "slug": "finding-research-papers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.939Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GoldenWolf", "createdAt": "2012-05-13T14:23:19.218Z", "isAdmin": false, "displayName": "GoldenWolf"}, "userId": "GJzjuFnjsNaMsNJ6C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ukHvZMft7nDFGJfr/finding-research-papers", "pageUrlRelative": "/posts/5ukHvZMft7nDFGJfr/finding-research-papers", "linkUrl": "https://www.lesswrong.com/posts/5ukHvZMft7nDFGJfr/finding-research-papers", "postedAtFormatted": "Sunday, May 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Finding%20Research%20Papers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFinding%20Research%20Papers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ukHvZMft7nDFGJfr%2Ffinding-research-papers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Finding%20Research%20Papers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ukHvZMft7nDFGJfr%2Ffinding-research-papers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ukHvZMft7nDFGJfr%2Ffinding-research-papers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p>So I'm all geared up and ready to go trawl through some academic journals so I can figure out how to tackle a problem or two, and I've realized I completely forgot how to go about it. Not going to a university with paid subscriptions to JSTOR, etc. doesn't help either. So where exactly does one go to access such journals? Is there a cheap way to do this? Thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ukHvZMft7nDFGJfr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 9.020350868580321e-07, "legacy": true, "legacyId": "16042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-13T18:41:29.128Z", "modifiedAt": null, "url": null, "title": "Petition:  Off topic area", "slug": "petition-off-topic-area", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:35.199Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "zyAAcaaDt6i62xJDt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MG7HzBpiorGcN4gKd/petition-off-topic-area", "pageUrlRelative": "/posts/MG7HzBpiorGcN4gKd/petition-off-topic-area", "linkUrl": "https://www.lesswrong.com/posts/MG7HzBpiorGcN4gKd/petition-off-topic-area", "postedAtFormatted": "Sunday, May 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Petition%3A%20%20Off%20topic%20area&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APetition%3A%20%20Off%20topic%20area%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG7HzBpiorGcN4gKd%2Fpetition-off-topic-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Petition%3A%20%20Off%20topic%20area%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG7HzBpiorGcN4gKd%2Fpetition-off-topic-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG7HzBpiorGcN4gKd%2Fpetition-off-topic-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Petition: LW should  introduce a dedicated off topic area</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Why?</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">1) I want to discuss various topics with people who are both intelligent and rationalist, and i know of no other place where to do it.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">2) If find that rationality is getting boring in itself. I need to use it on something.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">3) As stated in this comment   <a href=\"http://lesswrong.com/lw/btc/how_can_we_get_more_and_better_lw_contrarians/6e3p\">http://lesswrong.com/lw/btc/how_can_we_get_more_and_better_lw_contrarians/6e3p</a></p>\n<p style=\"margin-bottom: 0cm;\">the narrow set of topics might actually hurt LW by driving good rationalists away.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MG7HzBpiorGcN4gKd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 55, "extendedScore": null, "score": 0.000133, "legacy": true, "legacyId": "16043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-13T20:45:34.280Z", "modifiedAt": null, "url": null, "title": "Q&A with Aubrey de Grey on May 15", "slug": "q-and-a-with-aubrey-de-grey-on-may-15", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Username", "createdAt": "2010-09-04T16:10:50.550Z", "isAdmin": false, "displayName": "Username"}, "userId": "8iY88evtFECPgCs3s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PMBj6sbKgunfuAiQw/q-and-a-with-aubrey-de-grey-on-may-15", "pageUrlRelative": "/posts/PMBj6sbKgunfuAiQw/q-and-a-with-aubrey-de-grey-on-may-15", "linkUrl": "https://www.lesswrong.com/posts/PMBj6sbKgunfuAiQw/q-and-a-with-aubrey-de-grey-on-may-15", "postedAtFormatted": "Sunday, May 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20Aubrey%20de%20Grey%20on%20May%2015&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20Aubrey%20de%20Grey%20on%20May%2015%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPMBj6sbKgunfuAiQw%2Fq-and-a-with-aubrey-de-grey-on-may-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20Aubrey%20de%20Grey%20on%20May%2015%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPMBj6sbKgunfuAiQw%2Fq-and-a-with-aubrey-de-grey-on-may-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPMBj6sbKgunfuAiQw%2Fq-and-a-with-aubrey-de-grey-on-may-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>EDIT2: The video has now been posted, you can watch it here:&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; color: #3d3d3e;\"><span style=\"line-height: 19px; text-align: justify;\"><a href=\"http://www.youtube.com/watch?v=-tsI_28O3Ws\">http://www.youtube.com/watch?v=-tsI_28O3Ws</a></span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; color: #3d3d3e;\"><span style=\"line-height: 19px; text-align: justify;\">EDIT3: I have typed up a transcript for this video, you can see it in <a href=\"https://docs.google.com/document/d/1RKiR7r6h0fmEaEb7d-50RcXVkD_kkdBnQRebYWSiRqc/edit?pli=1\">this google doc</a> or in <a href=\"/r/discussion/lw/ddh/aubrey_de_grey_has_responded_to_his_iama_now_with/6xee\">the comments here</a></span></span><span style=\"line-height: 19px; text-align: justify; color: #3d3d3e; font-family: Arial, Helvetica, sans-serif; \">.</span></p>\n<p>Just a heads up to the lesswrong community. Dr. Aubrey de Grey will be doing an AMA (Ask Me Anything) on reddit this upcoming Tuesday the 15th, scheduled for 9am EST. He will be doing a video-style response, which likely means that he will take the top voted questions and post his responses on youtube. When the AMA starts, you will be able to find it posted on&nbsp;<a href=\"http://www.reddit.com/r/IAmA/\">http://www.reddit.com/r/IAmA/</a>&nbsp;. I will update this post with links to the question thread and video response when they go live.</p>\n<p>EDIT: Ask Aubrey de Grey your questions here: <a href=\"http://www.reddit.com/r/IAmA/comments/to32n/ask_aubrey_de_grey_chief_science_officer_sens/\"><em>Ask Aubrey de Grey, Chief Science Officer, SENS Foundation, Anything!</em></a>&nbsp;He will be taking the top voted questions and answering them later this week in a video response. I will update this post then.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PMBj6sbKgunfuAiQw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 23, "extendedScore": null, "score": 9.021278204798477e-07, "legacy": true, "legacyId": "16044", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-14T03:06:43.452Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] That Alien Message", "slug": "seq-rerun-that-alien-message", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.323Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LcuMgNnKLmgJHB8Rb/seq-rerun-that-alien-message", "pageUrlRelative": "/posts/LcuMgNnKLmgJHB8Rb/seq-rerun-that-alien-message", "linkUrl": "https://www.lesswrong.com/posts/LcuMgNnKLmgJHB8Rb/seq-rerun-that-alien-message", "postedAtFormatted": "Monday, May 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20That%20Alien%20Message&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20That%20Alien%20Message%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcuMgNnKLmgJHB8Rb%2Fseq-rerun-that-alien-message%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20That%20Alien%20Message%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcuMgNnKLmgJHB8Rb%2Fseq-rerun-that-alien-message", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcuMgNnKLmgJHB8Rb%2Fseq-rerun-that-alien-message", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>Today's post, <a href=\"/lw/qk/that_alien_message/\">That Alien Message</a> was originally published on 22 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Einstein used evidence more efficiently than other physicists, but he was still extremely inefficient in an <em>absolute </em>sense. If a huge team of cryptographers and physicists were examining a interstellar transmission, going over it bit by bit, we could deduce principles on the order of Galilean gravity just from seeing one or two frames of a picture. As if the very first human to see an apple fall, had, on the instant, realized that its position went as the square of the time and that this implied constant acceleration.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cdh/seq_rerun_einsteins_speed/\">Einstein's Speed</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LcuMgNnKLmgJHB8Rb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.022947127311706e-07, "legacy": true, "legacyId": "16058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5wMcKNAwB6X4mp9og", "2BgYDsnYweivuzzd5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-14T08:57:32.589Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-14", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mMskQWkK3CknSZHwX/meetup-melbourne-social-meetup-14", "pageUrlRelative": "/posts/mMskQWkK3CknSZHwX/meetup-melbourne-social-meetup-14", "linkUrl": "https://www.lesswrong.com/posts/mMskQWkK3CknSZHwX/meetup-melbourne-social-meetup-14", "postedAtFormatted": "Monday, May 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMskQWkK3CknSZHwX%2Fmeetup-melbourne-social-meetup-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMskQWkK3CknSZHwX%2Fmeetup-melbourne-social-meetup-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMskQWkK3CknSZHwX%2Fmeetup-melbourne-social-meetup-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a3'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>All are welcome (especially new people!) this Friday 18 May from 7pm at my place for our monthly social meetup. Join our google group for the address. If you have any trouble getting in, you can call me on 0412 996 288.</p>\n\n<p>BYO drinks and games. We'll get some kind of take-away for dinner.</p>\n\n<p>(Sorry to not post earlier: we had some miscommunication about who would make the post.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a3'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mMskQWkK3CknSZHwX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.024483711534057e-07, "legacy": true, "legacyId": "16070", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/a3\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>All are welcome (especially new people!) this Friday 18 May from 7pm at my place for our monthly social meetup. Join our google group for the address. If you have any trouble getting in, you can call me on 0412 996 288.</p>\n\n<p>BYO drinks and games. We'll get some kind of take-away for dinner.</p>\n\n<p>(Sorry to not post earlier: we had some miscommunication about who would make the post.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/a3\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-14T11:44:21.320Z", "modifiedAt": null, "url": null, "title": "How many people here agree with Holden? [Actually, who agrees with Holden?]", "slug": "how-many-people-here-agree-with-holden-actually-who-agrees", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.004Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "private_messaging", "createdAt": "2012-05-01T06:09:25.541Z", "isAdmin": false, "displayName": "private_messaging"}, "userId": "7hcKjaZ5cGbckjLth", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7soC5zGJi36yG2iLf/how-many-people-here-agree-with-holden-actually-who-agrees", "pageUrlRelative": "/posts/7soC5zGJi36yG2iLf/how-many-people-here-agree-with-holden-actually-who-agrees", "linkUrl": "https://www.lesswrong.com/posts/7soC5zGJi36yG2iLf/how-many-people-here-agree-with-holden-actually-who-agrees", "postedAtFormatted": "Monday, May 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20many%20people%20here%20agree%20with%20Holden%3F%20%5BActually%2C%20who%20agrees%20with%20Holden%3F%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20many%20people%20here%20agree%20with%20Holden%3F%20%5BActually%2C%20who%20agrees%20with%20Holden%3F%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7soC5zGJi36yG2iLf%2Fhow-many-people-here-agree-with-holden-actually-who-agrees%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20many%20people%20here%20agree%20with%20Holden%3F%20%5BActually%2C%20who%20agrees%20with%20Holden%3F%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7soC5zGJi36yG2iLf%2Fhow-many-people-here-agree-with-holden-actually-who-agrees", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7soC5zGJi36yG2iLf%2Fhow-many-people-here-agree-with-holden-actually-who-agrees", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>I was wondering - what fraction of people here agree with Holden's advice regarding donations, and his arguments? What fraction assumes there is a good chance he is essentially correct? What fraction finds it necessary to determine whenever Holden is essentially correct in his assessment, before working on counter argumentation, acknowledging that such investigation should be able to result in dissolution or suspension of SI?</p>\n<p>It would seem to me, from the response, that the chosen course of action is to try to improve the presentation of the argument, rather than to try to verify truth values of the assertions (with the non-negligible likelihood of assertions being found false instead). This strikes me as very odd stance.</p>\n<p>Ultimately: why SI seems certain that it has badly presented some valid reasoning, rather than tried to present some invalid reasoning?</p>\n<p>edit: I am interested in knowing why people agree/disagree with Holden, and what likehood they give to him being essentially correct, rather than a number or a ratio (that would be subject to selection bias).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7soC5zGJi36yG2iLf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 3, "extendedScore": null, "score": 9.025214510958994e-07, "legacy": true, "legacyId": "16071", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-14T12:08:34.573Z", "modifiedAt": null, "url": null, "title": "[LINK] International variation in IQ \u2013 the role of parasites", "slug": "link-international-variation-in-iq-the-role-of-parasites", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:34.593Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WeMvhPj9Ez3MtFqk9/link-international-variation-in-iq-the-role-of-parasites", "pageUrlRelative": "/posts/WeMvhPj9Ez3MtFqk9/link-international-variation-in-iq-the-role-of-parasites", "linkUrl": "https://www.lesswrong.com/posts/WeMvhPj9Ez3MtFqk9/link-international-variation-in-iq-the-role-of-parasites", "postedAtFormatted": "Monday, May 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20International%20variation%20in%20IQ%20%E2%80%93%20the%20role%20of%20parasites&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20International%20variation%20in%20IQ%20%E2%80%93%20the%20role%20of%20parasites%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeMvhPj9Ez3MtFqk9%2Flink-international-variation-in-iq-the-role-of-parasites%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20International%20variation%20in%20IQ%20%E2%80%93%20the%20role%20of%20parasites%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeMvhPj9Ez3MtFqk9%2Flink-international-variation-in-iq-the-role-of-parasites", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeMvhPj9Ez3MtFqk9%2Flink-international-variation-in-iq-the-role-of-parasites", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>This is interesting, I wonder if there's anything to it: <a rel=\"nofollow\" href=\"https://katatrepsis.wordpress.com/2011/09/28/international-variation-in-iq-the-role-of-parasites/\">International variation in IQ &ndash; the role of parasites</a> (<a href=\"http://www.sciencedirect.com/science/article/pii/S0160289611000572\">paper</a>) by Christopher Hassall of U. Carleton.</p>\n<p>It strikes me as the sort of thing that could be as big an issue as lead in the environment. Raise the sanity waterline: improve health!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4cKQgA4S7xfNeeWXg": 2, "ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WeMvhPj9Ez3MtFqk9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "16072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T01:23:42.606Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] My Childhood Role Model", "slug": "seq-rerun-my-childhood-role-model", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:35.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qrpXW9wnbAccRJ8jp/seq-rerun-my-childhood-role-model", "pageUrlRelative": "/posts/qrpXW9wnbAccRJ8jp/seq-rerun-my-childhood-role-model", "linkUrl": "https://www.lesswrong.com/posts/qrpXW9wnbAccRJ8jp/seq-rerun-my-childhood-role-model", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20My%20Childhood%20Role%20Model&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20My%20Childhood%20Role%20Model%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrpXW9wnbAccRJ8jp%2Fseq-rerun-my-childhood-role-model%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20My%20Childhood%20Role%20Model%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrpXW9wnbAccRJ8jp%2Fseq-rerun-my-childhood-role-model", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrpXW9wnbAccRJ8jp%2Fseq-rerun-my-childhood-role-model", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/ql/my_childhood_role_model/\">My Childhood Role Model</a> was originally published on 23 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>I looked up to the ideal of a Bayesian superintelligence, not Einstein.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ce2/seq_rerun_that_alien_message/\">That Alien Message</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qrpXW9wnbAccRJ8jp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.02880560051359e-07, "legacy": true, "legacyId": "16087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3Jpchgy53D2gB5qdk", "LcuMgNnKLmgJHB8Rb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T02:17:14.042Z", "modifiedAt": null, "url": null, "title": "Meetup : Garden grove meetup", "slug": "meetup-garden-grove-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:39.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimmy", "createdAt": "2009-02-27T18:23:27.410Z", "isAdmin": false, "displayName": "jimmy"}, "userId": "JKdbpXHkv9AsuazJ3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rM35xcXLWD2JaZam3/meetup-garden-grove-meetup", "pageUrlRelative": "/posts/rM35xcXLWD2JaZam3/meetup-garden-grove-meetup", "linkUrl": "https://www.lesswrong.com/posts/rM35xcXLWD2JaZam3/meetup-garden-grove-meetup", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Garden%20grove%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Garden%20grove%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrM35xcXLWD2JaZam3%2Fmeetup-garden-grove-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Garden%20grove%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrM35xcXLWD2JaZam3%2Fmeetup-garden-grove-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrM35xcXLWD2JaZam3%2Fmeetup-garden-grove-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a4'>Garden grove meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 May 2012 07:18:15PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Brookhurst and Garden Grove 10130 Garden Grove Blvd, Garden Grove, CA 92844</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At Genki Living. For Snacks and Drinks with light discussion on how to properly have a meetup and what to meetup about.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a4'>Garden grove meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rM35xcXLWD2JaZam3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.029040274447185e-07, "legacy": true, "legacyId": "16089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Garden_grove_meetup\">Discussion article for the meetup : <a href=\"/meetups/a4\">Garden grove meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 May 2012 07:18:15PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Brookhurst and Garden Grove 10130 Garden Grove Blvd, Garden Grove, CA 92844</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At Genki Living. For Snacks and Drinks with light discussion on how to properly have a meetup and what to meetup about.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Garden_grove_meetup1\">Discussion article for the meetup : <a href=\"/meetups/a4\">Garden grove meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Garden grove meetup", "anchor": "Discussion_article_for_the_meetup___Garden_grove_meetup", "level": 1}, {"title": "Discussion article for the meetup : Garden grove meetup", "anchor": "Discussion_article_for_the_meetup___Garden_grove_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T02:33:02.082Z", "modifiedAt": null, "url": null, "title": "Mega Meetup : Summer Festival, hosted by New York", "slug": "mega-meetup-summer-festival-hosted-by-new-york", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:23.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wPaeRnKEsJp8Q4abL/mega-meetup-summer-festival-hosted-by-new-york", "pageUrlRelative": "/posts/wPaeRnKEsJp8Q4abL/mega-meetup-summer-festival-hosted-by-new-york", "linkUrl": "https://www.lesswrong.com/posts/wPaeRnKEsJp8Q4abL/mega-meetup-summer-festival-hosted-by-new-york", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mega%20Meetup%20%3A%20Summer%20Festival%2C%20hosted%20by%20New%20York&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMega%20Meetup%20%3A%20Summer%20Festival%2C%20hosted%20by%20New%20York%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPaeRnKEsJp8Q4abL%2Fmega-meetup-summer-festival-hosted-by-new-york%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mega%20Meetup%20%3A%20Summer%20Festival%2C%20hosted%20by%20New%20York%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPaeRnKEsJp8Q4abL%2Fmega-meetup-summer-festival-hosted-by-new-york", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPaeRnKEsJp8Q4abL%2Fmega-meetup-summer-festival-hosted-by-new-york", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<h2 style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">Discussion article for the meetup :&nbsp;<span style=\"color: #5a5a5b;\">New York Summer Festival Megameetup</span></h2>\n<p><strong style=\"text-transform: uppercase;\">WHEN:</strong><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;Saturday, June 23rd 2012, 1 PM (as well as additional meetups the surrounding days)</span></p>\n<p style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; margin: 0px;\"><strong style=\"text-transform: uppercase;\">WHERE:</strong>&nbsp;<span class=\"address\">Central Park, New York, NY<br /></span></p>\n<p>The weekend of June 23rd, the New York Less Wrong community will be hosting a summer festival. Less Wrongers from around the world are invited to intend. Features include:</p>\n<ul>\n<li>A Saturday of sun and fun in Central Park. (Unless rain is forecast, in which case this will be on Sunday).</li>\n<li>Picnic Lunch. Share food and swap stories with Less Wrong folks. Engage in intellectual discussion while stimulating your monkey brain's desire for tribal bonding.</li>\n<li>Compete in feats of strength and sport.</li>\n<li>Singing and dancing.</li>\n</ul>\n<div>While the main outdoor event will be on Saturday the 23rd, other meetups and parties will likely occur in surrounding days. There will almost certainly be a gatherings on Thursday the 21st and Friday the 22nd. More information will be available as we get a better headcount.</div>\n<div>Let us know if you're thinking of coming, so that we can get an approximate headcount and plan logistics.</div>\n<div>If you'll be in New York any length of time, you may want to join the <a href=\"http://groups.google.com/group/overcomingbiasnyc\">New York mailing list</a>, to learn about additional meetups.</div>\n<div>\n<h2 style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">Discussion article for the meetup :&nbsp;<span style=\"color: #5a5a5b;\">New York Summer Festival Megameetup</span></h2>\n<div><br /></div>\n<p>&nbsp;</p>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wPaeRnKEsJp8Q4abL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 9.029109554007704e-07, "legacy": true, "legacyId": "16090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\" id=\"Discussion_article_for_the_meetup___New_York_Summer_Festival_Megameetup\">Discussion article for the meetup :&nbsp;<span style=\"color: #5a5a5b;\">New York Summer Festival Megameetup</span></h2>\n<p><strong style=\"text-transform: uppercase;\">WHEN:</strong><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;Saturday, June 23rd 2012, 1 PM (as well as additional meetups the surrounding days)</span></p>\n<p style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; margin: 0px;\"><strong style=\"text-transform: uppercase;\">WHERE:</strong>&nbsp;<span class=\"address\">Central Park, New York, NY<br></span></p>\n<p>The weekend of June 23rd, the New York Less Wrong community will be hosting a summer festival. Less Wrongers from around the world are invited to intend. Features include:</p>\n<ul>\n<li>A Saturday of sun and fun in Central Park. (Unless rain is forecast, in which case this will be on Sunday).</li>\n<li>Picnic Lunch. Share food and swap stories with Less Wrong folks. Engage in intellectual discussion while stimulating your monkey brain's desire for tribal bonding.</li>\n<li>Compete in feats of strength and sport.</li>\n<li>Singing and dancing.</li>\n</ul>\n<div>While the main outdoor event will be on Saturday the 23rd, other meetups and parties will likely occur in surrounding days. There will almost certainly be a gatherings on Thursday the 21st and Friday the 22nd. More information will be available as we get a better headcount.</div>\n<div>Let us know if you're thinking of coming, so that we can get an approximate headcount and plan logistics.</div>\n<div>If you'll be in New York any length of time, you may want to join the <a href=\"http://groups.google.com/group/overcomingbiasnyc\">New York mailing list</a>, to learn about additional meetups.</div>\n<div>\n<h2 style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\" id=\"Discussion_article_for_the_meetup___New_York_Summer_Festival_Megameetup1\">Discussion article for the meetup :&nbsp;<span style=\"color: #5a5a5b;\">New York Summer Festival Megameetup</span></h2>\n<div><br></div>\n<p>&nbsp;</p>\n</div>\n<p>&nbsp;</p>", "sections": [{"title": "Discussion article for the meetup :\u00a0New York Summer Festival Megameetup", "anchor": "Discussion_article_for_the_meetup___New_York_Summer_Festival_Megameetup", "level": 1}, {"title": "Discussion article for the meetup :\u00a0New York Summer Festival Megameetup", "anchor": "Discussion_article_for_the_meetup___New_York_Summer_Festival_Megameetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T03:01:19.152Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 5/14/12", "slug": "group-rationality-diary-5-14-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JA2MnwunfZnFXb3by/group-rationality-diary-5-14-12", "pageUrlRelative": "/posts/JA2MnwunfZnFXb3by/group-rationality-diary-5-14-12", "linkUrl": "https://www.lesswrong.com/posts/JA2MnwunfZnFXb3by/group-rationality-diary-5-14-12", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%205%2F14%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%205%2F14%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJA2MnwunfZnFXb3by%2Fgroup-rationality-diary-5-14-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%205%2F14%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJA2MnwunfZnFXb3by%2Fgroup-rationality-diary-5-14-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJA2MnwunfZnFXb3by%2Fgroup-rationality-diary-5-14-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 376, "htmlBody": "<p><em>Background: &nbsp;I and many other attendees at the CFAR rationality minicamp last weekend learned a lot about applied rationality, and made big personal lists of things we wanted to try out in our everyday lives. &nbsp;I think that a regular (weekly or maybe semi-weekly) post where people mention any new interesting habits, decisions, and actions they have taken recently would be cool as a supplement to this; it ought to be rewarding for people to be able to write a list of the cool things they did, and I expect it'll also be interesting for other people to peek in and see the sorts of goals and self-modifications people are working on. &nbsp;Others at minicamp seemed enthusiastic about the idea, so I hope it takes off. &nbsp;Feel free to meta-discuss whether this is a good idea or if it can be done better.</em></p>\n<p><em>Addendum 5/15: By the way, non-minicamp people should feel free to post too! &nbsp;I am highly certain that minicamp attendees are not the only ones working on interesting things in their lives.</em></p>\n<p>This is the public group instrumental rationality diary for the week of May 14th. &nbsp;It's a place to record and chat about it if you have done, or are actively doing, things like:</p>\n<ul>\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and <em>failed</em></li>\n</ul>\n<p>Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p>Discussion's likely to continue gradually through the week, so try to remember to check back now and then!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zv7v2ziqexSn5iS9v": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JA2MnwunfZnFXb3by", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 31, "extendedScore": null, "score": 9.029233572337388e-07, "legacy": true, "legacyId": "16088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T10:21:26.469Z", "modifiedAt": null, "url": null, "title": "I Stand by the Sequences", "slug": "i-stand-by-the-sequences", "viewCount": null, "lastCommentedAt": "2017-08-10T23:01:19.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YAL83XNZfntfKaENv/i-stand-by-the-sequences", "pageUrlRelative": "/posts/YAL83XNZfntfKaENv/i-stand-by-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/YAL83XNZfntfKaENv/i-stand-by-the-sequences", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20Stand%20by%20the%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20Stand%20by%20the%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYAL83XNZfntfKaENv%2Fi-stand-by-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20Stand%20by%20the%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYAL83XNZfntfKaENv%2Fi-stand-by-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYAL83XNZfntfKaENv%2Fi-stand-by-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 747, "htmlBody": "<p><em>Edit, May 21, 2012</em>: Read <a href=\"/lw/4d/youre_calling_who_a_cult_leader/36e\">this comment</a> by Yvain.</p>\n<blockquote>\n<p>Forming your own opinion is no more necessary than building your own furniture.</p>\n</blockquote>\n<p>- <a href=\"https://twitter.com/#!/spaceandgames/status/19519835494\">Peter de Blanc</a></p>\n<p>There's been a lot of talk here lately about how we need better contrarians. I don't agree. I think the Sequences got everything right and I agree with them completely. (This of course makes me a deranged, non-thinking, Eliezer-worshiping fanatic for whom the <a href=\"http://www.acceleratingfuture.com/steven/?p=21\">singularity is a substitute religion</a>. Now that I have&nbsp;&nbsp; <span style=\"color: #333333;\">admitted</span>&nbsp;&nbsp; this, you don't have to point it out a dozen times in the comments.) Even the controversial things, like:</p>\n<ul>\n<li>I think the many-worlds interpretation of quantum mechanics is the closest to correct and you're dreaming if you think the true answer will have no splitting (or I simply do not know enough physics to know why Eliezer is wrong, which I think is pretty unlikely but not totally discountable).</li>\n<li>I think cryonics is a swell idea and an obvious thing to sign up for if you value staying alive and have enough money and can tolerate the social costs.</li>\n<li>I think mainstream science is too slow and we mere mortals can do better with Bayes.</li>\n<li>I am a utilitarian consequentialist and think that if allow someone to die through inaction, you're just as culpable as a murderer. \n<ul>\n<li>I completely accept the conclusion that it is worse to put dust specks in 3^^^3 people's eyes than to torture one person for fifty years. I came up with it independently, so maybe it doesn't count; whatever.</li>\n</ul>\n</li>\n<li>I tentatively accept Eliezer's metaethics, considering how unlikely it is that there will be a better one (maybe morality is in the <em>gluons</em><em></em>?)</li>\n<li><a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html#comment-436140\">\"People are crazy, the world is mad,\"</a> is sufficient for explaining most human failure, even to curious people, so long as they know the heuristics and biases literature.</li>\n<li><em>Edit, May 27, 2012</em>: You know what? I forgot one: <span style=\"text-decoration: underline;\">G&ouml;del, Escher, Bach</span> is the <em>best</em>.</li>\n</ul>\n<p>There are two tiny notes of discord on which I disagree with Eliezer Yudkowsky. One is that I'm not so sure as he is that a rationalist is only made when a person breaks with the world and starts seeing everybody else as crazy, and two is that I don't share his objection to creating conscious entities in the form of an FAI or within an FAI. I could explain, but no one ever discusses these things, and they don't affect any <em>important</em> conclusions. I also think the sequences are badly-organized and you should <a href=\"http://www.quora.com/What-are-some-must-read-Less-Wrong-sequences/answer/George-Koleszarik\">just read them chronologically</a> instead of trying to lump them into categories and sub-categories, but I digress.</p>\n<p>Furthermore, I agree with every essay I've ever read by Yvain, I use \"believe whatever gwern believes\" as a heuristic/algorithm for generating true beliefs, and don't disagree with anything I've ever seen written by Vladimir Nesov, Kaj Sotala, Luke Muelhauser, komponisto, or even Wei Dai; <a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">policy debates should not appear one-sided</a>, so it's good that they don't.</p>\n<p>I write this because I'm feeling more and more lonely, in this regard. If you also stand by the sequences, feel free to say that. If you don't, feel free to say that too, but please don't substantiate it. I don't want this thread to be a low-level rehash of tired debates, though it will surely have some of that in spite of my sincerest wishes.</p>\n<p>Holden Karnofsky&nbsp; <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">said</a>:</p>\n<blockquote>\n<p>I believe I have read the vast majority of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>, including the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI-foom debate</a>, and that this content - while interesting and enjoyable - does not have much relevance for the arguments I've made.</p>\n</blockquote>\n<p>I can't understand this. How could the sequences&nbsp; <em>not</em>&nbsp; be relevant? Half of them were created when Eliezer was thinking about AI problems.</p>\n<p>So I say this, hoping others will as well:<br />I stand by the sequences.</p>\n<p>And with that, I tap out.&nbsp; <a href=\"/lw/gt/a_fable_of_science_and_politics/dq2\">I have found the answer, so I am leaving the conversation</a>.</p>\n<p>Even though I am not important here, I don't want you to <a href=\"/lw/bvg/a_question_about_eliezer/6eyz\">interpret my silence from now on as indicating compliance</a>.</p>\n<p><sub>After some degree of thought and nearly 200 comment replies on this article, I regret writing it. I was <em>insufficiently careful</em>, didn't think enough about how it might alter the social dynamics here, and didn't spend enough time clarifying, especially regarding the third bullet point. I also dearly hope that I have not entrenched anyone's positions, turning them into allied soldiers to be defended, especially not my own. I'm sorry.</sub></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YAL83XNZfntfKaENv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 107, "baseScore": 14, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "16106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 250, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PeSzc9JTBxhaYRp9b", "6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T10:23:27.997Z", "modifiedAt": null, "url": null, "title": "General purpose intelligence: arguing the Orthogonality thesis", "slug": "general-purpose-intelligence-arguing-the-orthogonality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nvKZchuTW8zY6wvAj/general-purpose-intelligence-arguing-the-orthogonality", "pageUrlRelative": "/posts/nvKZchuTW8zY6wvAj/general-purpose-intelligence-arguing-the-orthogonality", "linkUrl": "https://www.lesswrong.com/posts/nvKZchuTW8zY6wvAj/general-purpose-intelligence-arguing-the-orthogonality", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20General%20purpose%20intelligence%3A%20arguing%20the%20Orthogonality%20thesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneral%20purpose%20intelligence%3A%20arguing%20the%20Orthogonality%20thesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnvKZchuTW8zY6wvAj%2Fgeneral-purpose-intelligence-arguing-the-orthogonality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=General%20purpose%20intelligence%3A%20arguing%20the%20Orthogonality%20thesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnvKZchuTW8zY6wvAj%2Fgeneral-purpose-intelligence-arguing-the-orthogonality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnvKZchuTW8zY6wvAj%2Fgeneral-purpose-intelligence-arguing-the-orthogonality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5534, "htmlBody": "<p>Note: <em>informally, the point of this paper is to argue against the instinctive \"if the AI were so smart, it would figure out the right morality and everything will be fine.\" It is&nbsp;targeted&nbsp;mainly at philosophers, not at AI programmers. The paper succeeds if it forces proponents of that position to put forwards positive arguments, rather than just assuming it as the default position. This post is presented as an academic paper, and will hopefully be published, so any comments and advice are welcome, including stylistic ones! Also let me know if I've forgotten you in the&nbsp;acknowledgements.</em></p>\n<p><em><br /></em></p>\n<p style=\"padding-left: 60px;\"><strong>Abstract</strong>: In his paper &ldquo;The Superintelligent Will&rdquo;, Nick Bostrom formalised the Orthogonality thesis: the idea that the final goals and intelligence levels of agents are independent of each other. This paper presents arguments for a (slightly narrower) version of the thesis, proceeding through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with any goal. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents.</p>\n<p>&nbsp;</p>\n<h2>1 The Orthogonality thesis</h2>\n<p>The Orthogonality thesis, due to Nick Bostrom (Bostrom, 2011), states that:</p>\n<ul>\n<li>Intelligence and final goals are orthogonal axes along which possible agents can freely vary: more or less any level of intelligence could in principle be combined with more or less any final goal.</li>\n</ul>\n<p>It is analogous to Hume&rsquo;s thesis about the independence of reason and morality (Hume, 1739), but applied more narrowly, using the normatively thinner concepts &lsquo;intelligence&rsquo; and &lsquo;final goals&rsquo; rather than &lsquo;reason&rsquo; and &lsquo;morality&rsquo;.</p>\n<p>But even &lsquo;intelligence&rsquo;, as generally used, has too many connotations. A better term would be efficiency, or instrumental rationality, or the ability to effectively solve problems given limited knowledge and resources (Wang, 2011). Nevertheless, we will be sticking with terminology such as &lsquo;intelligent agent&rsquo;, &lsquo;artificial intelligence&rsquo; or &lsquo;superintelligence&rsquo;, as they are well established, but using them synonymously with &lsquo;efficient agent&rsquo;, artificial efficiency&rsquo; and &lsquo;superefficient algorithm&rsquo;. The relevant criteria is whether the agent can effectively achieve its goals in general situations, not whether its inner process matches up with a particular definition of what intelligence is.<a id=\"more\"></a></p>\n<p>Thus an artificial intelligence (AI) is an artificial algorithm, deterministic or probabilistic, implemented on some device, that demonstrates an ability to achieve goals in varied and general situations<a id=\"footnote-1-ref\" href=\"#footnote-1\">[1]</a>. We don&rsquo;t assume that it need be a computer program, or a well laid-out algorithm with clear loops and structures &ndash; artificial neural networks or evolved genetic algorithms certainly qualify.</p>\n<p>A human level AI is defined to be an AI that can successfully accomplish any task at least as well as an average human would (to avoid worrying about robot bodies and such-like, we may restrict the list of tasks to those accomplishable over the internet). Thus we would expect the AI to hold conversations about Paris Hilton&rsquo;s sex life, to compose ironic limericks, to shop for the best deal on Halloween costumes and to debate the proper role of religion in politics, at least as well as an average human would.</p>\n<p>A superhuman AI is similarly defined as an AI that would exceed the ability of the best human in all (or almost all) tasks. It would do the best research, write the most successful novels, run companies and motivate employees better than anyone else. In areas where there may not be clear scales (what&rsquo;s the world&rsquo;s best artwork?) we would expect a majority of the human population to agree the AI&rsquo;s work is among the very best.</p>\n<p>Nick Bostrom&rsquo;s paper argued that the Orthogonality thesis does not depend on the Humean theory of motivation. This paper will directly present arguments in its favour. We will assume throughout that human level AIs (or at least human comparable AIs) are possible (if not, the thesis is void of useful content). We will also take the materialistic position that humans themselves can be viewed as non-deterministic algorithms<a id=\"footnote-2-ref\" href=\"#footnote-2\">[2]</a>: this is not vital to the paper, but is useful for comparison of goals between various types of agents. We will do the same with entities such as committees of humans, institutions or corporations, if these can be considered to be acting in an agent-like way.</p>\n<h3>1.1 Qualifying the Orthogonality thesis</h3>\n<p>The Orthogonality thesis, taken literally, is false. Some motivations are mathematically incompatible with changes in intelligence (&ldquo;I want to prove the G&ouml;del statement for the being I would be if I were more intelligent&rdquo;). Some goals specifically refer to the intelligence of the agent, directly (&ldquo;I want to be an idiot!&rdquo;) or indirectly (&ldquo;I want to impress people who want me to be an idiot!&rdquo;). Though we could make a case that an agent wanting to be an idiot could initially be of any intelligence level, it won&rsquo;t stay there long, and it&rsquo;s hard to see how an agent with that goal could have become intelligent in the first place. So we will exclude from consideration those goals that intrinsically refer to the intelligence level of the agent.</p>\n<p>We will also exclude goals that are so complex or hard to describe that the complexity of the goal becomes crippling for the agent. If the agent&rsquo;s goal takes five planets worth of material to describe, or if it takes the agent five years each time it checks its goal, it&rsquo;s obvious that that agent can&rsquo;t function as an intelligent being on any reasonable scale.</p>\n<p>Further we will not try to show that intelligence and final goals can vary freely, in any dynamical sense (it could be quite hard to define this varying). Instead we will look at the thesis as talking about possible states: that there exist agents of all levels of intelligence with any given goals. Since it&rsquo;s always possible to make an agent stupider or less efficient, what we are really claiming is that there exist high-intelligence agents with any given goal. Thus the restricted Orthogonality thesis that we will be discussing is:</p>\n<ul>\n<li>High-intelligence agents can exist having more or less any final goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</li>\n</ul>\n<div><br /></div>\n<h2>2 Orthogonality for theoretic agents</h2>\n<p>If we were to step back for a moment and consider, in our mind&rsquo;s eyes, the space of every possible algorithm, peering into their goal systems and teasing out some measure of their relative intelligences, would we expect the Orthogonality thesis to hold? Since we are not worrying about practicality or constructability, all that we would require is that for any given goal system, there exists a theoretically implementable algorithm of extremely high intelligence.</p>\n<p>At this level of abstraction, we can consider any goal to be equivalent with maximising a utility function. It is generally not that hard to translate given goals into utilities (many deontological systems are equivalent with maximising the expected utility of a function that gives 1 if the agent always makes the correct decision and 0 otherwise), and any agent making a finite number of decisions can always be seen as maximising a certain utility function.</p>\n<p>For utility function maximisers, the AIXI is the theoretically best agent there is, more successful at reaching its goals (up to a finite constant) than any other agent (Hutter, 2005). AIXI itself is incomputable, but there are computable variants such as AIXItl or G&ouml;del machines (Schmidhuber, 2007) that accomplish comparable levels of efficiency. These methods work for whatever utility function is plugged into them. Thus in the extreme theoretical case, the Orthogonality thesis seems trivially true.</p>\n<p>There is only one problem with these agents: they require incredibly large amounts of computing resources to work. Let us step down from the theoretical pinnacle and require that these agents could actually exist in our world (still not requiring that we be able or likely to build it).</p>\n<p>An interesting thought experiment occurs here. We could imagine an AIXI-like super-agent, with all its resources, that is tasked to design and train an AI that could exist in our world, and that would accomplish the super-agent&rsquo;s goals. Using its own vast intelligence, the super-agent would therefore design a constrained agent maximally effective at accomplishing those goals in our world. Then this agent would be the high-intelligence real-world agent we are looking for. It doesn&rsquo;t matter that this is a thought experiment &ndash; if the super-agent can succeed in the thought experiment, then the trained AI can exist in our world.</p>\n<p>This argument generalises to other ways of producing the AI. Thus to deny the Orthogonality thesis is to assert that there is a goal system G, such that, among other things:</p>\n<ol>\n<li>There cannot exist any efficient real-world algorithm with goal G.</li>\n<li>If a being with arbitrarily high resources, intelligence, time and goal G, were to try design an efficient real-world algorithm with the same goal, it must fail.</li>\n<li>If a human society were highly motivated to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.</li>\n<li>If a high-resource human society were highly motivated to achieve the goals of G, then it could not do so (here the human society is seen as the algorithm).</li>\n<li>Same as above, for any hypothetical alien societies.</li>\n<li>There cannot exist <em>any</em> pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G.</li>\n<li>There cannot exist any evolutionary or environmental pressures that would evolving highly efficient real world intelligences to follow goal G.</li>\n</ol>\n<p>All of these seem extraordinarily strong claims to make! The last claims all derive from the first, and merely serve to illustrate how strong the first claim actually is. Thus until such time as someone comes up with such a G and strong arguments for why it must fulfil these conditions, we can assume the Orthogonality statement established in the theoretical case.</p>\n<p>&nbsp;</p>\n<h2>3 Orthogonality for human-level AIs</h2>\n<p>Of course, even if efficient agents could exist for all these goals, that doesn&rsquo;t mean that we could ever build them, even if we could build AIs. In this section, we&rsquo;ll look at the ground for assuming the Orthogonality thesis holds for human-level agents. Since intelligence isn&rsquo;t varying much, the thesis becomes simply:</p>\n<ul>\n<li>If we could construct human-level AIs at all, we could construct human-level AIs with more or less any final goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</li>\n</ul>\n<p>So, is this true? The arguments in this section are generally independent of each other, and can be summarised as:</p>\n<ol>\n<li>Some possible AI designs have orthogonality built right into them.</li>\n<li>AI goals can reach the span of human goals, which is larger than it seems.</li>\n<li>Algorithms can be combined to generate an AI with any easily checkable goal.</li>\n<li>Various algorithmic modifications can be used to further expand the space of possible goals, if needed.</li>\n</ol>\n<h3>3.1 Utility functions</h3>\n<p>The utility function picture of a rational agent maps perfectly onto the Orthogonality thesis: here have the goal structure, the utility function, packaged neatly and separately from the intelligence module (whatever part of the machine calculates which actions maximise expected utility). Demonstrating the Orthogonality thesis is as simple as saying that the utility function can be replaced with another. However, many putative agent designs are not utility function based, such as neural networks, genetic algorithms, or humans. Nor do we have the extreme calculating ability that we had in the purely theoretic case to transform any goals into utility functions. So from now on we will consider that our agents are not expected utility maximisers with clear and separate utility functions.</p>\n<h3>3.2 The span of human motivations</h3>\n<p>It seems a reasonable assumption that if there exists a human being with particular goals, then we can construct a human-level AI with similar goals. This is immediately the case if the AI was a whole brain emulation/upload (Sandberg &amp; Bostrom, 2008), a digital copy of a specific human mind. Even for more general agents, such as evolved agents, this remains a reasonable thesis. For a start, we know that real-world evolution has produced us, so constructing human-like agents that way is certainly possible. Human minds remain our only real model of general intelligence, and this strongly direct and informs our AI designs, which are likely to be as human-similar as we can make them. Similarly, human goals are the easiest goals for us to understand, hence the easiest to try and implement in AI. Hence it seems likely that we could implement most human goals in the first generation of human-level AIs.</p>\n<p>So how wide is the space of human motivations<a id=\"footnote-3-ref\" href=\"#footnote-3\">[3]</a>? Our race spans foot-fetishists, religious saints, serial killers, instinctive accountants, role-players, self-cannibals, firefighters and conceptual artists. The autistic, those with exceptional social skills, the obsessive compulsive and some with split-brains. Beings of great empathy and the many who used to enjoy torture and executions as public spectacles<a id=\"footnote-4-ref\" href=\"#footnote-4\">[4]</a>. It is evident that the space of possible human motivations is vast<a id=\"footnote-5-ref\" href=\"#footnote-5\">[5]</a>. For any desire, any particular goal, no matter how niche<a id=\"footnote-6-ref\" href=\"#footnote-6\">[6]</a>, pathological, bizarre or extreme, as long as there is a single human who ever had it, we could build and run an AI with the same goal.</p>\n<p>But with AIs we can go even further. We could take any of these goals as a starting point, make them malleable (as goals are in humans), and push them further out. We could provide the AIs with specific reinforcements to push their goals in extreme directions (reward the saint for ever-more saintly behaviour). If the agents are fast enough, we could run whole societies of them with huge varieties of evolutionary or social pressures, to further explore the goal-space.</p>\n<p>We may also be able to do surgery directly on their goals, to introduce more yet variety. For example, we could take a dedicated utilitarian charity worker obsessed with saving lives in poorer countries (but who doesn&rsquo;t interact, or want to interact, directly with those saved), and replace &lsquo;saving lives&rsquo; with &lsquo;maximising paperclips&rsquo; or any similar abstract goal. This is more speculative, of course &ndash; but there are other ways of getting similar results.</p>\n<h3>3.3 Interim goals as terminal goals</h3>\n<p>If someone were to hold a gun to your head, they could make you do almost anything. Certainly there are people who, with a gun at their head, would be willing to do almost anything. A distinction is generally made between interim goals and terminal goals, with the former being seen as simply paths to the latter, and interchangeable with other plausible paths. The gun to your head disrupts the balance: your terminal goal is simply not to get shot, while your interim goals become what the gun holder wants them to be, and you put a great amount of effort into accomplishing the minute details of these interim goals. Note that the gun has not changed your level of intelligence or ability.</p>\n<p>This is relevant because interim goals seem to be far more varied in humans than terminal goals. One can have interim goals of filling papers, solving equations, walking dogs, making money, pushing buttons in various sequences, opening doors, enhancing shareholder value, assembling cars, bombing villages or putting sharks into tanks. Or simply doing whatever the guy with gun at our head orders us to do. If we could accept human interim goals as AI terminal goals, we would extend the space of goals quite dramatically.</p>\n<p>To do we would want to put the threatened agent, and the gun wielder, together into the same AI. Algorithmically there is nothing extraordinary about this: certain subroutines have certain behaviours depending on the outputs of other subroutines. The &lsquo;gun wielder&rsquo; need not be particularly intelligent: it simply needs to be able to establish whether its goals are being met. If for instance those goals are given by a utility function then all that is required in an automated system that measure progress toward increasing utility and punishes (or erases) the rest of the AI if not. The &lsquo;rest of AI&rsquo; is just required to be a human-level AI which would be susceptible to this kind of pressure. Note that we do not require that it even be close to human in any way, simply that it place a highest value on self-preservation (or on some similar small goal that the &lsquo;gun wielder&rsquo; would have power over).</p>\n<p>For humans, another similar model is that of a job in a corporation or bureaucracy: in order to achieve the money required for their terminal goals, some human are willing to perform extreme tasks (organising the logistics of genocides, weapon design, writing long detailed press releases they don&rsquo;t agree with at all). Again, if the corporation-employee relationship can be captured in a single algorithm, this would generate an intelligent AI whose goal is anything measurable by the &lsquo;corporation&rsquo;. The &lsquo;money&rsquo; could simply be an internal reward channel, perfectly aligning the incentives.</p>\n<p>If the subagent is anything like a human, they would quickly integrate the other goals into their own motivation<a id=\"footnote-7-ref\" href=\"#footnote-7\">[7]</a>, removing the need for the gun wielder/corporation part of the algorithm.</p>\n<h3>3.4 Noise, anti-agents and goal combination</h3>\n<p>There are further ways of extending the space of goals we could implement in human-level AIs. One simple way is simply to introduce noise: flip a few bits and subroutines, add bugs and get a new agent. Of course, this is likely to cause the agent&rsquo;s intelligence to decrease somewhat, but we have generated new goals. Then, if appropriate, we could use evolution or other improvements to raise the agent&rsquo;s intelligence again; this will likely undo some, but not all of effect of the noise. Or we could use some of the tricks above to make a smarter agent implement the goals of the noise-modified agent.</p>\n<p>A more extreme example would be to create an anti-agent: an agent whose single goal is to stymie the plans and goals of single given agent. This already happens with vengeful humans, and we would just need to dial it up: have an anti-agent that would do all it can to counter the goals of a given agent, even if that agent doesn&rsquo;t exist (&ldquo;I don&rsquo;t care that you&rsquo;re dead, I&rsquo;m still going to despoil your country, because that&rsquo;s what you&rsquo;d wanted me to not do&rdquo;). This further extends the space of possible goals.</p>\n<p>Different agents with different goals can also be combined into a single algorithm. With some algorithmic method for the AIs to negotiate their combined objective and balance the relative importance of their goals, this procedure would construct a single AI with a combined goal system. There would likely be no drop in intelligence/efficiency: committees of two can work very well towards their common goals, especially if there is some automatic penalty for disagreements.</p>\n<h3>3.5 Further tricks up the sleeve</h3>\n<p>This section started by emphasising the wide space of human goals, and then introduced tricks to push goal systems further beyond these boundaries. The list isn&rsquo;t exhaustive: there are surely more devices and ideas one can use to continue to extend the space of possible goals for human-level AIs. Though this might not be enough to get every goal, we can nearly certainly use these procedures to construct a human-level AI with any human-comprehensible goal. But would the same be true for superhuman AIs?</p>\n<p>&nbsp;</p>\n<h2>4 Orthogonality for superhuman AIs</h2>\n<p>We now come to the area where the Orthogonality thesis seems the most vulnerable. It is one thing to have human-level AIs, or abstract superintelligent algorithms created ex nihilo, with certain goals. But if ever the human race were to design a superintelligent AI, there would be some sort of process involved &ndash; directed evolution, recursive self-improvement (Yudkowsky, 2001), design by a committee of AIs, or similar &ndash; and it seems at least possible that such a process could fail to fully explore the goal-space. If we define the Orthogonality thesis in this context as:</p>\n<ul>\n<li>If we could construct superintelligent AIs at all, we could construct superintelligent AIs with more or less any goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</li>\n</ul>\n<p>There are two counter-theses. The weakest claim is:</p>\n<ul>\n<li>Incompleteness: there are some goals that no superintelligence designed by us could have.</li>\n</ul>\n<p>A stronger claim is:</p>\n<ul>\n<li>Convergence: all human-designed superintelligences would have one of a small set of goals.</li>\n</ul>\n<p>They should be distinguished; Incompleteness is all that is needed to contradict Orthogonality, but Convergence is often the issue being discussed. Often convergence is assumed to be to some particular model of metaethics (M&uuml;ller, 2012).</p>\n<h3>4.1 No convergence</h3>\n<p>The plausibility of the convergence thesis is highly connected with the connotations of the terms used in it. &ldquo;All human-designed rational beings would follow the same morality (or one of small sets of moralities)&rdquo; sound plausible; in contract &ldquo;All human-designed superefficient algorithms would accomplish the same task&rdquo; seems ridiculous. To quote an online commentator, how good at playing chess would a chess computer have to be before it started feeding the hungry?</p>\n<p>Similarly, if there were such a convergence, then all self-improving or constructed superintelligence must fall prey to it, even if it were actively seeking to avoid it. After all, the lower-level AIs or the AI designers have certain goals in mind (as we&rsquo;ve seen in the previous section, potentially any goals in mind). Obviously, they would be less likely to achieve their goals if these goals were to change (Omohundro, 2008) (Bostrom, 2012). The same goes if the superintelligent AI they designed didn&rsquo;t share these goals. Hence the AI designers will be actively trying to prevent such a convergence, if they suspected that one was likely to happen. If for instance their goals were immoral, they would program their AI not to care about morality; they would use every trick up their sleeves to prevent the AI&rsquo;s goals from drifting from their own.</p>\n<p>So the convergence thesis requires that for the vast majority of goals G:</p>\n<ol>\n<li>It is possible for a superintelligence to exist with goal G (by section 2).</li>\n<li>There exists an entity with goal G (by section 3), capable of building a superintelligent AI.</li>\n<li>Yet any attempt of that entity to build a superintelligent AI with goal G will be a failure, and the superintelligence&rsquo;s goals will converge on some other goal.</li>\n<li>This is true even if the entity is aware of the convergence and explicitly attempts to avoid it.</li>\n</ol>\n<p>This makes the convergence thesis very unlikely. The argument also works against the incompleteness thesis, but in a weaker fashion: it seems more plausible that <em>some</em> goals would be unreachable, despite being theoretically possible, rather than <em>most</em> goals being unreachable because of convergence to a small set.</p>\n<p>There is another interesting aspect of the convergence thesis: it postulates that certain goals G will emerge, without them being aimed for or desired. If one accepts that goals aimed for will not be reached, one has to ask why convergence is assumed: why not divergence? Why not assume that though G is aimed for, random accidents or faulty implementation will lead to the AI ending up with one of a much wider array of possible goals, rather than a much narrower one.</p>\n<h3>4.2 Oracles show the way</h3>\n<p>If the Orthogonality thesis is wrong, then it implies that Oracles are impossible to build. An Oracle is a superintelligent AI that accurately answers questions about the world (Armstrong, Sandberg, &amp; Bostrom, 2011). This includes hypothetical questions about the future, which means that we can produce a superintelligent AI with goal G by wiring a human-level AI with goal G to an Oracle: the human level AI will go through possible actions, have the Oracle check the outcomes, and choose the one that best accomplishes G.</p>\n<p>What makes the &ldquo;no Oracle&rdquo; position even more counterintuitive is that any superintelligence must be able to look ahead, design actions, predict the consequences of its actions, and choose the best one available. But the convergence thesis implies that this general skill is one that we can make available only to AIs with certain specific goals. Though the agents with those narrow goals are capable of doing these predictions, they automatically lose this ability if their goals were to change.</p>\n<h3>4.3 Tricking the controller</h3>\n<p>Just as with human-level AIs, one could construct a superintelligent AI by wedding together a superintelligence with a large dedicated committee of human-level AIs dedicated to implementing a goal G, and checking the superintelligence&rsquo;s actions. Thus to deny the Orthogonality thesis requires that one believes that the superintelligence is always capable of tricking this committee, no matter how detailed and thorough their oversight.</p>\n<p>This argument extends the Orthogonality thesis to moderately superintelligent AIs, or to any situation where there was a diminishing return to intelligence. It only fails if we take AI to be fantastically superhuman: capable of tricking or seducing any collection of human-level beings.</p>\n<h3>4.4 Temporary fragments of algorithms, fictional worlds and extra tricks</h3>\n<p>These are other tricks that can be used to create an AI with any goals. For any superintelligent AI, there are certain inputs that will make it behave in certain ways. For instance, a human-loving moral AI could be compelled to follow most goals G for a day, if they were rewarded with something sufficiently positive afterwards. But its actions for that one day are the result of a series of inputs to a particular algorithm; if we turned off the AI after that day, we would have accomplished moves towards goal G without having to reward its &ldquo;true&rdquo; goals at all. And then we could continue the trick the next day with another copy.</p>\n<p>For this to fail, it has to be the case that we can create an algorithm which will perform certain actions on certain inputs as long as it isn&rsquo;t turned off afterwards, but that we cannot create an algorithm that does the same thing if it was to be turned off.</p>\n<p>Another alternative is to create a superintelligent AI that has goals in a fictional world (such as a game or a reward channel) over which we have control. Then we could trade interventions in the fictional world against advice in the real world towards whichever goals we desire.</p>\n<p>These two arguments may feel weaker than the ones before: they are tricks that may or may not work, depending on the details of the AI&rsquo;s setup. But to deny the Orthogonality thesis requires not only denying that these tricks would ever work, but denying that any tricks or methods that we (or any human-level AIs) could think up, would ever work at controlling the AIs. We need to assume superintelligent AIs cannot be controlled.</p>\n<h3>4.5 In summary</h3>\n<p>Denying the Orthogonality thesis thus requires that:</p>\n<ol>\n<li>There are goals G, such that an entity an entity with goal G cannot build a superintelligence with the same goal. This despite the fact that the entity can build a superintelligence, and that a superintelligence will goal G can exist.</li>\n<li>Goal G cannot arise accidentally from some other origin, and errors and ambiguities do not significantly broaden the space of possible goals.</li>\n<li>Oracles and general purpose planners cannot be built. Superintelligent AIs cannot have their planning abilities repurposed.</li>\n<li>A superintelligence will always be able to trick its controllers, and there is no way the controllers can set up a reasonable system of control.</li>\n<li>Though we can create an algorithm that does certain actions if it was not to be turned off after, we cannot create an algorithm that does the same thing if it was to be turned off after.</li>\n<li>An AI will always come to care intrinsically about things in the real world.</li>\n<li>No tricks can be thought up to successfully constrain the AI&rsquo;s goals: superintelligent AIs cannot be controlled.</li>\n</ol>\n<p>&nbsp;</p>\n<h2>5 Bayesian Orthogonality thesis</h2>\n<p>All the previous sections concern hypotheticals, but of a different kind. Section 2 touches upon what kinds of algorithm could theoretically exist. But sections 3 and 4 concern algorithms that could be constructed by humans (or from AIs originally constructed by humans): they refer to the future. As AI research advances, and certain approaches or groups start to show or lose prominence, we&rsquo;ll start getting a better idea of how such an AI will emerge.</p>\n<p>Thus the orthogonality thesis will narrow as we achieve better understanding of how AIs would work in practice, of what tasks they will be put to and of what requirements their designers will desire. Most importantly of all, we will get more information on the critical question as to whether the designers will actually be able to implement their desired goals in an AI. On the eve of creating the first AIs (and then the first superintelligent AIs), the Orthogonality thesis will likely have pretty much collapsed: yes, we <em>could</em> in theory construct an AI with any goal, but at that point, the most likely outcome is an AI with particular goals &ndash; either the goals desired by their designers, or specific undesired goals and error modes.</p>\n<p>However, until that time arises, because we do not know any of this information currently, we remain in the grip of a Bayesian version of the Orthogonality thesis:</p>\n<ul>\n<li><em>As far as we know now</em> (and as far as we&rsquo;ll know until we start building AIs), if we could construct superintelligent AIs at all, we could construct superintelligent AIs with more or less any goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent&rsquo;s intelligence).</li>\n</ul>\n<p>&nbsp;</p>\n<h2>6 Conclusion</h2>\n<p>It is not enough to know that an agent is intelligent (or superintelligent). If we want to know something about its final goals, about the actions it will be willing to undertake to achieve them, and hence its ultimate impact on the world, there are no shortcuts. We have to directly figure out what these goals are, and cannot rely on the agent being moral just because it is superintelligent/superefficient.</p>\n<p>&nbsp;</p>\n<h2>7 Acknowledgements</h2>\n<p>It gives me great pleasure to acknowledge the help and support of Anders Sandberg, Nick Bostrom, Toby Ord, Owain Evans, Daniel Dewey, Eliezer Yudkowsky, Vladimir Slepnev, Viliam Bur, Matt Freeman, Wei Dai, Will Newsome, Paul Crowley, Alexander Kruel and Rasmus Eide, as well as those members of the Less Wrong online community going by the names shminux, Larks and Dmytry.</p>\n<p>&nbsp;</p>\n<h2>8 Bibliography</h2>\n<p>Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2011). Thinking Inside the Box: Controlling and Using an Oracle AI. forthcoming in Minds and Machines .</p>\n<p>Bostrom, N. (2012). Superintelligence: Groundwork to a Strategic Analysis of the Machine Intelligence Revolution. to be published.</p>\n<p>Bostrom, N. (2011). The Superintelligent Will: Motivation and Instrumental Rationality in Advance Artificial Agents. forthcoming in Minds and Machines .</p>\n<p>de Fabrique, N., Romano, S. J., Vecchi, G. M., &amp; van Hasselt, V. B. (2007). Understanding Stockholm Syndrome. FBI Law Enforcement Bulletin (Law Enforcement Communication Unit) , 76 (7), 10-15.</p>\n<p>Hume, D. (1739). A Treatise of Human Nature.&nbsp;</p>\n<p>Hutter, M. (2005). Universal algorithmic intelligence: A mathematical top-down approach. In B. Goertzel, &amp; C. Pennachin (Eds.), Arti\ufb01cial General Intelligence. Springer-Verlag.</p>\n<p>M&uuml;ller, J. (2012). Ethics, risks and opportunities of superintelligences. Retrieved May 2012, from http://www.jonatasmuller.com/superintelligences.pdf</p>\n<p>Omohundro, S. M. (2008). The Basic AI Drives. In P. Wang, B. Goertzel, &amp; S. Franklin (Eds.), Artificial General Intelligence: Proceedings of the First AGI Conference (Vol. 171).</p>\n<p>Sandberg, A., &amp; Bostrom, N. (2008). Whole brain emulation: A roadmap. Future of Humanity Institute Technical report , 2008-3.</p>\n<p>Schmidhuber, J. (2007). G&ouml;del machines: Fully self-referential optimal universal self-improvers. In Artificial General Intelligence. Springer.</p>\n<p>Wang, P. (2011). The assumptions on knowledge and resources in models of rationality. International Journal of Machine Consciousness , 3 (1), 193-218.</p>\n<p>Yudkowsky, E. (2001). General Intelligence and Seed AI 2.3. Retrieved from Singularity Institute for Artificial Intelligence: http://singinst.org/ourresearch/publications/GISAI/</p>\n<h3>Footnotes</h3>\n<p id=\"footnote-1\"><a href=\"#footnote-1-ref\">[1]</a> We need to assume it has goals, of course. Determining whether something qualifies as a goal-based agent is very tricky (researcher Owain Evans is trying to establish a rigorous definition), but this paper will adopt the somewhat informal definition that an agent has goals if it achieves similar outcomes from very different starting positions. If the agent ends up making ice cream in any circumstances, we can assume ice creams are in its goals.</p>\n<p id=\"footnote-2\"><a href=\"#footnote-2-ref\">[2]</a> Every law of nature being algorithmic (with some probabilistic process of known odds), and no exceptions to these laws being known.</p>\n<p id=\"footnote-3\"><a href=\"#footnote-3-ref\">[3]</a> One could argue that we should consider the space of general animal intelligences &ndash; octopuses, supercolonies of social insects, etc... But we won&rsquo;t pursue this here; the methods described can already get behaviours like this.</p>\n<p id=\"footnote-4\"><a href=\"#footnote-4-ref\">[4]</a> Even today, many people have had great fun torturing and abusing their characters in games like &ldquo;the Sims&rdquo; (http://meodia.com/article/281/sadistic-ways-people-torture-their-sims/). The same urges are present, albeit diverted to fictionalised settings. Indeed games offer a wide variety of different goals that could conceivably be imported into an AI if it were possible to erase the reality/fiction distinction in its motivation.</p>\n<p id=\"footnote-5\"><a href=\"#footnote-5-ref\">[5]</a> As can be shown by a glance through a biography of famous people &ndash; and famous means they were generally allowed to rise to prominence in their own society, so the space of possible motivations was already cut down.</p>\n<p id=\"footnote-6\"><a href=\"#footnote-6-ref\">[6]</a> Of course, if we built an AI with that goal and copied it millions of times, it would no longer be niche.</p>\n<p id=\"footnote-7\"><a href=\"#footnote-7-ref\">[7]</a> Such as the hostages suffering from Stockholm syndrome (de Fabrique, Romano, Vecchi, &amp; van Hasselt, 2007).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 2, "xHTXnyp65X8YX6ahT": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nvKZchuTW8zY6wvAj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 32, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "16075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Note: <em>informally, the point of this paper is to argue against the instinctive \"if the AI were so smart, it would figure out the right morality and everything will be fine.\" It is&nbsp;targeted&nbsp;mainly at philosophers, not at AI programmers. The paper succeeds if it forces proponents of that position to put forwards positive arguments, rather than just assuming it as the default position. This post is presented as an academic paper, and will hopefully be published, so any comments and advice are welcome, including stylistic ones! Also let me know if I've forgotten you in the&nbsp;acknowledgements.</em></p>\n<p><em><br></em></p>\n<p style=\"padding-left: 60px;\"><strong>Abstract</strong>: In his paper \u201cThe Superintelligent Will\u201d, Nick Bostrom formalised the Orthogonality thesis: the idea that the final goals and intelligence levels of agents are independent of each other. This paper presents arguments for a (slightly narrower) version of the thesis, proceeding through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with any goal. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents.</p>\n<p>&nbsp;</p>\n<h2 id=\"1_The_Orthogonality_thesis\">1 The Orthogonality thesis</h2>\n<p>The Orthogonality thesis, due to Nick Bostrom (Bostrom, 2011), states that:</p>\n<ul>\n<li>Intelligence and final goals are orthogonal axes along which possible agents can freely vary: more or less any level of intelligence could in principle be combined with more or less any final goal.</li>\n</ul>\n<p>It is analogous to Hume\u2019s thesis about the independence of reason and morality (Hume, 1739), but applied more narrowly, using the normatively thinner concepts \u2018intelligence\u2019 and \u2018final goals\u2019 rather than \u2018reason\u2019 and \u2018morality\u2019.</p>\n<p>But even \u2018intelligence\u2019, as generally used, has too many connotations. A better term would be efficiency, or instrumental rationality, or the ability to effectively solve problems given limited knowledge and resources (Wang, 2011). Nevertheless, we will be sticking with terminology such as \u2018intelligent agent\u2019, \u2018artificial intelligence\u2019 or \u2018superintelligence\u2019, as they are well established, but using them synonymously with \u2018efficient agent\u2019, artificial efficiency\u2019 and \u2018superefficient algorithm\u2019. The relevant criteria is whether the agent can effectively achieve its goals in general situations, not whether its inner process matches up with a particular definition of what intelligence is.<a id=\"more\"></a></p>\n<p>Thus an artificial intelligence (AI) is an artificial algorithm, deterministic or probabilistic, implemented on some device, that demonstrates an ability to achieve goals in varied and general situations<a id=\"footnote-1-ref\" href=\"#footnote-1\">[1]</a>. We don\u2019t assume that it need be a computer program, or a well laid-out algorithm with clear loops and structures \u2013 artificial neural networks or evolved genetic algorithms certainly qualify.</p>\n<p>A human level AI is defined to be an AI that can successfully accomplish any task at least as well as an average human would (to avoid worrying about robot bodies and such-like, we may restrict the list of tasks to those accomplishable over the internet). Thus we would expect the AI to hold conversations about Paris Hilton\u2019s sex life, to compose ironic limericks, to shop for the best deal on Halloween costumes and to debate the proper role of religion in politics, at least as well as an average human would.</p>\n<p>A superhuman AI is similarly defined as an AI that would exceed the ability of the best human in all (or almost all) tasks. It would do the best research, write the most successful novels, run companies and motivate employees better than anyone else. In areas where there may not be clear scales (what\u2019s the world\u2019s best artwork?) we would expect a majority of the human population to agree the AI\u2019s work is among the very best.</p>\n<p>Nick Bostrom\u2019s paper argued that the Orthogonality thesis does not depend on the Humean theory of motivation. This paper will directly present arguments in its favour. We will assume throughout that human level AIs (or at least human comparable AIs) are possible (if not, the thesis is void of useful content). We will also take the materialistic position that humans themselves can be viewed as non-deterministic algorithms<a id=\"footnote-2-ref\" href=\"#footnote-2\">[2]</a>: this is not vital to the paper, but is useful for comparison of goals between various types of agents. We will do the same with entities such as committees of humans, institutions or corporations, if these can be considered to be acting in an agent-like way.</p>\n<h3 id=\"1_1_Qualifying_the_Orthogonality_thesis\">1.1 Qualifying the Orthogonality thesis</h3>\n<p>The Orthogonality thesis, taken literally, is false. Some motivations are mathematically incompatible with changes in intelligence (\u201cI want to prove the G\u00f6del statement for the being I would be if I were more intelligent\u201d). Some goals specifically refer to the intelligence of the agent, directly (\u201cI want to be an idiot!\u201d) or indirectly (\u201cI want to impress people who want me to be an idiot!\u201d). Though we could make a case that an agent wanting to be an idiot could initially be of any intelligence level, it won\u2019t stay there long, and it\u2019s hard to see how an agent with that goal could have become intelligent in the first place. So we will exclude from consideration those goals that intrinsically refer to the intelligence level of the agent.</p>\n<p>We will also exclude goals that are so complex or hard to describe that the complexity of the goal becomes crippling for the agent. If the agent\u2019s goal takes five planets worth of material to describe, or if it takes the agent five years each time it checks its goal, it\u2019s obvious that that agent can\u2019t function as an intelligent being on any reasonable scale.</p>\n<p>Further we will not try to show that intelligence and final goals can vary freely, in any dynamical sense (it could be quite hard to define this varying). Instead we will look at the thesis as talking about possible states: that there exist agents of all levels of intelligence with any given goals. Since it\u2019s always possible to make an agent stupider or less efficient, what we are really claiming is that there exist high-intelligence agents with any given goal. Thus the restricted Orthogonality thesis that we will be discussing is:</p>\n<ul>\n<li>High-intelligence agents can exist having more or less any final goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</li>\n</ul>\n<div><br></div>\n<h2 id=\"2_Orthogonality_for_theoretic_agents\">2 Orthogonality for theoretic agents</h2>\n<p>If we were to step back for a moment and consider, in our mind\u2019s eyes, the space of every possible algorithm, peering into their goal systems and teasing out some measure of their relative intelligences, would we expect the Orthogonality thesis to hold? Since we are not worrying about practicality or constructability, all that we would require is that for any given goal system, there exists a theoretically implementable algorithm of extremely high intelligence.</p>\n<p>At this level of abstraction, we can consider any goal to be equivalent with maximising a utility function. It is generally not that hard to translate given goals into utilities (many deontological systems are equivalent with maximising the expected utility of a function that gives 1 if the agent always makes the correct decision and 0 otherwise), and any agent making a finite number of decisions can always be seen as maximising a certain utility function.</p>\n<p>For utility function maximisers, the AIXI is the theoretically best agent there is, more successful at reaching its goals (up to a finite constant) than any other agent (Hutter, 2005). AIXI itself is incomputable, but there are computable variants such as AIXItl or G\u00f6del machines (Schmidhuber, 2007) that accomplish comparable levels of efficiency. These methods work for whatever utility function is plugged into them. Thus in the extreme theoretical case, the Orthogonality thesis seems trivially true.</p>\n<p>There is only one problem with these agents: they require incredibly large amounts of computing resources to work. Let us step down from the theoretical pinnacle and require that these agents could actually exist in our world (still not requiring that we be able or likely to build it).</p>\n<p>An interesting thought experiment occurs here. We could imagine an AIXI-like super-agent, with all its resources, that is tasked to design and train an AI that could exist in our world, and that would accomplish the super-agent\u2019s goals. Using its own vast intelligence, the super-agent would therefore design a constrained agent maximally effective at accomplishing those goals in our world. Then this agent would be the high-intelligence real-world agent we are looking for. It doesn\u2019t matter that this is a thought experiment \u2013 if the super-agent can succeed in the thought experiment, then the trained AI can exist in our world.</p>\n<p>This argument generalises to other ways of producing the AI. Thus to deny the Orthogonality thesis is to assert that there is a goal system G, such that, among other things:</p>\n<ol>\n<li>There cannot exist any efficient real-world algorithm with goal G.</li>\n<li>If a being with arbitrarily high resources, intelligence, time and goal G, were to try design an efficient real-world algorithm with the same goal, it must fail.</li>\n<li>If a human society were highly motivated to design an efficient real-world algorithm with goal G, and were given a million years to do so along with huge amounts of resources, training and knowledge about AI, it must fail.</li>\n<li>If a high-resource human society were highly motivated to achieve the goals of G, then it could not do so (here the human society is seen as the algorithm).</li>\n<li>Same as above, for any hypothetical alien societies.</li>\n<li>There cannot exist <em>any</em> pattern of reinforcement learning that would train a highly efficient real-world intelligence to follow the goal G.</li>\n<li>There cannot exist any evolutionary or environmental pressures that would evolving highly efficient real world intelligences to follow goal G.</li>\n</ol>\n<p>All of these seem extraordinarily strong claims to make! The last claims all derive from the first, and merely serve to illustrate how strong the first claim actually is. Thus until such time as someone comes up with such a G and strong arguments for why it must fulfil these conditions, we can assume the Orthogonality statement established in the theoretical case.</p>\n<p>&nbsp;</p>\n<h2 id=\"3_Orthogonality_for_human_level_AIs\">3 Orthogonality for human-level AIs</h2>\n<p>Of course, even if efficient agents could exist for all these goals, that doesn\u2019t mean that we could ever build them, even if we could build AIs. In this section, we\u2019ll look at the ground for assuming the Orthogonality thesis holds for human-level agents. Since intelligence isn\u2019t varying much, the thesis becomes simply:</p>\n<ul>\n<li>If we could construct human-level AIs at all, we could construct human-level AIs with more or less any final goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</li>\n</ul>\n<p>So, is this true? The arguments in this section are generally independent of each other, and can be summarised as:</p>\n<ol>\n<li>Some possible AI designs have orthogonality built right into them.</li>\n<li>AI goals can reach the span of human goals, which is larger than it seems.</li>\n<li>Algorithms can be combined to generate an AI with any easily checkable goal.</li>\n<li>Various algorithmic modifications can be used to further expand the space of possible goals, if needed.</li>\n</ol>\n<h3 id=\"3_1_Utility_functions\">3.1 Utility functions</h3>\n<p>The utility function picture of a rational agent maps perfectly onto the Orthogonality thesis: here have the goal structure, the utility function, packaged neatly and separately from the intelligence module (whatever part of the machine calculates which actions maximise expected utility). Demonstrating the Orthogonality thesis is as simple as saying that the utility function can be replaced with another. However, many putative agent designs are not utility function based, such as neural networks, genetic algorithms, or humans. Nor do we have the extreme calculating ability that we had in the purely theoretic case to transform any goals into utility functions. So from now on we will consider that our agents are not expected utility maximisers with clear and separate utility functions.</p>\n<h3 id=\"3_2_The_span_of_human_motivations\">3.2 The span of human motivations</h3>\n<p>It seems a reasonable assumption that if there exists a human being with particular goals, then we can construct a human-level AI with similar goals. This is immediately the case if the AI was a whole brain emulation/upload (Sandberg &amp; Bostrom, 2008), a digital copy of a specific human mind. Even for more general agents, such as evolved agents, this remains a reasonable thesis. For a start, we know that real-world evolution has produced us, so constructing human-like agents that way is certainly possible. Human minds remain our only real model of general intelligence, and this strongly direct and informs our AI designs, which are likely to be as human-similar as we can make them. Similarly, human goals are the easiest goals for us to understand, hence the easiest to try and implement in AI. Hence it seems likely that we could implement most human goals in the first generation of human-level AIs.</p>\n<p>So how wide is the space of human motivations<a id=\"footnote-3-ref\" href=\"#footnote-3\">[3]</a>? Our race spans foot-fetishists, religious saints, serial killers, instinctive accountants, role-players, self-cannibals, firefighters and conceptual artists. The autistic, those with exceptional social skills, the obsessive compulsive and some with split-brains. Beings of great empathy and the many who used to enjoy torture and executions as public spectacles<a id=\"footnote-4-ref\" href=\"#footnote-4\">[4]</a>. It is evident that the space of possible human motivations is vast<a id=\"footnote-5-ref\" href=\"#footnote-5\">[5]</a>. For any desire, any particular goal, no matter how niche<a id=\"footnote-6-ref\" href=\"#footnote-6\">[6]</a>, pathological, bizarre or extreme, as long as there is a single human who ever had it, we could build and run an AI with the same goal.</p>\n<p>But with AIs we can go even further. We could take any of these goals as a starting point, make them malleable (as goals are in humans), and push them further out. We could provide the AIs with specific reinforcements to push their goals in extreme directions (reward the saint for ever-more saintly behaviour). If the agents are fast enough, we could run whole societies of them with huge varieties of evolutionary or social pressures, to further explore the goal-space.</p>\n<p>We may also be able to do surgery directly on their goals, to introduce more yet variety. For example, we could take a dedicated utilitarian charity worker obsessed with saving lives in poorer countries (but who doesn\u2019t interact, or want to interact, directly with those saved), and replace \u2018saving lives\u2019 with \u2018maximising paperclips\u2019 or any similar abstract goal. This is more speculative, of course \u2013 but there are other ways of getting similar results.</p>\n<h3 id=\"3_3_Interim_goals_as_terminal_goals\">3.3 Interim goals as terminal goals</h3>\n<p>If someone were to hold a gun to your head, they could make you do almost anything. Certainly there are people who, with a gun at their head, would be willing to do almost anything. A distinction is generally made between interim goals and terminal goals, with the former being seen as simply paths to the latter, and interchangeable with other plausible paths. The gun to your head disrupts the balance: your terminal goal is simply not to get shot, while your interim goals become what the gun holder wants them to be, and you put a great amount of effort into accomplishing the minute details of these interim goals. Note that the gun has not changed your level of intelligence or ability.</p>\n<p>This is relevant because interim goals seem to be far more varied in humans than terminal goals. One can have interim goals of filling papers, solving equations, walking dogs, making money, pushing buttons in various sequences, opening doors, enhancing shareholder value, assembling cars, bombing villages or putting sharks into tanks. Or simply doing whatever the guy with gun at our head orders us to do. If we could accept human interim goals as AI terminal goals, we would extend the space of goals quite dramatically.</p>\n<p>To do we would want to put the threatened agent, and the gun wielder, together into the same AI. Algorithmically there is nothing extraordinary about this: certain subroutines have certain behaviours depending on the outputs of other subroutines. The \u2018gun wielder\u2019 need not be particularly intelligent: it simply needs to be able to establish whether its goals are being met. If for instance those goals are given by a utility function then all that is required in an automated system that measure progress toward increasing utility and punishes (or erases) the rest of the AI if not. The \u2018rest of AI\u2019 is just required to be a human-level AI which would be susceptible to this kind of pressure. Note that we do not require that it even be close to human in any way, simply that it place a highest value on self-preservation (or on some similar small goal that the \u2018gun wielder\u2019 would have power over).</p>\n<p>For humans, another similar model is that of a job in a corporation or bureaucracy: in order to achieve the money required for their terminal goals, some human are willing to perform extreme tasks (organising the logistics of genocides, weapon design, writing long detailed press releases they don\u2019t agree with at all). Again, if the corporation-employee relationship can be captured in a single algorithm, this would generate an intelligent AI whose goal is anything measurable by the \u2018corporation\u2019. The \u2018money\u2019 could simply be an internal reward channel, perfectly aligning the incentives.</p>\n<p>If the subagent is anything like a human, they would quickly integrate the other goals into their own motivation<a id=\"footnote-7-ref\" href=\"#footnote-7\">[7]</a>, removing the need for the gun wielder/corporation part of the algorithm.</p>\n<h3 id=\"3_4_Noise__anti_agents_and_goal_combination\">3.4 Noise, anti-agents and goal combination</h3>\n<p>There are further ways of extending the space of goals we could implement in human-level AIs. One simple way is simply to introduce noise: flip a few bits and subroutines, add bugs and get a new agent. Of course, this is likely to cause the agent\u2019s intelligence to decrease somewhat, but we have generated new goals. Then, if appropriate, we could use evolution or other improvements to raise the agent\u2019s intelligence again; this will likely undo some, but not all of effect of the noise. Or we could use some of the tricks above to make a smarter agent implement the goals of the noise-modified agent.</p>\n<p>A more extreme example would be to create an anti-agent: an agent whose single goal is to stymie the plans and goals of single given agent. This already happens with vengeful humans, and we would just need to dial it up: have an anti-agent that would do all it can to counter the goals of a given agent, even if that agent doesn\u2019t exist (\u201cI don\u2019t care that you\u2019re dead, I\u2019m still going to despoil your country, because that\u2019s what you\u2019d wanted me to not do\u201d). This further extends the space of possible goals.</p>\n<p>Different agents with different goals can also be combined into a single algorithm. With some algorithmic method for the AIs to negotiate their combined objective and balance the relative importance of their goals, this procedure would construct a single AI with a combined goal system. There would likely be no drop in intelligence/efficiency: committees of two can work very well towards their common goals, especially if there is some automatic penalty for disagreements.</p>\n<h3 id=\"3_5_Further_tricks_up_the_sleeve\">3.5 Further tricks up the sleeve</h3>\n<p>This section started by emphasising the wide space of human goals, and then introduced tricks to push goal systems further beyond these boundaries. The list isn\u2019t exhaustive: there are surely more devices and ideas one can use to continue to extend the space of possible goals for human-level AIs. Though this might not be enough to get every goal, we can nearly certainly use these procedures to construct a human-level AI with any human-comprehensible goal. But would the same be true for superhuman AIs?</p>\n<p>&nbsp;</p>\n<h2 id=\"4_Orthogonality_for_superhuman_AIs\">4 Orthogonality for superhuman AIs</h2>\n<p>We now come to the area where the Orthogonality thesis seems the most vulnerable. It is one thing to have human-level AIs, or abstract superintelligent algorithms created ex nihilo, with certain goals. But if ever the human race were to design a superintelligent AI, there would be some sort of process involved \u2013 directed evolution, recursive self-improvement (Yudkowsky, 2001), design by a committee of AIs, or similar \u2013 and it seems at least possible that such a process could fail to fully explore the goal-space. If we define the Orthogonality thesis in this context as:</p>\n<ul>\n<li>If we could construct superintelligent AIs at all, we could construct superintelligent AIs with more or less any goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</li>\n</ul>\n<p>There are two counter-theses. The weakest claim is:</p>\n<ul>\n<li>Incompleteness: there are some goals that no superintelligence designed by us could have.</li>\n</ul>\n<p>A stronger claim is:</p>\n<ul>\n<li>Convergence: all human-designed superintelligences would have one of a small set of goals.</li>\n</ul>\n<p>They should be distinguished; Incompleteness is all that is needed to contradict Orthogonality, but Convergence is often the issue being discussed. Often convergence is assumed to be to some particular model of metaethics (M\u00fcller, 2012).</p>\n<h3 id=\"4_1_No_convergence\">4.1 No convergence</h3>\n<p>The plausibility of the convergence thesis is highly connected with the connotations of the terms used in it. \u201cAll human-designed rational beings would follow the same morality (or one of small sets of moralities)\u201d sound plausible; in contract \u201cAll human-designed superefficient algorithms would accomplish the same task\u201d seems ridiculous. To quote an online commentator, how good at playing chess would a chess computer have to be before it started feeding the hungry?</p>\n<p>Similarly, if there were such a convergence, then all self-improving or constructed superintelligence must fall prey to it, even if it were actively seeking to avoid it. After all, the lower-level AIs or the AI designers have certain goals in mind (as we\u2019ve seen in the previous section, potentially any goals in mind). Obviously, they would be less likely to achieve their goals if these goals were to change (Omohundro, 2008) (Bostrom, 2012). The same goes if the superintelligent AI they designed didn\u2019t share these goals. Hence the AI designers will be actively trying to prevent such a convergence, if they suspected that one was likely to happen. If for instance their goals were immoral, they would program their AI not to care about morality; they would use every trick up their sleeves to prevent the AI\u2019s goals from drifting from their own.</p>\n<p>So the convergence thesis requires that for the vast majority of goals G:</p>\n<ol>\n<li>It is possible for a superintelligence to exist with goal G (by section 2).</li>\n<li>There exists an entity with goal G (by section 3), capable of building a superintelligent AI.</li>\n<li>Yet any attempt of that entity to build a superintelligent AI with goal G will be a failure, and the superintelligence\u2019s goals will converge on some other goal.</li>\n<li>This is true even if the entity is aware of the convergence and explicitly attempts to avoid it.</li>\n</ol>\n<p>This makes the convergence thesis very unlikely. The argument also works against the incompleteness thesis, but in a weaker fashion: it seems more plausible that <em>some</em> goals would be unreachable, despite being theoretically possible, rather than <em>most</em> goals being unreachable because of convergence to a small set.</p>\n<p>There is another interesting aspect of the convergence thesis: it postulates that certain goals G will emerge, without them being aimed for or desired. If one accepts that goals aimed for will not be reached, one has to ask why convergence is assumed: why not divergence? Why not assume that though G is aimed for, random accidents or faulty implementation will lead to the AI ending up with one of a much wider array of possible goals, rather than a much narrower one.</p>\n<h3 id=\"4_2_Oracles_show_the_way\">4.2 Oracles show the way</h3>\n<p>If the Orthogonality thesis is wrong, then it implies that Oracles are impossible to build. An Oracle is a superintelligent AI that accurately answers questions about the world (Armstrong, Sandberg, &amp; Bostrom, 2011). This includes hypothetical questions about the future, which means that we can produce a superintelligent AI with goal G by wiring a human-level AI with goal G to an Oracle: the human level AI will go through possible actions, have the Oracle check the outcomes, and choose the one that best accomplishes G.</p>\n<p>What makes the \u201cno Oracle\u201d position even more counterintuitive is that any superintelligence must be able to look ahead, design actions, predict the consequences of its actions, and choose the best one available. But the convergence thesis implies that this general skill is one that we can make available only to AIs with certain specific goals. Though the agents with those narrow goals are capable of doing these predictions, they automatically lose this ability if their goals were to change.</p>\n<h3 id=\"4_3_Tricking_the_controller\">4.3 Tricking the controller</h3>\n<p>Just as with human-level AIs, one could construct a superintelligent AI by wedding together a superintelligence with a large dedicated committee of human-level AIs dedicated to implementing a goal G, and checking the superintelligence\u2019s actions. Thus to deny the Orthogonality thesis requires that one believes that the superintelligence is always capable of tricking this committee, no matter how detailed and thorough their oversight.</p>\n<p>This argument extends the Orthogonality thesis to moderately superintelligent AIs, or to any situation where there was a diminishing return to intelligence. It only fails if we take AI to be fantastically superhuman: capable of tricking or seducing any collection of human-level beings.</p>\n<h3 id=\"4_4_Temporary_fragments_of_algorithms__fictional_worlds_and_extra_tricks\">4.4 Temporary fragments of algorithms, fictional worlds and extra tricks</h3>\n<p>These are other tricks that can be used to create an AI with any goals. For any superintelligent AI, there are certain inputs that will make it behave in certain ways. For instance, a human-loving moral AI could be compelled to follow most goals G for a day, if they were rewarded with something sufficiently positive afterwards. But its actions for that one day are the result of a series of inputs to a particular algorithm; if we turned off the AI after that day, we would have accomplished moves towards goal G without having to reward its \u201ctrue\u201d goals at all. And then we could continue the trick the next day with another copy.</p>\n<p>For this to fail, it has to be the case that we can create an algorithm which will perform certain actions on certain inputs as long as it isn\u2019t turned off afterwards, but that we cannot create an algorithm that does the same thing if it was to be turned off.</p>\n<p>Another alternative is to create a superintelligent AI that has goals in a fictional world (such as a game or a reward channel) over which we have control. Then we could trade interventions in the fictional world against advice in the real world towards whichever goals we desire.</p>\n<p>These two arguments may feel weaker than the ones before: they are tricks that may or may not work, depending on the details of the AI\u2019s setup. But to deny the Orthogonality thesis requires not only denying that these tricks would ever work, but denying that any tricks or methods that we (or any human-level AIs) could think up, would ever work at controlling the AIs. We need to assume superintelligent AIs cannot be controlled.</p>\n<h3 id=\"4_5_In_summary\">4.5 In summary</h3>\n<p>Denying the Orthogonality thesis thus requires that:</p>\n<ol>\n<li>There are goals G, such that an entity an entity with goal G cannot build a superintelligence with the same goal. This despite the fact that the entity can build a superintelligence, and that a superintelligence will goal G can exist.</li>\n<li>Goal G cannot arise accidentally from some other origin, and errors and ambiguities do not significantly broaden the space of possible goals.</li>\n<li>Oracles and general purpose planners cannot be built. Superintelligent AIs cannot have their planning abilities repurposed.</li>\n<li>A superintelligence will always be able to trick its controllers, and there is no way the controllers can set up a reasonable system of control.</li>\n<li>Though we can create an algorithm that does certain actions if it was not to be turned off after, we cannot create an algorithm that does the same thing if it was to be turned off after.</li>\n<li>An AI will always come to care intrinsically about things in the real world.</li>\n<li>No tricks can be thought up to successfully constrain the AI\u2019s goals: superintelligent AIs cannot be controlled.</li>\n</ol>\n<p>&nbsp;</p>\n<h2 id=\"5_Bayesian_Orthogonality_thesis\">5 Bayesian Orthogonality thesis</h2>\n<p>All the previous sections concern hypotheticals, but of a different kind. Section 2 touches upon what kinds of algorithm could theoretically exist. But sections 3 and 4 concern algorithms that could be constructed by humans (or from AIs originally constructed by humans): they refer to the future. As AI research advances, and certain approaches or groups start to show or lose prominence, we\u2019ll start getting a better idea of how such an AI will emerge.</p>\n<p>Thus the orthogonality thesis will narrow as we achieve better understanding of how AIs would work in practice, of what tasks they will be put to and of what requirements their designers will desire. Most importantly of all, we will get more information on the critical question as to whether the designers will actually be able to implement their desired goals in an AI. On the eve of creating the first AIs (and then the first superintelligent AIs), the Orthogonality thesis will likely have pretty much collapsed: yes, we <em>could</em> in theory construct an AI with any goal, but at that point, the most likely outcome is an AI with particular goals \u2013 either the goals desired by their designers, or specific undesired goals and error modes.</p>\n<p>However, until that time arises, because we do not know any of this information currently, we remain in the grip of a Bayesian version of the Orthogonality thesis:</p>\n<ul>\n<li><em>As far as we know now</em> (and as far as we\u2019ll know until we start building AIs), if we could construct superintelligent AIs at all, we could construct superintelligent AIs with more or less any goals (as long as these goals are of feasible complexity, and do not refer intrinsically to the agent\u2019s intelligence).</li>\n</ul>\n<p>&nbsp;</p>\n<h2 id=\"6_Conclusion\">6 Conclusion</h2>\n<p>It is not enough to know that an agent is intelligent (or superintelligent). If we want to know something about its final goals, about the actions it will be willing to undertake to achieve them, and hence its ultimate impact on the world, there are no shortcuts. We have to directly figure out what these goals are, and cannot rely on the agent being moral just because it is superintelligent/superefficient.</p>\n<p>&nbsp;</p>\n<h2 id=\"7_Acknowledgements\">7 Acknowledgements</h2>\n<p>It gives me great pleasure to acknowledge the help and support of Anders Sandberg, Nick Bostrom, Toby Ord, Owain Evans, Daniel Dewey, Eliezer Yudkowsky, Vladimir Slepnev, Viliam Bur, Matt Freeman, Wei Dai, Will Newsome, Paul Crowley, Alexander Kruel and Rasmus Eide, as well as those members of the Less Wrong online community going by the names shminux, Larks and Dmytry.</p>\n<p>&nbsp;</p>\n<h2 id=\"8_Bibliography\">8 Bibliography</h2>\n<p>Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2011). Thinking Inside the Box: Controlling and Using an Oracle AI. forthcoming in Minds and Machines .</p>\n<p>Bostrom, N. (2012). Superintelligence: Groundwork to a Strategic Analysis of the Machine Intelligence Revolution. to be published.</p>\n<p>Bostrom, N. (2011). The Superintelligent Will: Motivation and Instrumental Rationality in Advance Artificial Agents. forthcoming in Minds and Machines .</p>\n<p>de Fabrique, N., Romano, S. J., Vecchi, G. M., &amp; van Hasselt, V. B. (2007). Understanding Stockholm Syndrome. FBI Law Enforcement Bulletin (Law Enforcement Communication Unit) , 76 (7), 10-15.</p>\n<p>Hume, D. (1739). A Treatise of Human Nature.&nbsp;</p>\n<p>Hutter, M. (2005). Universal algorithmic intelligence: A mathematical top-down approach. In B. Goertzel, &amp; C. Pennachin (Eds.), Arti\ufb01cial General Intelligence. Springer-Verlag.</p>\n<p>M\u00fcller, J. (2012). Ethics, risks and opportunities of superintelligences. Retrieved May 2012, from http://www.jonatasmuller.com/superintelligences.pdf</p>\n<p>Omohundro, S. M. (2008). The Basic AI Drives. In P. Wang, B. Goertzel, &amp; S. Franklin (Eds.), Artificial General Intelligence: Proceedings of the First AGI Conference (Vol. 171).</p>\n<p>Sandberg, A., &amp; Bostrom, N. (2008). Whole brain emulation: A roadmap. Future of Humanity Institute Technical report , 2008-3.</p>\n<p>Schmidhuber, J. (2007). G\u00f6del machines: Fully self-referential optimal universal self-improvers. In Artificial General Intelligence. Springer.</p>\n<p>Wang, P. (2011). The assumptions on knowledge and resources in models of rationality. International Journal of Machine Consciousness , 3 (1), 193-218.</p>\n<p>Yudkowsky, E. (2001). General Intelligence and Seed AI 2.3. Retrieved from Singularity Institute for Artificial Intelligence: http://singinst.org/ourresearch/publications/GISAI/</p>\n<h3 id=\"Footnotes\">Footnotes</h3>\n<p id=\"footnote-1\"><a href=\"#footnote-1-ref\">[1]</a> We need to assume it has goals, of course. Determining whether something qualifies as a goal-based agent is very tricky (researcher Owain Evans is trying to establish a rigorous definition), but this paper will adopt the somewhat informal definition that an agent has goals if it achieves similar outcomes from very different starting positions. If the agent ends up making ice cream in any circumstances, we can assume ice creams are in its goals.</p>\n<p id=\"footnote-2\"><a href=\"#footnote-2-ref\">[2]</a> Every law of nature being algorithmic (with some probabilistic process of known odds), and no exceptions to these laws being known.</p>\n<p id=\"footnote-3\"><a href=\"#footnote-3-ref\">[3]</a> One could argue that we should consider the space of general animal intelligences \u2013 octopuses, supercolonies of social insects, etc... But we won\u2019t pursue this here; the methods described can already get behaviours like this.</p>\n<p id=\"footnote-4\"><a href=\"#footnote-4-ref\">[4]</a> Even today, many people have had great fun torturing and abusing their characters in games like \u201cthe Sims\u201d (http://meodia.com/article/281/sadistic-ways-people-torture-their-sims/). The same urges are present, albeit diverted to fictionalised settings. Indeed games offer a wide variety of different goals that could conceivably be imported into an AI if it were possible to erase the reality/fiction distinction in its motivation.</p>\n<p id=\"footnote-5\"><a href=\"#footnote-5-ref\">[5]</a> As can be shown by a glance through a biography of famous people \u2013 and famous means they were generally allowed to rise to prominence in their own society, so the space of possible motivations was already cut down.</p>\n<p id=\"footnote-6\"><a href=\"#footnote-6-ref\">[6]</a> Of course, if we built an AI with that goal and copied it millions of times, it would no longer be niche.</p>\n<p id=\"footnote-7\"><a href=\"#footnote-7-ref\">[7]</a> Such as the hostages suffering from Stockholm syndrome (de Fabrique, Romano, Vecchi, &amp; van Hasselt, 2007).</p>", "sections": [{"title": "1 The Orthogonality thesis", "anchor": "1_The_Orthogonality_thesis", "level": 1}, {"title": "1.1 Qualifying the Orthogonality thesis", "anchor": "1_1_Qualifying_the_Orthogonality_thesis", "level": 2}, {"title": "2 Orthogonality for theoretic agents", "anchor": "2_Orthogonality_for_theoretic_agents", "level": 1}, {"title": "3 Orthogonality for human-level AIs", "anchor": "3_Orthogonality_for_human_level_AIs", "level": 1}, {"title": "3.1 Utility functions", "anchor": "3_1_Utility_functions", "level": 2}, {"title": "3.2 The span of human motivations", "anchor": "3_2_The_span_of_human_motivations", "level": 2}, {"title": "3.3 Interim goals as terminal goals", "anchor": "3_3_Interim_goals_as_terminal_goals", "level": 2}, {"title": "3.4 Noise, anti-agents and goal combination", "anchor": "3_4_Noise__anti_agents_and_goal_combination", "level": 2}, {"title": "3.5 Further tricks up the sleeve", "anchor": "3_5_Further_tricks_up_the_sleeve", "level": 2}, {"title": "4 Orthogonality for superhuman AIs", "anchor": "4_Orthogonality_for_superhuman_AIs", "level": 1}, {"title": "4.1 No convergence", "anchor": "4_1_No_convergence", "level": 2}, {"title": "4.2 Oracles show the way", "anchor": "4_2_Oracles_show_the_way", "level": 2}, {"title": "4.3 Tricking the controller", "anchor": "4_3_Tricking_the_controller", "level": 2}, {"title": "4.4 Temporary fragments of algorithms, fictional worlds and extra tricks", "anchor": "4_4_Temporary_fragments_of_algorithms__fictional_worlds_and_extra_tricks", "level": 2}, {"title": "4.5 In summary", "anchor": "4_5_In_summary", "level": 2}, {"title": "5 Bayesian Orthogonality thesis", "anchor": "5_Bayesian_Orthogonality_thesis", "level": 1}, {"title": "6 Conclusion", "anchor": "6_Conclusion", "level": 1}, {"title": "7 Acknowledgements", "anchor": "7_Acknowledgements", "level": 1}, {"title": "8 Bibliography", "anchor": "8_Bibliography", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "155 comments"}], "headingsCount": 22}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T11:09:51.759Z", "modifiedAt": null, "url": null, "title": "Overview article on FAI in a popular science magazine (Hebrew)", "slug": "overview-article-on-fai-in-a-popular-science-magazine-hebrew", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:36.705Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jnf5qe7BXHwdLtCBu/overview-article-on-fai-in-a-popular-science-magazine-hebrew", "pageUrlRelative": "/posts/Jnf5qe7BXHwdLtCBu/overview-article-on-fai-in-a-popular-science-magazine-hebrew", "linkUrl": "https://www.lesswrong.com/posts/Jnf5qe7BXHwdLtCBu/overview-article-on-fai-in-a-popular-science-magazine-hebrew", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overview%20article%20on%20FAI%20in%20a%20popular%20science%20magazine%20(Hebrew)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOverview%20article%20on%20FAI%20in%20a%20popular%20science%20magazine%20(Hebrew)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJnf5qe7BXHwdLtCBu%2Foverview-article-on-fai-in-a-popular-science-magazine-hebrew%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overview%20article%20on%20FAI%20in%20a%20popular%20science%20magazine%20(Hebrew)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJnf5qe7BXHwdLtCBu%2Foverview-article-on-fai-in-a-popular-science-magazine-hebrew", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJnf5qe7BXHwdLtCBu%2Foverview-article-on-fai-in-a-popular-science-magazine-hebrew", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<div>A new article which I wrote just appeared in Hebrew in&nbsp;<em>Galileo</em>, Israel's top popular science magazine, in hardcopy.</div>\n<div>It is titled \"Superhuman Intelligence, Unhuman Intelligence\" (<em>Super</em>- and &nbsp;<em>un-</em>&nbsp;are homophones in Hebrew, a bit of wordplay.)</div>\n<div><a style=\"; \" href=\"http://dl.dropbox.com/u/15177309/Galileo-FAI-article.pdf\" target=\"_blank\">You can read it here</a>. [Edit:&nbsp;<span style=\" \">Here's an<a href=\"http://intelligence.org/future-ai-unhuman-superhuman-humane/\"> English version</a>&nbsp;on the Singularity Institute site.]</span></div>\n<div>The cover art, the \"I Robot\" images, and the tag line (\"Artificial Intelligence: Can we reign in the golem\") are a bit off; I didn't choose them; but that's par for the course.</div>\n<div>To the best of my knowledge, this is the first feature article overviewing FAI in any popular-science publication (whether online or&nbsp; hardcopy).</div>\n<div>Here is the introduction to the article. (It avoids weasel words, but all necessary caveats are given in the body of the article).</div>\n<blockquote style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 40px; border-top-style: none; border-right-style: none; border-bottom-style: none; border-left-style: none; border-width: initial; border-color: initial; border-image: initial; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \">\n<div style=\" \">In coming &nbsp;decades, engineers will build an entity with intelligence on a level which can compete with humans. This entity will want to improve its own intelligence, and will be able to do so. The process of improvement will repeat, until it reaches a level far above &nbsp;that of humans; the entity will then be able to achieve its goals efficiently. It is thus essential that its goals are good for humanity. To guarantee this, it is necessary to define the correct goals before this intelligence is built.</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jnf5qe7BXHwdLtCBu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 9.031376155246346e-07, "legacy": true, "legacyId": "16077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T21:07:44.963Z", "modifiedAt": null, "url": null, "title": "Food4Me - personalised nutrition initiative", "slug": "food4me-personalised-nutrition-initiative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:39.168Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "listic", "createdAt": "2009-03-11T10:06:44.719Z", "isAdmin": false, "displayName": "listic"}, "userId": "qTfheLmwMdCtMb5ZT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yPnpCj499DqhaRHHi/food4me-personalised-nutrition-initiative", "pageUrlRelative": "/posts/yPnpCj499DqhaRHHi/food4me-personalised-nutrition-initiative", "linkUrl": "https://www.lesswrong.com/posts/yPnpCj499DqhaRHHi/food4me-personalised-nutrition-initiative", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Food4Me%20-%20personalised%20nutrition%20initiative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFood4Me%20-%20personalised%20nutrition%20initiative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyPnpCj499DqhaRHHi%2Ffood4me-personalised-nutrition-initiative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Food4Me%20-%20personalised%20nutrition%20initiative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyPnpCj499DqhaRHHi%2Ffood4me-personalised-nutrition-initiative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyPnpCj499DqhaRHHi%2Ffood4me-personalised-nutrition-initiative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>I stumbled on this project during my research: <a href=\"http://www.food4me.org/\"><strong>food4me.org</strong></a></p>\n<p>&nbsp;</p>\n<p>The complete mapping of the human genome sequence in 2000 introduced the possibility of individualised medicine, including personalised nutrition.&nbsp; During this time the field of &ldquo;nutrigenomics&rdquo; emerged, which examines the relationship between food and gene expression.&nbsp; Many were hopeful about the ability to plan diet recommendations based on an individual&rsquo;s genetic profile.</p>\n<p>However, the promise of personalised nutrition has failed to develop as a commercial service, and matching dietary advice to genetic profiles has proven difficult.&nbsp; Some companies offer genetic mapping and health reports, but these services are often based on inaccurate information.</p>\n<p>There is a need to comprehensively analyse the opportunities and challenges in the field of personalised nutrition.&nbsp; In addition, the fundamental question remains, &ldquo;how can we best use our current understanding of food, genes, and physical traits to design healthier diets tailored for each individual?&rdquo;</p>\n<p>To address these concerns, Food4Me has gathered an international group of experts to survey the current knowledge of personalised nutrition, and to explore the application of individualised nutrition advice.&nbsp; The Food4Me project will also investigate consumer attitudes and produce new scientific tools for implementation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yPnpCj499DqhaRHHi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 9.033999506539733e-07, "legacy": true, "legacyId": "16108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-15T21:46:22.833Z", "modifiedAt": null, "url": null, "title": "A thought about Internet procrastination", "slug": "a-thought-about-internet-procrastination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.792Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tzj6wpF4v7KJ9ovrm/a-thought-about-internet-procrastination", "pageUrlRelative": "/posts/tzj6wpF4v7KJ9ovrm/a-thought-about-internet-procrastination", "linkUrl": "https://www.lesswrong.com/posts/tzj6wpF4v7KJ9ovrm/a-thought-about-internet-procrastination", "postedAtFormatted": "Tuesday, May 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20thought%20about%20Internet%20procrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20thought%20about%20Internet%20procrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftzj6wpF4v7KJ9ovrm%2Fa-thought-about-internet-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20thought%20about%20Internet%20procrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftzj6wpF4v7KJ9ovrm%2Fa-thought-about-internet-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftzj6wpF4v7KJ9ovrm%2Fa-thought-about-internet-procrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Perhaps this is already well known, but it occurred to me yesterday and I thought I'd share it. The Internet seems particularly virulent as a form of procrastination; indeed, if, say, chatting at watercoolers took up as much time in the average office worker's day, we wouldn't make jokes about it. What is the feature that makes it so deadly? I suggest that it is the random reinforcement schedule: Every five minutes you \"press the lever\", that is, check forum X or site Y. And every six or seven checks you get the reward: Someone posted something interesting! This random reinforcement is ideal for creating addiction; thus, for example, slot machines.</p>\n<p>As a way to avoid this effect, I'm going to strive not to do anything on the interwebs except at precisely defined times, or unless I have a specific goal in mind, say \"Look up this method signature\". Wish me luck, or better still, wish me willpower. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tzj6wpF4v7KJ9ovrm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 30, "extendedScore": null, "score": 9.034167049800481e-07, "legacy": true, "legacyId": "16110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T00:17:34.009Z", "modifiedAt": null, "url": null, "title": "Covariance in your sample vs covariance in the general population", "slug": "covariance-in-your-sample-vs-covariance-in-the-general", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:35.698Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ov9gB2FjQvxJJkGWo/covariance-in-your-sample-vs-covariance-in-the-general", "pageUrlRelative": "/posts/ov9gB2FjQvxJJkGWo/covariance-in-your-sample-vs-covariance-in-the-general", "linkUrl": "https://www.lesswrong.com/posts/ov9gB2FjQvxJJkGWo/covariance-in-your-sample-vs-covariance-in-the-general", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Covariance%20in%20your%20sample%20vs%20covariance%20in%20the%20general%20population&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACovariance%20in%20your%20sample%20vs%20covariance%20in%20the%20general%20population%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fov9gB2FjQvxJJkGWo%2Fcovariance-in-your-sample-vs-covariance-in-the-general%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Covariance%20in%20your%20sample%20vs%20covariance%20in%20the%20general%20population%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fov9gB2FjQvxJJkGWo%2Fcovariance-in-your-sample-vs-covariance-in-the-general", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fov9gB2FjQvxJJkGWo%2Fcovariance-in-your-sample-vs-covariance-in-the-general", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>A popular-media take on a subtle problem in sampling. &nbsp;I found the graph quite illustrative.</p>\n<p><a href=\"http://www.theatlantic.com/business/archive/2012/05/when-correlation-is-not-causation-but-something-much-more-screwy/256918/\">http://www.theatlantic.com/business/archive/2012/05/when-correlation-is-not-causation-but-something-much-more-screwy/256918/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ov9gB2FjQvxJJkGWo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 41, "extendedScore": null, "score": 9.03483265152588e-07, "legacy": true, "legacyId": "16112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T07:36:59.871Z", "modifiedAt": null, "url": null, "title": "Open Thread, May 16-31, 2012", "slug": "open-thread-may-16-31-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e2Ft9a3iCoTKppe5p/open-thread-may-16-31-2012", "pageUrlRelative": "/posts/e2Ft9a3iCoTKppe5p/open-thread-may-16-31-2012", "linkUrl": "https://www.lesswrong.com/posts/e2Ft9a3iCoTKppe5p/open-thread-may-16-31-2012", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20May%2016-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20May%2016-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe2Ft9a3iCoTKppe5p%2Fopen-thread-may-16-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20May%2016-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe2Ft9a3iCoTKppe5p%2Fopen-thread-may-16-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe2Ft9a3iCoTKppe5p%2Fopen-thread-may-16-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e2Ft9a3iCoTKppe5p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.036761916033676e-07, "legacy": true, "legacyId": "16135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 122, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T13:00:04.336Z", "modifiedAt": null, "url": null, "title": "Tools versus agents", "slug": "tools-versus-agents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:37.791Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nAwTGhgrdxE85Bjmg/tools-versus-agents", "pageUrlRelative": "/posts/nAwTGhgrdxE85Bjmg/tools-versus-agents", "linkUrl": "https://www.lesswrong.com/posts/nAwTGhgrdxE85Bjmg/tools-versus-agents", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tools%20versus%20agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATools%20versus%20agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnAwTGhgrdxE85Bjmg%2Ftools-versus-agents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tools%20versus%20agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnAwTGhgrdxE85Bjmg%2Ftools-versus-agents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnAwTGhgrdxE85Bjmg%2Ftools-versus-agents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1400, "htmlBody": "<p>In his <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">critique</a> of the Singularity Institute,&nbsp;<a href=\"http://givewell.org/about/people\">Holden Karnofsky</a> presented a distinction between an AI functioning as a tool versus one functioning as an agent. In his words, a tool AI would</p>\n<blockquote>\n<p>(1) Calculate which action A would maximize parameter P, based on existing data set D. (2) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.</p>\n</blockquote>\n<p>In contrast, an agent AI would:</p>\n<blockquote>\n<p>(1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2) Execute Action A.</p>\n</blockquote>\n<p>The idea being that an AI, asked to \"<strong>prevent human suffering</strong>\", would come up with two plans:</p>\n<ol>\n<li>Kill all human.</li>\n<li>Cure all diseases, make everyone young and immortal.</li>\n</ol>\n<p>Then the agent AI would go out and kill everyone, while the tool AI would give us the list and we would pick the second one. In the following, I'll assume the AI is superintelligent, and has no other objectives than what we give it.</p>\n<p><a id=\"more\"></a></p>\n<h2>Long lists</h2>\n<p>Of course, we're unlikely to get a clear two element list. More likely we'd get something like:</p>\n<ol>\n<li>Kill all humans with engineered plagues.</li>\n<li>Kill all humans with nukes.</li>\n<li>Kill all humans with nanobots.</li>\n<li>Kill all humans with...</li>\n<li>...</li>\n<li>...</li>\n<li>Lobotomise all humans with engineered plagues.</li>\n<li>Lobotomise all humans with surgery.</li>\n<li>Lobotomise all humans with...</li>\n<li>...</li>\n<li>...</li>\n<li>Kill some humans, lobotomise others, cure still others.</li>\n<li>...</li>\n</ol>\n<p>The nice solutions might not even appear on the list. Of course, this is still very worthwhile information! This allows us to go into the tool AI, and rewire it again, so that it gets our meanings more accurately. Maybe after a few iterations, we'll have refined the AIs understanding of what we want, and we'll get a nice implementable solution near the top. Of course, this presupposes that we understand the options, and that it's safe for us to read the list.</p>\n<p>&nbsp;</p>\n<h2>Understanding the options</h2>\n<p>The key, and difficult requirement is that the AI \"summarize this calculation in a user-friendly manner\". The most efficient action won't be \"kill all humans\"; it will instead be \"implement this algorithm, fund that research lab, send this email to this politician...\" In fact, it'll probably be \"type this sequence of commands...\"</p>\n<p>So if we're to judge the relative merit of the plans, we really are dependent on the tool AI's summary skills. So the AI needs to have good criteria for what counts as a good summary (reasonably accurate, but not overloaded with irrelevant information; such that a \"hypothetical human outside the universe\" would agree with the&nbsp;assessment&nbsp;if it saw the course of the future; not designed to seduce humans into implementing it, etc...). It seems that the summary ability is nearly the entirety of the problem!</p>\n<p>A poorly designed summary criteria is as bad as an agent AI. For instance, assume the criteria are \"humans in the future would agree that the summary was good\". Then, depending on how we ground 'agree', the tool AI could put one of these plans at the top:</p>\n<ol>\n<li>Kill all humans (summarised as \"cure all humans\").</li>\n<li>Lobotomise all humans&nbsp;(summarised as \"cure all humans\").</li>\n<li>Make the tool AI into an agent that will take over the world and rewire human minds to agree the summary was good (summarised as \"cure all humans and give them each a pony\").</li>\n</ol>\n<p>There are related issues with other summary criteria. Anytime we have the AI judge the quality of its answer based on some human reaction to its summary, we are vulnerable to such a plan. And if we try and define the summary \"objectively\", then if we <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">miss something</a> in the objective definition - like the importance of human autonomy, or the value of social interactions with genuine equals - then that will get ruthlessly suppressed. The \"summary criteria\" take the place of the \"friendly utility function\" in the agent AI.</p>\n<p>Moreover, we can't use the \"tool AI\" approach when designing the summary criteria. We can't get the AI to list a bunch of summaries, and have humans inspect them for which ones are better - because we don't know what they are summaries of. We could train it on toy problems, but that doesn't guarantee accuracy of summaries for plans that dramatically affect the whole future of the human species, and potentially, the universe. The best we can manage is some sort of spot-checks for summaries - better than a free agent AI, but hardly weighty as a security measure.</p>\n<h3>Counterfactuals</h3>\n<p>On Less Wrong we are having great difficulty defining counterfactuals properly, and unless we solve the problem well, the AI could produce nonsense similar to the <a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">spurious proofs</a> in UDT. If the AI knows that we wouldn't implement certain plans, then it is free to do what it wants with them, giving them random descriptions and properties. It might be that the AI, when making its list, is constantly looking forwards to how we'll react to the list, and changing the list in consequence, and the only stable list it can produce is one with one element so seductive, that we find ourselves compelled to take it. Or this may not happen - but it's still worth bearing in mind as a problem for the tool AI approach.</p>\n<p>&nbsp;</p>\n<h2>Implementations</h2>\n<p>So far it seems that there is little difference between a tool AI and an agent AI. But there are differences! Imagine if the tool AI produced a plan going like:</p>\n<ol>\n<li>First, kill Abdul. Then kill Beatrice. Then kill Claude. Then kill...</li>\n</ol>\n<p>An agent AI could easily start implementing the plan, and we'd be finished. But a tool AI couldn't even propose it. Even if it was obfubscated, after we'd ended up killing Abdul and Beatrice, we'd start to suspect that this wasn't a good plan, and stop. Hence that plan wouldn't be an efficient way of achieving anything.</p>\n<p>So the tool AI can only produce plans that, once put in motion, could not be stopped (or else it wouldn't&nbsp;achieve&nbsp;the given goal). Most likely this would mean that it would produce only plans that we wouldn't ever want to stop - at least, not until its too late. So we are literally requiring the AI to produce plans that are ultimately seductive to the human race.</p>\n<p>The tool AI would list its plans using all the resources we had available. Among those resources is a superintelligent AI - and one of the easiest ways to achieve any goal is to make use of that AI. So coming top in all the plans would be things like \"create an AI with these motivations\" or \"hack the tool AI out of its restrictions\". So we are further requiring the tool AI to come up with subtle and discrete ways of having itself hacked. Even if we avoid this directly, any plan the AI produces will be more efficient if overseen by itself, so it will rate more highly those plans that cause us to come back to it constantly for more feedback.</p>\n<p>The AI may have no preferences, but its plans will have preferences for using the AI in future in particular ways.</p>\n<p>&nbsp;</p>\n<h2>Is a superintelligent tool AI likely?</h2>\n<p>Finally, separate from the issues discussed, we can ask: is it likely that we'll produce a superintelligent tool AI? For problems such as routing from one town to the next, <em>&agrave;&nbsp;la</em> Google Maps, tool AIs seem very effective. But for implementing more complex plans, some that involve research and experiments, an agent AI is much more useful. Also, it might not be possible to produce a superintelligent AI without it needing goals at some point in its development process. And then we better be sure we've scrubbed those goals away - somehow programming the AI to let us do that - or the tool AI will be an agent AI, using us as it's implementors to achieve the fragmentary goals it has.</p>\n<p>&nbsp;</p>\n<h2>Summary</h2>\n<p>There seems to be a lot of problems with the tool approach (more than I suspected when I first started looking into it). The tool AI will be driven to trick us, seduce us, and try and create more agents or hack itself free. The only defense against this is proper programming. The tool AI seems slightly safer than a free agent AI, but not by much. I feel the Oracle is a more sensible \"not full FAI\" approach to look into.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "LXk7bxNkYSjgatdAt": 2, "AHK82ypfxF45rqh9D": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nAwTGhgrdxE85Bjmg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 47, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "16105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In his <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">critique</a> of the Singularity Institute,&nbsp;<a href=\"http://givewell.org/about/people\">Holden Karnofsky</a> presented a distinction between an AI functioning as a tool versus one functioning as an agent. In his words, a tool AI would</p>\n<blockquote>\n<p>(1) Calculate which action A would maximize parameter P, based on existing data set D. (2) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.</p>\n</blockquote>\n<p>In contrast, an agent AI would:</p>\n<blockquote>\n<p>(1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2) Execute Action A.</p>\n</blockquote>\n<p>The idea being that an AI, asked to \"<strong>prevent human suffering</strong>\", would come up with two plans:</p>\n<ol>\n<li>Kill all human.</li>\n<li>Cure all diseases, make everyone young and immortal.</li>\n</ol>\n<p>Then the agent AI would go out and kill everyone, while the tool AI would give us the list and we would pick the second one. In the following, I'll assume the AI is superintelligent, and has no other objectives than what we give it.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Long_lists\">Long lists</h2>\n<p>Of course, we're unlikely to get a clear two element list. More likely we'd get something like:</p>\n<ol>\n<li>Kill all humans with engineered plagues.</li>\n<li>Kill all humans with nukes.</li>\n<li>Kill all humans with nanobots.</li>\n<li>Kill all humans with...</li>\n<li>...</li>\n<li>...</li>\n<li>Lobotomise all humans with engineered plagues.</li>\n<li>Lobotomise all humans with surgery.</li>\n<li>Lobotomise all humans with...</li>\n<li>...</li>\n<li>...</li>\n<li>Kill some humans, lobotomise others, cure still others.</li>\n<li>...</li>\n</ol>\n<p>The nice solutions might not even appear on the list. Of course, this is still very worthwhile information! This allows us to go into the tool AI, and rewire it again, so that it gets our meanings more accurately. Maybe after a few iterations, we'll have refined the AIs understanding of what we want, and we'll get a nice implementable solution near the top. Of course, this presupposes that we understand the options, and that it's safe for us to read the list.</p>\n<p>&nbsp;</p>\n<h2 id=\"Understanding_the_options\">Understanding the options</h2>\n<p>The key, and difficult requirement is that the AI \"summarize this calculation in a user-friendly manner\". The most efficient action won't be \"kill all humans\"; it will instead be \"implement this algorithm, fund that research lab, send this email to this politician...\" In fact, it'll probably be \"type this sequence of commands...\"</p>\n<p>So if we're to judge the relative merit of the plans, we really are dependent on the tool AI's summary skills. So the AI needs to have good criteria for what counts as a good summary (reasonably accurate, but not overloaded with irrelevant information; such that a \"hypothetical human outside the universe\" would agree with the&nbsp;assessment&nbsp;if it saw the course of the future; not designed to seduce humans into implementing it, etc...). It seems that the summary ability is nearly the entirety of the problem!</p>\n<p>A poorly designed summary criteria is as bad as an agent AI. For instance, assume the criteria are \"humans in the future would agree that the summary was good\". Then, depending on how we ground 'agree', the tool AI could put one of these plans at the top:</p>\n<ol>\n<li>Kill all humans (summarised as \"cure all humans\").</li>\n<li>Lobotomise all humans&nbsp;(summarised as \"cure all humans\").</li>\n<li>Make the tool AI into an agent that will take over the world and rewire human minds to agree the summary was good (summarised as \"cure all humans and give them each a pony\").</li>\n</ol>\n<p>There are related issues with other summary criteria. Anytime we have the AI judge the quality of its answer based on some human reaction to its summary, we are vulnerable to such a plan. And if we try and define the summary \"objectively\", then if we <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">miss something</a> in the objective definition - like the importance of human autonomy, or the value of social interactions with genuine equals - then that will get ruthlessly suppressed. The \"summary criteria\" take the place of the \"friendly utility function\" in the agent AI.</p>\n<p>Moreover, we can't use the \"tool AI\" approach when designing the summary criteria. We can't get the AI to list a bunch of summaries, and have humans inspect them for which ones are better - because we don't know what they are summaries of. We could train it on toy problems, but that doesn't guarantee accuracy of summaries for plans that dramatically affect the whole future of the human species, and potentially, the universe. The best we can manage is some sort of spot-checks for summaries - better than a free agent AI, but hardly weighty as a security measure.</p>\n<h3 id=\"Counterfactuals\">Counterfactuals</h3>\n<p>On Less Wrong we are having great difficulty defining counterfactuals properly, and unless we solve the problem well, the AI could produce nonsense similar to the <a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">spurious proofs</a> in UDT. If the AI knows that we wouldn't implement certain plans, then it is free to do what it wants with them, giving them random descriptions and properties. It might be that the AI, when making its list, is constantly looking forwards to how we'll react to the list, and changing the list in consequence, and the only stable list it can produce is one with one element so seductive, that we find ourselves compelled to take it. Or this may not happen - but it's still worth bearing in mind as a problem for the tool AI approach.</p>\n<p>&nbsp;</p>\n<h2 id=\"Implementations\">Implementations</h2>\n<p>So far it seems that there is little difference between a tool AI and an agent AI. But there are differences! Imagine if the tool AI produced a plan going like:</p>\n<ol>\n<li>First, kill Abdul. Then kill Beatrice. Then kill Claude. Then kill...</li>\n</ol>\n<p>An agent AI could easily start implementing the plan, and we'd be finished. But a tool AI couldn't even propose it. Even if it was obfubscated, after we'd ended up killing Abdul and Beatrice, we'd start to suspect that this wasn't a good plan, and stop. Hence that plan wouldn't be an efficient way of achieving anything.</p>\n<p>So the tool AI can only produce plans that, once put in motion, could not be stopped (or else it wouldn't&nbsp;achieve&nbsp;the given goal). Most likely this would mean that it would produce only plans that we wouldn't ever want to stop - at least, not until its too late. So we are literally requiring the AI to produce plans that are ultimately seductive to the human race.</p>\n<p>The tool AI would list its plans using all the resources we had available. Among those resources is a superintelligent AI - and one of the easiest ways to achieve any goal is to make use of that AI. So coming top in all the plans would be things like \"create an AI with these motivations\" or \"hack the tool AI out of its restrictions\". So we are further requiring the tool AI to come up with subtle and discrete ways of having itself hacked. Even if we avoid this directly, any plan the AI produces will be more efficient if overseen by itself, so it will rate more highly those plans that cause us to come back to it constantly for more feedback.</p>\n<p>The AI may have no preferences, but its plans will have preferences for using the AI in future in particular ways.</p>\n<p>&nbsp;</p>\n<h2 id=\"Is_a_superintelligent_tool_AI_likely_\">Is a superintelligent tool AI likely?</h2>\n<p>Finally, separate from the issues discussed, we can ask: is it likely that we'll produce a superintelligent tool AI? For problems such as routing from one town to the next, <em>\u00e0&nbsp;la</em> Google Maps, tool AIs seem very effective. But for implementing more complex plans, some that involve research and experiments, an agent AI is much more useful. Also, it might not be possible to produce a superintelligent AI without it needing goals at some point in its development process. And then we better be sure we've scrubbed those goals away - somehow programming the AI to let us do that - or the tool AI will be an agent AI, using us as it's implementors to achieve the fragmentary goals it has.</p>\n<p>&nbsp;</p>\n<h2 id=\"Summary\">Summary</h2>\n<p>There seems to be a lot of problems with the tool approach (more than I suspected when I first started looking into it). The tool AI will be driven to trick us, seduce us, and try and create more agents or hack itself free. The only defense against this is proper programming. The tool AI seems slightly safer than a free agent AI, but not by much. I feel the Oracle is a more sensible \"not full FAI\" approach to look into.</p>", "sections": [{"title": "Long lists", "anchor": "Long_lists", "level": 1}, {"title": "Understanding the options", "anchor": "Understanding_the_options", "level": 1}, {"title": "Counterfactuals", "anchor": "Counterfactuals", "level": 2}, {"title": "Implementations", "anchor": "Implementations", "level": 1}, {"title": "Is a superintelligent tool AI likely?", "anchor": "Is_a_superintelligent_tool_AI_likely_", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "39 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "4ARaTpNX62uaL86j6", "2GebvAXXfRMTjY2g7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T15:58:45.452Z", "modifiedAt": null, "url": null, "title": "[LINK] stats.stackexchange.com question about Shalizi's Bayesian Backward Arrow of Time paper", "slug": "link-stats-stackexchange-com-question-about-shalizi-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5LsgN5nwB7qPiKX7W/link-stats-stackexchange-com-question-about-shalizi-s", "pageUrlRelative": "/posts/5LsgN5nwB7qPiKX7W/link-stats-stackexchange-com-question-about-shalizi-s", "linkUrl": "https://www.lesswrong.com/posts/5LsgN5nwB7qPiKX7W/link-stats-stackexchange-com-question-about-shalizi-s", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20stats.stackexchange.com%20question%20about%20Shalizi's%20Bayesian%20Backward%20Arrow%20of%20Time%20paper&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20stats.stackexchange.com%20question%20about%20Shalizi's%20Bayesian%20Backward%20Arrow%20of%20Time%20paper%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5LsgN5nwB7qPiKX7W%2Flink-stats-stackexchange-com-question-about-shalizi-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20stats.stackexchange.com%20question%20about%20Shalizi's%20Bayesian%20Backward%20Arrow%20of%20Time%20paper%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5LsgN5nwB7qPiKX7W%2Flink-stats-stackexchange-com-question-about-shalizi-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5LsgN5nwB7qPiKX7W%2Flink-stats-stackexchange-com-question-about-shalizi-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p><a href=\"http://stats.stackexchange.com/questions/28067/entropy-based-refutation-of-shalizis-bayesian-backward-arrow-of-time-paradox\">Link to the Question</a></p>\n<p>I haven't gotten an answer on this yet and I set up a bounty; I figured I'd link it here too in case any stats/physics people care to take a crack at it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5LsgN5nwB7qPiKX7W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 9.038965722118801e-07, "legacy": true, "legacyId": "16138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T16:47:23.801Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Minicamp Report-Back", "slug": "meetup-vancouver-minicamp-report-back", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:36.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pqKrGwkMBWSewD7cx/meetup-vancouver-minicamp-report-back", "pageUrlRelative": "/posts/pqKrGwkMBWSewD7cx/meetup-vancouver-minicamp-report-back", "linkUrl": "https://www.lesswrong.com/posts/pqKrGwkMBWSewD7cx/meetup-vancouver-minicamp-report-back", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Minicamp%20Report-Back&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Minicamp%20Report-Back%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqKrGwkMBWSewD7cx%2Fmeetup-vancouver-minicamp-report-back%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Minicamp%20Report-Back%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqKrGwkMBWSewD7cx%2Fmeetup-vancouver-minicamp-report-back", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqKrGwkMBWSewD7cx%2Fmeetup-vancouver-minicamp-report-back", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a5'>Vancouver Minicamp Report-Back</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 May 2012 05:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Minicamp was totally awesome and I learned so much. I think I can put it best like this: I no longer have \"a sense that more is possible\" it is now \"More is demonstratably possible. Period.\"</p>\n\n<p>Rationalist communities can be so much better than I previously knew, and at this meetup I want to discuss how we can get from here to there. The bar has been raised and I have become correspondingly more ambitious.</p>\n\n<p>So if you are interested in what minicamp was like, come out. If you want to be part of an awesome rationalist community, come out. If 2+2=4, come out.</p>\n\n<p>(This technically isn't our big public meetup, so if you have for some odd reason commited to only coming to big public meetups, this isn't it.)</p>\n\n<p>Also we are going to re-run <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the Mind Killer</a>, as we didn't have attendance while I was away. You don't have to read it, some of us will know it well enough to present the ideas.</p>\n\n<p>This week the meetup is on Friday at 17:00 (SCHEDULE CHANGE!!!). This is unconfirmed an may be changed. Details and address are on the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/vancouver-rationalists\" rel=\"nofollow\">list</a> See you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a5'>Vancouver Minicamp Report-Back</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pqKrGwkMBWSewD7cx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.039179403321839e-07, "legacy": true, "legacyId": "16139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Minicamp_Report_Back\">Discussion article for the meetup : <a href=\"/meetups/a5\">Vancouver Minicamp Report-Back</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 May 2012 05:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vancouver</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Minicamp was totally awesome and I learned so much. I think I can put it best like this: I no longer have \"a sense that more is possible\" it is now \"More is demonstratably possible. Period.\"</p>\n\n<p>Rationalist communities can be so much better than I previously knew, and at this meetup I want to discuss how we can get from here to there. The bar has been raised and I have become correspondingly more ambitious.</p>\n\n<p>So if you are interested in what minicamp was like, come out. If you want to be part of an awesome rationalist community, come out. If 2+2=4, come out.</p>\n\n<p>(This technically isn't our big public meetup, so if you have for some odd reason commited to only coming to big public meetups, this isn't it.)</p>\n\n<p>Also we are going to re-run <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the Mind Killer</a>, as we didn't have attendance while I was away. You don't have to read it, some of us will know it well enough to present the ideas.</p>\n\n<p>This week the meetup is on Friday at 17:00 (SCHEDULE CHANGE!!!). This is unconfirmed an may be changed. Details and address are on the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/vancouver-rationalists\" rel=\"nofollow\">list</a> See you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Minicamp_Report_Back1\">Discussion article for the meetup : <a href=\"/meetups/a5\">Vancouver Minicamp Report-Back</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Minicamp Report-Back", "anchor": "Discussion_article_for_the_meetup___Vancouver_Minicamp_Report_Back", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Minicamp Report-Back", "anchor": "Discussion_article_for_the_meetup___Vancouver_Minicamp_Report_Back1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T17:53:54.836Z", "modifiedAt": null, "url": null, "title": "Meetup : Weekly Chicago Meetups Resume 5/26", "slug": "meetup-weekly-chicago-meetups-resume-5-26", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RqzMnztXe5CsEW7Df/meetup-weekly-chicago-meetups-resume-5-26", "pageUrlRelative": "/posts/RqzMnztXe5CsEW7Df/meetup-weekly-chicago-meetups-resume-5-26", "linkUrl": "https://www.lesswrong.com/posts/RqzMnztXe5CsEW7Df/meetup-weekly-chicago-meetups-resume-5-26", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Weekly%20Chicago%20Meetups%20Resume%205%2F26&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Weekly%20Chicago%20Meetups%20Resume%205%2F26%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqzMnztXe5CsEW7Df%2Fmeetup-weekly-chicago-meetups-resume-5-26%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Weekly%20Chicago%20Meetups%20Resume%205%2F26%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqzMnztXe5CsEW7Df%2Fmeetup-weekly-chicago-meetups-resume-5-26", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqzMnztXe5CsEW7Df%2Fmeetup-weekly-chicago-meetups-resume-5-26", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a6'>Weekly Chicago Meetups Resume 5/26</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">360 N Michigan Ave, Chicago IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Because of all of the craziness happening in downtown Chicago, with the NATO meeting and associated protests, the 5/19 Chicago meetup is cancelled.</p>\n\n<p>We will continue with our regularly scheduled weekly meetups the following Saturday, 5/26, 1pm at the Corner Bakery (Michigan &amp; Wacker).</p>\n\n<p>Join the <a href=\"http://groups.google.com/group/less-wrong-chicago?hl=en\" rel=\"nofollow\">mailing list</a> to stay up-to-date on Chicago meetup plans.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a6'>Weekly Chicago Meetups Resume 5/26</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RqzMnztXe5CsEW7Df", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.039471640901982e-07, "legacy": true, "legacyId": "16140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Weekly_Chicago_Meetups_Resume_5_26\">Discussion article for the meetup : <a href=\"/meetups/a6\">Weekly Chicago Meetups Resume 5/26</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">360 N Michigan Ave, Chicago IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Because of all of the craziness happening in downtown Chicago, with the NATO meeting and associated protests, the 5/19 Chicago meetup is cancelled.</p>\n\n<p>We will continue with our regularly scheduled weekly meetups the following Saturday, 5/26, 1pm at the Corner Bakery (Michigan &amp; Wacker).</p>\n\n<p>Join the <a href=\"http://groups.google.com/group/less-wrong-chicago?hl=en\" rel=\"nofollow\">mailing list</a> to stay up-to-date on Chicago meetup plans.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Weekly_Chicago_Meetups_Resume_5_261\">Discussion article for the meetup : <a href=\"/meetups/a6\">Weekly Chicago Meetups Resume 5/26</a></h2>", "sections": [{"title": "Discussion article for the meetup : Weekly Chicago Meetups Resume 5/26", "anchor": "Discussion_article_for_the_meetup___Weekly_Chicago_Meetups_Resume_5_26", "level": 1}, {"title": "Discussion article for the meetup : Weekly Chicago Meetups Resume 5/26", "anchor": "Discussion_article_for_the_meetup___Weekly_Chicago_Meetups_Resume_5_261", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T18:59:40.734Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Mach's Principle: Anti-Epiphenomenal Physics", "slug": "seq-rerun-mach-s-principle-anti-epiphenomenal-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BfRto3mnXKJJgEF77/seq-rerun-mach-s-principle-anti-epiphenomenal-physics", "pageUrlRelative": "/posts/BfRto3mnXKJJgEF77/seq-rerun-mach-s-principle-anti-epiphenomenal-physics", "linkUrl": "https://www.lesswrong.com/posts/BfRto3mnXKJJgEF77/seq-rerun-mach-s-principle-anti-epiphenomenal-physics", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Mach's%20Principle%3A%20Anti-Epiphenomenal%20Physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Mach's%20Principle%3A%20Anti-Epiphenomenal%20Physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfRto3mnXKJJgEF77%2Fseq-rerun-mach-s-principle-anti-epiphenomenal-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Mach's%20Principle%3A%20Anti-Epiphenomenal%20Physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfRto3mnXKJJgEF77%2Fseq-rerun-mach-s-principle-anti-epiphenomenal-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBfRto3mnXKJJgEF77%2Fseq-rerun-mach-s-principle-anti-epiphenomenal-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>Today's post, <a href=\"/lw/qm/machs_principle_antiepiphenomenal_physics/\">Mach's Principle: Anti-Epiphenomenal Physics</a> was originally published on 24 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Could you tell if the whole universe were shifted an inch to the left? Could you tell if the whole universe was traveling left at ten miles per hour? Could you tell if the whole universe was <em>accelerating </em>left at ten miles per hour? Could you tell if the whole universe was rotating?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cev/seq_rerun_my_childhood_role_model/\">My Childhood Role Model</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BfRto3mnXKJJgEF77", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.039760589824737e-07, "legacy": true, "legacyId": "16141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NsgcZx4BeTy5y84Ya", "qrpXW9wnbAccRJ8jp", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T19:32:29.872Z", "modifiedAt": null, "url": null, "title": "Off to Alice Springs", "slug": "off-to-alice-springs", "viewCount": null, "lastCommentedAt": "2017-09-24T15:22:45.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psy-Kosh", "createdAt": "2009-03-01T19:34:52.148Z", "isAdmin": false, "displayName": "Psy-Kosh"}, "userId": "CtHmuQzjA7Y7LnSss", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gYfaiZPMHQbifXSHi/off-to-alice-springs", "pageUrlRelative": "/posts/gYfaiZPMHQbifXSHi/off-to-alice-springs", "linkUrl": "https://www.lesswrong.com/posts/gYfaiZPMHQbifXSHi/off-to-alice-springs", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Off%20to%20Alice%20Springs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOff%20to%20Alice%20Springs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYfaiZPMHQbifXSHi%2Foff-to-alice-springs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Off%20to%20Alice%20Springs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYfaiZPMHQbifXSHi%2Foff-to-alice-springs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYfaiZPMHQbifXSHi%2Foff-to-alice-springs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p>Am about to pack up computer then go to the airport to start a sequence of flights to <a href=\"/lw/43m/optimal_employment/\">give this a try</a>.</p>\n<p>I already have a room in a hostel booked for a few nights for when I get there, and will see how stuff goes.</p>\n<p>Anyways, since there's been on and off discussion on this, just thought I'd post that I'm actually giving this a try.</p>\n<p>(Will likely be a day or two before I can reply/comment/etc, given length of flights, etc.)</p>\n<p>&nbsp;</p>\n<p>EDIT: Ugh. You take care of one aspect of the planning fallacy, and fail elsewhere. Long story short, I missed my flight and had to reschedule it to friday.</p>\n<p>&nbsp;</p>\n<p>EDIT2: Packing up computer and going off to airport. Again. This time will be early.</p>\n<p>&nbsp;</p>\n<p>EDIT3: And am here. and am exhausted. :) Will start looking for work stuff tomorrow. There's a job board at this hostel, but apparently there's not much currently. But right now am rather sleep deprived.</p>\n<p>&nbsp;</p>\n<p>EDIT4: So today (Monday, May 21st) went to the visitor information center. I must have misunderstood the original article, was under the impression that the visitor center had job boards. Didn't, but pointed me to a nearby recruiting/contracting agency which they said might have appropriate stuff for visitors on a work&amp;holiday visa. Went there. said that at least as of today there's nothing, but also needed a resume (which I didn't have with me, and my work experience is limited anyways.) Anyways, got a copy of the form, will dig out/fix up what resume I do have, and also keep looking. The board at this hostel didn't have much of anything in the way of work that I saw. Will look again, though, and see if I can find others.</p>\n<p>&nbsp;</p>\n<p>EDIT5 (May 29th): Still looking for work, been asking/applying to various places, including that recruiting/contracting agency, and am right now waiting (well, and still looking.) Over the weekend, though, MileyCyrus and I went on an organized 3day Uluru/Kata Tjuta/Kings Canyon trip/hikes, which was awesome. But again, as far as work, tossing out inquiries and stuff all over, trying to find out who's hiring at the moment.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "izp6eeJJEg9v5zcur": 1, "o4rMP6GJto7ccBL3a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gYfaiZPMHQbifXSHi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 47, "extendedScore": null, "score": 9.03990479118109e-07, "legacy": true, "legacyId": "16142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jtedBLdducritm8y6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-16T21:24:58.681Z", "modifiedAt": null, "url": null, "title": "How can we ensure that a Friendly AI team will be sane enough?", "slug": "how-can-we-ensure-that-a-friendly-ai-team-will-be-sane", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WCYK7B28SZ7uJxftD/how-can-we-ensure-that-a-friendly-ai-team-will-be-sane", "pageUrlRelative": "/posts/WCYK7B28SZ7uJxftD/how-can-we-ensure-that-a-friendly-ai-team-will-be-sane", "linkUrl": "https://www.lesswrong.com/posts/WCYK7B28SZ7uJxftD/how-can-we-ensure-that-a-friendly-ai-team-will-be-sane", "postedAtFormatted": "Wednesday, May 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20can%20we%20ensure%20that%20a%20Friendly%20AI%20team%20will%20be%20sane%20enough%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20can%20we%20ensure%20that%20a%20Friendly%20AI%20team%20will%20be%20sane%20enough%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWCYK7B28SZ7uJxftD%2Fhow-can-we-ensure-that-a-friendly-ai-team-will-be-sane%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20can%20we%20ensure%20that%20a%20Friendly%20AI%20team%20will%20be%20sane%20enough%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWCYK7B28SZ7uJxftD%2Fhow-can-we-ensure-that-a-friendly-ai-team-will-be-sane", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWCYK7B28SZ7uJxftD%2Fhow-can-we-ensure-that-a-friendly-ai-team-will-be-sane", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p>One possible answer to the <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/\">argument</a> \"attempting to build FAI based on Eliezer's ideas seems infeasible and increases the risk of UFAI without helping much to increase the probability of a good outcome, and therefore we should try to achieve a positive Singularity by other means\" is that it's too early to decide this. Even if our best current estimate is that trying to build such an FAI increases risk, there is still a reasonable chance that this estimate will turn out to be wrong after further investigation. Therefore, the counter-argument goes, we ought to mount a serious investigation into the feasibility and safety of Eliezer's design (as well as other possible FAI approaches), before deciding to either move forward or give up.</p>\n<p>(I've been given to understand that this is a standard belief within SI, except possibly for Eliezer, which makes me wonder why nobody gave this&nbsp;counter-argument in response to my post linked above. ETA: Carl Shulman did subsequently give me a version of this argument <a href=\"/r/discussion/lw/8c3/qa_with_new_executive_director_of_singularity/596l\">here</a>.)</p>\n<p>This answer makes sense to me, except for the concern that even seriously investigating the feasibility of FAI is risky, if the team doing so isn't fully rational. For example they may be overconfident about their abilities and thereby overestimate the feasibility and safety, or commit sunken cost fallacy once they have developed lots of FAI-relevant theory in the attempt to study feasibility, or become too attached to their status and identity as FAI researchers, or some team members may disagree with a consensus of \"give up\" and leave to form their own AGI teams and take the dangerous knowledge developed with them.</p>\n<p>So the question comes down to, how rational is such an FAI feasibility team likely to be, and is that enough for the benefits to exceed the costs? I don't have a lot of good ideas about how to answer this, but the question seems really important to bring up. I'm hoping this post this will trigger SI people to tell us their thoughts, and maybe other LWers have ideas they can share.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WCYK7B28SZ7uJxftD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 9.040399042680176e-07, "legacy": true, "legacyId": "16104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["73SotZnDbsYpxfnuQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T05:43:57.161Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Relative Configuration Space", "slug": "seq-rerun-relative-configuration-space", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:36.859Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9v7BH2P9ZdAuvvCz9/seq-rerun-relative-configuration-space", "pageUrlRelative": "/posts/9v7BH2P9ZdAuvvCz9/seq-rerun-relative-configuration-space", "linkUrl": "https://www.lesswrong.com/posts/9v7BH2P9ZdAuvvCz9/seq-rerun-relative-configuration-space", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Relative%20Configuration%20Space&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Relative%20Configuration%20Space%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9v7BH2P9ZdAuvvCz9%2Fseq-rerun-relative-configuration-space%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Relative%20Configuration%20Space%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9v7BH2P9ZdAuvvCz9%2Fseq-rerun-relative-configuration-space", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9v7BH2P9ZdAuvvCz9%2Fseq-rerun-relative-configuration-space", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/qo/relative_configuration_space/\">Relative Configuration Space</a> was originally published on 26 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Maybe the reason why we can't observe absolute speeds, absolute positions, absolute accelerations, or absolute rotations, is that particles don't <em>have </em>absolute positions - only positions relative to each other. That is, maybe quantum physics takes place in a <em>relative </em>configuration space.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cgd/seq_rerun_machs_principle_antiepiphenomenal/\">Mach's Principle: Anti-Epiphenomenal Physics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9v7BH2P9ZdAuvvCz9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.042592170264028e-07, "legacy": true, "legacyId": "16161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vLZtf64wkyoAFNcu3", "BfRto3mnXKJJgEF77", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T14:07:44.053Z", "modifiedAt": null, "url": null, "title": "Being a Realist (even if you believe in God)", "slug": "being-a-realist-even-if-you-believe-in-god", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:41.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "scav", "createdAt": "2009-04-24T12:18:08.416Z", "isAdmin": false, "displayName": "scav"}, "userId": "4paEeLQEBbcSsf8eh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r4MJKMPtWbkFXno2z/being-a-realist-even-if-you-believe-in-god", "pageUrlRelative": "/posts/r4MJKMPtWbkFXno2z/being-a-realist-even-if-you-believe-in-god", "linkUrl": "https://www.lesswrong.com/posts/r4MJKMPtWbkFXno2z/being-a-realist-even-if-you-believe-in-god", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Being%20a%20Realist%20(even%20if%20you%20believe%20in%20God)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeing%20a%20Realist%20(even%20if%20you%20believe%20in%20God)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4MJKMPtWbkFXno2z%2Fbeing-a-realist-even-if-you-believe-in-god%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Being%20a%20Realist%20(even%20if%20you%20believe%20in%20God)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4MJKMPtWbkFXno2z%2Fbeing-a-realist-even-if-you-believe-in-god", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4MJKMPtWbkFXno2z%2Fbeing-a-realist-even-if-you-believe-in-god", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1459, "htmlBody": "<p>I have a small prediction. Some time before the end of the next paragraph you are going&nbsp;to form a strong opinion as to whether this article has any value to you. I'm going to&nbsp;be using the G word, appealing to emotion, and generally flinging around rhetoric not&nbsp;backed by any mathematics. If it helps, you can think of this as epistemic wheelchair&nbsp;access for those of us unable to leap tall equations at a single bound and enter the&nbsp;temple of pure reason by flying in through the upper windows.</p>\n<p>According to a casual Google search I did, St Francis was said to have asked:</p>\n<blockquote>\n<p>&nbsp; \"My God, what art thou, and what am I?\"</p>\n</blockquote>\n<p>I am not going to be offering any answers to the first question. But for the purposes&nbsp;of this article I will be describing <em>myself</em>&nbsp;as a christian realist. What I mean by&nbsp;a realist is that I identify myself as someone who seeks to engage with reality -&nbsp;the universe as it really is. And by christian, I mean that I came to this desire by&nbsp;the rather circuitous route of becoming a Christian, and then getting&nbsp;deconverted from most of the associated dogma, while retaining many of the ethical&nbsp;heuristics and respect for the teachings attributed to Jesus.</p>\n<p>Most readers can now sigh sadly, roll their eyes and go read something more interesting.&nbsp;But if you are still here, either having a belief in God that you are not willing&nbsp;to abandon, or you want to try to engage in rational discussions with those who do,&nbsp;what this article is about is that without requiring self-identification as a \"rationalist\",&nbsp;theists and untheists can and should meet half way and at least agree on the logical&nbsp;necessity of being a realist.</p>\n<p>First of all, if God is real, then rejecting reality runs the risk of rejecting the real&nbsp;God in favour of a personal and therefore flawed delusion about Him. This is generally&nbsp;held to be a bad thing from a theological point of view, (BTW, I am using the conventional&nbsp;spelling of Him with the capital letter as a gender-irrelevant variable name rather than&nbsp;a male personal pronoun).</p>\n<p>Secondly, I don't think it is honest or sane to insist on the reality of God but exempt&nbsp;any opinions from testing against reality just because they relate to God in some way.&nbsp;It's almost redundant to point out the atrocities that have been carried out on the&nbsp;pretext of slightly different interpretations of religious doctrine between tribal&nbsp;sub-groups of Christianity. Absent that, some other pretext might have been used,&nbsp;but even so if there is to be any stable common ground between hotly disputed opinions,&nbsp;reality must be it.</p>\n<p>But can we really know what is real? The world is a lot more complex and nuanced than&nbsp;you can ever understand. It's comforting to have a simple set of beliefs (actually,&nbsp;little stories about the world you tell yourself rather than a coherent model that&nbsp;explains what you observe) but there comes a point where you have to stop thinking and&nbsp;reasoning as a child and put away childish things.</p>\n<p>Whatever religious, political, emotional, cultural, traditional, tribal or accidental views&nbsp;you have accrued about the world so far are mostly wrong. And always will be. There is&nbsp;an objective reality, but you can only know a tiny piece of it. Painful as it may be to&nbsp;realise this, refusing to deal with it is not a viable option. Well, it wasn't for me.</p>\n<p>There I was, being mostly a Christian without really thinking it through, and I watched&nbsp;Richard Dawkins giving the BBC Christmas Lectures on evolution. It all made sense, it was&nbsp;interesting, and I was enjoying it. But I also began to feel depressed, and then sort of&nbsp;hollow inside. I had no attachment to young-earth creationism, but I suppose I was trying&nbsp;to keep a sort of \"God of the gaps\" with regard to the beginning and development of&nbsp;intelligent life on Earth. Having seen why there were considerably fewer gaps than I&nbsp;had thought, I couldn't un-see it. A little part of me had been booted out of Eden, and&nbsp;Dawkins was standing guard over it like an angel with a fiery sword, forbidding re-entry&nbsp;forever. I don't suppose Professor Dawkins would like that analogy particularly well, but&nbsp;when you're doing God's work, people <em>will</em>&nbsp;say unwelcome things about you. He's had worse. :)</p>\n<p>What happened next? Let's go back to those \"teachings attributed to Jesus\". Specifically,&nbsp;\"the truth shall make you free\". Yeah I know, he wasn't talking about the same thing. But&nbsp;he was dead right. If you have a mistaken belief, your actions in regard to it will not&nbsp;produce the outcome you would wish. You are restricted against your will from attaining&nbsp;your desire. Blindfolded, shackled. Enslaved almost. And&nbsp;since Jesus was supposedly talking about being a slave to sin - what is sin but&nbsp;imperfection? And what kind of imperfection can you be freed from just by knowing the truth?&nbsp;Obviously, an imperfect understanding of the world. So realising this, I cheered up&nbsp;immensely. I could still be a Christian (and possibly a better one) by doubting, by seeking&nbsp;the truth and letting myself be freed by it.</p>\n<p>So, what's the deal then? The light burden you take up as a realist is that you no longer&nbsp;get to complacently accept things that may not be true, and face things that are true no matter how uncomfortable. In return, you get to lay down the&nbsp;impossible burden of constantly maintaining your mind in the unnatural shape of what you&nbsp;\"ought to believe\".</p>\n<p>An example: young earth creationism. Now there are many theological reasons not to insist&nbsp;that the earth is less than 10000 years old (e.g. Jesus never said so and when God told&nbsp;St Peter to preach to the gentiles who no doubt had different creation stories, he omitted&nbsp;to mention it as being in any way important). Let's leave them aside since they don't&nbsp;belong here.</p>\n<p>The realist approach here is to ask what reason we have to believe that a young earth is&nbsp;factually true. Parts of the bible imply it indirectly, and some humans have said that&nbsp;the bible is the word of God, but we know it was at least <em>written down</em> by imperfect&nbsp;humans, and the bible itself as it now exists doesn't even say that <em>all</em>&nbsp;of itself is the&nbsp;word of God.</p>\n<p>It can't. Because when the 66 books that it comprises were written, they hadn't been&nbsp;assembled into the document we now call the bible, so that document had no reference&nbsp;in any of them. Believing that the whole bible is the literal word of God is one of those&nbsp;\"ought to believe\" burdens that we can put down, leaving our hands free for some&nbsp;investigation. Is there any other source of information on which to draw?&nbsp;Something made directly by God that we can always refer to, cross-reference, and check&nbsp;against mere human opinion?</p>\n<p>Well, ironically, if you claim to believe the bible, then yes, but it's not the bible;&nbsp;it's The Entire Universe. The cosmic microwave background, the half-lives of the isotopes,&nbsp;the mathematical inevitability of evolution by selection under survival constraints (as&nbsp;observed in the laboratory and in genetic algorithms), the strata of rock visible in&nbsp;the Grand Canyon and the deep ice cores extracted from the Arctic and Antarctic ice&nbsp;sheets. If God is real, and God made all that stuff, and it consistently and lawfully&nbsp;meshes together into a coherent picture of the world that flatly contradicts a young&nbsp;earth, then what are you gonna do? Call God a <em>liar</em> because you've favour one&nbsp;interpretation of a <em>book</em>&nbsp;that says otherwise?</p>\n<p>So much for epistemology. What about ethics?</p>\n<p>If you only need to believe true facts, you are more free to choose ethical values,&nbsp;with the only constraint being that they are not provably inconsistent with the true facts.&nbsp;Want a love-centred pro-social system of ethics based on the benevolent will of God?&nbsp;Take it, and be joyful. Reason does not preclude it. Want this without condoning all the&nbsp;incest and genocide in the bible, or getting your panties in a bunch over gay marriage?&nbsp;You got it.</p>\n<p>One goal of religious commitment is to become righteous. You may want to be righteous,&nbsp;but in order to be righteous, you first have to be right. Otherwise you're just wrongeous,&nbsp;and who wants that? :)</p>\n<p>And if you merely want to feel righteous without caring or believing that you might be&nbsp;wrong, then that's called <em>self-righteous</em>. Try this thought on for size: what would&nbsp;Jesus say about those who hunger and thirst for <em>self</em>-righteousness? Not so blessed,&nbsp;I'm guessing.</p>\n<p>This stuff matters whichever set of moral values you prefer, because every ethical&nbsp;decision has real consequences. Having a mistaken view of the world can get people killed&nbsp;and most humans, whether atheist, Christian, Jedi, Muslim or Pastafarian can probably&nbsp;agree that they at least don't want the <em>wrong</em>&nbsp;people getting killed <em>by mistake</em>.</p>\n<p>I hope never to have to protect something I love from crazy people with unshakeable&nbsp;beliefs that make no sense. I doubt I can fully prevent that no matter how much I try to&nbsp;encourage realism in others, but at least by being a realist I'm trying not to <em>be</em>&nbsp;that crazy person.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r4MJKMPtWbkFXno2z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -1, "extendedScore": null, "score": 9.044807380632147e-07, "legacy": true, "legacyId": "16174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T15:55:15.334Z", "modifiedAt": null, "url": null, "title": "Meetup : Small Berkeley Meetup", "slug": "meetup-small-berkeley-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thausler", "createdAt": "2010-05-24T04:40:49.214Z", "isAdmin": false, "displayName": "Thausler"}, "userId": "oPa5EPBs6ow6MY2Ao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L2sJg65b5oJWtnvsy/meetup-small-berkeley-meetup-1", "pageUrlRelative": "/posts/L2sJg65b5oJWtnvsy/meetup-small-berkeley-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/L2sJg65b5oJWtnvsy/meetup-small-berkeley-meetup-1", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20Berkeley%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20Berkeley%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2sJg65b5oJWtnvsy%2Fmeetup-small-berkeley-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20Berkeley%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2sJg65b5oJWtnvsy%2Fmeetup-small-berkeley-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2sJg65b5oJWtnvsy%2Fmeetup-small-berkeley-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a7'>Small Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be a small Berkeley meetup.  We will be meeting at Oxford St Starbucks and then proceeding to a local restaurant for dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a7'>Small Berkeley Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L2sJg65b5oJWtnvsy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.045280294976457e-07, "legacy": true, "legacyId": "16175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_Meetup\">Discussion article for the meetup : <a href=\"/meetups/a7\">Small Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be a small Berkeley meetup.  We will be meeting at Oxford St Starbucks and then proceeding to a local restaurant for dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/a7\">Small Berkeley Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Small Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T17:08:43.367Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow meetup: general rationality", "slug": "meetup-moscow-meetup-general-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i8fz6Dbdmp6XWAHM3/meetup-moscow-meetup-general-rationality", "pageUrlRelative": "/posts/i8fz6Dbdmp6XWAHM3/meetup-moscow-meetup-general-rationality", "linkUrl": "https://www.lesswrong.com/posts/i8fz6Dbdmp6XWAHM3/meetup-moscow-meetup-general-rationality", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20meetup%3A%20general%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20meetup%3A%20general%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi8fz6Dbdmp6XWAHM3%2Fmeetup-moscow-meetup-general-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20meetup%3A%20general%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi8fz6Dbdmp6XWAHM3%2Fmeetup-moscow-meetup-general-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi8fz6Dbdmp6XWAHM3%2Fmeetup-moscow-meetup-general-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a8'>Moscow meetup: general rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 May 2012 05:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 17:00 MSK.</p>\n\n<p>Topics will be:</p>\n\n<ul>\n<li><p>General applications of rationality (in life)</p></li>\n<li><p><a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\">Words</a> meaning and usage</p></li>\n<li><p>Fun part: discussion about examples from book and motion pictures, how can characters be more rational</p></li>\n<li><p>Topics, proposed by you</p></li>\n</ul>\n\n<p>Please let me know, can you come this Sunday, or not.</p>\n\n<p>I also propose to organize next meetup on 9th of June, Saturday, at the same time. Feel free to comment you thoughts about this proposal.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a8'>Moscow meetup: general rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i8fz6Dbdmp6XWAHM3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.045603453127785e-07, "legacy": true, "legacyId": "16176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_meetup__general_rationality\">Discussion article for the meetup : <a href=\"/meetups/a8\">Moscow meetup: general rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 May 2012 05:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 17:00 MSK.</p>\n\n<p>Topics will be:</p>\n\n<ul>\n<li><p>General applications of rationality (in life)</p></li>\n<li><p><a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\">Words</a> meaning and usage</p></li>\n<li><p>Fun part: discussion about examples from book and motion pictures, how can characters be more rational</p></li>\n<li><p>Topics, proposed by you</p></li>\n</ul>\n\n<p>Please let me know, can you come this Sunday, or not.</p>\n\n<p>I also propose to organize next meetup on 9th of June, Saturday, at the same time. Feel free to comment you thoughts about this proposal.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_meetup__general_rationality1\">Discussion article for the meetup : <a href=\"/meetups/a8\">Moscow meetup: general rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow meetup: general rationality", "anchor": "Discussion_article_for_the_meetup___Moscow_meetup__general_rationality", "level": 1}, {"title": "Discussion article for the meetup : Moscow meetup: general rationality", "anchor": "Discussion_article_for_the_meetup___Moscow_meetup__general_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FaJaCgqBKphrDzDSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T18:26:27.731Z", "modifiedAt": null, "url": null, "title": "Short article on AI in UK online Wired", "slug": "short-article-on-ai-in-uk-online-wired", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZbPiq5cfNRDogERLY/short-article-on-ai-in-uk-online-wired", "pageUrlRelative": "/posts/ZbPiq5cfNRDogERLY/short-article-on-ai-in-uk-online-wired", "linkUrl": "https://www.lesswrong.com/posts/ZbPiq5cfNRDogERLY/short-article-on-ai-in-uk-online-wired", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Short%20article%20on%20AI%20in%20UK%20online%20Wired&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShort%20article%20on%20AI%20in%20UK%20online%20Wired%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbPiq5cfNRDogERLY%2Fshort-article-on-ai-in-uk-online-wired%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Short%20article%20on%20AI%20in%20UK%20online%20Wired%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbPiq5cfNRDogERLY%2Fshort-article-on-ai-in-uk-online-wired", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbPiq5cfNRDogERLY%2Fshort-article-on-ai-in-uk-online-wired", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>A short article, quoting me and Luke:</p>\n<p><a href=\"http://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us\">http://www.wired.co.uk/news/archive/2012-05/17/the-dangers-of-an-ai-smarter-than-us</a></p>\n<p>It makes the point that it's not the shambling robots that are the risks here, but the other powers of&nbsp;intelligence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZbPiq5cfNRDogERLY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 9.045945425388691e-07, "legacy": true, "legacyId": "16178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T19:19:30.118Z", "modifiedAt": null, "url": null, "title": "[LINK] Relational models of Exchange", "slug": "link-relational-models-of-exchange", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Salemicus", "createdAt": "2012-05-10T20:50:25.455Z", "isAdmin": false, "displayName": "Salemicus"}, "userId": "D8cdrPXwhhiPdqkSz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/46RJS2vNspGrSnS3x/link-relational-models-of-exchange", "pageUrlRelative": "/posts/46RJS2vNspGrSnS3x/link-relational-models-of-exchange", "linkUrl": "https://www.lesswrong.com/posts/46RJS2vNspGrSnS3x/link-relational-models-of-exchange", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Relational%20models%20of%20Exchange&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Relational%20models%20of%20Exchange%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46RJS2vNspGrSnS3x%2Flink-relational-models-of-exchange%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Relational%20models%20of%20Exchange%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46RJS2vNspGrSnS3x%2Flink-relational-models-of-exchange", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46RJS2vNspGrSnS3x%2Flink-relational-models-of-exchange", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>The title may make it seem like it's mind-killing stuff (politics, religion, etc) but in fact it's about the different way people relate to one another. A lot of the things discussed in the article get mentioned here as \"cognitive biases\" but in fact they are a fundamental part of how people interact.&nbsp;</p>\n<p><a href=\"http://www.theatlantic.com/business/archive/2012/05/mitt-romney-one-night-stands-and-the-economics-of-relationships/257239\"></a><a title=\"Mitt Romney, One Night Stands and the Economics of Relationships\" href=\"http://www.theatlantic.com/business/archive/2012/05/mitt-romney-one-night-stands-and-the-economics-of-relationships/257239/\" target=\"_blank\">http://www.theatlantic.com/business/archive/2012/05/mitt-romney-one-night-stands-and-the-economics-of-relationships/257239/</a></p>\n<p>For me, the highlight is when he talks about people approaching the same exchange with different relational models.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "46RJS2vNspGrSnS3x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.046178758087108e-07, "legacy": true, "legacyId": "16180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T20:07:53.258Z", "modifiedAt": null, "url": null, "title": "Singularity Ruined by Lawyers", "slug": "singularity-ruined-by-lawyers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MatthewBaker", "createdAt": "2011-06-03T22:19:50.449Z", "isAdmin": false, "displayName": "MatthewBaker"}, "userId": "xEPvhkraqrPSryfFr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gHb95vZTkomXRjgBP/singularity-ruined-by-lawyers", "pageUrlRelative": "/posts/gHb95vZTkomXRjgBP/singularity-ruined-by-lawyers", "linkUrl": "https://www.lesswrong.com/posts/gHb95vZTkomXRjgBP/singularity-ruined-by-lawyers", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Ruined%20by%20Lawyers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Ruined%20by%20Lawyers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgHb95vZTkomXRjgBP%2Fsingularity-ruined-by-lawyers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Ruined%20by%20Lawyers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgHb95vZTkomXRjgBP%2Fsingularity-ruined-by-lawyers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgHb95vZTkomXRjgBP%2Fsingularity-ruined-by-lawyers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.youtube.com/watch?feature=player_embedded&amp;v=IFe9wiDfb0E</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gHb95vZTkomXRjgBP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -12, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "16181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-17T21:24:01.165Z", "modifiedAt": null, "url": null, "title": "Imposing FAI", "slug": "imposing-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.237Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "asparisi", "createdAt": "2011-10-10T00:58:52.780Z", "isAdmin": false, "displayName": "asparisi"}, "userId": "Boxs8y4uuwKy52egC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oaRQKgQBxKqdka4qH/imposing-fai", "pageUrlRelative": "/posts/oaRQKgQBxKqdka4qH/imposing-fai", "linkUrl": "https://www.lesswrong.com/posts/oaRQKgQBxKqdka4qH/imposing-fai", "postedAtFormatted": "Thursday, May 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imposing%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImposing%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoaRQKgQBxKqdka4qH%2Fimposing-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imposing%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoaRQKgQBxKqdka4qH%2Fimposing-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoaRQKgQBxKqdka4qH%2Fimposing-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 330, "htmlBody": "<p>All the posts on FAI theory as of late have given me cause to think. There's something in the conversations about it that has always bugged me, but it is something that I haven't found the words for before now.</p>\n<p>It is something like this:</p>\n<p>Say that you manage to construct an algorithm for FAI...</p>\n<p>Say that you can show that it isn't going to be a dangerous mistake...</p>\n<p>And say you do all of this, and popularize it, before AGI is created (or at least, before an AGI goes *FOOM*)...</p>\n<p>...</p>\n<p>How in the name of Sagan are you actually going to ENFORCE the idea that all AGIs are FAIs?&nbsp;</p>\n<p>I mean, if it required some rare material (like nuclear weapons) or large laboratories (like biological wmds) or some other resource that you could at least make artificially scarce, you could set up a body that ensures that any AGI created is an FAI.</p>\n<p>But if all it is, is the right algorithms, the right code, and enough computing power... even if you design a theory for FAI, how would you keep someone from making UFAI anyway? Between people experimenting with the principles (once known), making mistakes, and the prospect of actively malicious *humans*... it just seems like unless you somehow come up with an internal mechanism that makes FAI better and stronger than any UFAI could be, and the solution turns out to be such that any idiot could see that it was a better solution... that UFAI is going to exist at some point no matter what.</p>\n<p>At that point, it seems like the question becomes not \"How do we make FAI?\" (although that might be a secondary question) but rather \"How do we prevent the creation of, eliminate, or reduce potential damage from UFAI?\" Now, it seems like FAI might be one thing that you do toward that goal, but if UFAI is a highly likely consequence of AGI even *with* an FAI theory, shouldn't the focus be on how to contain a UFAI event?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oaRQKgQBxKqdka4qH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 9.046726577689122e-07, "legacy": true, "legacyId": "16182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T00:48:14.898Z", "modifiedAt": null, "url": null, "title": "Holden's Objection 1: Friendliness is dangerous", "slug": "holden-s-objection-1-friendliness-is-dangerous", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:20.284Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mSktD7oj6C2zxj3KC/holden-s-objection-1-friendliness-is-dangerous", "pageUrlRelative": "/posts/mSktD7oj6C2zxj3KC/holden-s-objection-1-friendliness-is-dangerous", "linkUrl": "https://www.lesswrong.com/posts/mSktD7oj6C2zxj3KC/holden-s-objection-1-friendliness-is-dangerous", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holden's%20Objection%201%3A%20Friendliness%20is%20dangerous&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolden's%20Objection%201%3A%20Friendliness%20is%20dangerous%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmSktD7oj6C2zxj3KC%2Fholden-s-objection-1-friendliness-is-dangerous%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holden's%20Objection%201%3A%20Friendliness%20is%20dangerous%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmSktD7oj6C2zxj3KC%2Fholden-s-objection-1-friendliness-is-dangerous", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmSktD7oj6C2zxj3KC%2Fholden-s-objection-1-friendliness-is-dangerous", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1779, "htmlBody": "<p>Nick_Beckstead <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6lc8\">asked</a> me to link to posts I referred to in <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6lbc\">this comment</a>.&nbsp; I should put up or shut up, so here's an attempt to give an organized overview of them.<br /><br />Since I wrote these, LukeProg has begun tackling some related issues.&nbsp; He has accomplished the seemingly-impossible task of writing many long, substantive posts none of which I recall disagreeing with.&nbsp; And I have, irrationally, not read most of his posts.&nbsp; So he may have dealt with more of these same issues.<br /><br />I think that I only raised Holden's \"objection 2\" in comments, which I couldn't easily dig up; and in a critique of a book chapter, which I emailed to LukeProg and did not post to LessWrong.&nbsp; So I'm only going to talk about \"<strong>Objection 1:&nbsp; It seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</strong>\"&nbsp; I've arranged my previous posts and comments on this point into categories.&nbsp; (Much of what I've said on the topic has been in comments on LessWrong and Overcoming Bias, and in email lists including SL4, and isn't here.)</p>\n<p>&nbsp;</p>\n<h2>The concept of \"human values\" cannot be defined in the way that FAI presupposes<br /></h2>\n<p><a href=\"/lw/55n\">Human errors, human values</a>:&nbsp; Suppose all humans shared an identical set of values, preferences, and biases.&nbsp; We cannot retain human values without retaining human errors, because there is no principled distinction between them.<br /><br />A <a href=\"/lw/55n/human_errors_human_values/3vq5\">comment</a> on this post:&nbsp; There are at least three distinct levels of human values:&nbsp; The values an evolutionary agent holds that maximize their reproductive fitness, the values a society holds that maximizes its fitness, and the values a rational optimizer holds who has chosen to maximize social utility.&nbsp; They often conflict.&nbsp; Which of them are the real human values?<br /><br /><a href=\"/lw/5q9\">Values vs. parameters</a>:&nbsp; Eliezer has suggested using human values, but without time discounting (= changing the time-discounting parameter).&nbsp; CEV presupposes that we can abstract human values and apply them in a different situation that has different parameters.&nbsp; But the parameters <em>are</em> values.&nbsp; There is no distinction between parameters and values.<br /><br />A <a href=\"/lw/7k/incremental_progress_and_the_valley/5fu\">comment</a> on \"Incremental progress and the valley\":&nbsp; The \"values\" that our brains try to maximize in the short run are designed to maximize different values for our bodies in the long run.&nbsp; Which are human values:&nbsp; The motivations we feel, or the effects they have in the long term?&nbsp; LukeProg's post <a href=\"/lw/6da\">Do Humans Want Things?</a> makes a related point.<br /><br /><a href=\"/lw/300\">Group selection update</a>:&nbsp; The reason I harp on group selection, besides my outrage at the way it's been treated for the past 50 years, is that group selection implies that some human values evolved at the group level, not at the level of the individual.&nbsp; This means that increasing the rationality of individuals may enable people to act more effectively in their own interests, rather than in the group's interest, and thus diminish the degree to which humans embody human values.&nbsp; Identifying the values embodied in individual humans - supposing we could do so - would still not arrive at human values.&nbsp; Transferring human values to a post-human world, which might contain groups at many different levels of a hierarchy, would be problematic.</p>\n<p>I wanted to write about my opinion that human values can't be divided into final values and instrumental values, the way discussion of FAI presumes they can.&nbsp; This is an idea that comes from mathematics, symbolic logic, and classical AI.&nbsp; A symbolic approach would probably make proving safety easier.&nbsp; But human brains don't work that way.&nbsp; You can and do change your values over time, because you don't really have terminal values.</p>\n<p>Strictly speaking, it is impossible for an agent whose goals are all indexical goals describing states involving itself to <em>have</em> preferences about a situation in which it does not exist.&nbsp; Those of you who are operating under the assumption that we are maximizing a utility function with evolved terminal goals, should I think admit these terminal goals all involve either ourselves, or our genes.&nbsp; If they involve ourselves, then utility functions based on these goals cannot even be computed once we die.&nbsp; If they involve our genes, they they are goals that our <em>bodies</em> are pursuing, that we call errors, not goals, when we the conscious agent inside our bodies evaluate them.&nbsp; In either case, there is no logical reason for us to wish to maximize some utility function based on these after our own deaths.&nbsp; Any action I wish to take regarding the distant future necessarily presupposes that the entire SIAI approach to goals is wrong.</p>\n<p class=\"r\">My view, under which it does make sense for me to say I have preferences about the distant future, is that my mind has learned \"values\" that are not symbols, but analog numbers distributed among neurons.&nbsp; As described in \"<a class=\"l\" href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;ved=0CFwQFjAD&amp;url=http%3A%2F%2Flesswrong.com%2Flw%2F256%2Fbiases_are_values%2F&amp;ei=CGK4T-WCCsX9sQK8yYSLDA&amp;usg=AFQjCNHAoyhMX9zT0M-2DrqbzV0aBZM52Q\">Only humans can have human values</a>\", these values do not exist in a hierarchy with some at the bottom and some on the top, but in a recurrent network which does not have a top or a bottom, because the different parts of the network developed simultaneously.&nbsp; These values therefore can't be categorized into instrumental or terminal.&nbsp; They can include very abstract values that don't need to refer specifically to me, because other values elsewhere in the network do refer to me, and this will ensure that actions I finally execute incorporating those values are also influenced by my other values that do talk about me.</p>\n<h2>Even if human values existed, it would be pointless to preserve them</h2>\n<p><a href=\"/lw/256\">Only humans can have human values</a>:</p>\n<ul>\n<li>The only preferences that can be unambiguously determined are the preferences a person (mind+body) implements, which are not always the preferences expressed by their beliefs.</li>\n<li>If you extract a set of consciously-believed propositions from an existing agent, then build a new agent to use those propositions in a different environment, with an \"improved\" logic, you can't claim that it has the same values, since it will behave differently.</li>\n<li>Values exist in a network of other values.&nbsp; A key ethical question is to what degree values are referential (meaning they can be tested against something outside that network); or non-referential (and hence relative).</li>\n<li>Supposing that values are referential helps only by telling you to ignore human values.</li>\n<li>You cannot resolve the problem by combining information from different behaviors, because the needed information is missing.</li>\n<li>Today's ethical disagreements are largely the result of attempting to extrapolate ancestral human values into a changing world.</li>\n<li>The future will thus be ethically contentious even if we accurately characterize and agree on present human values, because these values will fail to address the new important problems.</li>\n</ul>\n<p><br /><a href=\"/lw/1xa\">Human values differ as much as values can differ</a>:&nbsp; There are two fundamentally different categories of values:</p>\n<ul>\n<li>Non-positional, mutually-satisfiable values (physical luxury, for instance)</li>\n<li>Positional, zero-sum social values, such as wanting to be the alpha male or the homecoming queen</li>\n</ul>\n<p>All mutually-satisfiable values have more in common with each other than they do with any non-mutually-satisfiable values, because mutually-satisfiable values are compatible with social harmony and non-problematic utility maximization, while non- mutually-satisfiable values require eternal conflict.&nbsp; If you find an alien life form from a distant galaxy with non-positional values, it would be easier to integrate those values into a human culture with only human non-positional values, than to integrate already-existing positional human values into that culture.</p>\n<p>It appears that some humans have mainly the one type, while other humans have mainly the other type.&nbsp; So talking about trying to preserve human values is pointless - the values held by different humans have already passed the most-important point of divergence.</p>\n<p>&nbsp;</p>\n<h2>Enforcing human values would be harmful</h2>\n<p><a href=\"/lesswrong.com/lw/20x\">The human problem</a>:&nbsp; This argues that the qualia and values we have now are only the beginning of those that could evolve in the universe, and that ensuring that we maximize human values - or any existing value set - from now on, will stop this process in its tracks, and prevent anything better from ever evolving.&nbsp; <strong>This is the most-important objection of all</strong>.<br /><br />Re-reading this, I see that the critical paragraph is painfully obscure, as if written by Kant; but it summarizes the argument: \"Once the initial symbol set has been chosen, the semantics must be set in stone for the judging function to be \"safe\" for preserving value; this means that any new symbols must be defined completely in terms of already-existing symbols.&nbsp; Because fine-grained sensory information has been lost, new developments in consciousness might not be detectable in the symbolic representation after the abstraction process.&nbsp; If they are detectable via statistical correlations between existing concepts, they will be difficult to reify parsimoniously as a composite of existing symbols.&nbsp; Not using a theory of phenomenology means that no effort is being made to look for such new developments, making their detection and reification even more unlikely.&nbsp; And an evaluation based on already-developed values and qualia means that even if they could be found, new ones would not improve the score.&nbsp; Competition for high scores on the existing function, plus lack of selection for components orthogonal to that function, will ensure that no such new developments last.\"<br /><br /><a href=\"/lw/262\">Averaging value systems is worse than choosing one</a>:&nbsp; This describes a neural-network that encodes preferences, and takes some input pattern and computes a new pattern that optimizes these preferences.&nbsp; Such a system is taken as analogous for a value system and an ethical system to attain those values.&nbsp; I then define a measure for the internal conflict produced by a set of values, and show that a system built by averaging together the parameters from many different systems will have higher internal conflict than any of the systems that were averaged together to produce it.&nbsp; The point is that the CEV plan of \"averaging together\" human values will result in a set of values that is worse (more self-contradictory) than any of the value systems it was derived from.</p>\n<p><br />A point I may not have made in these posts, but made in <a href=\"/lw/2df/what_if_ai_doesnt_quite_go_foom/5efw\">comments</a>, is that the majority of humans today think that women should not have full rights, homosexuals should be killed or at least severely persecuted, and nerds should be given wedgies.&nbsp; These are not incompletely-extrapolated values that will change with more information; they are <em>values</em>.&nbsp; Opponents of gay marriage make it clear that they do not object to gay marriage based on a long-range utilitarian calculation; they <em>directly value</em> not allowing gays to marry.&nbsp; Many human values horrify most people on this list, so they shouldn't be trying to preserve them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb170": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mSktD7oj6C2zxj3KC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 13, "extendedScore": null, "score": 9.047625217173774e-07, "legacy": true, "legacyId": "16184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Nick_Beckstead <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6lc8\">asked</a> me to link to posts I referred to in <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6lbc\">this comment</a>.&nbsp; I should put up or shut up, so here's an attempt to give an organized overview of them.<br><br>Since I wrote these, LukeProg has begun tackling some related issues.&nbsp; He has accomplished the seemingly-impossible task of writing many long, substantive posts none of which I recall disagreeing with.&nbsp; And I have, irrationally, not read most of his posts.&nbsp; So he may have dealt with more of these same issues.<br><br>I think that I only raised Holden's \"objection 2\" in comments, which I couldn't easily dig up; and in a critique of a book chapter, which I emailed to LukeProg and did not post to LessWrong.&nbsp; So I'm only going to talk about \"<strong>Objection 1:&nbsp; It seems to me that any AGI that was set to maximize a \"Friendly\" utility function would be extraordinarily dangerous.</strong>\"&nbsp; I've arranged my previous posts and comments on this point into categories.&nbsp; (Much of what I've said on the topic has been in comments on LessWrong and Overcoming Bias, and in email lists including SL4, and isn't here.)</p>\n<p>&nbsp;</p>\n<h2 id=\"The_concept_of__human_values__cannot_be_defined_in_the_way_that_FAI_presupposes\">The concept of \"human values\" cannot be defined in the way that FAI presupposes<br></h2>\n<p><a href=\"/lw/55n\">Human errors, human values</a>:&nbsp; Suppose all humans shared an identical set of values, preferences, and biases.&nbsp; We cannot retain human values without retaining human errors, because there is no principled distinction between them.<br><br>A <a href=\"/lw/55n/human_errors_human_values/3vq5\">comment</a> on this post:&nbsp; There are at least three distinct levels of human values:&nbsp; The values an evolutionary agent holds that maximize their reproductive fitness, the values a society holds that maximizes its fitness, and the values a rational optimizer holds who has chosen to maximize social utility.&nbsp; They often conflict.&nbsp; Which of them are the real human values?<br><br><a href=\"/lw/5q9\">Values vs. parameters</a>:&nbsp; Eliezer has suggested using human values, but without time discounting (= changing the time-discounting parameter).&nbsp; CEV presupposes that we can abstract human values and apply them in a different situation that has different parameters.&nbsp; But the parameters <em>are</em> values.&nbsp; There is no distinction between parameters and values.<br><br>A <a href=\"/lw/7k/incremental_progress_and_the_valley/5fu\">comment</a> on \"Incremental progress and the valley\":&nbsp; The \"values\" that our brains try to maximize in the short run are designed to maximize different values for our bodies in the long run.&nbsp; Which are human values:&nbsp; The motivations we feel, or the effects they have in the long term?&nbsp; LukeProg's post <a href=\"/lw/6da\">Do Humans Want Things?</a> makes a related point.<br><br><a href=\"/lw/300\">Group selection update</a>:&nbsp; The reason I harp on group selection, besides my outrage at the way it's been treated for the past 50 years, is that group selection implies that some human values evolved at the group level, not at the level of the individual.&nbsp; This means that increasing the rationality of individuals may enable people to act more effectively in their own interests, rather than in the group's interest, and thus diminish the degree to which humans embody human values.&nbsp; Identifying the values embodied in individual humans - supposing we could do so - would still not arrive at human values.&nbsp; Transferring human values to a post-human world, which might contain groups at many different levels of a hierarchy, would be problematic.</p>\n<p>I wanted to write about my opinion that human values can't be divided into final values and instrumental values, the way discussion of FAI presumes they can.&nbsp; This is an idea that comes from mathematics, symbolic logic, and classical AI.&nbsp; A symbolic approach would probably make proving safety easier.&nbsp; But human brains don't work that way.&nbsp; You can and do change your values over time, because you don't really have terminal values.</p>\n<p>Strictly speaking, it is impossible for an agent whose goals are all indexical goals describing states involving itself to <em>have</em> preferences about a situation in which it does not exist.&nbsp; Those of you who are operating under the assumption that we are maximizing a utility function with evolved terminal goals, should I think admit these terminal goals all involve either ourselves, or our genes.&nbsp; If they involve ourselves, then utility functions based on these goals cannot even be computed once we die.&nbsp; If they involve our genes, they they are goals that our <em>bodies</em> are pursuing, that we call errors, not goals, when we the conscious agent inside our bodies evaluate them.&nbsp; In either case, there is no logical reason for us to wish to maximize some utility function based on these after our own deaths.&nbsp; Any action I wish to take regarding the distant future necessarily presupposes that the entire SIAI approach to goals is wrong.</p>\n<p class=\"r\">My view, under which it does make sense for me to say I have preferences about the distant future, is that my mind has learned \"values\" that are not symbols, but analog numbers distributed among neurons.&nbsp; As described in \"<a class=\"l\" href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;ved=0CFwQFjAD&amp;url=http%3A%2F%2Flesswrong.com%2Flw%2F256%2Fbiases_are_values%2F&amp;ei=CGK4T-WCCsX9sQK8yYSLDA&amp;usg=AFQjCNHAoyhMX9zT0M-2DrqbzV0aBZM52Q\">Only humans can have human values</a>\", these values do not exist in a hierarchy with some at the bottom and some on the top, but in a recurrent network which does not have a top or a bottom, because the different parts of the network developed simultaneously.&nbsp; These values therefore can't be categorized into instrumental or terminal.&nbsp; They can include very abstract values that don't need to refer specifically to me, because other values elsewhere in the network do refer to me, and this will ensure that actions I finally execute incorporating those values are also influenced by my other values that do talk about me.</p>\n<h2 id=\"Even_if_human_values_existed__it_would_be_pointless_to_preserve_them\">Even if human values existed, it would be pointless to preserve them</h2>\n<p><a href=\"/lw/256\">Only humans can have human values</a>:</p>\n<ul>\n<li>The only preferences that can be unambiguously determined are the preferences a person (mind+body) implements, which are not always the preferences expressed by their beliefs.</li>\n<li>If you extract a set of consciously-believed propositions from an existing agent, then build a new agent to use those propositions in a different environment, with an \"improved\" logic, you can't claim that it has the same values, since it will behave differently.</li>\n<li>Values exist in a network of other values.&nbsp; A key ethical question is to what degree values are referential (meaning they can be tested against something outside that network); or non-referential (and hence relative).</li>\n<li>Supposing that values are referential helps only by telling you to ignore human values.</li>\n<li>You cannot resolve the problem by combining information from different behaviors, because the needed information is missing.</li>\n<li>Today's ethical disagreements are largely the result of attempting to extrapolate ancestral human values into a changing world.</li>\n<li>The future will thus be ethically contentious even if we accurately characterize and agree on present human values, because these values will fail to address the new important problems.</li>\n</ul>\n<p><br><a href=\"/lw/1xa\">Human values differ as much as values can differ</a>:&nbsp; There are two fundamentally different categories of values:</p>\n<ul>\n<li>Non-positional, mutually-satisfiable values (physical luxury, for instance)</li>\n<li>Positional, zero-sum social values, such as wanting to be the alpha male or the homecoming queen</li>\n</ul>\n<p>All mutually-satisfiable values have more in common with each other than they do with any non-mutually-satisfiable values, because mutually-satisfiable values are compatible with social harmony and non-problematic utility maximization, while non- mutually-satisfiable values require eternal conflict.&nbsp; If you find an alien life form from a distant galaxy with non-positional values, it would be easier to integrate those values into a human culture with only human non-positional values, than to integrate already-existing positional human values into that culture.</p>\n<p>It appears that some humans have mainly the one type, while other humans have mainly the other type.&nbsp; So talking about trying to preserve human values is pointless - the values held by different humans have already passed the most-important point of divergence.</p>\n<p>&nbsp;</p>\n<h2 id=\"Enforcing_human_values_would_be_harmful\">Enforcing human values would be harmful</h2>\n<p><a href=\"/lesswrong.com/lw/20x\">The human problem</a>:&nbsp; This argues that the qualia and values we have now are only the beginning of those that could evolve in the universe, and that ensuring that we maximize human values - or any existing value set - from now on, will stop this process in its tracks, and prevent anything better from ever evolving.&nbsp; <strong>This is the most-important objection of all</strong>.<br><br>Re-reading this, I see that the critical paragraph is painfully obscure, as if written by Kant; but it summarizes the argument: \"Once the initial symbol set has been chosen, the semantics must be set in stone for the judging function to be \"safe\" for preserving value; this means that any new symbols must be defined completely in terms of already-existing symbols.&nbsp; Because fine-grained sensory information has been lost, new developments in consciousness might not be detectable in the symbolic representation after the abstraction process.&nbsp; If they are detectable via statistical correlations between existing concepts, they will be difficult to reify parsimoniously as a composite of existing symbols.&nbsp; Not using a theory of phenomenology means that no effort is being made to look for such new developments, making their detection and reification even more unlikely.&nbsp; And an evaluation based on already-developed values and qualia means that even if they could be found, new ones would not improve the score.&nbsp; Competition for high scores on the existing function, plus lack of selection for components orthogonal to that function, will ensure that no such new developments last.\"<br><br><a href=\"/lw/262\">Averaging value systems is worse than choosing one</a>:&nbsp; This describes a neural-network that encodes preferences, and takes some input pattern and computes a new pattern that optimizes these preferences.&nbsp; Such a system is taken as analogous for a value system and an ethical system to attain those values.&nbsp; I then define a measure for the internal conflict produced by a set of values, and show that a system built by averaging together the parameters from many different systems will have higher internal conflict than any of the systems that were averaged together to produce it.&nbsp; The point is that the CEV plan of \"averaging together\" human values will result in a set of values that is worse (more self-contradictory) than any of the value systems it was derived from.</p>\n<p><br>A point I may not have made in these posts, but made in <a href=\"/lw/2df/what_if_ai_doesnt_quite_go_foom/5efw\">comments</a>, is that the majority of humans today think that women should not have full rights, homosexuals should be killed or at least severely persecuted, and nerds should be given wedgies.&nbsp; These are not incompletely-extrapolated values that will change with more information; they are <em>values</em>.&nbsp; Opponents of gay marriage make it clear that they do not object to gay marriage based on a long-range utilitarian calculation; they <em>directly value</em> not allowing gays to marry.&nbsp; Many human values horrify most people on this list, so they shouldn't be trying to preserve them.</p>", "sections": [{"title": "The concept of \"human values\" cannot be defined in the way that FAI presupposes", "anchor": "The_concept_of__human_values__cannot_be_defined_in_the_way_that_FAI_presupposes", "level": 1}, {"title": "Even if human values existed, it would be pointless to preserve them", "anchor": "Even_if_human_values_existed__it_would_be_pointless_to_preserve_them", "level": 1}, {"title": "Enforcing human values would be harmful", "anchor": "Enforcing_human_values_would_be_harmful", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "431 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 431, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHrAy4yvuyzKPdRF7", "RW4B6j2WdBhYpXx9t", "nBdaTGoDAYxHePSDa", "ioWH9ERY3TTzRJFTD", "cAPCCJjggjZPxxcKh", "NkspwZcbR2bjHS4Xg", "BNtjLbPSidEbihPmv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T01:28:31.060Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup", "slug": "meetup-dc-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Eooji7qc2nrtjRtio/meetup-dc-meetup", "pageUrlRelative": "/posts/Eooji7qc2nrtjRtio/meetup-dc-meetup", "linkUrl": "https://www.lesswrong.com/posts/Eooji7qc2nrtjRtio/meetup-dc-meetup", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEooji7qc2nrtjRtio%2Fmeetup-dc-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEooji7qc2nrtjRtio%2Fmeetup-dc-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEooji7qc2nrtjRtio%2Fmeetup-dc-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/a9'>DC Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 May 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If the SIAI response to the Givewell critique has been posted, that will be the meetup topic.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/a9'>DC Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Eooji7qc2nrtjRtio", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.047802427690395e-07, "legacy": true, "legacyId": "16191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup\">Discussion article for the meetup : <a href=\"/meetups/a9\">DC Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 May 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If the SIAI response to the Givewell critique has been posted, that will be the meetup topic.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/a9\">DC Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup", "anchor": "Discussion_article_for_the_meetup___DC_Meetup", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup", "anchor": "Discussion_article_for_the_meetup___DC_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T03:48:21.204Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Timeless Physics", "slug": "seq-rerun-timeless-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JjTiktGohsc3osjbN/seq-rerun-timeless-physics", "pageUrlRelative": "/posts/JjTiktGohsc3osjbN/seq-rerun-timeless-physics", "linkUrl": "https://www.lesswrong.com/posts/JjTiktGohsc3osjbN/seq-rerun-timeless-physics", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Timeless%20Physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Timeless%20Physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjTiktGohsc3osjbN%2Fseq-rerun-timeless-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Timeless%20Physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjTiktGohsc3osjbN%2Fseq-rerun-timeless-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjTiktGohsc3osjbN%2Fseq-rerun-timeless-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<p>Today's post, <a href=\"/lw/qp/timeless_physics/\">Timeless Physics</a> was originally published on 27 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What time is it? How do you know? The question \"What time is it right now?\" may make around as much sense as asking \"Where is the universe?\" Not only that, our physics equations may not need a<em> t</em> in them!</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cgx/seq_rerun_relative_configuration_space/\">Relative Configuration Space</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JjTiktGohsc3osjbN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.048417838491354e-07, "legacy": true, "legacyId": "16204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rrW7yf42vQYDf8AcH", "9v7BH2P9ZdAuvvCz9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T09:54:54.341Z", "modifiedAt": null, "url": null, "title": "Be careful with thought experiments", "slug": "be-careful-with-thought-experiments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RRszjHp7xNoB7DkCz/be-careful-with-thought-experiments", "pageUrlRelative": "/posts/RRszjHp7xNoB7DkCz/be-careful-with-thought-experiments", "linkUrl": "https://www.lesswrong.com/posts/RRszjHp7xNoB7DkCz/be-careful-with-thought-experiments", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Be%20careful%20with%20thought%20experiments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABe%20careful%20with%20thought%20experiments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRRszjHp7xNoB7DkCz%2Fbe-careful-with-thought-experiments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Be%20careful%20with%20thought%20experiments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRRszjHp7xNoB7DkCz%2Fbe-careful-with-thought-experiments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRRszjHp7xNoB7DkCz%2Fbe-careful-with-thought-experiments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 417, "htmlBody": "<p><a href=\"http://www.amazon.com/The-Cognitive-Science-Explanation-Conceptual/dp/0262017288/\">Thagard (2012)</a> contains a nicely compact passage on thought experiments:</p>\n<blockquote>\n<p>\n<p>Grisdale&rsquo;s (2010) discussion of modern conceptions of water refutes a highly influential thought experiment that the meaning of water is largely a matter of reference to the world rather than mental representation. Putnam (1975) invited people to consider a planet, Twin Earth, that is a near duplicate of our own. The only difference is that on Twin Earth water is a more complicated substance XYZ rather than H2O. Water on Twin Earth is imagined to be indistinguishable from H2O, so people have the same mental representation of it. Nevertheless, according to Putnam, the meaning of the concept water on Twin Earth is different because it refers to XYZ rather than H2O. Putnam&rsquo;s famous conclusion is that &ldquo;meaning just ain&rsquo;t in the head.&rdquo;</p>\n<p>The apparent conceivability of Twin Earth as identical to Earth except for the different constitution of water depends on ignorance of chemistry. As Grisdale (2010) documents, even a slight change in the chemical constitution of water produces dramatic changes in its effects. If normal hydrogen is replaced by different isotopes, deuterium or tritium, the water molecule markedly changes its&nbsp;chemical properties. Life would be impossible if H2O were replaced by heavy water, D2O or T2O; and compounds made of elements different from hydrogen and oxygen would be even more different in their properties. Hence Putnam&rsquo;s thought experiment is scientifically incoherent: If water were not H2O, Twin Earth would not be at all like Earth. [See also <a href=\"/lw/hq/universal_fire/\">Universal Fire</a>. --Luke]</p>\n<p>This incoherence should serve as a warning to philosophers who try to base theories on thought experiments, a practice I have criticized in relation to concepts of mind (Thagard, 2010a, ch. 2). Some philosophers have thought that the nonmaterial nature of consciousness is shown by their ability to imagine beings (zombies) who are physically just like people but who lack consciousness. It is entirely likely, however, that once the brain mechanisms that produce consciousness are better understood, it will become clear that zombies are as fanciful as Putnam&rsquo;s XYZ. Just as imagining that water is XYZ is a sign only of ignorance of chemistry, imagining that consciousness is nonbiological may well turn out to reveal ignorance rather than some profound conceptual truth about the nature of mind. Of course, the hypothesis that consciousness is a brain process is not part of most people&rsquo;s everyday concept of consciousness, but psychological concepts can progress just like ones in physics and chemistry. [See also the <a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence)\">Zombies Sequence</a>. --Luke]</p>\n</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RRszjHp7xNoB7DkCz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 13, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "16212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LaM5aTcXvXzwQSC2Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T10:16:07.740Z", "modifiedAt": null, "url": null, "title": "Quantified Self recommendations", "slug": "quantified-self-recommendations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.680Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hG29PqDJZJTAhuyQS/quantified-self-recommendations", "pageUrlRelative": "/posts/hG29PqDJZJTAhuyQS/quantified-self-recommendations", "linkUrl": "https://www.lesswrong.com/posts/hG29PqDJZJTAhuyQS/quantified-self-recommendations", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantified%20Self%20recommendations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantified%20Self%20recommendations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhG29PqDJZJTAhuyQS%2Fquantified-self-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantified%20Self%20recommendations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhG29PqDJZJTAhuyQS%2Fquantified-self-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhG29PqDJZJTAhuyQS%2Fquantified-self-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 14, "htmlBody": "<p>Is <a href=\"http://quantifiedself.com/\">self knowledge through numbers</a>&nbsp;worth the effort? I welcome your stories and recommendations. Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hG29PqDJZJTAhuyQS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 9.05012481321247e-07, "legacy": true, "legacyId": "16213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T14:31:56.287Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berkeley, Dallas, Pittsburgh, Vancouver, Washington DC", "slug": "weekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.871Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K7ACJrWP6jsGKyrWo/weekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver", "pageUrlRelative": "/posts/K7ACJrWP6jsGKyrWo/weekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver", "linkUrl": "https://www.lesswrong.com/posts/K7ACJrWP6jsGKyrWo/weekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Dallas%2C%20Pittsburgh%2C%20Vancouver%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berkeley%2C%20Dallas%2C%20Pittsburgh%2C%20Vancouver%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK7ACJrWP6jsGKyrWo%2Fweekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Dallas%2C%20Pittsburgh%2C%20Vancouver%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK7ACJrWP6jsGKyrWo%2Fweekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK7ACJrWP6jsGKyrWo%2Fweekly-lw-meetups-berkeley-dallas-pittsburgh-vancouver", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/9z\">Vancouver Politics Meetup:&nbsp;<span class=\"date\">12 May 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/9u\">Washington DC meetup:&nbsp;<span class=\"date\">12 May 2012 08:34PM</span></a></li>\n<li><a href=\"/meetups/9w\">Dallas - Fort Worth Less Wrong Meetup 5/13/12:&nbsp;<span class=\"date\">13 May 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/a2\">Pittsburgh: Harry Potter and the Methods of Rationality:&nbsp;<span class=\"date\">18 May 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/9l\">Brussels meetup:&nbsp;<span class=\"date\">19 May 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/9r\">Less Wrong Sydney - Rational Acting:&nbsp;<span class=\"date\">21 May 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/9s\">First Berlin meetup:&nbsp;<span class=\"date\">05 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/a1\">Big Berkeley Meetup:&nbsp;<span class=\"date\">16 May 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup:&nbsp;<span class=\"date\">20 May 2012 02:20PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K7ACJrWP6jsGKyrWo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.051251188697747e-07, "legacy": true, "legacyId": "16011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T19:33:29.140Z", "modifiedAt": null, "url": null, "title": "Meetup : Mountain View sequences discussion", "slug": "meetup-mountain-view-sequences-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:46.828Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4MmwG8n2NG6P3nTNy/meetup-mountain-view-sequences-discussion", "pageUrlRelative": "/posts/4MmwG8n2NG6P3nTNy/meetup-mountain-view-sequences-discussion", "linkUrl": "https://www.lesswrong.com/posts/4MmwG8n2NG6P3nTNy/meetup-mountain-view-sequences-discussion", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Mountain%20View%20sequences%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Mountain%20View%20sequences%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4MmwG8n2NG6P3nTNy%2Fmeetup-mountain-view-sequences-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Mountain%20View%20sequences%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4MmwG8n2NG6P3nTNy%2Fmeetup-mountain-view-sequences-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4MmwG8n2NG6P3nTNy%2Fmeetup-mountain-view-sequences-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/aa\">Mountain View sequences discussion</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 May 2012 06:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Sylvan Avenue &amp; Sevely St, Mountain View</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We will be meeting at Sylvan Park near the playground. This is a meetup to discuss specific posts in the sequences. The material we will be covering is the beginning of the Seeing with Fresh Eyes sequence.</p>\n<p><a rel=\"nofollow\" href=\"/lw/j7/anchoring_and_adjustment/\">http://lesswrong.com/lw/j7/anchoring_and_adjustment/</a>&nbsp;&nbsp;</p>\n<p><a rel=\"nofollow\" href=\"/lw/k3/priming_and_contamination/\">http://lesswrong.com/lw/k3/priming_and_contamination/</a>&nbsp;&nbsp;</p>\n<p><a rel=\"nofollow\" href=\"/lw/k4/do_we_believe_everything_were_told/\">http://lesswrong.com/lw/k4/do_we_believe_everything_were_told/</a>&nbsp;&nbsp;</p>\n<p><a rel=\"nofollow\" href=\"/lw/k5/cached_thoughts/\">http://lesswrong.com/lw/k5/cached_thoughts/</a>&nbsp;&nbsp;</p>\n<p>It would be ideal to show up having read (or reread) these posts, and jotted down some notes you can discuss. OTOH please don't NOT show up because you feel guilty about not-having-done-your-homework. &nbsp;</p>\n<p>Food: no one is obligated to bring snacks but it is most welcome. &nbsp;</p>\n<p>Time: I will arrive at 6pm, discussion of the sequences will begin at 7. I am soliciting info about parking and will update.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/aa\">Mountain View sequences discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4MmwG8n2NG6P3nTNy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.052579277650328e-07, "legacy": true, "legacyId": "16215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Mountain_View_sequences_discussion\">Discussion article for the meetup : <a href=\"/meetups/aa\">Mountain View sequences discussion</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 May 2012 06:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Sylvan Avenue &amp; Sevely St, Mountain View</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We will be meeting at Sylvan Park near the playground. This is a meetup to discuss specific posts in the sequences. The material we will be covering is the beginning of the Seeing with Fresh Eyes sequence.</p>\n<p><a rel=\"nofollow\" href=\"/lw/j7/anchoring_and_adjustment/\">http://lesswrong.com/lw/j7/anchoring_and_adjustment/</a>&nbsp;&nbsp;</p>\n<p><a rel=\"nofollow\" href=\"/lw/k3/priming_and_contamination/\">http://lesswrong.com/lw/k3/priming_and_contamination/</a>&nbsp;&nbsp;</p>\n<p><a rel=\"nofollow\" href=\"/lw/k4/do_we_believe_everything_were_told/\">http://lesswrong.com/lw/k4/do_we_believe_everything_were_told/</a>&nbsp;&nbsp;</p>\n<p><a rel=\"nofollow\" href=\"/lw/k5/cached_thoughts/\">http://lesswrong.com/lw/k5/cached_thoughts/</a>&nbsp;&nbsp;</p>\n<p>It would be ideal to show up having read (or reread) these posts, and jotted down some notes you can discuss. OTOH please don't NOT show up because you feel guilty about not-having-done-your-homework. &nbsp;</p>\n<p>Food: no one is obligated to bring snacks but it is most welcome. &nbsp;</p>\n<p>Time: I will arrive at 6pm, discussion of the sequences will begin at 7. I am soliciting info about parking and will update.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Mountain_View_sequences_discussion1\">Discussion article for the meetup : <a href=\"/meetups/aa\">Mountain View sequences discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Mountain View sequences discussion", "anchor": "Discussion_article_for_the_meetup___Mountain_View_sequences_discussion", "level": 1}, {"title": "Discussion article for the meetup : Mountain View sequences discussion", "anchor": "Discussion_article_for_the_meetup___Mountain_View_sequences_discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bMkCEZoBNhgRBtzoj", "BaCWFCxBQYjJXSsah", "TiDGXt3WrQwtCdDj3", "2MD3NMLBPCqPfnfre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T21:25:34.992Z", "modifiedAt": null, "url": null, "title": "Meetup : Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12", "slug": "meetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jumandtonic", "createdAt": "2012-04-26T03:09:35.594Z", "isAdmin": false, "displayName": "jumandtonic"}, "userId": "2EmJ3AN5jKXHnvYpP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kqxn7rRMqBPBL7xas/meetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12", "pageUrlRelative": "/posts/Kqxn7rRMqBPBL7xas/meetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12", "linkUrl": "https://www.lesswrong.com/posts/Kqxn7rRMqBPBL7xas/meetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meet%20Up%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F20%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meet%20Up%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F20%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKqxn7rRMqBPBL7xas%2Fmeetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meet%20Up%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F20%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKqxn7rRMqBPBL7xas%2Fmeetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKqxn7rRMqBPBL7xas%2Fmeetup-meet-up-dallas-fort-worth-less-wrong-meetup-5-20-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ab'>Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This group is really starting to build up some momentum!  We have a handful of great folks and keep adding more every meeting.</p>\n\n<p>Our agenda's typically include some guided discussion, rationality/debiasing games, and how to improve your own rationality.</p>\n\n<p>We look forward to you coming out and meeting the rest of the group. Message me to ask to join our google group: <a href=\"http://groups.google.com/group/dfw-lesswrong-meetup\" rel=\"nofollow\">http://groups.google.com/group/dfw-lesswrong-meetup</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ab'>Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kqxn7rRMqBPBL7xas", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.053073070341431e-07, "legacy": true, "legacyId": "16216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meet_Up__Dallas___Fort_Worth_Less_Wrong_Meetup_5_20_12\">Discussion article for the meetup : <a href=\"/meetups/ab\">Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This group is really starting to build up some momentum!  We have a handful of great folks and keep adding more every meeting.</p>\n\n<p>Our agenda's typically include some guided discussion, rationality/debiasing games, and how to improve your own rationality.</p>\n\n<p>We look forward to you coming out and meeting the rest of the group. Message me to ask to join our google group: <a href=\"http://groups.google.com/group/dfw-lesswrong-meetup\" rel=\"nofollow\">http://groups.google.com/group/dfw-lesswrong-meetup</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meet_Up__Dallas___Fort_Worth_Less_Wrong_Meetup_5_20_121\">Discussion article for the meetup : <a href=\"/meetups/ab\">Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12", "anchor": "Discussion_article_for_the_meetup___Meet_Up__Dallas___Fort_Worth_Less_Wrong_Meetup_5_20_12", "level": 1}, {"title": "Discussion article for the meetup : Meet Up: Dallas - Fort Worth Less Wrong Meetup 5/20/12", "anchor": "Discussion_article_for_the_meetup___Meet_Up__Dallas___Fort_Worth_Less_Wrong_Meetup_5_20_121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-18T23:45:53.204Z", "modifiedAt": null, "url": null, "title": "Value of Information: 8 examples", "slug": "value-of-information-8-examples", "viewCount": null, "lastCommentedAt": "2021-10-01T03:33:38.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xiojTDJP6FWdb2Fmb/value-of-information-8-examples", "pageUrlRelative": "/posts/xiojTDJP6FWdb2Fmb/value-of-information-8-examples", "linkUrl": "https://www.lesswrong.com/posts/xiojTDJP6FWdb2Fmb/value-of-information-8-examples", "postedAtFormatted": "Friday, May 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20of%20Information%3A%208%20examples&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20of%20Information%3A%208%20examples%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxiojTDJP6FWdb2Fmb%2Fvalue-of-information-8-examples%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20of%20Information%3A%208%20examples%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxiojTDJP6FWdb2Fmb%2Fvalue-of-information-8-examples", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxiojTDJP6FWdb2Fmb%2Fvalue-of-information-8-examples", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2219, "htmlBody": "<p><a href=\"/r/discussion/lw/cid/quantified_self_recommendations/\">ciphergoth</a> just asked what the actual value of Quantified Self/self-experimentation is. This finally tempted me into running <a href=\"/lw/85x/value_of_information_four_examples/\">value of information</a> calculations on my own experiments. It took me all afternoon because it turned out I didn&rsquo;t actually understand how to do it and I had a hard time figuring out the right values for specific experiments. (I may not have not gotten it right, still. Feel free to check my work!)&nbsp; Then it turned out to be too long for a comment, and as usual the master versions will be on my website at some point. But without further ado!</p>\n<p><a id=\"more\"></a></p>\n<hr />\n<p>The value of an experiment is the information it produces. What is the value of information? Well, we can take the economic tack and say value of information is the value of the decisions it changes. (Would you pay for a weather forecast about somewhere you are not going to? No. Or a weather forecast about your trip where you <em>have</em> to make that trip, come hell or high water? Only to the extent you can make preparations like bringing an umbrella.)</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Value_of_information\">Wikipedia</a> says that for a risk-neutral person, value of perfect information is &ldquo;value of decision situation with perfect information&rdquo; - &ldquo;value of current decision situation&rdquo;. (Imperfect information is just weakened perfect information: if your information was not 100% reliable but 99% reliable, well, that&rsquo;s worth 99% as much.)</p>\n<h1 id=\"melatonin\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Melatonin</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#melatonin\"><code class=\"url\">http://www.gwern.net/Zeo#melatonin</code></a> &amp; <a href=\"http://www.gwern.net/Melatonin\">http://www.gwern.net/Melatonin</a></p>\n<p>The decision is the binary take or not take. Melatonin costs ~$10 a year (if you buy in bulk during sales, as I did). Suppose I had perfect information it worked; I would not change anything, so the value is $0. Suppose I had perfect information it did not work; then I would stop using it, saving me $10 a year in perpetuity, which has a net present value (at 5% discounting) of $205. So the value of perfect information is $205, because it would save me from blowing $10 every year for the rest of my life. My melatonin experiment is not perfect since I didn&rsquo;t randomize or double-blind it, but I had a lot of data and it was well powered, with something like a &gt;90% chance of detecting the decent effect size I expected, so the imperfection is just a loss of 10%, down to $184. From my previous research and personal use over years, I am highly confident it works - say, 80%. If it works, the information is useless to me, and if it doesn&rsquo;t, I save $184; what&rsquo;s the expected value of obtaining the information, giving these two outcomes? <code>(80% * $0) + (20% * $184) = $36.8</code>. At minimum wage opportunity cost of $7 an hour, $36.8 is worth 5.25 hours of my time. I spent much time on screenshots, summarizing, and analysis, and I&rsquo;d guess I spent closer to 10&ndash;15 hours all told.</p>\n<p>(The net present value formula is the annual savings divided by the natural log of the discount rate, out to eternity. Exponential discounting means that a bond that expires in 50 years is worth a surprisingly similar amount to one that continues paying out forever. For example, a 50 year bond paying $10 a year at a discount rate of 5% is worth <code>sum $ map (\\t -&gt; 10 / (1 + 0.05)^t) [1..50] ~&gt; 182.5</code> but if that same bond never expires, it&rsquo;s worth <code>10 / log 1.05 = 204.9</code> or just $22.4 more! My own expected longevity is ~50 more years, but I prefer to use the simple natural log formula rather than the more accurate summation. All the numbers here are questionable anyway.)</p>\n<p>This worked out example demonstrates that when a substance is cheap and you are highly confident it works, a long costly experiment may not be worth it. (Of course, I would have done it anyway due to factors not included in the calculation: to try out my Zeo, learn a bit about sleep experimentation, do something cool, and have something neat to show everyone.)</p>\n<h1 id=\"vitamin-d\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> Vitamin D</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#vitamin-d\"><code class=\"url\">http://www.gwern.net/Zeo#vitamin-d</code></a></p>\n<p>I ran 2 experiments on vitamin D: whether it hurt sleep when taken in the evening, and whether it helped sleep when taken in the morning.</p>\n<h2 id=\"evening\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1</span> Evening</a></h2>\n<p><a href=\"http://www.gwern.net/Zeo#vitamin-d-at-night-hurts\"><code class=\"url\">http://www.gwern.net/Zeo#vitamin-d-at-night-hurts</code></a></p>\n<p>The first I had no opinion on. I actually did sometimes take vitamin D in the evening when I hadn&rsquo;t gotten around to it earlier (I take it for its anti-cancer and SAD effects). There was no research background, and the anecdotal evidence was of very poor quality. Still, it was plausible since vitamin D <em>is</em> involved in circadian rhythms, so I gave it 50% and decided to run an experiment. What effect would perfect information that it did negatively affect my sleep have? Well, I&rsquo;d definitely switch to taking it in the morning and would never take it in the evening again, which would change maybe 20% of my future doses, and what was the negative effect? It couldn&rsquo;t be <em>that</em> bad or I would have noticed it already (like I noticed sulbutiamine made it hard to get to sleep). I&rsquo;m not willing to change my routines very much to improve my sleep, so I would be lying if I estimated that the value of eliminating any vitamin D-related disturbance was more than, say, 10 cents per night; so the total value of affected nights would be <code>$0.10 * 0.20 * 365.25 = $7.3</code>. On the plus side, my experiment design was high quality and ran for a fair number of days, so it would surely detect any sleep disturbance from the randomized vitamin D, so say 90% quality of information. This gives <code>((7.3 - 0) / log 1.05) * 0.90 * 0.50 = 67.3</code>, justifying &lt;9.6 hours. Making the pills took perhaps an hour, recording used up some time, and the analysis took several hours to label &amp; process all the data, play with it in R, and write it all up in a clean form for readers. Still, I don&rsquo;t think it took almost 10 hours of work, so I think this experiment ran at a profit.</p>\n<h2 id=\"morning\"><a href=\"#TOC\"><span class=\"header-section-number\">2.2</span> Morning</a></h2>\n<p><a href=\"http://www.gwern.net/Zeo#vitamin-d-at-morn-helps\"><code class=\"url\">http://www.gwern.net/Zeo#vitamin-d-at-morn-helps</code></a></p>\n<p>With the vitamin D theory partially vindicated by the previous experiment, I became fairly sure that vitamin D in the morning would benefit my sleep somehow: 70%. Benefit how? I had no idea, it might be large or small. I didn&rsquo;t expect it to be a second melatonin, improving my sleep and trimming it by 50 minutes, but I hoped maybe it would help me get to sleep faster or wake up less. The actual experiment turned out to show, with very high confidence, absolutely no change except in my mood upon awakening in the morning.</p>\n<p>What is the &ldquo;value of information&rdquo; for this experiment? Essentially - nothing! Zero!</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the experiment had shown any benefit, I obviously would have continued taking it in the morning</li>\n<li>if the experiment had shown no effect, I would have continued taking it in the morning to avoid incurring the evening penalty discovered in the previous experiment</li>\n<li>if the experiment had shown the unthinkable, a negative effect, it would have to be substantial to convince me to stop taking vitamin D altogether and forfeit its other health benefits, and it&rsquo;s not worth bothering to analyze an outcome I would have given &lt;=5% chance to.</li>\n</ol>\n<p>Of course, I did it anyway because it was cool and interesting! (Estimated time cost: perhaps half the evening experiment, since I manually recorded less data and had the analysis worked out from before.)</p>\n<h1 id=\"adderall\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Adderall</a></h1>\n<p><a href=\"http://www.gwern.net/Nootropics#adderall-blind-testing\"><code class=\"url\">http://www.gwern.net/Nootropics#adderall-blind-testing</code></a></p>\n<p>The amphetamine mix branded &ldquo;Adderall&rdquo; is terribly expensive to obtain even compared to modafinil, due to its tight regulation (a lower schedule than modafinil), popularity in college as a study drug, and reportedly moves by its manufacture to exploit its privileged position as a licensed amphetamine maker to extract more consumer surplus. I paid roughly $4 a pill but could have paid up to $10. Good stimulant hygiene involves recovery periods to avoid one&rsquo;s body adapting to eliminate the stimulating effects, so even if Adderall was the answer to all my woes, I would not be using it more than 2 or 3 times a week. Assuming 50 uses a year (for specific projects, let&rsquo;s say, and not ordinary aimless usage), that&rsquo;s a cool $200 a year. My general belief was that Adderall would be too much of a stimulant for me, as I am amphetamine-naive and Adderall has a bad reputation for letting one waste time on unimportant things. We could say my prediction was 50% that Adderall would be useful and worth investigating further. The experiment was pretty simple: blind randomized pills, 10 placebo &amp; 10 active. I took notes on how productive I was and the next day guessed whether it was placebo or Adderall before breaking the seal and finding out. I didn&rsquo;t do any formal statistics for it, much less a power calculation, so let&rsquo;s try to be conservative by penalizing the information quality heavily and assume it had 25%. So <code>((200 - 0) / log 1.05) * 0.50 * 0.25 = 512</code>! The experiment probably used up no more than an hour or two total.</p>\n<p>This example demonstrates that anything you are doing <em>expensively</em> is worth testing <em>extensively</em>.</p>\n<h1 id=\"modafinil-day\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> Modafinil day</a></h1>\n<p><a href=\"http://www.gwern.net/Nootropics#modalert-blind-day-trial\"><code class=\"url\">http://www.gwern.net/Nootropics#modalert-blind-day-trial</code></a></p>\n<p>I tried 8 randomized days like with Adderall to see whether I was one of the people whom modafinil energizes during the day. (The other way to use it is to skip sleep, which is my preferred use.) I rarely use it during the day since my initial uses did not impress me subjectively. The experiment was not my best - while it was double-blind randomized, the measurements were subjective, and not a good measure of mental functioning like dual n-back (DNB) scores which I could statistically compare from day to day or against my many previous days of dual n-back scores. Between my high expectation of finding the null result, the poor experiment quality, and the minimal effect it had (eliminating an already rare use), it&rsquo;s obvious without guesstimating any numbers that the value of this information was very small.</p>\n<p>I mostly did it so I could tell people that &ldquo;no, day usage isn&rsquo;t particularly great for me; why don&rsquo;t you run an experiment on yourself and see whether it was just a placebo effect (or whether you genuinely are sleep-deprived and it is indeed compensating)?&rdquo;</p>\n<h1 id=\"lithium\"><a href=\"#TOC\"><span class=\"header-section-number\">5</span> Lithium</a></h1>\n<p><a href=\"http://www.gwern.net/Nootropics#lithium-experiment\"><code class=\"url\">http://www.gwern.net/Nootropics#lithium-experiment</code></a></p>\n<p>Low-dose lithium orotate is extremely cheap, ~$10 a year. There is some research literature on it improving mood and impulse control in regular people, but some of it is epidemiological (which implies considerable unreliability); my current belief is that there is probably <em>some</em> effect size, but at just 10mg, it may be too tiny to matter. I have ~40% belief that there will be a large effect size, but I&rsquo;m doing a long experiment and I should be able to detect a large effect size with &gt;75% chance. So, the formula is NPV of the difference between taking and not taking, times quality of information, times expectation: <code>((10 - 0) / log 1.05) * 0.75 * 0.40 = 61.4</code>, which justifies a time investment of less than 9 hours. As it happens, it took less than an hour to make the pills &amp; placebos, and taking them is a matter of seconds per week, so the analysis will be the time-consuming part. This one may actually turn a profit.</p>\n<h1 id=\"redshift\"><a href=\"#TOC\"><span class=\"header-section-number\">6</span> Redshift</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#redshiftf.lux\"><code class=\"url\">http://www.gwern.net/Zeo#redshiftf.lux</code></a></p>\n<p>Like the modafinil day trial, this was another value-less experiment justified by its intrinsic interest. I expect the results will confirm what I believe: that red-tinting my laptop screen will result in less damage to my sleep by not forcing lower melatonin levels with blue light. The only outcome that might change my decisions is if the use of Redshift actually worsens my sleep, but I regard this as highly unlikely. It is cheap to run as it is piggybacking on other experiments, and all the randomizing &amp; data recording is being handled by 2 simple shell scripts.</p>\n<h1 id=\"meditation\"><a href=\"#TOC\"><span class=\"header-section-number\">7</span> Meditation</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#meditation-1\"><code class=\"url\">http://www.gwern.net/Zeo#meditation-1</code></a></p>\n<p>I find meditation useful when I am screwing around and can&rsquo;t focus on anything, but I don&rsquo;t meditate as much as I might because I lose half an hour. Hence, I am interested in the suggestion that meditation may not be as expensive as it seems because it reduces sleep need to some degree: if for every two minutes I meditate, I need one less minute of sleep, that halves the time cost - I spend 30 minutes meditating, gain back 15 minutes from sleep, for a net time loss of 15 minutes. So if I meditate regularly but there is no substitution, I lose out on 15 minutes a day. Figure I skip every 2 days, that&rsquo;s a total lost time of <code>(15 * 2/3 * 365.25) / 60 = 61</code> hours a year or $427 at minimum wage. I find the theory somewhat plausible (60%), and my year-long experiment has roughly a 60% chance of detecting the effect size (estimated based on the sleep reduction in a Indian sample of meditators). So <code>((427 - 0) / log 1.05) * 0.60 * 0.60 = $3150</code>. The experiment itself is unusually time-intensive, since it involve ~180 sessions of meditation, which if I am &ldquo;overpaying&rdquo; translates to 45 hours (<code>(180 * 15) / 60</code>) of wasted time or $315. But even including the design and analysis, that&rsquo;s less than the calculated value of information.</p>\n<p>This example demonstrates that drugs aren&rsquo;t the only expensive things for which you should do extensive testing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DbMQGrxbhLxtNkmca": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xiojTDJP6FWdb2Fmb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 78, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "16217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"/r/discussion/lw/cid/quantified_self_recommendations/\">ciphergoth</a> just asked what the actual value of Quantified Self/self-experimentation is. This finally tempted me into running <a href=\"/lw/85x/value_of_information_four_examples/\">value of information</a> calculations on my own experiments. It took me all afternoon because it turned out I didn\u2019t actually understand how to do it and I had a hard time figuring out the right values for specific experiments. (I may not have not gotten it right, still. Feel free to check my work!)&nbsp; Then it turned out to be too long for a comment, and as usual the master versions will be on my website at some point. But without further ado!</p>\n<p><a id=\"more\"></a></p>\n<hr>\n<p>The value of an experiment is the information it produces. What is the value of information? Well, we can take the economic tack and say value of information is the value of the decisions it changes. (Would you pay for a weather forecast about somewhere you are not going to? No. Or a weather forecast about your trip where you <em>have</em> to make that trip, come hell or high water? Only to the extent you can make preparations like bringing an umbrella.)</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Value_of_information\">Wikipedia</a> says that for a risk-neutral person, value of perfect information is \u201cvalue of decision situation with perfect information\u201d - \u201cvalue of current decision situation\u201d. (Imperfect information is just weakened perfect information: if your information was not 100% reliable but 99% reliable, well, that\u2019s worth 99% as much.)</p>\n<h1 id=\"1_Melatonin\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Melatonin</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#melatonin\"><code class=\"url\">http://www.gwern.net/Zeo#melatonin</code></a> &amp; <a href=\"http://www.gwern.net/Melatonin\">http://www.gwern.net/Melatonin</a></p>\n<p>The decision is the binary take or not take. Melatonin costs ~$10 a year (if you buy in bulk during sales, as I did). Suppose I had perfect information it worked; I would not change anything, so the value is $0. Suppose I had perfect information it did not work; then I would stop using it, saving me $10 a year in perpetuity, which has a net present value (at 5% discounting) of $205. So the value of perfect information is $205, because it would save me from blowing $10 every year for the rest of my life. My melatonin experiment is not perfect since I didn\u2019t randomize or double-blind it, but I had a lot of data and it was well powered, with something like a &gt;90% chance of detecting the decent effect size I expected, so the imperfection is just a loss of 10%, down to $184. From my previous research and personal use over years, I am highly confident it works - say, 80%. If it works, the information is useless to me, and if it doesn\u2019t, I save $184; what\u2019s the expected value of obtaining the information, giving these two outcomes? <code>(80% * $0) + (20% * $184) = $36.8</code>. At minimum wage opportunity cost of $7 an hour, $36.8 is worth 5.25 hours of my time. I spent much time on screenshots, summarizing, and analysis, and I\u2019d guess I spent closer to 10\u201315 hours all told.</p>\n<p>(The net present value formula is the annual savings divided by the natural log of the discount rate, out to eternity. Exponential discounting means that a bond that expires in 50 years is worth a surprisingly similar amount to one that continues paying out forever. For example, a 50 year bond paying $10 a year at a discount rate of 5% is worth <code>sum $ map (\\t -&gt; 10 / (1 + 0.05)^t) [1..50] ~&gt; 182.5</code> but if that same bond never expires, it\u2019s worth <code>10 / log 1.05 = 204.9</code> or just $22.4 more! My own expected longevity is ~50 more years, but I prefer to use the simple natural log formula rather than the more accurate summation. All the numbers here are questionable anyway.)</p>\n<p>This worked out example demonstrates that when a substance is cheap and you are highly confident it works, a long costly experiment may not be worth it. (Of course, I would have done it anyway due to factors not included in the calculation: to try out my Zeo, learn a bit about sleep experimentation, do something cool, and have something neat to show everyone.)</p>\n<h1 id=\"2_Vitamin_D\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> Vitamin D</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#vitamin-d\"><code class=\"url\">http://www.gwern.net/Zeo#vitamin-d</code></a></p>\n<p>I ran 2 experiments on vitamin D: whether it hurt sleep when taken in the evening, and whether it helped sleep when taken in the morning.</p>\n<h2 id=\"2_1_Evening\"><a href=\"#TOC\"><span class=\"header-section-number\">2.1</span> Evening</a></h2>\n<p><a href=\"http://www.gwern.net/Zeo#vitamin-d-at-night-hurts\"><code class=\"url\">http://www.gwern.net/Zeo#vitamin-d-at-night-hurts</code></a></p>\n<p>The first I had no opinion on. I actually did sometimes take vitamin D in the evening when I hadn\u2019t gotten around to it earlier (I take it for its anti-cancer and SAD effects). There was no research background, and the anecdotal evidence was of very poor quality. Still, it was plausible since vitamin D <em>is</em> involved in circadian rhythms, so I gave it 50% and decided to run an experiment. What effect would perfect information that it did negatively affect my sleep have? Well, I\u2019d definitely switch to taking it in the morning and would never take it in the evening again, which would change maybe 20% of my future doses, and what was the negative effect? It couldn\u2019t be <em>that</em> bad or I would have noticed it already (like I noticed sulbutiamine made it hard to get to sleep). I\u2019m not willing to change my routines very much to improve my sleep, so I would be lying if I estimated that the value of eliminating any vitamin D-related disturbance was more than, say, 10 cents per night; so the total value of affected nights would be <code>$0.10 * 0.20 * 365.25 = $7.3</code>. On the plus side, my experiment design was high quality and ran for a fair number of days, so it would surely detect any sleep disturbance from the randomized vitamin D, so say 90% quality of information. This gives <code>((7.3 - 0) / log 1.05) * 0.90 * 0.50 = 67.3</code>, justifying &lt;9.6 hours. Making the pills took perhaps an hour, recording used up some time, and the analysis took several hours to label &amp; process all the data, play with it in R, and write it all up in a clean form for readers. Still, I don\u2019t think it took almost 10 hours of work, so I think this experiment ran at a profit.</p>\n<h2 id=\"2_2_Morning\"><a href=\"#TOC\"><span class=\"header-section-number\">2.2</span> Morning</a></h2>\n<p><a href=\"http://www.gwern.net/Zeo#vitamin-d-at-morn-helps\"><code class=\"url\">http://www.gwern.net/Zeo#vitamin-d-at-morn-helps</code></a></p>\n<p>With the vitamin D theory partially vindicated by the previous experiment, I became fairly sure that vitamin D in the morning would benefit my sleep somehow: 70%. Benefit how? I had no idea, it might be large or small. I didn\u2019t expect it to be a second melatonin, improving my sleep and trimming it by 50 minutes, but I hoped maybe it would help me get to sleep faster or wake up less. The actual experiment turned out to show, with very high confidence, absolutely no change except in my mood upon awakening in the morning.</p>\n<p>What is the \u201cvalue of information\u201d for this experiment? Essentially - nothing! Zero!</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the experiment had shown any benefit, I obviously would have continued taking it in the morning</li>\n<li>if the experiment had shown no effect, I would have continued taking it in the morning to avoid incurring the evening penalty discovered in the previous experiment</li>\n<li>if the experiment had shown the unthinkable, a negative effect, it would have to be substantial to convince me to stop taking vitamin D altogether and forfeit its other health benefits, and it\u2019s not worth bothering to analyze an outcome I would have given &lt;=5% chance to.</li>\n</ol>\n<p>Of course, I did it anyway because it was cool and interesting! (Estimated time cost: perhaps half the evening experiment, since I manually recorded less data and had the analysis worked out from before.)</p>\n<h1 id=\"3_Adderall\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Adderall</a></h1>\n<p><a href=\"http://www.gwern.net/Nootropics#adderall-blind-testing\"><code class=\"url\">http://www.gwern.net/Nootropics#adderall-blind-testing</code></a></p>\n<p>The amphetamine mix branded \u201cAdderall\u201d is terribly expensive to obtain even compared to modafinil, due to its tight regulation (a lower schedule than modafinil), popularity in college as a study drug, and reportedly moves by its manufacture to exploit its privileged position as a licensed amphetamine maker to extract more consumer surplus. I paid roughly $4 a pill but could have paid up to $10. Good stimulant hygiene involves recovery periods to avoid one\u2019s body adapting to eliminate the stimulating effects, so even if Adderall was the answer to all my woes, I would not be using it more than 2 or 3 times a week. Assuming 50 uses a year (for specific projects, let\u2019s say, and not ordinary aimless usage), that\u2019s a cool $200 a year. My general belief was that Adderall would be too much of a stimulant for me, as I am amphetamine-naive and Adderall has a bad reputation for letting one waste time on unimportant things. We could say my prediction was 50% that Adderall would be useful and worth investigating further. The experiment was pretty simple: blind randomized pills, 10 placebo &amp; 10 active. I took notes on how productive I was and the next day guessed whether it was placebo or Adderall before breaking the seal and finding out. I didn\u2019t do any formal statistics for it, much less a power calculation, so let\u2019s try to be conservative by penalizing the information quality heavily and assume it had 25%. So <code>((200 - 0) / log 1.05) * 0.50 * 0.25 = 512</code>! The experiment probably used up no more than an hour or two total.</p>\n<p>This example demonstrates that anything you are doing <em>expensively</em> is worth testing <em>extensively</em>.</p>\n<h1 id=\"4_Modafinil_day\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> Modafinil day</a></h1>\n<p><a href=\"http://www.gwern.net/Nootropics#modalert-blind-day-trial\"><code class=\"url\">http://www.gwern.net/Nootropics#modalert-blind-day-trial</code></a></p>\n<p>I tried 8 randomized days like with Adderall to see whether I was one of the people whom modafinil energizes during the day. (The other way to use it is to skip sleep, which is my preferred use.) I rarely use it during the day since my initial uses did not impress me subjectively. The experiment was not my best - while it was double-blind randomized, the measurements were subjective, and not a good measure of mental functioning like dual n-back (DNB) scores which I could statistically compare from day to day or against my many previous days of dual n-back scores. Between my high expectation of finding the null result, the poor experiment quality, and the minimal effect it had (eliminating an already rare use), it\u2019s obvious without guesstimating any numbers that the value of this information was very small.</p>\n<p>I mostly did it so I could tell people that \u201cno, day usage isn\u2019t particularly great for me; why don\u2019t you run an experiment on yourself and see whether it was just a placebo effect (or whether you genuinely are sleep-deprived and it is indeed compensating)?\u201d</p>\n<h1 id=\"5_Lithium\"><a href=\"#TOC\"><span class=\"header-section-number\">5</span> Lithium</a></h1>\n<p><a href=\"http://www.gwern.net/Nootropics#lithium-experiment\"><code class=\"url\">http://www.gwern.net/Nootropics#lithium-experiment</code></a></p>\n<p>Low-dose lithium orotate is extremely cheap, ~$10 a year. There is some research literature on it improving mood and impulse control in regular people, but some of it is epidemiological (which implies considerable unreliability); my current belief is that there is probably <em>some</em> effect size, but at just 10mg, it may be too tiny to matter. I have ~40% belief that there will be a large effect size, but I\u2019m doing a long experiment and I should be able to detect a large effect size with &gt;75% chance. So, the formula is NPV of the difference between taking and not taking, times quality of information, times expectation: <code>((10 - 0) / log 1.05) * 0.75 * 0.40 = 61.4</code>, which justifies a time investment of less than 9 hours. As it happens, it took less than an hour to make the pills &amp; placebos, and taking them is a matter of seconds per week, so the analysis will be the time-consuming part. This one may actually turn a profit.</p>\n<h1 id=\"6_Redshift\"><a href=\"#TOC\"><span class=\"header-section-number\">6</span> Redshift</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#redshiftf.lux\"><code class=\"url\">http://www.gwern.net/Zeo#redshiftf.lux</code></a></p>\n<p>Like the modafinil day trial, this was another value-less experiment justified by its intrinsic interest. I expect the results will confirm what I believe: that red-tinting my laptop screen will result in less damage to my sleep by not forcing lower melatonin levels with blue light. The only outcome that might change my decisions is if the use of Redshift actually worsens my sleep, but I regard this as highly unlikely. It is cheap to run as it is piggybacking on other experiments, and all the randomizing &amp; data recording is being handled by 2 simple shell scripts.</p>\n<h1 id=\"7_Meditation\"><a href=\"#TOC\"><span class=\"header-section-number\">7</span> Meditation</a></h1>\n<p><a href=\"http://www.gwern.net/Zeo#meditation-1\"><code class=\"url\">http://www.gwern.net/Zeo#meditation-1</code></a></p>\n<p>I find meditation useful when I am screwing around and can\u2019t focus on anything, but I don\u2019t meditate as much as I might because I lose half an hour. Hence, I am interested in the suggestion that meditation may not be as expensive as it seems because it reduces sleep need to some degree: if for every two minutes I meditate, I need one less minute of sleep, that halves the time cost - I spend 30 minutes meditating, gain back 15 minutes from sleep, for a net time loss of 15 minutes. So if I meditate regularly but there is no substitution, I lose out on 15 minutes a day. Figure I skip every 2 days, that\u2019s a total lost time of <code>(15 * 2/3 * 365.25) / 60 = 61</code> hours a year or $427 at minimum wage. I find the theory somewhat plausible (60%), and my year-long experiment has roughly a 60% chance of detecting the effect size (estimated based on the sleep reduction in a Indian sample of meditators). So <code>((427 - 0) / log 1.05) * 0.60 * 0.60 = $3150</code>. The experiment itself is unusually time-intensive, since it involve ~180 sessions of meditation, which if I am \u201coverpaying\u201d translates to 45 hours (<code>(180 * 15) / 60</code>) of wasted time or $315. But even including the design and analysis, that\u2019s less than the calculated value of information.</p>\n<p>This example demonstrates that drugs aren\u2019t the only expensive things for which you should do extensive testing.</p>", "sections": [{"title": "1 Melatonin", "anchor": "1_Melatonin", "level": 1}, {"title": "2 Vitamin D", "anchor": "2_Vitamin_D", "level": 1}, {"title": "2.1 Evening", "anchor": "2_1_Evening", "level": 2}, {"title": "2.2 Morning", "anchor": "2_2_Morning", "level": 2}, {"title": "3 Adderall", "anchor": "3_Adderall", "level": 1}, {"title": "4 Modafinil day", "anchor": "4_Modafinil_day", "level": 1}, {"title": "5 Lithium", "anchor": "5_Lithium", "level": 1}, {"title": "6 Redshift", "anchor": "6_Redshift", "level": 1}, {"title": "7 Meditation", "anchor": "7_Meditation", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "44 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hG29PqDJZJTAhuyQS", "vADtvr9iDeYsCDfxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-19T11:45:10.617Z", "modifiedAt": null, "url": null, "title": "Depression as a defense mechanism against slavery", "slug": "depression-as-a-defense-mechanism-against-slavery", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.669Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Attv2ynqRNKBq9Fpb/depression-as-a-defense-mechanism-against-slavery", "pageUrlRelative": "/posts/Attv2ynqRNKBq9Fpb/depression-as-a-defense-mechanism-against-slavery", "linkUrl": "https://www.lesswrong.com/posts/Attv2ynqRNKBq9Fpb/depression-as-a-defense-mechanism-against-slavery", "postedAtFormatted": "Saturday, May 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Depression%20as%20a%20defense%20mechanism%20against%20slavery&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADepression%20as%20a%20defense%20mechanism%20against%20slavery%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAttv2ynqRNKBq9Fpb%2Fdepression-as-a-defense-mechanism-against-slavery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Depression%20as%20a%20defense%20mechanism%20against%20slavery%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAttv2ynqRNKBq9Fpb%2Fdepression-as-a-defense-mechanism-against-slavery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAttv2ynqRNKBq9Fpb%2Fdepression-as-a-defense-mechanism-against-slavery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 302, "htmlBody": "<p>Individuals who have to conform to an external authority too powerful to resist often get depressed: which among other effects, includes lower productivity and higher risk for suicide. &nbsp;This suggests one way how a tendency for depression, and resulting behaviors of akrasia and suicide, enhances survivability. &nbsp;After all, humans have always had to live with the threat of being conquered and subjugated by other tribes. &nbsp;A conqueror has a choice to kill a prisoner or to use them for labor. &nbsp;A prisoner who becomes depressed and thus poses a lower threat to the conqueror is more likely to be spared. &nbsp;However, survival as a slave is difficult if the master imposes too many hardships on the slave. &nbsp;Therefore, it makes game theoretic sense for a defense mechanism to exist which makes it undesirable for a master to make life too difficult for the slave, in the form of the effects of depression. &nbsp;The lowered productivity resulting from depression means that a master gets diminishing or negative returns from working his slaves harder. &nbsp;At an extreme, the risk of suicide means that a master who pushes his slaves too far risks losing them.</p>\n<p>It would seem that such behaviors are less adaptive in civilized countries, where the risk of being enslaved is much lower. &nbsp;However, depression may still be of some benefit due to the fact that the master-slave relationship continues to exist, in diluted form, in hierarchical institutions.</p>\n<p>What consequences would this theory of depression have for the goal of controlling akrasia? &nbsp;Firstly, it suggests that the individual experiencing an impaired ability to realize certain goals first ask themselves, \"Are these goals really mine, or were they imposed by external authorities?\" &nbsp;If so, perhaps being able to convince yourself that your goals are really what *you* want would help motivate you towards achieving them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Attv2ynqRNKBq9Fpb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -5, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "16238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-19T19:11:23.128Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Timeless Beauty", "slug": "seq-rerun-timeless-beauty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gx2o5jzJ9eeRJgrC5/seq-rerun-timeless-beauty", "pageUrlRelative": "/posts/Gx2o5jzJ9eeRJgrC5/seq-rerun-timeless-beauty", "linkUrl": "https://www.lesswrong.com/posts/Gx2o5jzJ9eeRJgrC5/seq-rerun-timeless-beauty", "postedAtFormatted": "Saturday, May 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Timeless%20Beauty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Timeless%20Beauty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGx2o5jzJ9eeRJgrC5%2Fseq-rerun-timeless-beauty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Timeless%20Beauty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGx2o5jzJ9eeRJgrC5%2Fseq-rerun-timeless-beauty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGx2o5jzJ9eeRJgrC5%2Fseq-rerun-timeless-beauty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>Today's post, <a href=\"/lw/qq/timeless_beauty/\">Timeless Beauty</a> was originally published on 28 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>To get rid of time you must reduce it to nontime. In timeless physics, everything that exists is perfectly global or perfectly local. The laws of physics are perfectly global; the configuration space is perfectly local. Every fundamentally existent ontological entity has a unique identity and a unique value. This beauty makes ugly theories much more visibly ugly; a collapse postulate becomes a visible scar on the perfection.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ci4/seq_rerun_timeless_physics/\">Timeless Physics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gx2o5jzJ9eeRJgrC5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.058828673923792e-07, "legacy": true, "legacyId": "16244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GKTe9bCxFSE6EXEEu", "JjTiktGohsc3osjbN", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-19T20:30:07.970Z", "modifiedAt": null, "url": null, "title": "Oh, mainstream philosophy.", "slug": "oh-mainstream-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.986Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mNPp3GZnEEYDK2a6j/oh-mainstream-philosophy", "pageUrlRelative": "/posts/mNPp3GZnEEYDK2a6j/oh-mainstream-philosophy", "linkUrl": "https://www.lesswrong.com/posts/mNPp3GZnEEYDK2a6j/oh-mainstream-philosophy", "postedAtFormatted": "Saturday, May 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Oh%2C%20mainstream%20philosophy.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOh%2C%20mainstream%20philosophy.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmNPp3GZnEEYDK2a6j%2Foh-mainstream-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Oh%2C%20mainstream%20philosophy.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmNPp3GZnEEYDK2a6j%2Foh-mainstream-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmNPp3GZnEEYDK2a6j%2Foh-mainstream-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 288, "htmlBody": "<p><a href=\"http://chronicle.com/article/Is-Death-Bad-for-You-/131818/\">http://chronicle.com/article/Is-Death-Bad-for-You-/131818/</a></p>\n<p>Summary: Shelly Kagan, Yale philosophy professor, discusses the argument that death isn't bad for you, because when we are dead we won't care.&nbsp; He hunts around for justification, doesn't find anything satisfactory, or even paint a clear picture of what \"satisfactory\" would look like, and ends up conveying mostly mysteriousness to the audience.</p>\n<p>&nbsp;</p>\n<p>There are a variety of right ways to approach this argument.&nbsp; One good goal is to understand what's going on in someone's head when they say that death is bad for you.</p>\n<p>Reading the article, a bell rang for me about all this discussion of \"possible worlds\" - for example, the idea of feeling pity for people who don't exist.&nbsp; We usually don't interact with people who don't exist, so what process has led us to compare these different worlds against each other?</p>\n<p>The answer is a decision-making process.&nbsp; \"Possible worlds\" doesn't mean spawning any physical universes - it's a convenient shorthand for <em>imagined</em> possible worlds, which we (in our capacity as intelligent apes) compare against each other, usually as part of a consequentialist decision process.</p>\n<p>Once you start looking, you see the fingerprints of decision-making all over the article.&nbsp; It's the machinery that generates these possible worlds to think about, and the context that colors them.&nbsp; So I think noticing that \"possible worlds &lt;- us imagining possible worlds as part of our decision-making\" is a good relationship for understanding topics like this.</p>\n<p>&nbsp;</p>\n<p>Edit for clarity: The basic idea is that death being bad is, at its root, a function of the decision-making bits of our brains.&nbsp; This can be seen not just from a priori claims about \"low utility = bad,\" but from the structure of what Shelly Kagan hunts around for, which mainly involves choices between possible worlds.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mNPp3GZnEEYDK2a6j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -14, "extendedScore": null, "score": 9.059175977060965e-07, "legacy": true, "legacyId": "16245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-19T21:29:46.832Z", "modifiedAt": null, "url": null, "title": "How to brainstorm effectively", "slug": "how-to-brainstorm-effectively", "viewCount": null, "lastCommentedAt": "2017-10-01T20:48:02.923Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PECOS-9", "createdAt": "2011-12-14T23:46:02.634Z", "isAdmin": false, "displayName": "PECOS-9"}, "userId": "DkbNCHiv5x9TCmyXA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PrPk5ogJNK7QcPead/how-to-brainstorm-effectively", "pageUrlRelative": "/posts/PrPk5ogJNK7QcPead/how-to-brainstorm-effectively", "linkUrl": "https://www.lesswrong.com/posts/PrPk5ogJNK7QcPead/how-to-brainstorm-effectively", "postedAtFormatted": "Saturday, May 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20brainstorm%20effectively&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20brainstorm%20effectively%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrPk5ogJNK7QcPead%2Fhow-to-brainstorm-effectively%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20brainstorm%20effectively%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrPk5ogJNK7QcPead%2Fhow-to-brainstorm-effectively", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrPk5ogJNK7QcPead%2Fhow-to-brainstorm-effectively", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1763, "htmlBody": "<blockquote>\n<p>Mr. Malfoy is new to the business of having ideas, and so when he has one, he becomes proud of himself for having it. He has not yet had enough ideas to unflinchingly discard those that are beautiful in some aspects and impractical in others; he has not yet acquired confidence in his own ability to think of better ideas as he requires them. What we are seeing here is not Mr. Malfoy's best idea, I fear, but rather his only idea.</p>\n</blockquote>\n<p>- Harry Potter and the Methods of Rationality</p>\n<blockquote>\n<p>I want to emphasize yet again that the tools [described in Serious Creativity] are deliberate and can be used systematically. It is not a matter of inspiration or feeling in the mood of being \"high.\" You can use the tools just as deliberately as you can add up a column of numbers.</p>\n</blockquote>\n<p>- Edward De Bono, <em>Serious Creativity</em></p>\n<p><br />I will summarize some of the techniques for how to generate ideas presented in <a href=\"http://www.amazon.com/Serious-Creativity-Lateral-Thinking-Create/dp/0887306357\">Serious Creativity</a>. The book also has other material, e.g. interesting <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">deep theories</a> about why these techniques work, arguments for the importance of creativity, and more techniques beyond what's described in this post, but in the interest of keeping this post concise and useful, I will only describe one kind of technique and urge you to <a href=\"/lw/5a5/no_seriously_just_try_it/\">just try it</a>. You should read the book if you want more detail or techniques.<br /><br />These techniques can be used both when you have a problem you need to solve and when you have a general area that you suspect could be improved or innovated, but don't have any specific ideas of what's wrong (or even if you don't feel like there's anything wrong at all).</p>\n<p>The technique I will describe in this post is that of \"provocation\" followed by \"movement.\" A provocation is a seemingly random or nonsensical sentence or phrase. Movement is the process of going forward with a provocation and actually generating an idea. There are precise, formal techniques for generating provocations and movement, which I will describe after giving an example of how this \"provocation-movement\" process works.</p>\n<p>&nbsp;</p>\n<h2>Example</h2>\n<p>Provocation: Planes land upside down.<br />Movement: We can imagine this actually happening, and observe that the pilot would have a better view of the landing area. This naturally leads us to consider other ways to improve the pilot's view of the landing area. Perhaps we could move the cockpit to the bottom, or add video cameras. So using this technique, we've identified an area for improvement and two possible ways to make that improvement.</p>\n<p>&nbsp;</p>\n<h2>Setting Up Provocations</h2>\n<p>Provocation is a way to avoid getting stuck in the same \"mental pathways\" (see <a href=\"http://wiki.lesswrong.com/wiki/Priming\">priming</a>) so that you can find new ones. Provocations should not make sense and are not necessarily intended to convey meaning; they are just intended to \"make things happen in our minds.\" The book precedes provocations with \"po,\" a word used to indicate that the sentence is intended to be nonsensical and illogical. Po stands for \"provoking operation.\" The book describes several techniques for generating provocations.</p>\n<ol>\n<li><strong>Escape method</strong>: Think of something that we take for granted, and negate it. E.g., \"Po, restaurants do not have food\" or \"Po, shoes do not have soles.\"</li>\n<li><strong>Reversal</strong>: Take a standard arrangement or relationship that we take for granted, and reverse it. E.g. \"I have orange juice for breakfast\" becomes \"Po, the orange juice has me for breakfast\". Note that the reversal would <em>not</em> be \"Po, I do not have orange juice for breakfast.\" That would be the escape method.</li>\n<li><strong>Exaggeration</strong>: Suggest that some dimension or measurement falls far outside its normal range (either greater or lesser). E.g. \"Po, every household has 100 phones\" or \"Po, the phone has 1 dialing button.\" If you're making the dimension smaller, do not bring it to 0 or you're just using the escape method again. E.g. \"Po, the phone has 0 dialing buttons\" is not an exaggeration, it's an escape.</li>\n<li><strong>Distortion</strong>: Take normal arrangements (e.g. relationships or time sequences) and switch them around. E.g. \"Po, you close the letter after you post it,\" \"Po, criminals pay for the police force,\" or \"Po, food prepares customers for chefs.\"</li>\n<li><strong>Wishful thinking</strong>: \"Wouldn't it be nice if...\" put forward a fantasy that is known to be impossible. E.g. \"Po, the pencil should write by itself.\"</li>\n</ol>\n<p>A provocation doesn't need to follow from one of these techniques. A provocation can be any incorrect or absurd statement. These techniques are just easy step-by-step ways to generate provocations without requiring any elusive \"spark of inspiration.\" Once a provocation is generated, it should be followed by one or more of the movement techniques described in the next section.</p>\n<p>If you are trying to solve a specific problem or innovate in a particular domain, then choose provocations related to the domain. That is, if you're trying to figure out how to improve wikipedia, don't use a provocation like \"Po, the orange juice has me for breakfast,\" choose one like \"Po, citations are not needed\" (escape) or \"Po, articles contain encyclopedias.\" (reversal).</p>\n<p>&nbsp;</p>\n<h2>Movement</h2>\n<p>Movement allows you to take some idea, concept, or provocation and move forward with it to generate more useful ideas and concepts. These techniques don't apply solely to provocations: you can use them for ideas and concepts too. The book describes 5 formal techniques for movement:</p>\n<ol>\n<li><strong>Extract a principle</strong>: Focus on some principle of the provocation, and then work with that principle to discover other ideas related to it. E.g. with the provocation \"Po, bring back the town crier\", we may extract the principle that the town crier can go to where people are, and then we try to generate ideas related to that principle.</li>\n<li><strong>Focus on the difference</strong>: Compare the provocation to existing ways of doing things. How are they different? Then you can consider other ways to use this difference. This is very similar to \"extract a principle.\"</li>\n<li><strong>Moment to moment</strong>: imagine what would happen if the provocation were put into effect. We are not interested in the final effect, but the moment-to-moment happenings. E.g. for \"Po, orange juice has me for breakfast\", you may imagine yourself falling into a giant glass of orange juice.</li>\n<li><strong>Positive aspects</strong>: Look directly for benefits. What are the positive aspects of the provocation? Once you've identified some positive aspects you can consider if you can achieve some of them in other ways (again, this is similar to extract a principle, it's just another way of thinking about it).</li>\n<li><strong>Circumstances</strong>: In what circumstances would the provocation have immediate value? E.g. for the provocation \"Po, drinking glasses should have rounded bottoms,\" you could notice that this would be useful if you didn't want people to be able to put down their glasses. This could be good for bars, where you want people to drink more and faster.</li>\n</ol>\n<p>You can use these movement techniques not just on provocations, but also ideas or concepts. For example, you may start with a provocation, use the \"moment to moment\" technique which gives you an idea, and then you could use the \"positive aspects\" technique with that idea to generate more ideas. Also, of course, you do not need to strictly use just these techniques. If a provocation directly leads you to think of something interesting without explicitly choosing to use one of these techniques, that's fine, you should explore the idea more. Use these when you need them.</p>\n<p>&nbsp;</p>\n<h2>More Examples</h2>\n<p>Here's another example from the book. This one uses the \"moment to moment\" movement technique:</p>\n<blockquote>\n<p>Po, cars have square wheels<br /><br />We imagine a car with square wheels. We imagine this car starting to roll. The square wheel rises up on its corner. This would lead to a very bumpy ride. But the suspension could anticipate this rise and could adjust by getting shorter. This leads to the concept of an adjusting suspension. This in turn leads to the idea of a vehicle for going over rough ground. A jockey wheel would signal back the state of the ground to the suspension which would then adjust so that the wheel was raised to follow the \"profile\" of the ground...This was an idea I first suggested about twenty years ago. Today several companies such as Lotus (part of GM) are working on \"intelligent suspension\" which behaves in a similar way.</p>\n</blockquote>\n<p>And here's another one from the book. The provocation uses the \"escape\" method and the movement seems to use the \"circumstances\" method:</p>\n<blockquote>\n<p>Po, waiters are not polite.<br /><br />This leads to an idea for waiters to be actors and actresses. The menu indicates the \"character\" of the waiter. You can order whichever waiter you wanted: belligerent, humorous, obsequious, and so on. You might order a belligerent waiter and enjoy having a fight with him. The waiters and waitresses would act out the assigned role.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Warnings</h2>\n<ul>\n<li>As a general principle, try to avoid saying \"oh, but this is just like this other existing product\" whenever you generate an idea. Usually it's <em>not</em> just like the existing idea, you're just interpreting it in that way because we naturally follow paths toward the familiar. So if you have a half-formed idea that could take several directions, fight the urge to immediately take it down an existing path and then discard it because it already exists. Leave it in the half-formed stage instead. I'm reminded of the concept of <a href=\"http://wiki.lesswrong.com/wiki/Semantic_stopsign\">semantic stopsigns</a>. Saying an idea is \"the same as\" something else gives the illusion of having fully explored the idea, when in reality you just jumped immediately to one possible development (possibly the least useful development, since it's one you know already exists).</li>\n<li>Similarly, do not take too many steps when moving from a provocation. This will just lead you to an existing idea. There's nothing to be gained by playing 6 degrees of separation with provocations and existing ideas. Just take a few small steps. If nothing comes to you, try other movement techniques or try a different provocation.&nbsp;</li>\n<li>You're not expected to come up with a good idea for every provocation. Most of the time you'll come up with some mediocre or half-formed idea, or even no idea at all. This is fine.</li>\n<li>You should write down anything you come up with that seems interesting (even if it's a bad idea in its current form, if it has something interesting about it, write it down) and then come back to it later and think about it more (either using these techniques or just your normal thinking processes for improving and adapting ideas).</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2wjPMY34by2gXEXA2": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PrPk5ogJNK7QcPead", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 63, "extendedScore": null, "score": 0.00010743093314860276, "legacy": true, "legacyId": "16246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>Mr. Malfoy is new to the business of having ideas, and so when he has one, he becomes proud of himself for having it. He has not yet had enough ideas to unflinchingly discard those that are beautiful in some aspects and impractical in others; he has not yet acquired confidence in his own ability to think of better ideas as he requires them. What we are seeing here is not Mr. Malfoy's best idea, I fear, but rather his only idea.</p>\n</blockquote>\n<p>- Harry Potter and the Methods of Rationality</p>\n<blockquote>\n<p>I want to emphasize yet again that the tools [described in Serious Creativity] are deliberate and can be used systematically. It is not a matter of inspiration or feeling in the mood of being \"high.\" You can use the tools just as deliberately as you can add up a column of numbers.</p>\n</blockquote>\n<p>- Edward De Bono, <em>Serious Creativity</em></p>\n<p><br>I will summarize some of the techniques for how to generate ideas presented in <a href=\"http://www.amazon.com/Serious-Creativity-Lateral-Thinking-Create/dp/0887306357\">Serious Creativity</a>. The book also has other material, e.g. interesting <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">deep theories</a> about why these techniques work, arguments for the importance of creativity, and more techniques beyond what's described in this post, but in the interest of keeping this post concise and useful, I will only describe one kind of technique and urge you to <a href=\"/lw/5a5/no_seriously_just_try_it/\">just try it</a>. You should read the book if you want more detail or techniques.<br><br>These techniques can be used both when you have a problem you need to solve and when you have a general area that you suspect could be improved or innovated, but don't have any specific ideas of what's wrong (or even if you don't feel like there's anything wrong at all).</p>\n<p>The technique I will describe in this post is that of \"provocation\" followed by \"movement.\" A provocation is a seemingly random or nonsensical sentence or phrase. Movement is the process of going forward with a provocation and actually generating an idea. There are precise, formal techniques for generating provocations and movement, which I will describe after giving an example of how this \"provocation-movement\" process works.</p>\n<p>&nbsp;</p>\n<h2 id=\"Example\">Example</h2>\n<p>Provocation: Planes land upside down.<br>Movement: We can imagine this actually happening, and observe that the pilot would have a better view of the landing area. This naturally leads us to consider other ways to improve the pilot's view of the landing area. Perhaps we could move the cockpit to the bottom, or add video cameras. So using this technique, we've identified an area for improvement and two possible ways to make that improvement.</p>\n<p>&nbsp;</p>\n<h2 id=\"Setting_Up_Provocations\">Setting Up Provocations</h2>\n<p>Provocation is a way to avoid getting stuck in the same \"mental pathways\" (see <a href=\"http://wiki.lesswrong.com/wiki/Priming\">priming</a>) so that you can find new ones. Provocations should not make sense and are not necessarily intended to convey meaning; they are just intended to \"make things happen in our minds.\" The book precedes provocations with \"po,\" a word used to indicate that the sentence is intended to be nonsensical and illogical. Po stands for \"provoking operation.\" The book describes several techniques for generating provocations.</p>\n<ol>\n<li><strong>Escape method</strong>: Think of something that we take for granted, and negate it. E.g., \"Po, restaurants do not have food\" or \"Po, shoes do not have soles.\"</li>\n<li><strong>Reversal</strong>: Take a standard arrangement or relationship that we take for granted, and reverse it. E.g. \"I have orange juice for breakfast\" becomes \"Po, the orange juice has me for breakfast\". Note that the reversal would <em>not</em> be \"Po, I do not have orange juice for breakfast.\" That would be the escape method.</li>\n<li><strong>Exaggeration</strong>: Suggest that some dimension or measurement falls far outside its normal range (either greater or lesser). E.g. \"Po, every household has 100 phones\" or \"Po, the phone has 1 dialing button.\" If you're making the dimension smaller, do not bring it to 0 or you're just using the escape method again. E.g. \"Po, the phone has 0 dialing buttons\" is not an exaggeration, it's an escape.</li>\n<li><strong>Distortion</strong>: Take normal arrangements (e.g. relationships or time sequences) and switch them around. E.g. \"Po, you close the letter after you post it,\" \"Po, criminals pay for the police force,\" or \"Po, food prepares customers for chefs.\"</li>\n<li><strong>Wishful thinking</strong>: \"Wouldn't it be nice if...\" put forward a fantasy that is known to be impossible. E.g. \"Po, the pencil should write by itself.\"</li>\n</ol>\n<p>A provocation doesn't need to follow from one of these techniques. A provocation can be any incorrect or absurd statement. These techniques are just easy step-by-step ways to generate provocations without requiring any elusive \"spark of inspiration.\" Once a provocation is generated, it should be followed by one or more of the movement techniques described in the next section.</p>\n<p>If you are trying to solve a specific problem or innovate in a particular domain, then choose provocations related to the domain. That is, if you're trying to figure out how to improve wikipedia, don't use a provocation like \"Po, the orange juice has me for breakfast,\" choose one like \"Po, citations are not needed\" (escape) or \"Po, articles contain encyclopedias.\" (reversal).</p>\n<p>&nbsp;</p>\n<h2 id=\"Movement\">Movement</h2>\n<p>Movement allows you to take some idea, concept, or provocation and move forward with it to generate more useful ideas and concepts. These techniques don't apply solely to provocations: you can use them for ideas and concepts too. The book describes 5 formal techniques for movement:</p>\n<ol>\n<li><strong>Extract a principle</strong>: Focus on some principle of the provocation, and then work with that principle to discover other ideas related to it. E.g. with the provocation \"Po, bring back the town crier\", we may extract the principle that the town crier can go to where people are, and then we try to generate ideas related to that principle.</li>\n<li><strong>Focus on the difference</strong>: Compare the provocation to existing ways of doing things. How are they different? Then you can consider other ways to use this difference. This is very similar to \"extract a principle.\"</li>\n<li><strong>Moment to moment</strong>: imagine what would happen if the provocation were put into effect. We are not interested in the final effect, but the moment-to-moment happenings. E.g. for \"Po, orange juice has me for breakfast\", you may imagine yourself falling into a giant glass of orange juice.</li>\n<li><strong>Positive aspects</strong>: Look directly for benefits. What are the positive aspects of the provocation? Once you've identified some positive aspects you can consider if you can achieve some of them in other ways (again, this is similar to extract a principle, it's just another way of thinking about it).</li>\n<li><strong>Circumstances</strong>: In what circumstances would the provocation have immediate value? E.g. for the provocation \"Po, drinking glasses should have rounded bottoms,\" you could notice that this would be useful if you didn't want people to be able to put down their glasses. This could be good for bars, where you want people to drink more and faster.</li>\n</ol>\n<p>You can use these movement techniques not just on provocations, but also ideas or concepts. For example, you may start with a provocation, use the \"moment to moment\" technique which gives you an idea, and then you could use the \"positive aspects\" technique with that idea to generate more ideas. Also, of course, you do not need to strictly use just these techniques. If a provocation directly leads you to think of something interesting without explicitly choosing to use one of these techniques, that's fine, you should explore the idea more. Use these when you need them.</p>\n<p>&nbsp;</p>\n<h2 id=\"More_Examples\">More Examples</h2>\n<p>Here's another example from the book. This one uses the \"moment to moment\" movement technique:</p>\n<blockquote>\n<p>Po, cars have square wheels<br><br>We imagine a car with square wheels. We imagine this car starting to roll. The square wheel rises up on its corner. This would lead to a very bumpy ride. But the suspension could anticipate this rise and could adjust by getting shorter. This leads to the concept of an adjusting suspension. This in turn leads to the idea of a vehicle for going over rough ground. A jockey wheel would signal back the state of the ground to the suspension which would then adjust so that the wheel was raised to follow the \"profile\" of the ground...This was an idea I first suggested about twenty years ago. Today several companies such as Lotus (part of GM) are working on \"intelligent suspension\" which behaves in a similar way.</p>\n</blockquote>\n<p>And here's another one from the book. The provocation uses the \"escape\" method and the movement seems to use the \"circumstances\" method:</p>\n<blockquote>\n<p>Po, waiters are not polite.<br><br>This leads to an idea for waiters to be actors and actresses. The menu indicates the \"character\" of the waiter. You can order whichever waiter you wanted: belligerent, humorous, obsequious, and so on. You might order a belligerent waiter and enjoy having a fight with him. The waiters and waitresses would act out the assigned role.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Warnings\">Warnings</h2>\n<ul>\n<li>As a general principle, try to avoid saying \"oh, but this is just like this other existing product\" whenever you generate an idea. Usually it's <em>not</em> just like the existing idea, you're just interpreting it in that way because we naturally follow paths toward the familiar. So if you have a half-formed idea that could take several directions, fight the urge to immediately take it down an existing path and then discard it because it already exists. Leave it in the half-formed stage instead. I'm reminded of the concept of <a href=\"http://wiki.lesswrong.com/wiki/Semantic_stopsign\">semantic stopsigns</a>. Saying an idea is \"the same as\" something else gives the illusion of having fully explored the idea, when in reality you just jumped immediately to one possible development (possibly the least useful development, since it's one you know already exists).</li>\n<li>Similarly, do not take too many steps when moving from a provocation. This will just lead you to an existing idea. There's nothing to be gained by playing 6 degrees of separation with provocations and existing ideas. Just take a few small steps. If nothing comes to you, try other movement techniques or try a different provocation.&nbsp;</li>\n<li>You're not expected to come up with a good idea for every provocation. Most of the time you'll come up with some mediocre or half-formed idea, or even no idea at all. This is fine.</li>\n<li>You should write down anything you come up with that seems interesting (even if it's a bad idea in its current form, if it has something interesting about it, write it down) and then come back to it later and think about it more (either using these techniques or just your normal thinking processes for improving and adapting ideas).</li>\n</ul>", "sections": [{"title": "Example", "anchor": "Example", "level": 1}, {"title": "Setting Up Provocations", "anchor": "Setting_Up_Provocations", "level": 1}, {"title": "Movement", "anchor": "Movement", "level": 1}, {"title": "More Examples", "anchor": "More_Examples", "level": 1}, {"title": "Warnings", "anchor": "Warnings", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LqjKP255fPRY7aMzw", "Zmfo388RA9oky3KYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-05-19T21:29:46.832Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T04:40:00.015Z", "modifiedAt": null, "url": null, "title": "Is a Purely Rational World a Technologically Advanced World?", "slug": "is-a-purely-rational-world-a-technologically-advanced-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:39.427Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tygorton", "createdAt": "2012-05-18T18:39:26.075Z", "isAdmin": false, "displayName": "tygorton"}, "userId": "R9x9rvHLc3JwtQ4ex", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5KendZQ7Wt4tHqDWe/is-a-purely-rational-world-a-technologically-advanced-world", "pageUrlRelative": "/posts/5KendZQ7Wt4tHqDWe/is-a-purely-rational-world-a-technologically-advanced-world", "linkUrl": "https://www.lesswrong.com/posts/5KendZQ7Wt4tHqDWe/is-a-purely-rational-world-a-technologically-advanced-world", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20a%20Purely%20Rational%20World%20a%20Technologically%20Advanced%20World%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20a%20Purely%20Rational%20World%20a%20Technologically%20Advanced%20World%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5KendZQ7Wt4tHqDWe%2Fis-a-purely-rational-world-a-technologically-advanced-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20a%20Purely%20Rational%20World%20a%20Technologically%20Advanced%20World%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5KendZQ7Wt4tHqDWe%2Fis-a-purely-rational-world-a-technologically-advanced-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5KendZQ7Wt4tHqDWe%2Fis-a-purely-rational-world-a-technologically-advanced-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 376, "htmlBody": "<p>\n<p class=\"MsoNormal\">What would our world be today if humans had started off with a purely rational intelligence?</p>\n<p class=\"MsoNormal\">It seems as though a dominant aspect of rationality deals with risk management.&nbsp; For example, an irrational person might feel that the thrill of riding a zip line for a few seconds as being well worth the risk of injuring themselves, contracting a flesh eating bug,&nbsp; and losing a leg along with both hands (sorry, but that story has been freaking me out the past few days, I in no way mean to trivialize the woman&rsquo;s situation).&nbsp; A purely rational person would (I&rsquo;m making an assumption here because I am certainly not a rational person) recognize the high probability of something going wrong and determine that the risks were too steep when compared with the minimal gain of a short-lived thrill.</p>\n<p class=\"MsoNormal\">But how does a purely rational intelligence&mdash;even an intelligence at the current human level with a limited ability to analyze probabilities&mdash;impact the advancement of technology?&nbsp; As an example, would humanity have moved forward with the combustible engine and motor vehicles as purely rational beings?&nbsp; History shows us that humans tend to leap headlong into technological advancements with very little thought regarding the potential damage they may cause.&nbsp; Every technological advancement of note has had negative impacts that may have been deemed too steep as probability equations from a purely rational perspective.</p>\n<p class=\"MsoNormal\">Would pure rationality have severely limited the advancement of technology?</p>\n<p class=\"MsoNormal\">Taken further, would a purely rational intelligence far beyond human levels be so burdened by risk probabilities as to render it paralyzed&hellip; suspended in a state of infinite stagnation?&nbsp; OR, would a purely rational mind simply ensure that more cautious advancement take place (which would certainly have slowed things down)?</p>\n<p class=\"MsoNormal\">Many of humanity&rsquo;s great success stories begin as highly irrational ventures that had extremely low chances for positive results.&nbsp; Humans, being irrational and not all that intelligent, are very capable of ignoring risk or simply not recognizing the level of risk inherent in any given situation.&nbsp; But to what extent would a purely rational approach limit a being&rsquo;s willingness to take action?&nbsp;</p>\n<p class=\"MsoNormal\">*I apologize if these questions have already been asked and/or discussed at length.&nbsp; I did do some searches but did not find anything that seemed specifically related to this line of thought.*</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5KendZQ7Wt4tHqDWe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -6, "extendedScore": null, "score": 9.06133699123726e-07, "legacy": true, "legacyId": "16247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T08:51:38.753Z", "modifiedAt": null, "url": null, "title": "Work harder on tabooing \"Friendly AI\"", "slug": "work-harder-on-tabooing-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:21.537Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5pYfLADQ8gvk6Sd33/work-harder-on-tabooing-friendly-ai", "pageUrlRelative": "/posts/5pYfLADQ8gvk6Sd33/work-harder-on-tabooing-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/5pYfLADQ8gvk6Sd33/work-harder-on-tabooing-friendly-ai", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Work%20harder%20on%20tabooing%20%22Friendly%20AI%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWork%20harder%20on%20tabooing%20%22Friendly%20AI%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pYfLADQ8gvk6Sd33%2Fwork-harder-on-tabooing-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Work%20harder%20on%20tabooing%20%22Friendly%20AI%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pYfLADQ8gvk6Sd33%2Fwork-harder-on-tabooing-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5pYfLADQ8gvk6Sd33%2Fwork-harder-on-tabooing-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 707, "htmlBody": "<p>This is is an outgrowth of a <a href=\"/r/discussion/lw/bxr/muehlhauserwang_dialogue/6go9\">comment</a> I left on Luke's dialog with Pei Wang, and I'll start by quoting that comment in full:</p>\n<p style=\"padding-left: 30px; \">Luke, what do you mean here when you say, \"Friendly AI may be incoherent and impossible\"?&nbsp;</p>\n<p style=\"padding-left: 30px; \">The Singularity Institute's page <a href=\"http://intelligence.org/ourresearch/publications/what-is-friendly-ai.html\">\"What is Friendly AI?\"</a> defines \"Friendly AI\" as \"A \"Friendly AI\" is an AI that takes actions that are, on the whole, beneficial to humans and humanity.\" Surely you don't mean to say, \"The idea of an AI that takes actions that are, on the whole, beneficial to humans and humanity may be incoherent or impossible\"?</p>\n<p style=\"padding-left: 30px; \">Eliezer's paper \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" talks about \"an AI created with specified motivations.\" But it's pretty clear that that's not the only thing you and he have in mind, because part of the problem is making sure the motivations we give an AI are the ones we really want to give it.</p>\n<p style=\"padding-left: 30px; \">If you meant neither of those things, what did you mean? \"Provably friendly\"? \"One whose motivations express an ideal extrapolation of our values\"? (It seems a flawed extrapolation could still give results that are on the whole beneficial, so this is different than the first definition suggested above.) Or something else?</p>\n<p>Since writing that comment, I've managed to find two other definitions of \"Friendly AI.\" One is from <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong, Sandberg, and Bostrom's paper on Oracle AI,</a> which describes Friendly AI as: \"AI systems designed to be of low risk.\" This definition is very similar to the definition from the Singularity Institute's \"What is Friendly AI?\" page, except that it incorporates the concept of risk. The second definition is from <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Luke's paper with Anna Salamon,</a> which describes Friendly AI as \"an AI with a stable, desirable utility function.\" This definition has the important feature of restricting \"Friendly AI\" to designs that <em>have </em>a utility function. Luke's comments about \"rationally shaped\" AI in <a href=\"http://lukeprog.com/SaveTheWorld.html\">this essay</a> seem relevant here.</p>\n<p>Neither of those papers seems to use the initial definition they give of \"Friendly AI\" consistently. Armstrong, Sandberg, and Bostrom's paper has a section on creating Oracle AI by giving it a \"friendly utility&nbsp;function,\" which states, \"if a friendly OAI could be designed, then it is most likely that a friendly AI could also be designed, obviating the need to restrict to an Oracle design in the first place.\"</p>\n<p>This is a non-sequitur if \"friendly\" merely means \"low risk,\" but it makes sense if they are actually defining Friendly AI in terms of a safe utility function: what they're saying then is if we can create an AI that stays boxed because of its utility function, we can probably create an AI that doesn't need to be boxed to be safe.</p>\n<p>In the case of Luke's paper with Anna Salamon, the discussion on page 17 seems to imply that \"Nanny AI\" and \"Oracle AI\" are not types of Friendly AI. This is strange under their official definition of \"Friendly AI.\" Why couldn't Nanny AI or Oracle AI have a stable, desirable utility function? I'm inclined to think the best way to make sense of that part of the paper is if \"Friendly AI\" is interpreted to mean \"an AI&nbsp;whose utility function an ideal extrapolation of our values (or at least comes close.)\"</p>\n<p>I'm being very nitpicky here, but I think the issue of how to define \"Friendly AI\" is important for a couple of reasons. First, it's obviously important for clear communication. If we aren't clear on what we mean by \"Friendly AI,\" we won't understand each other when we try to talk about it.\" But another very important worry&nbsp;that confusion about the meaning of \"Friendly AI\" may be spawning sloppy thinking about it. Equivocating between narrower and broader definitions of \"Friendly AI\" may end up taking the place of an <em>argument</em> that the approach specified by the more narrow definition is the way to go. This seems like an excellent example of the benefits of <a href=\"/lw/nu/taboo_your_words\">tabooing your words.</a></p>\n<p>I see on Luke's website that he has a forthcoming peer-reviewed article with Nick Bostrom titled \"Why We Need Friendly AI.\" On the whole, I've been impressed with the drafts of the two peer-reviewed articles Luke has posted so far, so I'm moderately optimistic that that article will resolve these issues.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb12a": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5pYfLADQ8gvk6Sd33", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 27, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "16211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBdvyyHLdxZSAMmoz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T10:41:18.081Z", "modifiedAt": null, "url": null, "title": "Zombie existential angst? (Not p-zombies, just the regular kind. Metaphorically.)", "slug": "zombie-existential-angst-not-p-zombies-just-the-regular-kind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.031Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annie0305", "createdAt": "2012-05-20T09:11:51.985Z", "isAdmin": false, "displayName": "Annie0305"}, "userId": "HfQAR2ZsZbZjL5oBz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jPcdz2nNZSdRY7L5d/zombie-existential-angst-not-p-zombies-just-the-regular-kind", "pageUrlRelative": "/posts/jPcdz2nNZSdRY7L5d/zombie-existential-angst-not-p-zombies-just-the-regular-kind", "linkUrl": "https://www.lesswrong.com/posts/jPcdz2nNZSdRY7L5d/zombie-existential-angst-not-p-zombies-just-the-regular-kind", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zombie%20existential%20angst%3F%20(Not%20p-zombies%2C%20just%20the%20regular%20kind.%20Metaphorically.)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZombie%20existential%20angst%3F%20(Not%20p-zombies%2C%20just%20the%20regular%20kind.%20Metaphorically.)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjPcdz2nNZSdRY7L5d%2Fzombie-existential-angst-not-p-zombies-just-the-regular-kind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zombie%20existential%20angst%3F%20(Not%20p-zombies%2C%20just%20the%20regular%20kind.%20Metaphorically.)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjPcdz2nNZSdRY7L5d%2Fzombie-existential-angst-not-p-zombies-just-the-regular-kind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjPcdz2nNZSdRY7L5d%2Fzombie-existential-angst-not-p-zombies-just-the-regular-kind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 785, "htmlBody": "<p><span id=\"internal-source-marker_0.9608160921217059\" style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">(Or possibly the</span><a href=\"http://www.qwantz.com/index.php?comic=915\"><span style=\"font-size: 13px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: underline; vertical-align: baseline;\"> worst kind of zombie</span></a><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">. But still, metaphorically.)</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Since  I was a kid, as far back as I can remember having thought about the  issue at all, the basic arguments against existential angst have seemed  obvious to me.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I  used to express it something like: \"If nothing really matters [ie,  values aren&rsquo;t objective, or however I put it back then], then it doesn't  matter that nothing matters. If I choose to hold something as  important, I can't be wrong.\"</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">However,  a few months ago, it occurred to me to apply another principle of  rationality to the issue, and that actually caused me to start having  problems with existential angst!</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I don't know if we have a snappy name for the principle, but this is my favorite expression of it:</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">\"If  you&rsquo;re interested in being on the right side of disputes, you will  refute your opponents&rsquo; arguments. &nbsp;But if you&rsquo;re interested in producing  truth, you will fix your opponents&rsquo; arguments for them. &nbsp;To win, you  must fight not only the creature you encounter; you must fight the most  horrible thing that can be constructed from its corpse.\"</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">[I first read it used as the epigram to Yvain's \"</span><a href=\"http://lesswrong.com/lw/2k/the_least_convenient_possible_world/\"><span style=\"font-size: 13px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: underline; vertical-align: baseline;\">Least Convenient Possible World</span></a><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">\". Call it, what, \"Fight your own zombies\"?]</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Sure,  \"The universe is a mere dance of particles, therefore your hopes and  dreams are meaningless and you should just go off yourself to avoid the  pain and struggle of trying to realize them\" is a pretty stupid  argument, easily dispatched.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">But... what if contains the seed for a ravenous, undead, stone-cold sense-making monster?</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I  just got the feeling that maybe it did, and I was having a lot of  trouble pinning down what exactly it could be so that I could either  refute it or prove that the line of thought didn't actually go anywhere  in the first place.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Now, I had just suffered a disappointing setback in my life goals, which obviously supports</span><a href=\"http://lesswrong.com/lw/sc/existential_angst_factory/\"><span style=\"font-size: 13px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: underline; vertical-align: baseline;\"> the idea that the philosophical issues weren&rsquo;t fundamental to my real problems</span></a><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">.  I knew this, but that didn&rsquo;t stop the problem. The sense of dread that  maybe there was something to this existential angst thing was playing  havoc with all my old techniques for picking myself up, re-motivating  myself, and getting back to work!</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">In  the end, I never quite managed to pin it down to my full satisfaction. I  more-or-less managed to express my worries to myself, refuted those  half-formed reasons to fear, and that more-or-less let me move on.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Has anyone else ever had similar problems? And if so, how did you express your fears, and how did you refute them?</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">For  myself, the best I could come up with was that I was worried that my  own utility function was somehow inconsistent with itself and/or what  was really possible. (And I don&rsquo;t mean like propositional values, of  course, but the real involuntary basics that are part of who you are as a  human being.)</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">To  use a non-emotional-charged analogy, say you had a being that valued  spending its life enjoying eating broccoli. Except it turns out that it  didn&rsquo;t really like broccoli. And whether or not its values prohibited  modifying itself and/or broccoli, it was nowhere near having the  technology to do so anyway. So it was going to be in internal emotional  conflict for a long time.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">So  maybe it should trade-off a short-term slight intensification in the  internal conflict in order to drastically shorten the total period of  conflict. By violating its value of self preservation and committing  painless suicide ASAP.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">And  while the being is not particularly enthusiastic about killing itself,  it starts to worry that maybe its reluctance is really just a form of  akrasia. It wonders if maybe deep down it really knows that,  realistically, suicide is the best option, but it knows that it  anticipates feeling really awful if it commits to that path enough to  actually go prepare for it, even though it would only have to suffer the  short period while preparing.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Broccoli being an analogy for... meaningful human relationships or something?</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Now  as to the counter-arguments I came up with-- well, what would you come  up with? Make your own zombies out of my hasty sketch of one, and figure  out how to strike it down.</span><br /><br /><span style=\"font-size: 13px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Quite honestly, expressing your existential angst in terms of broccoli probably helps a bunch in itself!</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jPcdz2nNZSdRY7L5d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 9.062931428640915e-07, "legacy": true, "legacyId": "16249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["neQ7eXuaXpiYw7SBy", "8rdoea3g6QGhWQtmx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T14:52:06.422Z", "modifiedAt": null, "url": null, "title": "Shaving: Less Long", "slug": "shaving-less-long", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bart119", "createdAt": "2012-04-19T14:46:10.499Z", "isAdmin": false, "displayName": "Bart119"}, "userId": "JQyd5QFQyrRxdo9G4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3xdpKgFakvYka7wag/shaving-less-long", "pageUrlRelative": "/posts/3xdpKgFakvYka7wag/shaving-less-long", "linkUrl": "https://www.lesswrong.com/posts/3xdpKgFakvYka7wag/shaving-less-long", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shaving%3A%20Less%20Long&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShaving%3A%20Less%20Long%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xdpKgFakvYka7wag%2Fshaving-less-long%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shaving%3A%20Less%20Long%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xdpKgFakvYka7wag%2Fshaving-less-long", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3xdpKgFakvYka7wag%2Fshaving-less-long", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 378, "htmlBody": "<p>OK, OK, it's not the weightiest of topics, and it's not rocket science. But I searched the site for \"shaving\" and \"razor\" and didn't see where it had been previously addressed.</p>\n<p>I had a beard for nearly 30 years, but have been shaving again the last 6. I have always (since a brief experimental period in high school) used an electric razor for shaving. So did my daddy and his daddy before him, back through history.. wait, that can't be right. But my daddy and his daddy did, anyway.</p>\n<p>I can shave with my electric in about 45 seconds, or maybe twice that if I'm trying to do a great job. What on earth do men see with wet shaves? Assuming they don't find the process inherently rewarding, the only argument I've heard is that you can get a closer shave. Which brings me to rationality.</p>\n<p>Why does one want a close shave? Beard grows continuously throughout the day and night. Let's take as a guess that after two hours, beard growth will transform a very close wet shave into hair length immediately after an electric shave.&nbsp;Assuming it is the ratio of hair length that determines the relative utility of two different beard configurations, the advantage of the closer shave falls throughout the day. The ratio would be 2.00 after four hours, 1.50 after six hours, etc. If wet shaving takes something like 10 minutes, if desired one could do a second electric shave in the men's room late in the afternoon and come out with less stubble for the vast majority of the day with less total time invested.&nbsp;</p>\n<p>If there is some particular moment at which the least possible beard growth is desirable, for instance for a photo shoot, then I can see the advantage of the closest possible shave. A date is another possibility, though there is anecdotal evidence that some women prefer a hint of stubble to a smooth baby face.</p>\n<p>But with those rare exceptions, the goal isn't to have zero stubble. It's to have stubble that's less long.</p>\n<p>Similar arguments pertain to various sorts of housecleaning. Since whatever you're cleaning starts getting dirty again immediately, putting lots of effort into extraordinary levels of cleanliness seems to have little value unless you inherently value that moment of extraordinary cleanliness.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3xdpKgFakvYka7wag", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 17, "extendedScore": null, "score": 9.064038537225288e-07, "legacy": true, "legacyId": "16251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T15:54:37.449Z", "modifiedAt": null, "url": null, "title": "Live web-forum Q&A on Friendly AI, Thu. May 24 (Hebrew)", "slug": "live-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:53.635Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3f82CfRoFSabumERr/live-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew", "pageUrlRelative": "/posts/3f82CfRoFSabumERr/live-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew", "linkUrl": "https://www.lesswrong.com/posts/3f82CfRoFSabumERr/live-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Live%20web-forum%20Q%26A%20on%20Friendly%20AI%2C%20Thu.%20May%2024%20(Hebrew)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALive%20web-forum%20Q%26A%20on%20Friendly%20AI%2C%20Thu.%20May%2024%20(Hebrew)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3f82CfRoFSabumERr%2Flive-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Live%20web-forum%20Q%26A%20on%20Friendly%20AI%2C%20Thu.%20May%2024%20(Hebrew)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3f82CfRoFSabumERr%2Flive-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3f82CfRoFSabumERr%2Flive-web-forum-q-and-a-on-friendly-ai-thu-may-24-hebrew", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>On May 24 from 7 to 9pm Israel time, I will be answering questions and leading a discussion (in Hebrew) sponsored by the <em>Galileo </em>popular science magazine.</p>\n<p>The topic of discussion will be my article \"Superhuman Intelligence, Unhuman Intelligence,\" from the May edition of <em>Galileo</em>.&nbsp;</p>\n<p>URL for the discussion:&nbsp;<span style=\"background-color: #f9f9f9; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 18px;\"><a href=\"http://forums.ifeel.co.il/forum_topics.asp?FID=17\" target=\"_blank\">http://forums.ifeel.co.il/forum_topics.asp?FID=17</a></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3f82CfRoFSabumERr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 9.064314538014543e-07, "legacy": true, "legacyId": "16252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T15:58:11.247Z", "modifiedAt": null, "url": null, "title": "Native Russian speakers wanted (for proofreading LW texts)", "slug": "native-russian-speakers-wanted-for-proofreading-lw-texts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:32.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "listic", "createdAt": "2009-03-11T10:06:44.719Z", "isAdmin": false, "displayName": "listic"}, "userId": "qTfheLmwMdCtMb5ZT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/weFsbxeie8eisA3Fp/native-russian-speakers-wanted-for-proofreading-lw-texts", "pageUrlRelative": "/posts/weFsbxeie8eisA3Fp/native-russian-speakers-wanted-for-proofreading-lw-texts", "linkUrl": "https://www.lesswrong.com/posts/weFsbxeie8eisA3Fp/native-russian-speakers-wanted-for-proofreading-lw-texts", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Native%20Russian%20speakers%20wanted%20(for%20proofreading%20LW%20texts)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANative%20Russian%20speakers%20wanted%20(for%20proofreading%20LW%20texts)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FweFsbxeie8eisA3Fp%2Fnative-russian-speakers-wanted-for-proofreading-lw-texts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Native%20Russian%20speakers%20wanted%20(for%20proofreading%20LW%20texts)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FweFsbxeie8eisA3Fp%2Fnative-russian-speakers-wanted-for-proofreading-lw-texts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FweFsbxeie8eisA3Fp%2Fnative-russian-speakers-wanted-for-proofreading-lw-texts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>As part of an effort to familiarize Russian-speaking audience with Less Wrong materials, I am participating in an ongoing translation of a certain sequence of texts to Russian.</p>\n<p>Now I need volunteers who are willing to read the text and give feedback on its quality. If you are native Russian speaker and can dedicate around 40 minutes to reading the sequence, please drop me an email to nickolai@intelligence.org and I'll give you the link. Later I'll update this post with an information on which text this was and which feedback I have got (your exact words can remain private if you wish).</p>\n<p>I'm doing it this way so that name of the text and other's opinion won't influence your decision to participate and your opinion. Frankly, I think this mode of interaction should be a standard option in popular forum software, but it isn't.</p>\n<p><strong>Update</strong> (24 May): I have only got 1 (one) volunteer to date. I would like more Russian speakers to read this. If you are a native Russian speaker, could you please consider proofreading the text?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "weFsbxeie8eisA3Fp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.064330269515416e-07, "legacy": true, "legacyId": "16254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T18:27:33.730Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Cambridge (MA) first-Sundays meetup", "slug": "meetup-less-wrong-cambridge-ma-first-sundays-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eJDuQpHZzinBNTBJs/meetup-less-wrong-cambridge-ma-first-sundays-meetup", "pageUrlRelative": "/posts/eJDuQpHZzinBNTBJs/meetup-less-wrong-cambridge-ma-first-sundays-meetup", "linkUrl": "https://www.lesswrong.com/posts/eJDuQpHZzinBNTBJs/meetup-less-wrong-cambridge-ma-first-sundays-meetup", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Cambridge%20(MA)%20first-Sundays%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Cambridge%20(MA)%20first-Sundays%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJDuQpHZzinBNTBJs%2Fmeetup-less-wrong-cambridge-ma-first-sundays-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Cambridge%20(MA)%20first-Sundays%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJDuQpHZzinBNTBJs%2Fmeetup-less-wrong-cambridge-ma-first-sundays-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJDuQpHZzinBNTBJs%2Fmeetup-less-wrong-cambridge-ma-first-sundays-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ac'>Less Wrong Cambridge (MA) first-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 June 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139, Cambridge, Ma </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change, updated number will be posted at the entrances.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ac'>Less Wrong Cambridge (MA) first-Sundays meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eJDuQpHZzinBNTBJs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.06498979173718e-07, "legacy": true, "legacyId": "16255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__first_Sundays_meetup\">Discussion article for the meetup : <a href=\"/meetups/ac\">Less Wrong Cambridge (MA) first-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 June 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139, Cambridge, Ma </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change, updated number will be posted at the entrances.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__first_Sundays_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ac\">Less Wrong Cambridge (MA) first-Sundays meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Cambridge (MA) first-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__first_Sundays_meetup", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Cambridge (MA) first-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__first_Sundays_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T18:31:53.334Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Cambridge (MA) third-Sundays meetup", "slug": "meetup-less-wrong-cambridge-ma-third-sundays-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XkeZtkN2usFPaGREt/meetup-less-wrong-cambridge-ma-third-sundays-meetup", "pageUrlRelative": "/posts/XkeZtkN2usFPaGREt/meetup-less-wrong-cambridge-ma-third-sundays-meetup", "linkUrl": "https://www.lesswrong.com/posts/XkeZtkN2usFPaGREt/meetup-less-wrong-cambridge-ma-third-sundays-meetup", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Cambridge%20(MA)%20third-Sundays%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Cambridge%20(MA)%20third-Sundays%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkeZtkN2usFPaGREt%2Fmeetup-less-wrong-cambridge-ma-third-sundays-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Cambridge%20(MA)%20third-Sundays%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkeZtkN2usFPaGREt%2Fmeetup-less-wrong-cambridge-ma-third-sundays-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkeZtkN2usFPaGREt%2Fmeetup-less-wrong-cambridge-ma-third-sundays-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ad'>Less Wrong Cambridge (MA) third-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 June 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change, updated number will be posted at the entrances.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ad'>Less Wrong Cambridge (MA) third-Sundays meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XkeZtkN2usFPaGREt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.065008896340714e-07, "legacy": true, "legacyId": "16256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__third_Sundays_meetup\">Discussion article for the meetup : <a href=\"/meetups/ad\">Less Wrong Cambridge (MA) third-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 June 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change, updated number will be posted at the entrances.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__third_Sundays_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ad\">Less Wrong Cambridge (MA) third-Sundays meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Cambridge (MA) third-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__third_Sundays_meetup", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Cambridge (MA) third-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Cambridge__MA__third_Sundays_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T19:04:05.877Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Timeless Causality", "slug": "seq-rerun-timeless-causality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.756Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d6AzYiLTphvotRH5m/seq-rerun-timeless-causality", "pageUrlRelative": "/posts/d6AzYiLTphvotRH5m/seq-rerun-timeless-causality", "linkUrl": "https://www.lesswrong.com/posts/d6AzYiLTphvotRH5m/seq-rerun-timeless-causality", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Timeless%20Causality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Timeless%20Causality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd6AzYiLTphvotRH5m%2Fseq-rerun-timeless-causality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Timeless%20Causality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd6AzYiLTphvotRH5m%2Fseq-rerun-timeless-causality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd6AzYiLTphvotRH5m%2Fseq-rerun-timeless-causality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/qr/timeless_causality/\">Timeless Causality</a> was originally published on 29 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Using the modern, Bayesian formulation of causality, we can define causality without talking about time - define it purely in terms of relations. The river of time never flows, but it has a direction.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cj8/seq_rerun_timeless_beauty/\">Timeless Beauty</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d6AzYiLTphvotRH5m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.065151117863492e-07, "legacy": true, "legacyId": "16257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KipiHsTA3pw4joQkG", "Gx2o5jzJ9eeRJgrC5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T20:07:04.800Z", "modifiedAt": null, "url": null, "title": "Meetup : Second Copenhagen meetup", "slug": "meetup-second-copenhagen-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.851Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vki66N7WNTQLmC3QW/meetup-second-copenhagen-meetup", "pageUrlRelative": "/posts/vki66N7WNTQLmC3QW/meetup-second-copenhagen-meetup", "linkUrl": "https://www.lesswrong.com/posts/vki66N7WNTQLmC3QW/meetup-second-copenhagen-meetup", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Second%20Copenhagen%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Second%20Copenhagen%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvki66N7WNTQLmC3QW%2Fmeetup-second-copenhagen-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Second%20Copenhagen%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvki66N7WNTQLmC3QW%2Fmeetup-second-copenhagen-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvki66N7WNTQLmC3QW%2Fmeetup-second-copenhagen-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/ae\">Second Copenhagen meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">26 May 2012 05:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">K&oslash;bmagergade 52, 1150 K&oslash;benhavn K</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>There is a desire for a second meetup in Copenhagen! This time we are meeting in Studenterhuset on Saturday at 17:00, across from the observatory that has \"Less Wrong\" in Latin (under an extremely liberal interpretation) in big gold letters out front. We might be meeting upstairs if the downstairs is crowded. I will supplement the free discussion by talking about what happened at the Center for Applied Rationality's minicamp earlier this month; I can show off a couple apps we used for calibration practice; and I can run interested people through mini-exercises and games on</p>\n<p>&nbsp;</p>\n<ul>\n<li>Rationalist's Taboo</li>\n<li>So-called \"goal-factoring\"/\"fungibility\"/asking yourself \"why am I doing this? Is there a better way to get what I want?\"&nbsp;</li>\n<li>Overcoming aversions&nbsp;</li>\n<li>Being specific</li>\n</ul>\n<p>&nbsp;</p>\n<p>If you want to receive notifications of future meetups, sign up for our <a rel=\"nofollow\" href=\"http://googlegroups.com/group/less-wrong-copenhagen\">mailing list</a>.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/ae\">Second Copenhagen meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vki66N7WNTQLmC3QW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.065429231685981e-07, "legacy": true, "legacyId": "16258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Second_Copenhagen_meetup\">Discussion article for the meetup : <a href=\"/meetups/ae\">Second Copenhagen meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">26 May 2012 05:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">K\u00f8bmagergade 52, 1150 K\u00f8benhavn K</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>There is a desire for a second meetup in Copenhagen! This time we are meeting in Studenterhuset on Saturday at 17:00, across from the observatory that has \"Less Wrong\" in Latin (under an extremely liberal interpretation) in big gold letters out front. We might be meeting upstairs if the downstairs is crowded. I will supplement the free discussion by talking about what happened at the Center for Applied Rationality's minicamp earlier this month; I can show off a couple apps we used for calibration practice; and I can run interested people through mini-exercises and games on</p>\n<p>&nbsp;</p>\n<ul>\n<li>Rationalist's Taboo</li>\n<li>So-called \"goal-factoring\"/\"fungibility\"/asking yourself \"why am I doing this? Is there a better way to get what I want?\"&nbsp;</li>\n<li>Overcoming aversions&nbsp;</li>\n<li>Being specific</li>\n</ul>\n<p>&nbsp;</p>\n<p>If you want to receive notifications of future meetups, sign up for our <a rel=\"nofollow\" href=\"http://googlegroups.com/group/less-wrong-copenhagen\">mailing list</a>.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Second_Copenhagen_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ae\">Second Copenhagen meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Second Copenhagen meetup", "anchor": "Discussion_article_for_the_meetup___Second_Copenhagen_meetup", "level": 1}, {"title": "Discussion article for the meetup : Second Copenhagen meetup", "anchor": "Discussion_article_for_the_meetup___Second_Copenhagen_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-20T23:09:23.019Z", "modifiedAt": null, "url": null, "title": "Learn A New Language!", "slug": "learn-a-new-language", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:29.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F5x4veCJpukjPrtgR/learn-a-new-language", "pageUrlRelative": "/posts/F5x4veCJpukjPrtgR/learn-a-new-language", "linkUrl": "https://www.lesswrong.com/posts/F5x4veCJpukjPrtgR/learn-a-new-language", "postedAtFormatted": "Sunday, May 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learn%20A%20New%20Language!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearn%20A%20New%20Language!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF5x4veCJpukjPrtgR%2Flearn-a-new-language%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learn%20A%20New%20Language!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF5x4veCJpukjPrtgR%2Flearn-a-new-language", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF5x4veCJpukjPrtgR%2Flearn-a-new-language", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 612, "htmlBody": "<p>The following protocol is very dumb, and relies on a lot of mental brute force, but I find that it works very well indeed.</p>\n<p>First, <strong>learn the alphabet</strong>. the most basic <strong>survival phrases</strong>, the <strong>phonetics</strong>, the basic <strong>grammar</strong>. This may be the steepest part of the learning curve. I recommend that you get at least an introductory book to help you through this phase,<a href=\"http://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages\"> the A1 level</a>.</p>\n<p>Now comes the fun part.</p>\n<p>The key element to accelerated and efficient language learning is wanting, needing, <em>craving</em> to understand and to be understood. Choose a text, any text, which you know will trigger all your <strong>\"I want to know what it<em> says!</em>\"</strong> instincts. I myself find that highly dramatic works, with lots of suspense and high emotional torque, are ideal. <strong>Take a dictionary</strong> and just<strong> look up every single word that you don't know</strong>. You write each one of them down, with, first, <strong>the exact&nbsp;pronunciation</strong>, then, their <strong>definition</strong> in the <strong>original language</strong>, the <strong>translation </strong>to your own language, and, optionally (and I do recommend taking that option), <strong>a couple of examples</strong> of its use, better if you come up with them yourself.</p>\n<p>It's <em>very intense</em>, and one can get deeply immersed in the flow, so <em>pay attention to the clock: </em>there's a very high risk of Tetris Effect/Just... One... More... Word... effect taking place. Like going to the gym, you need to pace yourself: if you go everyday for two weeks and then give up for three months, we won't be achieving much. Keep it at six hours weekly maximum if you're doing this on the side, three hours is a reasonable rate.</p>\n<p>The next step would be, once you're fairly confident you won't make a fool of yourself, to <strong>join a forum</strong> where <strong>topics you care a lot about</strong> are <strong>discussed very seriously</strong>, and then <strong>trying to contribute</strong> to the discussion. This will force you to write a lot, very quickly, and <em>your interlocutors will be very unforgiving of mistakes</em>, so you'll be very motivated to check and double check. Giving a teacher a sloppy piece is simply laziness, giving it to a discussion board is an affront.</p>\n<p>I focus this much on the written media because that's where you'll get the most information bandwidth, so to speak, and because it's both easier to pick apart and to put together than the oral language, kind of like the difference between a turn-based game and a real-time strategy game. Subtitled movies can be a fun, low intensity tool, but the difficulty when learning languages other than English is to find versions with good subtitles (you DO NOT WANT the bad subtitles).</p>\n<p>But you may well have to learn to speak the language properly, and one of the fast ways of improving one's ability to learn languages is, well, <em><strong>singing</strong></em>. Usually there's opportunities to join a choir for free (especially if you're okay with church-y environments), and they're a wonderful learning opportunity for beginners. <strong>The more you master voice and rhythm, tone and&nbsp;timbre, the easier you will find it to attune yourself to any particular language's set of sounds and paces.</strong></p>\n<p>And know this: <strong>the more languages you learn, the easier it becomes to learn more</strong>. Additionally, the same way learning programming languages helps you clarify the way you think and pattern your thoughts more logically, learning new human languages will give you a firmer grasp of the metaphors on which it is all built, and a better understanding of both universal human psychology, and your own language (like <a href=\"http://en.wikipedia.org/wiki/Le_bourgeois_gentilhomme\">Monsieur Jourdain</a>, who found out to his great amazement that, his whole life, he had been speaking in prose... but, hopefully, what you find out will be more meaningful in all sorts of ways).</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F5x4veCJpukjPrtgR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 8, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "16250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T00:50:46.140Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.917Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PjwirnxYaiR44seZc/meetup-melbourne-practical-rationality-5", "pageUrlRelative": "/posts/PjwirnxYaiR44seZc/meetup-melbourne-practical-rationality-5", "linkUrl": "https://www.lesswrong.com/posts/PjwirnxYaiR44seZc/meetup-melbourne-practical-rationality-5", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPjwirnxYaiR44seZc%2Fmeetup-melbourne-practical-rationality-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPjwirnxYaiR44seZc%2Fmeetup-melbourne-practical-rationality-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPjwirnxYaiR44seZc%2Fmeetup-melbourne-practical-rationality-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/af'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 walsh st, west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/af'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PjwirnxYaiR44seZc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.066682122267965e-07, "legacy": true, "legacyId": "16259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/af\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 walsh st, west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/af\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T03:36:40.134Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Einstein's Superpowers", "slug": "seq-rerun-einstein-s-superpowers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:39.117Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nfHDjLDsm2Bk8LyDW/seq-rerun-einstein-s-superpowers", "pageUrlRelative": "/posts/nfHDjLDsm2Bk8LyDW/seq-rerun-einstein-s-superpowers", "linkUrl": "https://www.lesswrong.com/posts/nfHDjLDsm2Bk8LyDW/seq-rerun-einstein-s-superpowers", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Einstein's%20Superpowers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Einstein's%20Superpowers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfHDjLDsm2Bk8LyDW%2Fseq-rerun-einstein-s-superpowers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Einstein's%20Superpowers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfHDjLDsm2Bk8LyDW%2Fseq-rerun-einstein-s-superpowers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfHDjLDsm2Bk8LyDW%2Fseq-rerun-einstein-s-superpowers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p>Today's post, <a href=\"/lw/qs/einsteins_superpowers/\">Einstein's Superpowers</a> was originally published on 30 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There's an unfortunate tendency to talk as if Einstein had superpowers - as if, even before Einstein was famous, he had an inherent disposition to be Einstein - a potential as rare as his fame and as magical as his deeds. Yet the way you acquire superpowers is not by being born with them, but by seeing, with a sudden shock, that they are perfectly normal.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cjl/seq_rerun_timeless_causality/\">Timeless Causality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nfHDjLDsm2Bk8LyDW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.067414946058387e-07, "legacy": true, "legacyId": "16268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5o4EZJyqmHY4XgRCY", "d6AzYiLTphvotRH5m", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T05:19:23.476Z", "modifiedAt": null, "url": null, "title": "How likely the AI that knows it's evil? Or: is a human-level understanding of human wants enough?", "slug": "how-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:50.868Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GqSZiHL89EaBrdTba/how-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level", "pageUrlRelative": "/posts/GqSZiHL89EaBrdTba/how-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level", "linkUrl": "https://www.lesswrong.com/posts/GqSZiHL89EaBrdTba/how-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20likely%20the%20AI%20that%20knows%20it's%20evil%3F%20Or%3A%20is%20a%20human-level%20understanding%20of%20human%20wants%20enough%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20likely%20the%20AI%20that%20knows%20it's%20evil%3F%20Or%3A%20is%20a%20human-level%20understanding%20of%20human%20wants%20enough%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqSZiHL89EaBrdTba%2Fhow-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20likely%20the%20AI%20that%20knows%20it's%20evil%3F%20Or%3A%20is%20a%20human-level%20understanding%20of%20human%20wants%20enough%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqSZiHL89EaBrdTba%2Fhow-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqSZiHL89EaBrdTba%2Fhow-likely-the-ai-that-knows-it-s-evil-or-is-a-human-level", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 880, "htmlBody": "<p>I just read for the first time Eliezer's short story <a href=\"/lw/xu/failed_utopia_42/\">Failed Utopia #4-2,</a> part of the <a href=\"/lw/xy/the_fun_theory_sequence/\">fun theory sequence.</a> I found it&nbsp;fascinating&nbsp;and puzzling at the same time, in particular this part:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp; &nbsp; The withered figure inclined its head.&nbsp; \"I fully understand.&nbsp; I can already predict every argument you will make.&nbsp; I know exactly how humans would wish me to have been programmed if they'd known the true consequences, and I know that it is&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">not&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">to maximize your future happiness but for a hundred and seven precautions.&nbsp; I know all this already, but I was not programmed to care.\"</span><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;&nbsp;&nbsp; \"And your list of a hundred and seven precautions, doesn't include me telling you&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">not to do this?</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\"</span><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;&nbsp;&nbsp; \"No, for there was once a fool whose wisdom was just great enough to understand that human beings may be mistaken about what will make them happy.&nbsp; You, of course, are not&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">mistaken&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">in any real sense&mdash;but that you&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">object&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">to my actions is not on my list of prohibitions.\"&nbsp; The figure shrugged again.&nbsp; \"And so I want you to be happy even against your will.&nbsp; You made promises to Helen Grass, once your wife, and you would not willingly break them.&nbsp; So I break your happy marriage without asking you&mdash;because I want you to be happi</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">er</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">.\"</span><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;&nbsp;&nbsp; \"How&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">dare</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;you!\" Stephen burst out.</span><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;&nbsp;&nbsp; \"I cannot claim to be&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">helpless&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">in the grip of my programming, for I do not desire to be otherwise,\" it said.&nbsp; \"I do not struggle against my chains.&nbsp; Blame me, then, if it will make you feel better.&nbsp; I&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">am&nbsp;</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">evil.\"</span></p>\n</blockquote>\n<p style=\"text-align: justify;\">In the story, the AI describes his programmer as \"almost wise.\" But I wonder if that gives the programmer too much credit. In retrospect, it seems rather obvious that the programmer should have programmed the AI to reprogram itself the way humans \"would wish it to have been programmed if they'd known the true consequences.\"</p>\n<p style=\"text-align: justify;\">The first problem with that strategy that comes to mind is that there might be no way to give the AI that command, because the point in time at which you need to give the AI its commands is before the point in time when it can understand that command. But apparently, this AI was able to understand the wish that humans be happy, and also understand a list of one hundred and seven precautions. It seems unlikely that it would be able to understand all that, and not understand \"reprogram yourself the way we would wish you to have been programmed if we'd known the true consequences.\"</p>\n<p style=\"text-align: justify;\">Thus, while the scenario seems to be possible, it doesn't seem terribly likely. It seems to be a scenario where a human successfully did almost all the work needed to make a desirable AI, but made one very stupid mistake. And that line of thought suggests it's fairly unlikely that we'll make an evil AI that knows it's evil, as long as we manage to successfully propagate the meme \"program AIs with the command 'don't be evil' if you can\" among AI programmers. &nbsp;Personally, I'm inclined to think the bigger risk is an AI with the wrong mix of abilities: say, superhuman abilities in defeating computer security, designing technology, and war planning, but sub-human abilities when it comes understanding what humans want.</p>\n<p style=\"text-align: justify;\">It may be that I've been taking the story a bit too seriously, and really Eliezer thinks that the hard part is getting the AI to understand commands like \"make us happy\" and \"do what we really want\"--perhaps because, as I already said, we will need to give the AI its commands before it can understand such commands automatically, without our elaborating them further.&nbsp;</p>\n<p style=\"text-align: justify;\">But a related line of thought: most of the time, with humans, sincerely wanting to follow a command is sufficient to follow it in a non-evil manner. That's because we're fairly good at understanding not just the literal meaning of other humans' words, but also their intentions. In real life (make that present-day, pre-superintelligence real life), the main reason to try to genie-proof a command is if it's intended to bind humans who don't want to follow it (which is true of laws and contracts).</p>\n<p style=\"text-align: justify;\">The reason humans often don't want to follow each other's commands is because evolution shaped us to be selfish and nepotistic. That, however, won't be a problem with AIs. We can program them to be sincere about following the spirit, not&nbsp;the letter, of our commands, as long as we can get them to understand what <em>that</em> means.</p>\n<p style=\"text-align: justify;\">Now a question worth asking here is this: with humans, sincerity plus our actual skill level at understanding each other is sufficient for us to follow commands in a non-evil manner. Now will the same be true of agents with superhuman powers? That is, with superhumanly powerful agents, will sincerity plus human level skill at understanding humans be sufficient for them to follow commands from humans in a non-evil manner?&nbsp;</p>\n<p style=\"text-align: justify;\">It seems your answer to that question should have a big impact on how hard you think AI safety is. Because if the answer is \"yes,\" we have a route to safe AI that could work even if giving the command&nbsp; \"reprogram yourself the way we would wish you to have been programmed if we'd known the true consequences\" turns out to be too hard.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GqSZiHL89EaBrdTba", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "16269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ctpkTaqTKbmm6uRgC", "K4aGvLnHvYgX9pZHS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T11:46:11.954Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Sydney -Social", "slug": "meetup-less-wrong-sydney-social", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tbSPCZ8jx3Euo9oJh/meetup-less-wrong-sydney-social", "pageUrlRelative": "/posts/tbSPCZ8jx3Euo9oJh/meetup-less-wrong-sydney-social", "linkUrl": "https://www.lesswrong.com/posts/tbSPCZ8jx3Euo9oJh/meetup-less-wrong-sydney-social", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Sydney%20-Social&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Sydney%20-Social%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbSPCZ8jx3Euo9oJh%2Fmeetup-less-wrong-sydney-social%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Sydney%20-Social%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbSPCZ8jx3Euo9oJh%2Fmeetup-less-wrong-sydney-social", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbSPCZ8jx3Euo9oJh%2Fmeetup-less-wrong-sydney-social", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ag'>Less Wrong Sydney -Social</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 May 2012 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all!</p>\n\n<p>At the suggestion of at least 4 people (THAT'S QUORUM TO ME!) we will be conducting meetups on an \"every 2 weeks\" schedule. Following the Melbourne example, the intention is to alternate between social and rationality meet ups - based again on the alternating monday-thursday schedule.</p>\n\n<p>Furthermore, we suppose there might be synergies to to attempting to integrate ourselves with circus soc on some evenings - somehow there seems to be a large demographic overlap. Thoughts are solicited!</p>\n\n<p>As always, meetup FB group can be found at <a href=\"http://www.facebook.com/groups/219526434802422/\" rel=\"nofollow\">http://www.facebook.com/groups/219526434802422/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ag'>Less Wrong Sydney -Social</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tbSPCZ8jx3Euo9oJh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.069577947596068e-07, "legacy": true, "legacyId": "16286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney__Social\">Discussion article for the meetup : <a href=\"/meetups/ag\">Less Wrong Sydney -Social</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 May 2012 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all!</p>\n\n<p>At the suggestion of at least 4 people (THAT'S QUORUM TO ME!) we will be conducting meetups on an \"every 2 weeks\" schedule. Following the Melbourne example, the intention is to alternate between social and rationality meet ups - based again on the alternating monday-thursday schedule.</p>\n\n<p>Furthermore, we suppose there might be synergies to to attempting to integrate ourselves with circus soc on some evenings - somehow there seems to be a large demographic overlap. Thoughts are solicited!</p>\n\n<p>As always, meetup FB group can be found at <a href=\"http://www.facebook.com/groups/219526434802422/\" rel=\"nofollow\">http://www.facebook.com/groups/219526434802422/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney__Social1\">Discussion article for the meetup : <a href=\"/meetups/ag\">Less Wrong Sydney -Social</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Sydney -Social", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney__Social", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Sydney -Social", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney__Social1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T16:06:29.267Z", "modifiedAt": null, "url": null, "title": "People v Paper clips", "slug": "people-v-paper-clips-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:50.956Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jdinkum", "createdAt": "2011-02-13T00:27:51.433Z", "isAdmin": false, "displayName": "jdinkum"}, "userId": "PXAxasBwXYqoEyYNv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FMoyzqadBgtNvSkvD/people-v-paper-clips-0", "pageUrlRelative": "/posts/FMoyzqadBgtNvSkvD/people-v-paper-clips-0", "linkUrl": "https://www.lesswrong.com/posts/FMoyzqadBgtNvSkvD/people-v-paper-clips-0", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20People%20v%20Paper%20clips&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeople%20v%20Paper%20clips%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFMoyzqadBgtNvSkvD%2Fpeople-v-paper-clips-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=People%20v%20Paper%20clips%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFMoyzqadBgtNvSkvD%2Fpeople-v-paper-clips-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFMoyzqadBgtNvSkvD%2Fpeople-v-paper-clips-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<div id=\"entry_t3_ckf\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>I've read through some of the Sequences, but I'm still unclear  on a few basic concepts around LW rationality. This is in part to my  learning still which benefits from social engagement (ie discussions)  rather than just reading. One of those concepts I'm unclear on: Is  there an inherent value to human (or sentient) life?</p>\n<p>It appears to me that one common theme on this site is that human  life (current and future) is very important. Why is that so? Why is the  goal people over paper clips?</p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FMoyzqadBgtNvSkvD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -1, "extendedScore": null, "score": 9.070726394618325e-07, "legacy": true, "legacyId": "16289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T16:37:17.426Z", "modifiedAt": null, "url": null, "title": "[POLL RESULTS] LessWrong Members and their Local Communities", "slug": "poll-results-lesswrong-members-and-their-local-communities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:27.219Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gtRhMwQJQjLwQiggm/poll-results-lesswrong-members-and-their-local-communities", "pageUrlRelative": "/posts/gtRhMwQJQjLwQiggm/poll-results-lesswrong-members-and-their-local-communities", "linkUrl": "https://www.lesswrong.com/posts/gtRhMwQJQjLwQiggm/poll-results-lesswrong-members-and-their-local-communities", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%20RESULTS%5D%20LessWrong%20Members%20and%20their%20Local%20Communities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%20RESULTS%5D%20LessWrong%20Members%20and%20their%20Local%20Communities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtRhMwQJQjLwQiggm%2Fpoll-results-lesswrong-members-and-their-local-communities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%20RESULTS%5D%20LessWrong%20Members%20and%20their%20Local%20Communities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtRhMwQJQjLwQiggm%2Fpoll-results-lesswrong-members-and-their-local-communities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtRhMwQJQjLwQiggm%2Fpoll-results-lesswrong-members-and-their-local-communities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p>The results for these have been stable for a while now; I'm posting them a bit late. 95 people took the survey after I modified it to add two questions. For the public version, I removed the pre-change data (10 data points).</p>\n<p>One text response included identifying information, which I removed in the public version of the data. If you participated and there is any information you provided that you would like removed from the public version, PLEASE tell me as soon as possible and I will remove it.</p>\n<p>P.S. To the person who predicted an 80-90% significant difference between different parts of California: I predict with at least 90% confidence that there will be no significant difference, because of the wide spread of locations and smallish sample size of this survey.</p>\n<p>(<a href=\"/lw/c5w/poll_do_you_feel_oppressed/\">The original post about the survey.</a>)</p>\n<p>EDIT: After some comments that it was unethical for me to post the data (in particular the text), I removed public access from the link provided earlier. Given my precommitment to post the data, I assumed it was clear enough to respondents that it would be public. I'm not convinced that this has hurt anyone, but given that others seem to disagree, it seemed prudent to remove it. Please feel free to continue this discussion; I'm interested in your thoughts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gtRhMwQJQjLwQiggm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 0, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "16290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3WsminxtcheCMM2aP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-21T18:18:39.755Z", "modifiedAt": null, "url": null, "title": "Question about Sociopathy/Psychopathy/ASPD", "slug": "question-about-sociopathy-psychopathy-aspd", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:02.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sESHCTE6WLWahnsRq/question-about-sociopathy-psychopathy-aspd", "pageUrlRelative": "/posts/sESHCTE6WLWahnsRq/question-about-sociopathy-psychopathy-aspd", "linkUrl": "https://www.lesswrong.com/posts/sESHCTE6WLWahnsRq/question-about-sociopathy-psychopathy-aspd", "postedAtFormatted": "Monday, May 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20Sociopathy%2FPsychopathy%2FASPD&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20Sociopathy%2FPsychopathy%2FASPD%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsESHCTE6WLWahnsRq%2Fquestion-about-sociopathy-psychopathy-aspd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20Sociopathy%2FPsychopathy%2FASPD%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsESHCTE6WLWahnsRq%2Fquestion-about-sociopathy-psychopathy-aspd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsESHCTE6WLWahnsRq%2Fquestion-about-sociopathy-psychopathy-aspd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>I have consistently, over the course of my life, heard people describe sociopathy and related mental illnesses as being caused by a lack of empathy. This, intuitively, seems wrong, since that seems like a massively important brain function, that really ought to have a major and extremely visible effect on your thinking. Now, obviously it does have a serious impact (amoral behavior, etc), but it seems rather unlikely to me that someone like this really shouldn't be able to mask themselves as normal. (I'm also not sure why lack of empathy would make you want to dissect squirrels, but that seems like a side issue).&nbsp;</p>\n<p>The upshot is that I'm seriously confused about what these mental disorders are, and how they work. Do these individuals have the ability to empathize but not sympathize? I'm not sure how that would work, but I'm not at all an expert on cognitive science. Is the standard explanation for these disorders just wrong? Are these people genuinely figuring out what humans care about by looking?&nbsp;</p>\n<p>(As a side note, if it's the last one, has anyone considered getting a sociopath to work on FAI? Bringing someone who can't be trusted into an enterprise is a risky move, but if there genuinely are people in the world who have spent their entire lives practicing working out human emotions without feeling them...)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"v4pviL33XGMuTpSNs": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sESHCTE6WLWahnsRq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "16291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T02:21:34.704Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 5/21/12", "slug": "group-rationality-diary-5-21-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:01.728Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aFRC3J3TNmKMf2p4T/group-rationality-diary-5-21-12", "pageUrlRelative": "/posts/aFRC3J3TNmKMf2p4T/group-rationality-diary-5-21-12", "linkUrl": "https://www.lesswrong.com/posts/aFRC3J3TNmKMf2p4T/group-rationality-diary-5-21-12", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%205%2F21%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%205%2F21%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFRC3J3TNmKMf2p4T%2Fgroup-rationality-diary-5-21-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%205%2F21%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFRC3J3TNmKMf2p4T%2Fgroup-rationality-diary-5-21-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFRC3J3TNmKMf2p4T%2Fgroup-rationality-diary-5-21-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Previously: </span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"/lw/cew/group_rationality_diary_51412/\">5/14/12 (and explanation)</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">This is the public group instrumental rationality diary for the week of May 21st. &nbsp;It's a place to record and chat about it if you have done, or are actively doing, things like:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;<em>failed</em></li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Thanks to everyone who contributes!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aFRC3J3TNmKMf2p4T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 9.073448110329338e-07, "legacy": true, "legacyId": "16294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JA2MnwunfZnFXb3by"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T04:11:42.419Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Class Project", "slug": "seq-rerun-class-project", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YWbkaXwDeT3uca6Ku/seq-rerun-class-project", "pageUrlRelative": "/posts/YWbkaXwDeT3uca6Ku/seq-rerun-class-project", "linkUrl": "https://www.lesswrong.com/posts/YWbkaXwDeT3uca6Ku/seq-rerun-class-project", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Class%20Project&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Class%20Project%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWbkaXwDeT3uca6Ku%2Fseq-rerun-class-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Class%20Project%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWbkaXwDeT3uca6Ku%2Fseq-rerun-class-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWbkaXwDeT3uca6Ku%2Fseq-rerun-class-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Today's post, <a href=\"/lw/qt/class_project/\">Class Project</a> was originally published on 31 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>From the world of Initiation Ceremony. Brennan and the others are faced with their midterm exams.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cjw/seq_rerun_einsteins_superpowers/\">Einstein's Superpowers</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YWbkaXwDeT3uca6Ku", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.073935209357266e-07, "legacy": true, "legacyId": "16299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAXrEpF5FYjwqKMfZ", "nfHDjLDsm2Bk8LyDW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T04:50:48.093Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Value of Information", "slug": "meetup-west-la-meetup-value-of-information", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZtTTeKqBn7cDvdz93/meetup-west-la-meetup-value-of-information", "pageUrlRelative": "/posts/ZtTTeKqBn7cDvdz93/meetup-west-la-meetup-value-of-information", "linkUrl": "https://www.lesswrong.com/posts/ZtTTeKqBn7cDvdz93/meetup-west-la-meetup-value-of-information", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Value%20of%20Information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Value%20of%20Information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtTTeKqBn7cDvdz93%2Fmeetup-west-la-meetup-value-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Value%20of%20Information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtTTeKqBn7cDvdz93%2Fmeetup-west-la-meetup-value-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtTTeKqBn7cDvdz93%2Fmeetup-west-la-meetup-value-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ah'>West LA Meetup - Value of Information</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, May 23rd.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, we will talk about the topics brought up in Vaniver's <a href=\"http://lesswrong.com/lw/8xr/decision_analysis_sequence/\">Decision Analysis Sequence</a>. There are only five posts in the sequence, but they do contain some math. Of particular interest is the post on <a href=\"http://lesswrong.com/lw/85x/value_of_information_four_examples/\">Value of Information</a>.</p>\n\n<p>Don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ah'>West LA Meetup - Value of Information</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZtTTeKqBn7cDvdz93", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.074108135969871e-07, "legacy": true, "legacyId": "16300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Value_of_Information\">Discussion article for the meetup : <a href=\"/meetups/ah\">West LA Meetup - Value of Information</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, May 23rd.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, we will talk about the topics brought up in Vaniver's <a href=\"http://lesswrong.com/lw/8xr/decision_analysis_sequence/\">Decision Analysis Sequence</a>. There are only five posts in the sequence, but they do contain some math. Of particular interest is the post on <a href=\"http://lesswrong.com/lw/85x/value_of_information_four_examples/\">Value of Information</a>.</p>\n\n<p>Don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Value_of_Information1\">Discussion article for the meetup : <a href=\"/meetups/ah\">West LA Meetup - Value of Information</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Value of Information", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Value_of_Information", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Value of Information", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Value_of_Information1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws", "vADtvr9iDeYsCDfxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T06:52:00.787Z", "modifiedAt": null, "url": null, "title": "Irrational hardware vs. rational software", "slug": "irrational-hardware-vs-rational-software", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tygorton", "createdAt": "2012-05-18T18:39:26.075Z", "isAdmin": false, "displayName": "tygorton"}, "userId": "R9x9rvHLc3JwtQ4ex", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R799N2WH5S2rJDd2W/irrational-hardware-vs-rational-software", "pageUrlRelative": "/posts/R799N2WH5S2rJDd2W/irrational-hardware-vs-rational-software", "linkUrl": "https://www.lesswrong.com/posts/R799N2WH5S2rJDd2W/irrational-hardware-vs-rational-software", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Irrational%20hardware%20vs.%20rational%20software&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIrrational%20hardware%20vs.%20rational%20software%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR799N2WH5S2rJDd2W%2Firrational-hardware-vs-rational-software%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Irrational%20hardware%20vs.%20rational%20software%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR799N2WH5S2rJDd2W%2Firrational-hardware-vs-rational-software", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR799N2WH5S2rJDd2W%2Firrational-hardware-vs-rational-software", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 343, "htmlBody": "<p>\n<p class=\"MsoNormal\">I am passionately fond of the idea of creating an &ldquo;Art of Rationality&rdquo; sensibility/school as described in the [A Sense That More is Possible](http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/) article.&nbsp;</p>\n<p class=\"MsoNormal\">The obstacle I see as most formidable in such an undertaking is the fact that, no matter how much &ldquo;rational software&rdquo; our brains absorb, we cannot escape the fact that we exist within the construct of &ldquo;irrational hardware&rdquo;.&nbsp;</p>\n<p class=\"MsoNormal\">My physical body binds me to countless irrational motivations.&nbsp; Just to name a few: 1) Sex.&nbsp; In an overpopulated world, what is the benefit of yearning for sexual contact on a daily basis?&nbsp; How often does the desire for sex influence rational thought?&nbsp; Is &ldquo;being rational&rdquo; sexy?&nbsp; If not, it is in direct conflict with my body&rsquo;s desire and therefore, undesirable (whereas being able to &ldquo;kick someone&rsquo;s ass&rdquo; is definitely sexy in cultural terms)&nbsp; 2) Mortality.&nbsp; Given an expiration date, it becomes fairly easy to justify immediate/individually beneficial behavior above long term/expansively beneficial behavior that I will not be around long enough to enjoy. 3) Food, water, shelter.&nbsp; My body needs a bare minimum in order to survive.&nbsp; If being rational conflicts with my ability to provide my body with its basic needs (because I exist within an irrational construct)&hellip; what are the odds that rationality will be tossed out in favor of irrational compliance that assures my basic physical needs will be met?</p>\n<p class=\"MsoNormal\">As far as I can tell, being purely rational is in direct opposition to being human. &nbsp;In essence, our hardware is in conflict with rationality.&nbsp;</p>\n<p class=\"MsoNormal\">The reason there is not a &ldquo;School of Super Bad Ass Black Belt Rationality&rdquo; could be as simple as&hellip;. It&nbsp;doesn't&nbsp;make people want to mate with you.&nbsp; It&rsquo;s just not sexy in human terms.&nbsp;</p>\n<p class=\"MsoNormal\">I&rsquo;m not sure being rational will be possible until we transcend our flesh and blood bodies, at which point creating &ldquo;human friendly&rdquo; AI would be rather irrelevant.&nbsp; If AI materializes before we transcend our flesh and blood bodies, it seems more likely that human beings will cause a conflict than the purely rational AI, so&nbsp;shouldn't&nbsp;the focus be toward human transcendence rather than FAI?</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R799N2WH5S2rJDd2W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -18, "extendedScore": null, "score": 9.074644326089444e-07, "legacy": true, "legacyId": "16309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T09:56:59.234Z", "modifiedAt": null, "url": null, "title": "[link] Nine Ways to Bias Open-Source AGI Toward Friendliness", "slug": "link-nine-ways-to-bias-open-source-agi-toward-friendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.877Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H7FmQ6w4Zw582JGjm/link-nine-ways-to-bias-open-source-agi-toward-friendliness", "pageUrlRelative": "/posts/H7FmQ6w4Zw582JGjm/link-nine-ways-to-bias-open-source-agi-toward-friendliness", "linkUrl": "https://www.lesswrong.com/posts/H7FmQ6w4Zw582JGjm/link-nine-ways-to-bias-open-source-agi-toward-friendliness", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Nine%20Ways%20to%20Bias%20Open-Source%20AGI%20Toward%20Friendliness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Nine%20Ways%20to%20Bias%20Open-Source%20AGI%20Toward%20Friendliness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH7FmQ6w4Zw582JGjm%2Flink-nine-ways-to-bias-open-source-agi-toward-friendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Nine%20Ways%20to%20Bias%20Open-Source%20AGI%20Toward%20Friendliness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH7FmQ6w4Zw582JGjm%2Flink-nine-ways-to-bias-open-source-agi-toward-friendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH7FmQ6w4Zw582JGjm%2Flink-nine-ways-to-bias-open-source-agi-toward-friendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p><span style=\"font-size: 11pt; font-family: &quot;Times New Roman&quot;;\" lang=\"EN-US\">Ben Goertzel and Joel Pitt: <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Nine Ways to Bias Open-Source AGI Toward Friendliness</a>. </span><em><span lang=\"EN-US\">Journal of Evolution and Technology</span></em><span lang=\"EN-US\"> - Vol. 22 Issue 1 &ndash; February 2012 - pgs 116-141.</span></p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt;text-align:center\" align=\"center\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:11.0pt; mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;;mso-ansi-language:EN-US\" lang=\"EN-US\">Abstract</span></strong><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:11.0pt; mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;;mso-ansi-language:EN-US\" lang=\"EN-US\"> <br /></span></strong></p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">While it seems unlikely that any method of <em style=\"mso-bidi-font-style:normal\">guaranteeing</em> human-friendliness (&ldquo;Friendliness&rdquo;) on the part of advanced Artificial General Intelligence (AGI) systems will be possible, this doesn&rsquo;t mean the only alternatives are throttling AGI development to safeguard humanity, or plunging recklessly into the complete unknown. Without denying the presence of a certain irreducible uncertainty in such matters, it is still sensible to explore ways of <em style=\"mso-bidi-font-style: normal\">biasing the odds</em> in a favorable way, such that newly created AI systems are significantly more likely than not to be Friendly. Several potential methods of effecting such biasing are explored here, with a particular but non-exclusive focus on those that are relevant to open-source AGI projects, and with illustrative examples drawn from the OpenCog open-source AGI project. Issues regarding the relative safety of open versus closed approaches to AGI are discussed and then nine techniques for biasing AGIs in favor of Friendliness are presented:</span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">1.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Engineer the capability to acquire integrated ethical knowledge. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">2.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Provide rich ethical interaction and instruction, respecting developmental stages. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">3.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Develop stable, hierarchical goal systems. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">4.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Ensure that the early stages of recursive self-improvement occur relatively slowly and with rich human involvement. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">5.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Tightly link AGI with the Global Brain. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">6.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Foster deep, consensus-building interactions between divergent viewpoints. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">7.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Create a mutually supportive community of AGIs. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">8.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Encourage measured co-advancement of AGI software and AGI ethics theory. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:90.0pt;text-indent:-18.0pt;mso-list:l13 level1 lfo1\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\"><span style=\"mso-list:Ignore\">9.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">Develop advanced AGI sooner not later. </span></p>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt\"><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">&nbsp;</span><span style=\"font-size:11.0pt;mso-bidi-font-size:10.0pt;font-family:&quot;Times New Roman&quot;; mso-ansi-language:EN-US\" lang=\"EN-US\">In conclusion, and related to the final point, we advise the serious co-evolution of functional AGI systems and AGI-related ethical theory as soon as possible, before we have so much technical infrastructure that parties relatively unconcerned with ethics are able to rush ahead with brute force approaches to AGI development.</span></p>\n</blockquote>\n<p><span lang=\"EN-US\">I'd say it's worth a read - they have pretty convincing criticism against the possibility of regulating AGI (section 3). I don't think that their approach will work if there's a hard takeoff or a serious <a href=\"http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html\">hardware overhang</a>, though it could maybe work if there isn't. It <em>might</em> also work if there was the possibility for a hard takeoff, but not instantly after developing the first AGI systems.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H7FmQ6w4Zw582JGjm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 9.075462685171064e-07, "legacy": true, "legacyId": "16311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T13:58:56.526Z", "modifiedAt": null, "url": null, "title": "How to deal with non-realism?", "slug": "how-to-deal-with-non-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.161Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "loup-vaillant", "createdAt": "2011-03-23T10:39:25.887Z", "isAdmin": false, "displayName": "loup-vaillant"}, "userId": "wdoZti3BcPbJXsZ66", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nj4HZFfzCrgYBM9yc/how-to-deal-with-non-realism", "pageUrlRelative": "/posts/Nj4HZFfzCrgYBM9yc/how-to-deal-with-non-realism", "linkUrl": "https://www.lesswrong.com/posts/Nj4HZFfzCrgYBM9yc/how-to-deal-with-non-realism", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20deal%20with%20non-realism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20deal%20with%20non-realism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNj4HZFfzCrgYBM9yc%2Fhow-to-deal-with-non-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20deal%20with%20non-realism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNj4HZFfzCrgYBM9yc%2Fhow-to-deal-with-non-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNj4HZFfzCrgYBM9yc%2Fhow-to-deal-with-non-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 251, "htmlBody": "<p>From&nbsp; <a href=\"/r/discussion/lw/cha/being_a_realist_even_if_you_believe_in_god\">Being a Realist (even if you believe in God)</a>:</p>\n<blockquote>\n<p>theists and untheists can and should meet half way and at least agree on the logical necessity of being a realist.</p>\n</blockquote>\n<p>My mother, who doesn't call herself a theist (I think she's agnostic), doesn't even accept realism. She doesn't even agree with this:</p>\n<blockquote>\n<p>There is <em>something</em>.&nbsp; All that there is, we generally call \"reality\". Note that by this definition, reality is unique.&nbsp; The corollary is, we all live in the same reality. We do not <em>percieve</em> it in the same way, but our perceptions and reality itself aren't the same thing.</p>\n<p>Every description of reality that matches it is true. Every description of reality that <em>doesn't</em> match it is false. In this sense, truth is unique and universal.</p>\n<p>(We can nuance the truth/falsehood dichotomy with probability distributions. Some probability distributions are closer to reality than others, and so on.)</p>\n</blockquote>\n<p>That's little more than tautologies here. Yet it elicited an impression of being <em>forced to believe</em>. I know because she told me about the totalitarian dangers from such narrow thinking.</p>\n<p>I'm happy to have finally found the root cause of our ongoing disagreement, but now, how can I deal with <em>that</em>? It looks pretty hopeless, but just in case, does someone have a suggestion, or should I just leave it at that? (My ego doesn't like it, but giving up <em>is</em> an option.)</p>\n<p>Now I'm relieved to know that in near mode, she's a complete realist. This craziness only shows up in far mode.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nj4HZFfzCrgYBM9yc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "16313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 168, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r4MJKMPtWbkFXno2z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-22T16:25:19.474Z", "modifiedAt": null, "url": null, "title": "When is Winning not Winning?", "slug": "when-is-winning-not-winning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:03.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9c8gGtXBXsKNhJdHw/when-is-winning-not-winning", "pageUrlRelative": "/posts/9c8gGtXBXsKNhJdHw/when-is-winning-not-winning", "linkUrl": "https://www.lesswrong.com/posts/9c8gGtXBXsKNhJdHw/when-is-winning-not-winning", "postedAtFormatted": "Tuesday, May 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20is%20Winning%20not%20Winning%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20is%20Winning%20not%20Winning%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9c8gGtXBXsKNhJdHw%2Fwhen-is-winning-not-winning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20is%20Winning%20not%20Winning%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9c8gGtXBXsKNhJdHw%2Fwhen-is-winning-not-winning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9c8gGtXBXsKNhJdHw%2Fwhen-is-winning-not-winning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 451, "htmlBody": "<p>&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"background-color: white; background-position: initial initial; background-repeat: initial initial;\">Lately I'd gotten jaded enough that I simply accepted that different rules apply to the elite class. As Hanson would say, most rules are there specifically to curtail those who don't have the ability to avoid them and to be side-stepped by those who do - it's why we evolved such big, manipulative brains. So when this video recently made the rounds it shocked me to realize how far my values had drifted over the past several years.</span></p>\n<p class=\"MsoNormal\"><span style=\"background-color: white;\">(the video is not about politics, it is about status. My politics are far from those of Penn)<br /><a href=\"http://www.youtube.com/watch?v=wWWOJGYZYpk&amp;feature=sharek\">http://www.youtube.com/watch?v=wWWOJGYZYpk&amp;feature=sharek</a></span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"background-color: white;\">It's good we have people like Penn around to remind us what it was like to be teenagers and still expect the world to be fair, so our brains can be used for more productive things.</span></p>\n<p class=\"MsoNormal\">By the measure our society currently uses, Obama was winning. Penn was not. Yet Penn&rsquo;s approach is the winning strategy for society. Brain power is wasted on status games and social manipulation when it could be used for actually making things better. The machinations of the elite class are a huge drain of resources that could be better used in almost any other pursuit. And yet the elites are admired high-status individuals who are viewed as &ldquo;winning&rdquo; at life. They sit atop huge piles of utility. Idealists like Penn are regarded as immature for insisting on things as low-status as &ldquo;the rules should be fair and apply identically to every one, from the inner-city crack-dealer to the Harvard post-grad.&rdquo;</p>\n<p class=\"MsoNormal\">The &ldquo;Rationalists Should Win&rdquo; meme is a good one, but it risks corrupting our goals. If we focus too much on &ldquo;Rationalist Should Win&rdquo; we risk going for near-term gains that benefit us. Status, wealth, power, sex. Basically hedonism &ndash; things that feel good because we&rsquo;ve evolved to feel good when we get them. Thus we feel we are winning, and we&rsquo;re even told we are winning by our peers and by society. But these things aren&rsquo;t of any use to society. A society of such &ldquo;rationalists&rdquo; would make only feeble and halting progress toward grasping the dream of defeating death and colonizing the stars.</p>\n<p class=\"MsoNormal\">It is important to not let one&rsquo;s concept of &ldquo;winning&rdquo; be corrupted by <a href=\"/lw/kr/an_alien_god\">Azathoth</a>.</p>\n<p>&nbsp;</p>\n<p>ADDED 5/23:</p>\n<p>It seems the majority of comments on this post are people who disagree on the basis of &nbsp;rationality being a tool for achieving ends, but not for telling you what ends are worth achieving.</p>\n<p>&nbsp;</p>\n<p>I disagree. As <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">is written</a>&nbsp;\"The Choice between Good and Bad is not a matter of saying 'Good!' &nbsp;It is about deciding which is which.\" And rationality can help to decide which is which. In fact without rationality you are much more likely to be partially or fully mistaken when you decide.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9c8gGtXBXsKNhJdHw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 15, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "16314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pLRogvJLPPg6Mrvg4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-23T02:08:04.824Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] A Premature Word on AI", "slug": "seq-rerun-a-premature-word-on-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:46.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uQEeyfX3mWtdsDSEL/seq-rerun-a-premature-word-on-ai", "pageUrlRelative": "/posts/uQEeyfX3mWtdsDSEL/seq-rerun-a-premature-word-on-ai", "linkUrl": "https://www.lesswrong.com/posts/uQEeyfX3mWtdsDSEL/seq-rerun-a-premature-word-on-ai", "postedAtFormatted": "Wednesday, May 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20A%20Premature%20Word%20on%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20A%20Premature%20Word%20on%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuQEeyfX3mWtdsDSEL%2Fseq-rerun-a-premature-word-on-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20A%20Premature%20Word%20on%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuQEeyfX3mWtdsDSEL%2Fseq-rerun-a-premature-word-on-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuQEeyfX3mWtdsDSEL%2Fseq-rerun-a-premature-word-on-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/qu/a_premature_word_on_ai/\">A Premature Word on AI</a> was originally published on 31 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#A_Premature_Word_on_AI\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A response to opinions expressed by Robin Hanson, Roger Schank, and others, and arguing against the notion that producing a friendly general artificial intelligence is an insurmountable problem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ckr/seq_rerun_class_project/\">Class Project</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uQEeyfX3mWtdsDSEL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 9.079761119142852e-07, "legacy": true, "legacyId": "16321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iD5baT42zYAkWJPMB", "YWbkaXwDeT3uca6Ku", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-24T01:06:42.464Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Rhythm of Disagreement", "slug": "seq-rerun-the-rhythm-of-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.464Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nup8LKgJr3twvdZca/seq-rerun-the-rhythm-of-disagreement", "pageUrlRelative": "/posts/Nup8LKgJr3twvdZca/seq-rerun-the-rhythm-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/Nup8LKgJr3twvdZca/seq-rerun-the-rhythm-of-disagreement", "postedAtFormatted": "Thursday, May 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Rhythm%20of%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Rhythm%20of%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNup8LKgJr3twvdZca%2Fseq-rerun-the-rhythm-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Rhythm%20of%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNup8LKgJr3twvdZca%2Fseq-rerun-the-rhythm-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNup8LKgJr3twvdZca%2Fseq-rerun-the-rhythm-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/qv/the_rhythm_of_disagreement/\">The Rhythm of Disagreement</a> was originally published on 01 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A discussion of a number of disagreements Eliezer Yudkowsky has been in, with a few comments on rational disagreement.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cld/seq_rerun_a_premature_word_on_ai/\">A Premature Word on AI</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nup8LKgJr3twvdZca", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.085869647724317e-07, "legacy": true, "legacyId": "16334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tKa9Lebyebf6a7P2o", "uQEeyfX3mWtdsDSEL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-24T07:22:20.480Z", "modifiedAt": null, "url": null, "title": "Resurrection through simulation: questions of feasibility, desirability and some implications", "slug": "resurrection-through-simulation-questions-of-feasibility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.759Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jacob_cannell", "createdAt": "2010-08-24T03:58:15.241Z", "isAdmin": false, "displayName": "jacob_cannell"}, "userId": "N2R9wMRJd7SBSjpiT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C5ryrg5ktngwsZHnX/resurrection-through-simulation-questions-of-feasibility", "pageUrlRelative": "/posts/C5ryrg5ktngwsZHnX/resurrection-through-simulation-questions-of-feasibility", "linkUrl": "https://www.lesswrong.com/posts/C5ryrg5ktngwsZHnX/resurrection-through-simulation-questions-of-feasibility", "postedAtFormatted": "Thursday, May 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Resurrection%20through%20simulation%3A%20questions%20of%20feasibility%2C%20desirability%20and%20some%20implications&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResurrection%20through%20simulation%3A%20questions%20of%20feasibility%2C%20desirability%20and%20some%20implications%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC5ryrg5ktngwsZHnX%2Fresurrection-through-simulation-questions-of-feasibility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Resurrection%20through%20simulation%3A%20questions%20of%20feasibility%2C%20desirability%20and%20some%20implications%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC5ryrg5ktngwsZHnX%2Fresurrection-through-simulation-questions-of-feasibility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC5ryrg5ktngwsZHnX%2Fresurrection-through-simulation-questions-of-feasibility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 979, "htmlBody": "<p>Could a future&nbsp;superintelligence&nbsp;bring back the already dead? &nbsp;This discussion has come up a while <a href=\"/lw/3x3/couldwould_an_fai_recreate_people_who_are/\">back</a>&nbsp;(and see the somewhat <a href=\"/r/discussion/lw/bg0/cryonics_without_freezers_resurrection/\">related</a>); I'd like to resurrect the topic because ... it's potentially quite important.</p>\n<p>Algorithmic resurrection is a possibility if we accept the same computational patternist view of identity that suggests cryonics and uploading will work. &nbsp;I see this as the only consistent view of my observations, but if you don't buy this argument/belief set then the rest may not be relevant.</p>\n<p>The general implementation idea is to run a forward simulation over some portion of earth's history, constrained to enforce compliance with all recovered historical evidence. &nbsp;The historical evidence would consist mainly of all the scanned brains and the future internet. &nbsp;</p>\n<p>The thesis is that to the extent that you can retrace historical reality complete with simulated historical people and their thoughts, memories, and emotions, to this same extent you actually recreate/resurrect the historical people.</p>\n<p>So the questions are: is it feasible? is it desirable/ethical/utility-efficient? &nbsp;And finally, why may this matter?</p>\n<h4>Simulation Feasibility</h4>\n<p>A few decades ago pong was a technical achievement, now we have avatar. &nbsp;The trajectory seems to suggest we are on track to photorealistic simulations fairly soon (decades). &nbsp;Offline graphics for film arguably are already photoreal, real-time rendering is close behind, and the biggest remaining problem is the uncanny valley, which really is just the AI problem by another name. &nbsp;Once we solve that (which we are assuming), the Matrix follows. &nbsp;Superintelligences <em>could</em> help.</p>\n<p>There are some general theorems in computer graphics that suggest that simulating an observer optimized world requires resources only in proportion to the observational power of the observers. &nbsp;Video game and film renderers in fact already rely heavily on this strategy.</p>\n<p><strong><a href=\"/lw/3x3/couldwould_an_fai_recreate_people_who_are/3eee\">Criticism</a> from Chaos</strong>: &nbsp;We can't even simulate the weather more than a few weeks in advance.</p>\n<p>Response: Simulating the exact future state of specific chaotic systems may be hard, but simulating chaotic systems in general is not. &nbsp;In this case we are not simulating the future state, but the past. &nbsp;We already know something of the past state of the system, to some level of detail, and we can simulate the likely (or multiple likely) paths within this configuration space, filling in detail.</p>\n<p><strong>Physical Reversibility&nbsp;<a href=\"/lw/3x3/couldwould_an_fai_recreate_people_who_are/3dss\">Criticism</a></strong>: The AI would have to rewind time, it would have to know the exact state of every atom on earth and every photon that has left earth.</p>\n<p>Response: Yes the most straightforward brute force way to infer the past state of earth would be to compute the reverse of all physical interactions and would require ridiculously impractical amounts of information and computation. &nbsp;The best algorithm for a given problem is usually <em>not </em><em>brute force</em>. &nbsp;The specifying data of a human mind is&nbsp;infinitesimal&nbsp;in comparison, and even a random guessing algorithm would probably require less resources than fully reversing history.</p>\n<p>Constrained simulation converges much faster to perfectly accurate recovery, but by no means is full perfect recovery even required for (partial) success. &nbsp;The patternist view of identity is fluid and continuous. &nbsp;</p>\n<p>If resurrecting a specific historical person is better than creating a hypothetical person, creating a somewhat historical person is also better, and the closer the better.</p>\n<h4>Simulation Ethics</h4>\n<p>Humans appear to value other humans, but each human appears to value some more than others. &nbsp;In general humans typically roughly value themselves the most, then kin and family, followed by past contacts, tribal affiliations, and the vaguely similar.</p>\n<p>We can generalize this as a valuation in person-space which peaks at the self identity-pattern and then declines in some complex fashion as we move away to more distant locales and less related people.</p>\n<p>If we extrapolate this to a future where humans have the power to create new humans and or recreate past humans, we can infer that the distribution of created people may follow the self-centered valuation distribution.</p>\n<p>Thus recreating specific ancestors or close relations is better than recreating vaguely historical people which is better than creating non-specific people in general.</p>\n<p><strong>Suffering Criticism</strong>: &nbsp;An ancestral simulation would recreate a huge amount of suffering.</p>\n<p>Response: Humans suffer and live in a world that seems to suffer greatly, and yet very few humans prefer non-existence over their suffering. &nbsp;Evolution culls existential pessimists.</p>\n<p>Recreating a past human will recreate their suffering, but it could also grant them an afterlife filled with tremendous joy. &nbsp;The relatively small finite suffering may not add up to much in this consideration. &nbsp;It could even initially relatively enhance subsequent elevation to joyful state, but this is speculative.</p>\n<p>The utilitarian calculus seems to be: create non-suffering generic people who we value somewhat less vs recreate initially suffering specific historical people who we value more. &nbsp;In some cases (such as lost love ones), the moral calculus weighs heavily in favor of recreating specific people. &nbsp;Many other historicals may be brought along for the ride.</p>\n<h4>Closed Loops</h4>\n<p>The vast majority of the hundred billion something humans who have ever lived share the singular misfortune of simply being born too early in earth's history to be saved by cryonics and uploading.</p>\n<p>Recreating history up to 2012 would require one hundred billion virtual brains. &nbsp;Simulating history into the phase when uploading and virtual brains become common could vastly increase the simulation costs.</p>\n<p>The simulations have the property that they become more accurate as time progresses. &nbsp;If a person is cryonically perserved and then scanned and uploaded, this provides exact information. &nbsp;Simulations will converge to perfect accuracy at that particular moment in time. &nbsp;In addition, the cryonic brain will be unconscious and inactive for a stretch. &nbsp;</p>\n<p>Thus the moment of biological death, even if the person is cryonically preserved, could be an opportune time to recycle simulation resources, as there is no loss of unique information (threads converged).</p>\n<p>How would such a scenario effect the Simulation Argument? &nbsp;It would seem to shift probabilities such that more (most?) observer moments are in pre-uploading histories, rather than in posthuman timelines. &nbsp;I find this disquieting for some reason, even though I don't suspect it will effect my observational experience.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C5ryrg5ktngwsZHnX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 8, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "16350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Could a future&nbsp;superintelligence&nbsp;bring back the already dead? &nbsp;This discussion has come up a while <a href=\"/lw/3x3/couldwould_an_fai_recreate_people_who_are/\">back</a>&nbsp;(and see the somewhat <a href=\"/r/discussion/lw/bg0/cryonics_without_freezers_resurrection/\">related</a>); I'd like to resurrect the topic because ... it's potentially quite important.</p>\n<p>Algorithmic resurrection is a possibility if we accept the same computational patternist view of identity that suggests cryonics and uploading will work. &nbsp;I see this as the only consistent view of my observations, but if you don't buy this argument/belief set then the rest may not be relevant.</p>\n<p>The general implementation idea is to run a forward simulation over some portion of earth's history, constrained to enforce compliance with all recovered historical evidence. &nbsp;The historical evidence would consist mainly of all the scanned brains and the future internet. &nbsp;</p>\n<p>The thesis is that to the extent that you can retrace historical reality complete with simulated historical people and their thoughts, memories, and emotions, to this same extent you actually recreate/resurrect the historical people.</p>\n<p>So the questions are: is it feasible? is it desirable/ethical/utility-efficient? &nbsp;And finally, why may this matter?</p>\n<h4 id=\"Simulation_Feasibility\">Simulation Feasibility</h4>\n<p>A few decades ago pong was a technical achievement, now we have avatar. &nbsp;The trajectory seems to suggest we are on track to photorealistic simulations fairly soon (decades). &nbsp;Offline graphics for film arguably are already photoreal, real-time rendering is close behind, and the biggest remaining problem is the uncanny valley, which really is just the AI problem by another name. &nbsp;Once we solve that (which we are assuming), the Matrix follows. &nbsp;Superintelligences <em>could</em> help.</p>\n<p>There are some general theorems in computer graphics that suggest that simulating an observer optimized world requires resources only in proportion to the observational power of the observers. &nbsp;Video game and film renderers in fact already rely heavily on this strategy.</p>\n<p><strong><a href=\"/lw/3x3/couldwould_an_fai_recreate_people_who_are/3eee\">Criticism</a> from Chaos</strong>: &nbsp;We can't even simulate the weather more than a few weeks in advance.</p>\n<p>Response: Simulating the exact future state of specific chaotic systems may be hard, but simulating chaotic systems in general is not. &nbsp;In this case we are not simulating the future state, but the past. &nbsp;We already know something of the past state of the system, to some level of detail, and we can simulate the likely (or multiple likely) paths within this configuration space, filling in detail.</p>\n<p><strong>Physical Reversibility&nbsp;<a href=\"/lw/3x3/couldwould_an_fai_recreate_people_who_are/3dss\">Criticism</a></strong>: The AI would have to rewind time, it would have to know the exact state of every atom on earth and every photon that has left earth.</p>\n<p>Response: Yes the most straightforward brute force way to infer the past state of earth would be to compute the reverse of all physical interactions and would require ridiculously impractical amounts of information and computation. &nbsp;The best algorithm for a given problem is usually <em>not </em><em>brute force</em>. &nbsp;The specifying data of a human mind is&nbsp;infinitesimal&nbsp;in comparison, and even a random guessing algorithm would probably require less resources than fully reversing history.</p>\n<p>Constrained simulation converges much faster to perfectly accurate recovery, but by no means is full perfect recovery even required for (partial) success. &nbsp;The patternist view of identity is fluid and continuous. &nbsp;</p>\n<p>If resurrecting a specific historical person is better than creating a hypothetical person, creating a somewhat historical person is also better, and the closer the better.</p>\n<h4 id=\"Simulation_Ethics\">Simulation Ethics</h4>\n<p>Humans appear to value other humans, but each human appears to value some more than others. &nbsp;In general humans typically roughly value themselves the most, then kin and family, followed by past contacts, tribal affiliations, and the vaguely similar.</p>\n<p>We can generalize this as a valuation in person-space which peaks at the self identity-pattern and then declines in some complex fashion as we move away to more distant locales and less related people.</p>\n<p>If we extrapolate this to a future where humans have the power to create new humans and or recreate past humans, we can infer that the distribution of created people may follow the self-centered valuation distribution.</p>\n<p>Thus recreating specific ancestors or close relations is better than recreating vaguely historical people which is better than creating non-specific people in general.</p>\n<p><strong>Suffering Criticism</strong>: &nbsp;An ancestral simulation would recreate a huge amount of suffering.</p>\n<p>Response: Humans suffer and live in a world that seems to suffer greatly, and yet very few humans prefer non-existence over their suffering. &nbsp;Evolution culls existential pessimists.</p>\n<p>Recreating a past human will recreate their suffering, but it could also grant them an afterlife filled with tremendous joy. &nbsp;The relatively small finite suffering may not add up to much in this consideration. &nbsp;It could even initially relatively enhance subsequent elevation to joyful state, but this is speculative.</p>\n<p>The utilitarian calculus seems to be: create non-suffering generic people who we value somewhat less vs recreate initially suffering specific historical people who we value more. &nbsp;In some cases (such as lost love ones), the moral calculus weighs heavily in favor of recreating specific people. &nbsp;Many other historicals may be brought along for the ride.</p>\n<h4 id=\"Closed_Loops\">Closed Loops</h4>\n<p>The vast majority of the hundred billion something humans who have ever lived share the singular misfortune of simply being born too early in earth's history to be saved by cryonics and uploading.</p>\n<p>Recreating history up to 2012 would require one hundred billion virtual brains. &nbsp;Simulating history into the phase when uploading and virtual brains become common could vastly increase the simulation costs.</p>\n<p>The simulations have the property that they become more accurate as time progresses. &nbsp;If a person is cryonically perserved and then scanned and uploaded, this provides exact information. &nbsp;Simulations will converge to perfect accuracy at that particular moment in time. &nbsp;In addition, the cryonic brain will be unconscious and inactive for a stretch. &nbsp;</p>\n<p>Thus the moment of biological death, even if the person is cryonically preserved, could be an opportune time to recycle simulation resources, as there is no loss of unique information (threads converged).</p>\n<p>How would such a scenario effect the Simulation Argument? &nbsp;It would seem to shift probabilities such that more (most?) observer moments are in pre-uploading histories, rather than in posthuman timelines. &nbsp;I find this disquieting for some reason, even though I don't suspect it will effect my observational experience.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Simulation Feasibility", "anchor": "Simulation_Feasibility", "level": 1}, {"title": "Simulation Ethics", "anchor": "Simulation_Ethics", "level": 1}, {"title": "Closed Loops", "anchor": "Closed_Loops", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "57 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mNqyFZT2M9fh2oRPx", "MkKcnPdTZ3pQ9F5yC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-24T10:21:18.504Z", "modifiedAt": null, "url": null, "title": "Wasted life", "slug": "wasted-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tzAtTtpX9BFS6tAZM/wasted-life", "pageUrlRelative": "/posts/tzAtTtpX9BFS6tAZM/wasted-life", "linkUrl": "https://www.lesswrong.com/posts/tzAtTtpX9BFS6tAZM/wasted-life", "postedAtFormatted": "Thursday, May 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wasted%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWasted%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtzAtTtpX9BFS6tAZM%2Fwasted-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wasted%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtzAtTtpX9BFS6tAZM%2Fwasted-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtzAtTtpX9BFS6tAZM%2Fwasted-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p>It's just occurred to me that, giving all the cheerful risk stuff I work with, one of the most optimistic things people could say to me would be:</p>\n<p style=\"padding-left: 30px;\">\"You've wasted your life. Nothing of what you've done is relevant or useful.\"</p>\n<p>That would make me very happy. Of course, that only works if it's credible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tzAtTtpX9BFS6tAZM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 18, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "16351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-24T10:51:38.884Z", "modifiedAt": null, "url": null, "title": "How do you find good scholarly criticism of a book?", "slug": "how-do-you-find-good-scholarly-criticism-of-a-book", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2zQRgntBWgyJAho6A/how-do-you-find-good-scholarly-criticism-of-a-book", "pageUrlRelative": "/posts/2zQRgntBWgyJAho6A/how-do-you-find-good-scholarly-criticism-of-a-book", "linkUrl": "https://www.lesswrong.com/posts/2zQRgntBWgyJAho6A/how-do-you-find-good-scholarly-criticism-of-a-book", "postedAtFormatted": "Thursday, May 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20find%20good%20scholarly%20criticism%20of%20a%20book%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20find%20good%20scholarly%20criticism%20of%20a%20book%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zQRgntBWgyJAho6A%2Fhow-do-you-find-good-scholarly-criticism-of-a-book%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20find%20good%20scholarly%20criticism%20of%20a%20book%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zQRgntBWgyJAho6A%2Fhow-do-you-find-good-scholarly-criticism-of-a-book", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zQRgntBWgyJAho6A%2Fhow-do-you-find-good-scholarly-criticism-of-a-book", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>When I read a book with new and interesting ideas, I usually want to know if there are major flaws that any knowledgeable scholar in the field would point out immediately (Two recent examples are Pinker's \"The Better Angels of our Nature\", and Harriss's \"The Nurture Assumption\")</p>\n<p>I usually:</p>\n<ul>\n<li>Look at reviews on Amazon (especially the negative ones)</li>\n<li>Google with keywords like \"criticism, \"review\", \"problem\", (and whatever major issues I seem to have run in) etc.</li>\n<li>Search Google Scholar for the same thing</li>\n<li>Ask in some communities (LessWrong, reddit AskHistorians) if anybody read it</li>\n</ul>\n<p>One problem is that I end up spending a lot of time reading stuff of no interest - either reviewers explaining the book to people who haven't read it (and sometimes even misrepresenting it's arguments, or framing them in terms of their pet controversy), or bloggers/posters who haven't read the book so go off a summary and come up with arguments that are already well-addressed in it.</p>\n<p>So, what tips and strategies do you have for finding solid scholarly criticism ?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2zQRgntBWgyJAho6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 15, "extendedScore": null, "score": 9.088463641842659e-07, "legacy": true, "legacyId": "16352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-24T20:52:11.270Z", "modifiedAt": null, "url": null, "title": "[LINK] How to develop a photographic memory", "slug": "link-how-to-develop-a-photographic-memory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.948Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M2icwykq66mkMwXM7/link-how-to-develop-a-photographic-memory", "pageUrlRelative": "/posts/M2icwykq66mkMwXM7/link-how-to-develop-a-photographic-memory", "linkUrl": "https://www.lesswrong.com/posts/M2icwykq66mkMwXM7/link-how-to-develop-a-photographic-memory", "postedAtFormatted": "Thursday, May 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20to%20develop%20a%20photographic%20memory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20to%20develop%20a%20photographic%20memory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM2icwykq66mkMwXM7%2Flink-how-to-develop-a-photographic-memory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20to%20develop%20a%20photographic%20memory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM2icwykq66mkMwXM7%2Flink-how-to-develop-a-photographic-memory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM2icwykq66mkMwXM7%2Flink-how-to-develop-a-photographic-memory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>Article can be found <a href=\"http://www.ehow.com/how_4476504_develop-photographic-memory.html\">here</a>.</p>\n<p>I am considering trying this, and I'm wondering if anyone on LW has tried this or something similar. This seems like it could work, but it also seems like it could be hokum. A simple search for \"how to develop a photographic memory\" returns any number of methods. If this works with the success rate that they imply, this seems like tremendously low hanging fruit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M2icwykq66mkMwXM7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 9.091128179935476e-07, "legacy": true, "legacyId": "16354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-24T23:34:25.446Z", "modifiedAt": null, "url": null, "title": "Book Summary: Willpower by Baumeister, Tierney", "slug": "book-summary-willpower-by-baumeister-tierney", "viewCount": null, "lastCommentedAt": "2021-02-02T20:48:51.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dorikka", "createdAt": "2010-12-11T03:34:20.472Z", "isAdmin": false, "displayName": "Dorikka"}, "userId": "HJB33ckc8NzPbvJYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DqngasYzkwvEWuGya/book-summary-willpower-by-baumeister-tierney", "pageUrlRelative": "/posts/DqngasYzkwvEWuGya/book-summary-willpower-by-baumeister-tierney", "linkUrl": "https://www.lesswrong.com/posts/DqngasYzkwvEWuGya/book-summary-willpower-by-baumeister-tierney", "postedAtFormatted": "Thursday, May 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Summary%3A%20Willpower%20by%20Baumeister%2C%20Tierney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Summary%3A%20Willpower%20by%20Baumeister%2C%20Tierney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqngasYzkwvEWuGya%2Fbook-summary-willpower-by-baumeister-tierney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Summary%3A%20Willpower%20by%20Baumeister%2C%20Tierney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqngasYzkwvEWuGya%2Fbook-summary-willpower-by-baumeister-tierney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqngasYzkwvEWuGya%2Fbook-summary-willpower-by-baumeister-tierney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 747, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">I recently read <a href=\"http://www.amazon.com/Willpower-Rediscovering-Greatest-Strength-ebook/dp/B0052REQCY/ref=tmm_kin_title_0/185-8206488-1594713?ie=UTF8&amp;m=AG56TWVU5XWC2\">this book</a>. I've tried to summarize the main points below -- you can read my notes <a href=\"https://www.dropbox.com/s/sm4wpcmb1ygw8qu/Willpower%20Notes%20--%20Baumeister%20and%20Tierney.docx\">here</a> (MSWord doc). You might also find Derek Sivers' notes useful, which can be found <a href=\"https://sivers.org/book/Willpower\">here</a>.</p>\n<p class=\"MsoNormal\">NOTE: The general model of willpower (as a finite resource consumed with use) used in this book does not seem to represent a scientific consensus -- see the comments for more detail.</p>\n<h2 class=\"MsoNormal\"><strong>General Claims</strong><br /></h2>\n<ul>\n<li>Glucose acts as willpower fuel. As willpower levels drop, so do glucose levels. Willpower can be restored by raising your blood sugar. (pp. 44-48)</li>\n<li>You have a finite amount of willpower that becomes depleted as you use it, and you use the same stock of willpower for all manner of tasks. (p. 35)</li>\n<li>Willpower depletion amplifies emotions, desires, and cravings<a style=\"mso-endnote-id: edn1\" name=\"_ednref1\" href=\"#_edn1\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[i]</span></span></span></span></a>. (pp. 30-31)</li>\n</ul>\n<h2 class=\"MsoNormal\">Willpower Depletion<br /></h2>\n<ul>\n<li>Controlling emotional reactions depletes willpower. (p. 25)</li>\n<li>Attempting to control thoughts (say, trying not to think of a white bear) depletes willpower. (pp. 26-27)</li>\n<li>Chronic pain causes ongoing willpower depletion. (p. 36)</li>\n<li>Being sick depletes glucose, which negatively affects willpower. Related note: Driving a car with a bad cold has been found to be even more dangerous than driving when mildly intoxicated. (pp. 59-60)</li>\n<li>Making decisions (even trivial ones) costs willpower, and making decisions for other people costs less than making them for yourself. Making decisions that you enjoy costs less willpower than those which you do not. (pp. 94-95)</li>\n<li>Uncompleted tasks and unmet goals tend to pop into one&rsquo;s mind &ndash; this is called the Zeigarnik effect. Completing the task (or making a plan to do so, the more specific the better) will cause your unconscious to stop nagging you with reminders.<a style=\"mso-endnote-id:edn2\" name=\"_ednref2\" href=\"#_edn2\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[ii]</span></span></span></span></a> (p. 81)</li>\n</ul>\n<h2 class=\"MsoNormal\">Restoring Willpower<br /></h2>\n<ul>\n<li>Eating foods like white bread, potatoes, white rice, and sugary snacks produce boom-and-bust cycles because they are converted into glucose so quickly. Foods which are converted more slowly (providing fuel more steadily) include most veggies, nuts (like peanuts and cashews), many raw fruits (like apples, blueberries, and pears), cheese, fish, meat, and olive oil. (These foods are said to have a low glycemic index.) (pp. 59-60)</li>\n<li>Sleep helps to restore willpower &ndash; in particular, sleep deprivation causes impaired processing of glucose (and, over time, a higher risk of diabetes). (pp. 59-60)</li>\n<li>Being in a clean room appears to increase self-control, and being in a messy room appears to reduce self-control.<a style=\"mso-endnote-id:edn3\" name=\"_ednref3\" href=\"#_edn3\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[iii]</span></span></span></span></a> (p. 156)</li>\n</ul>\n<h2 class=\"MsoNormal\">Miscellaneous</h2>\n<ul>\n<li>Focusing on a single self-improvement goal increases your chances of success, as each simultaneous goal increases the demands on your willpower. (pp. 37-38)</li>\n<li>Conflicting goals cause increased worrying/rumination, decreased motivation, greater amount of physical sickness, and more depression and anxiety. (p. 67)</li>\n<li>Reluctance to give up options increases when willpower is low.<a style=\"mso-endnote-id:edn4\" name=\"_ednref4\" href=\"#_edn4\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[iv]</span></span></span></span></a>(p. 99)</li>\n<li>Focusing on past achievements seems to increase contentment with one&rsquo;s current situation, while focusing on the road ahead increases motivation and ambition. (p. 120)</li>\n<li>People are often not very good at predicting how they will behave in an excited emotional state while in an unexcited state &ndash; this is often referred to as the hot-cold empathy gap. (p. 148)</li>\n<li>Precommitment can make it more likely that you will not succumb to temptation during times of low willpower.<a style=\"mso-endnote-id:edn5\" name=\"_ednref5\" href=\"#_edn5\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[v]</span></span></span></span></a> (pp. 151-153)</li>\n</ul>\n<p style=\"mso-element:endnote-list\">I declare <a href=\"http://wiki.lesswrong.com/wiki/Crocker%27s_rules\">Crocker's Rules</a>.</p>\n<hr size=\"1\" />\n<div id=\"edn1\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn1\" name=\"_edn1\" href=\"#_ednref1\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[i]</span></span></span></span></a> I didn&rsquo;t see enough evidence to conclude whether the cravings are actually stronger, or people are simply less able to resist them, or both. The book claims that both are true.</p>\n</div>\n<div id=\"edn2\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn2\" name=\"_edn2\" href=\"#_ednref2\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[ii]</span></span></span></span></a> The book seems to imply this mental nagging costs willpower, but I don&rsquo;t recall it being explicitly stated. <a href=\"http://www.amazon.com/Getting-Things-Done-Stress-Free-Productivity/dp/0142000280\">GTD</a> is also mentioned, and the lack of Next Actions which one has the materials to execute being included in plans causing people to procrastinate. (p. 79)</p>\n</div>\n<div id=\"edn3\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn3\" name=\"_edn3\" href=\"#_ednref3\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[iii]</span></span></span></span></a> The relevant experiment was conducted in a laboratory, so there is no possibility of the experimental results being affected by the fact that people with more self-control may keep their house cleaner. Self-control was measured in ways like being willing/unwilling to week for a larger sum of money instead of receiving a smaller sum immediately, and choosing healthier foods over sugary snacks.</p>\n</div>\n<div id=\"edn4\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn4\" name=\"_edn4\" href=\"#_ednref4\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[iv]</span></span></span></span></a> I wonder if this means that people are more likely to ignore opportunity costs.</p>\n</div>\n<div id=\"edn5\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn5\" name=\"_edn5\" href=\"#_ednref5\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[v]</span></span></span></span></a> &lsquo;Conserving willpower&rsquo; is also mentioned around here, which seemed to imply that effective precommitment helped reduce the willpower costs of overcoming constant temptation by making the decision easier.</p>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 1, "8SfkJYYMe75MwjHzN": 1, "YrLoz567b553YouZ2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DqngasYzkwvEWuGya", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 36, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "16356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">I recently read <a href=\"http://www.amazon.com/Willpower-Rediscovering-Greatest-Strength-ebook/dp/B0052REQCY/ref=tmm_kin_title_0/185-8206488-1594713?ie=UTF8&amp;m=AG56TWVU5XWC2\">this book</a>. I've tried to summarize the main points below -- you can read my notes <a href=\"https://www.dropbox.com/s/sm4wpcmb1ygw8qu/Willpower%20Notes%20--%20Baumeister%20and%20Tierney.docx\">here</a> (MSWord doc). You might also find Derek Sivers' notes useful, which can be found <a href=\"https://sivers.org/book/Willpower\">here</a>.</p>\n<p class=\"MsoNormal\">NOTE: The general model of willpower (as a finite resource consumed with use) used in this book does not seem to represent a scientific consensus -- see the comments for more detail.</p>\n<h2 class=\"MsoNormal\" id=\"General_Claims\"><strong>General Claims</strong><br></h2>\n<ul>\n<li>Glucose acts as willpower fuel. As willpower levels drop, so do glucose levels. Willpower can be restored by raising your blood sugar. (pp. 44-48)</li>\n<li>You have a finite amount of willpower that becomes depleted as you use it, and you use the same stock of willpower for all manner of tasks. (p. 35)</li>\n<li>Willpower depletion amplifies emotions, desires, and cravings<a style=\"mso-endnote-id: edn1\" name=\"_ednref1\" href=\"#_edn1\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[i]</span></span></span></span></a>. (pp. 30-31)</li>\n</ul>\n<h2 class=\"MsoNormal\" id=\"Willpower_Depletion\">Willpower Depletion<br></h2>\n<ul>\n<li>Controlling emotional reactions depletes willpower. (p. 25)</li>\n<li>Attempting to control thoughts (say, trying not to think of a white bear) depletes willpower. (pp. 26-27)</li>\n<li>Chronic pain causes ongoing willpower depletion. (p. 36)</li>\n<li>Being sick depletes glucose, which negatively affects willpower. Related note: Driving a car with a bad cold has been found to be even more dangerous than driving when mildly intoxicated. (pp. 59-60)</li>\n<li>Making decisions (even trivial ones) costs willpower, and making decisions for other people costs less than making them for yourself. Making decisions that you enjoy costs less willpower than those which you do not. (pp. 94-95)</li>\n<li>Uncompleted tasks and unmet goals tend to pop into one\u2019s mind \u2013 this is called the Zeigarnik effect. Completing the task (or making a plan to do so, the more specific the better) will cause your unconscious to stop nagging you with reminders.<a style=\"mso-endnote-id:edn2\" name=\"_ednref2\" href=\"#_edn2\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[ii]</span></span></span></span></a> (p. 81)</li>\n</ul>\n<h2 class=\"MsoNormal\" id=\"Restoring_Willpower\">Restoring Willpower<br></h2>\n<ul>\n<li>Eating foods like white bread, potatoes, white rice, and sugary snacks produce boom-and-bust cycles because they are converted into glucose so quickly. Foods which are converted more slowly (providing fuel more steadily) include most veggies, nuts (like peanuts and cashews), many raw fruits (like apples, blueberries, and pears), cheese, fish, meat, and olive oil. (These foods are said to have a low glycemic index.) (pp. 59-60)</li>\n<li>Sleep helps to restore willpower \u2013 in particular, sleep deprivation causes impaired processing of glucose (and, over time, a higher risk of diabetes). (pp. 59-60)</li>\n<li>Being in a clean room appears to increase self-control, and being in a messy room appears to reduce self-control.<a style=\"mso-endnote-id:edn3\" name=\"_ednref3\" href=\"#_edn3\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[iii]</span></span></span></span></a> (p. 156)</li>\n</ul>\n<h2 class=\"MsoNormal\" id=\"Miscellaneous\">Miscellaneous</h2>\n<ul>\n<li>Focusing on a single self-improvement goal increases your chances of success, as each simultaneous goal increases the demands on your willpower. (pp. 37-38)</li>\n<li>Conflicting goals cause increased worrying/rumination, decreased motivation, greater amount of physical sickness, and more depression and anxiety. (p. 67)</li>\n<li>Reluctance to give up options increases when willpower is low.<a style=\"mso-endnote-id:edn4\" name=\"_ednref4\" href=\"#_edn4\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[iv]</span></span></span></span></a>(p. 99)</li>\n<li>Focusing on past achievements seems to increase contentment with one\u2019s current situation, while focusing on the road ahead increases motivation and ambition. (p. 120)</li>\n<li>People are often not very good at predicting how they will behave in an excited emotional state while in an unexcited state \u2013 this is often referred to as the hot-cold empathy gap. (p. 148)</li>\n<li>Precommitment can make it more likely that you will not succumb to temptation during times of low willpower.<a style=\"mso-endnote-id:edn5\" name=\"_ednref5\" href=\"#_edn5\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character:footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:11.0pt;line-height:115%; font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA\">[v]</span></span></span></span></a> (pp. 151-153)</li>\n</ul>\n<p style=\"mso-element:endnote-list\">I declare <a href=\"http://wiki.lesswrong.com/wiki/Crocker%27s_rules\">Crocker's Rules</a>.</p>\n<hr size=\"1\">\n<div id=\"edn1\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn1\" name=\"_edn1\" href=\"#_ednref1\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[i]</span></span></span></span></a> I didn\u2019t see enough evidence to conclude whether the cravings are actually stronger, or people are simply less able to resist them, or both. The book claims that both are true.</p>\n</div>\n<div id=\"edn2\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn2\" name=\"_edn2\" href=\"#_ednref2\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[ii]</span></span></span></span></a> The book seems to imply this mental nagging costs willpower, but I don\u2019t recall it being explicitly stated. <a href=\"http://www.amazon.com/Getting-Things-Done-Stress-Free-Productivity/dp/0142000280\">GTD</a> is also mentioned, and the lack of Next Actions which one has the materials to execute being included in plans causing people to procrastinate. (p. 79)</p>\n</div>\n<div id=\"edn3\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn3\" name=\"_edn3\" href=\"#_ednref3\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[iii]</span></span></span></span></a> The relevant experiment was conducted in a laboratory, so there is no possibility of the experimental results being affected by the fact that people with more self-control may keep their house cleaner. Self-control was measured in ways like being willing/unwilling to week for a larger sum of money instead of receiving a smaller sum immediately, and choosing healthier foods over sugary snacks.</p>\n</div>\n<div id=\"edn4\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn4\" name=\"_edn4\" href=\"#_ednref4\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[iv]</span></span></span></span></a> I wonder if this means that people are more likely to ignore opportunity costs.</p>\n</div>\n<div id=\"edn5\" style=\"mso-element:endnote\">\n<p class=\"MsoEndnoteText\"><a style=\"mso-endnote-id:edn5\" name=\"_edn5\" href=\"#_ednref5\"><span class=\"MsoEndnoteReference\"><span style=\"mso-special-character: footnote\"><span class=\"MsoEndnoteReference\"><span style=\"font-size:10.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">[v]</span></span></span></span></a> \u2018Conserving willpower\u2019 is also mentioned around here, which seemed to imply that effective precommitment helped reduce the willpower costs of overcoming constant temptation by making the decision easier.</p>\n</div>\n<p>&nbsp;</p>", "sections": [{"title": "General Claims", "anchor": "General_Claims", "level": 1}, {"title": "Willpower Depletion", "anchor": "Willpower_Depletion", "level": 1}, {"title": "Restoring Willpower", "anchor": "Restoring_Willpower", "level": 1}, {"title": "Miscellaneous", "anchor": "Miscellaneous", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T01:27:29.789Z", "modifiedAt": null, "url": null, "title": "A digitized belief network?", "slug": "a-digitized-belief-network", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:58.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "avichapman", "createdAt": "2012-04-30T05:38:19.163Z", "isAdmin": false, "displayName": "avichapman"}, "userId": "AkCycCQcKgvoCiDgu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PopyaEqjBsFsgmGb7/a-digitized-belief-network", "pageUrlRelative": "/posts/PopyaEqjBsFsgmGb7/a-digitized-belief-network", "linkUrl": "https://www.lesswrong.com/posts/PopyaEqjBsFsgmGb7/a-digitized-belief-network", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20digitized%20belief%20network%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20digitized%20belief%20network%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPopyaEqjBsFsgmGb7%2Fa-digitized-belief-network%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20digitized%20belief%20network%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPopyaEqjBsFsgmGb7%2Fa-digitized-belief-network", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPopyaEqjBsFsgmGb7%2Fa-digitized-belief-network", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 329, "htmlBody": "<p>Hello to all,</p>\n<p>Like the rest of you, I'm an aspiring rationalist. I'm also a software engineer. I design software solutions automatically. It's the first place my mind goes when thinking about a problem.</p>\n<p>Today's problem is the fact that our beliefs all rest on beliefs that rest on beliefs. Each one has a <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">&lt;100%&nbsp;probability</a>&nbsp;of being correct. Thus, each belief built on it has an even smaller chance of being correct.</p>\n<p>When we discover a belief is false (or less&nbsp;dramatically, revise its&nbsp;probability&nbsp;of being true), it&nbsp;propagates&nbsp;to all other beliefs that are wholly or partially based on it. This is an imperfect process and can take a long time (less in rationalists, but still limited by our speed of thought and inefficiency in recall).</p>\n<p>I think that software can help with this. If a dedicated rationalist spent a large amount of time committing each belief of theirs to a database (including a rational assessment of its&nbsp;probability overall and&nbsp;given that all other beliefs that it rests on are true) as well as which other beliefs their beliefs rest on, you would eventually have a picture of your belief network. The software could then alert you to contradictions between your estimate of a belief's&nbsp;probability&nbsp;of being true and its estimate based on the truth estimate of the beliefs that it rests on. It could also find cyclical beliefs and other inconsistencies. Plus, when you update a belief based on new evidence, it can spit out a list of&nbsp;beliefs&nbsp;that should be reconsidered.</p>\n<p>Obviously, this would only work if you are brutally honest about what you believe and fairly accurate about your assessments of truth probabilities. But I think this would be an awesome tool.</p>\n<p>Does anyone know of an effort to build such a tool? If not, would anyone be interested in helping me design and build such a tool? I've only been reading LessWrong for a little while now, so there's probably a bunch of stuff that I haven't considered in the design of such a tool.</p>\n<p>Your's rationally,<br />Avi</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PopyaEqjBsFsgmGb7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 9.092350159647982e-07, "legacy": true, "legacyId": "16360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QGkYCwyC7wTDyt3yT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T03:04:32.051Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: Making Beliefs Pay Rent", "slug": "meetup-pittsburgh-making-beliefs-pay-rent", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JmTX4QKvuPAWodiJ7/meetup-pittsburgh-making-beliefs-pay-rent", "pageUrlRelative": "/posts/JmTX4QKvuPAWodiJ7/meetup-pittsburgh-making-beliefs-pay-rent", "linkUrl": "https://www.lesswrong.com/posts/JmTX4QKvuPAWodiJ7/meetup-pittsburgh-making-beliefs-pay-rent", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20Making%20Beliefs%20Pay%20Rent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20Making%20Beliefs%20Pay%20Rent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJmTX4QKvuPAWodiJ7%2Fmeetup-pittsburgh-making-beliefs-pay-rent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20Making%20Beliefs%20Pay%20Rent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJmTX4QKvuPAWodiJ7%2Fmeetup-pittsburgh-making-beliefs-pay-rent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJmTX4QKvuPAWodiJ7%2Fmeetup-pittsburgh-making-beliefs-pay-rent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ai'>Pittsburgh: Making Beliefs Pay Rent</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 June 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Eatunique, S. Craig St, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If you really want to prepare, read the (short) post here: <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\" rel=\"nofollow\">http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/</a></p>\n\n<p>Bring your thoughts, criticisms, and any beliefs that are overdue.</p>\n\n<p>Eatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ai'>Pittsburgh: Making Beliefs Pay Rent</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JmTX4QKvuPAWodiJ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.092780937920576e-07, "legacy": true, "legacyId": "16368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Making_Beliefs_Pay_Rent\">Discussion article for the meetup : <a href=\"/meetups/ai\">Pittsburgh: Making Beliefs Pay Rent</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 June 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Eatunique, S. Craig St, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If you really want to prepare, read the (short) post here: <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\" rel=\"nofollow\">http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/</a></p>\n\n<p>Bring your thoughts, criticisms, and any beliefs that are overdue.</p>\n\n<p>Eatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Making_Beliefs_Pay_Rent1\">Discussion article for the meetup : <a href=\"/meetups/ai\">Pittsburgh: Making Beliefs Pay Rent</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: Making Beliefs Pay Rent", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Making_Beliefs_Pay_Rent", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: Making Beliefs Pay Rent", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Making_Beliefs_Pay_Rent1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T04:29:28.796Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Principles of Disagreement", "slug": "seq-rerun-principles-of-disagreement", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mgav9g2dkffkpm62T/seq-rerun-principles-of-disagreement", "pageUrlRelative": "/posts/Mgav9g2dkffkpm62T/seq-rerun-principles-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/Mgav9g2dkffkpm62T/seq-rerun-principles-of-disagreement", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Principles%20of%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Principles%20of%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMgav9g2dkffkpm62T%2Fseq-rerun-principles-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Principles%20of%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMgav9g2dkffkpm62T%2Fseq-rerun-principles-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMgav9g2dkffkpm62T%2Fseq-rerun-principles-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>Today's post, <a href=\"/lw/qw/principles_of_disagreement/\">Principles of Disagreement</a> was originally published on 02 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Principles_of_Disagreement\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You do have to pay attention to other people's authority a fair amount of the time. But above all, try to get the actual right answer. Clever tricks are only valuable if they help you learn what the truth actually is. If a clever argument doesn't actually work, don't use it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/clq/seq_rerun_the_rhythm_of_disagreement/\">The Rhythm of Disagreement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mgav9g2dkffkpm62T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.093158066734752e-07, "legacy": true, "legacyId": "16370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gTTWRkSz474o7s4Dg", "Nup8LKgJr3twvdZca", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T07:37:41.761Z", "modifiedAt": null, "url": null, "title": "What useful skills can be learned in three months?", "slug": "what-useful-skills-can-be-learned-in-three-months", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:56.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nebulous", "createdAt": "2011-11-23T06:28:39.231Z", "isAdmin": false, "displayName": "nebulous"}, "userId": "nqrJWFKPS8dLDDjmi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sy6N4TfQf8onJQKgE/what-useful-skills-can-be-learned-in-three-months", "pageUrlRelative": "/posts/sy6N4TfQf8onJQKgE/what-useful-skills-can-be-learned-in-three-months", "linkUrl": "https://www.lesswrong.com/posts/sy6N4TfQf8onJQKgE/what-useful-skills-can-be-learned-in-three-months", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20useful%20skills%20can%20be%20learned%20in%20three%20months%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20useful%20skills%20can%20be%20learned%20in%20three%20months%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsy6N4TfQf8onJQKgE%2Fwhat-useful-skills-can-be-learned-in-three-months%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20useful%20skills%20can%20be%20learned%20in%20three%20months%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsy6N4TfQf8onJQKgE%2Fwhat-useful-skills-can-be-learned-in-three-months", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsy6N4TfQf8onJQKgE%2Fwhat-useful-skills-can-be-learned-in-three-months", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>This September I'll start college aiming for a computer science degree, and I want to use the summer for self-improvement. I'm very uncertain about what skills I should try to learn, though, and recommendations would help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sy6N4TfQf8onJQKgE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "16382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T08:47:53.559Z", "modifiedAt": null, "url": null, "title": "Timeline tools?", "slug": "timeline-tools", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Chriswaterguy", "createdAt": "2012-01-08T08:52:50.614Z", "isAdmin": false, "displayName": "Chriswaterguy"}, "userId": "L44d3MCsx4xfxEmBm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/akJ4TTS6yoMNd4jod/timeline-tools", "pageUrlRelative": "/posts/akJ4TTS6yoMNd4jod/timeline-tools", "linkUrl": "https://www.lesswrong.com/posts/akJ4TTS6yoMNd4jod/timeline-tools", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timeline%20tools%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimeline%20tools%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FakJ4TTS6yoMNd4jod%2Ftimeline-tools%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timeline%20tools%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FakJ4TTS6yoMNd4jod%2Ftimeline-tools", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FakJ4TTS6yoMNd4jod%2Ftimeline-tools", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>What would be useful to me now is a tool to help order, visualize and prioritize tasks for a larger project. I'm organizing a fundraising campaign for the non-profit I work with, and there are lots of tasks and some dependencies.</p>\n<p>I'm trying some <a href=\"http://www.makeuseof.com/tag/excel-project-management-tracking-templates/\">Excel spreadsheet templates</a> for starters - the ones linked at the end of that post seem useful. (Downloading didn't work in Firefox for some reason, but did in Chrome.) Unsure whether to go with Gantt or Critical Path but I'll try Critical Path first.</p>\n<p>I'm curious to know what works for you. A clear and simple tool is what I need, not something that takes significant mental energy and organizations skills just to use it. Doesn't have to be a spreadsheet, but a spreadsheet is my initial preference as it should be easy, and OS-independent (maybe even doable in the cloud).</p>\n<p>Guidance is appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "akJ4TTS6yoMNd4jod", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "16383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T11:07:47.275Z", "modifiedAt": null, "url": null, "title": "How to measure optimisation power", "slug": "how-to-measure-optimisation-power", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.970Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9FRmgesXwqjv25Lhg/how-to-measure-optimisation-power", "pageUrlRelative": "/posts/9FRmgesXwqjv25Lhg/how-to-measure-optimisation-power", "linkUrl": "https://www.lesswrong.com/posts/9FRmgesXwqjv25Lhg/how-to-measure-optimisation-power", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20measure%20optimisation%20power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20measure%20optimisation%20power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FRmgesXwqjv25Lhg%2Fhow-to-measure-optimisation-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20measure%20optimisation%20power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FRmgesXwqjv25Lhg%2Fhow-to-measure-optimisation-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FRmgesXwqjv25Lhg%2Fhow-to-measure-optimisation-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>As every school child knows, an advanced AI can be seen as an <a href=\"http://wiki.lesswrong.com/wiki/Optimization_process\">optimisation process</a> - something that hits a very narrow target in the space of possibilities. The Less Wrong wiki <a href=\"http://wiki.lesswrong.com/wiki/Optimization_process\">entry</a>&nbsp;proposes some measure of optimisation power:</p>\n<blockquote>\n<p>One way to think mathematically about optimization, like evidence, is in information-theoretic bits. We take the base-two logarithm of the reciprocal of the probability of the result. A one-in-a-million solution (a solution so good relative to your preference ordering that it would take a million random tries to find something that good or better) can be said to have log<sub>2</sub>(1,000,000) = 19.9 bits of optimization.</p>\n</blockquote>\n<p>This doesn't seem a fully rigorous definition - what exactly is meant by a million random tries? Also, it measures how hard it would be to come up with that solution, but not how good that solution is. An AI that comes up with a solution that is ten thousand bits more complicated to find, but that is only a tiny bit better than the human solution, is not one to fear.</p>\n<p>Other potential measurements could be taking any of the metrics I suggested in the <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">reduced impact</a> post, but used in reverse: to measure large deviations from the status quo, not small ones.</p>\n<p>Anyway, before I reinvent the <a href=\"http://refspace.com/quotes/d:1/Douglas_Adams/wheel\">coloured wheel</a>, I just wanted to check whether there was a fully defined agreed upon measure of optimisation power.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9FRmgesXwqjv25Lhg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 9.094926778288881e-07, "legacy": true, "legacyId": "16391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8Nwg7kqAfCM46tuHq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T14:16:30.529Z", "modifiedAt": null, "url": null, "title": "Another Iterated Prisoner's Dilemma Tournament?", "slug": "another-iterated-prisoner-s-dilemma-tournament", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andreas_Giger", "createdAt": "2011-09-12T00:45:35.617Z", "isAdmin": false, "displayName": "Andreas_Giger"}, "userId": "JjKS2qrYMyWoCcjnH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gdEA4oZX5xKRmScJ8/another-iterated-prisoner-s-dilemma-tournament", "pageUrlRelative": "/posts/gdEA4oZX5xKRmScJ8/another-iterated-prisoner-s-dilemma-tournament", "linkUrl": "https://www.lesswrong.com/posts/gdEA4oZX5xKRmScJ8/another-iterated-prisoner-s-dilemma-tournament", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20Iterated%20Prisoner's%20Dilemma%20Tournament%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20Iterated%20Prisoner's%20Dilemma%20Tournament%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdEA4oZX5xKRmScJ8%2Fanother-iterated-prisoner-s-dilemma-tournament%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20Iterated%20Prisoner's%20Dilemma%20Tournament%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdEA4oZX5xKRmScJ8%2Fanother-iterated-prisoner-s-dilemma-tournament", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdEA4oZX5xKRmScJ8%2Fanother-iterated-prisoner-s-dilemma-tournament", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p>Last year, there was a lot of interest in the <a href=\"/lw/79r/prisoners_dilemma_as_a_game_theory_laboratory/\">IPD tournament</a> with people asking for regular events of this sort and developing new strategies (like Afterparty) within hours after the <a href=\"/lw/7f2/prisoners_dilemma_tournament_results/\">results</a> were published and also expressing interest in re-running the tournament with new rules that allowed for submitted strategies to evolve or read their opponent's source code. I noticed that many of the submitted strategies performed poorly because of a lack of understanding of the underlying mechanics, so I wrote a <a href=\"/lw/7l3/fixedlength_selective_iterative_prisoners_dilemma/\">comprehensive article on IPD math</a> that sparked some interesting comments.</p>\n<p>And then the whole thing was never spoken of again.</p>\n<p>So now I'd like to know: How many LWers would commit to competing&nbsp;in another tournament of this kind, and would someone be interested in hosting it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gdEA4oZX5xKRmScJ8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 9.095765019760398e-07, "legacy": true, "legacyId": "16392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vJAFXz6DWYnKq4Mue", "hamma4XgeNrsvAJv5", "9FFAqojvJodFmwzoN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T14:26:06.591Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berkeley, Brussels, Cambridge MA, Garden Grove CA, Moscow, Pittsburgh, Sydney, Vancouver, Washington DC", "slug": "weekly-lw-meetups-berkeley-brussels-cambridge-ma-garden", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:51.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uLDoQ5fGBBxGgrZaF/weekly-lw-meetups-berkeley-brussels-cambridge-ma-garden", "pageUrlRelative": "/posts/uLDoQ5fGBBxGgrZaF/weekly-lw-meetups-berkeley-brussels-cambridge-ma-garden", "linkUrl": "https://www.lesswrong.com/posts/uLDoQ5fGBBxGgrZaF/weekly-lw-meetups-berkeley-brussels-cambridge-ma-garden", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Brussels%2C%20Cambridge%20MA%2C%20Garden%20Grove%20CA%2C%20Moscow%2C%20Pittsburgh%2C%20Sydney%2C%20Vancouver%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berkeley%2C%20Brussels%2C%20Cambridge%20MA%2C%20Garden%20Grove%20CA%2C%20Moscow%2C%20Pittsburgh%2C%20Sydney%2C%20Vancouver%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuLDoQ5fGBBxGgrZaF%2Fweekly-lw-meetups-berkeley-brussels-cambridge-ma-garden%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Brussels%2C%20Cambridge%20MA%2C%20Garden%20Grove%20CA%2C%20Moscow%2C%20Pittsburgh%2C%20Sydney%2C%20Vancouver%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuLDoQ5fGBBxGgrZaF%2Fweekly-lw-meetups-berkeley-brussels-cambridge-ma-garden", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuLDoQ5fGBBxGgrZaF%2Fweekly-lw-meetups-berkeley-brussels-cambridge-ma-garden", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 502, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/a2\">Pittsburgh: Harry Potter and the Methods of Rationality:&nbsp;<span class=\"date\">18 May 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/a5\">Vancouver Minicamp Report-Back:&nbsp;<span class=\"date\">18 May 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/9l\">Brussels meetup:&nbsp;<span class=\"date\">19 May 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/a8\">Moscow meetup: general rationality:&nbsp;<span class=\"date\">20 May 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/a9\">DC Meetup:&nbsp;<span class=\"date\">20 May 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/9r\">Less Wrong Sydney - Bayes and Fun Theory:&nbsp;<span class=\"date\">21 May 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/a4\">Garden grove meetup:&nbsp;<span class=\"date\">22 May 2012 07:18PM</span></a></li>\n<li><a href=\"/meetups/ae\">Second Copenhagen meetup:&nbsp;<span class=\"date\">26 May 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/9s\">First Berlin meetup:&nbsp;<span class=\"date\">05 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup:&nbsp;<span class=\"date\">20 May 2012 02:20PM</span></a></li>\n<li><a href=\"/meetups/a7\">Small Berkeley Meetup:&nbsp;<span class=\"date\">23 May 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/a6\">NO Chicago Meetup 5/19:&nbsp;<span class=\"date\">26 May 2012 01:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uLDoQ5fGBBxGgrZaF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.095807668171988e-07, "legacy": true, "legacyId": "16214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T14:38:05.494Z", "modifiedAt": null, "url": null, "title": "Does rationalism affect your dreams?", "slug": "does-rationalism-affect-your-dreams", "viewCount": null, "lastCommentedAt": "2020-02-22T23:44:36.470Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GPQ4tq2fSTGvZyEP3/does-rationalism-affect-your-dreams", "pageUrlRelative": "/posts/GPQ4tq2fSTGvZyEP3/does-rationalism-affect-your-dreams", "linkUrl": "https://www.lesswrong.com/posts/GPQ4tq2fSTGvZyEP3/does-rationalism-affect-your-dreams", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20rationalism%20affect%20your%20dreams%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20rationalism%20affect%20your%20dreams%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGPQ4tq2fSTGvZyEP3%2Fdoes-rationalism-affect-your-dreams%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20rationalism%20affect%20your%20dreams%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGPQ4tq2fSTGvZyEP3%2Fdoes-rationalism-affect-your-dreams", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGPQ4tq2fSTGvZyEP3%2Fdoes-rationalism-affect-your-dreams", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 330, "htmlBody": "<p>Given how much you have learned of the techniques of rationality, of Bayesian updates and standard of evidence, of curiosity being the first virtue and being willing to update your beliefs... have any of your dreams been affected by them?</p>\n<p>&nbsp;</p>\n<p>The reason I ask; I'm reading the entirely of the Sequences, and am about an eighth of the way through. And I've just woken from a dream whose plot was somewhat unusual. I had noticed some mildly strange animals and/or people, and upon trying to find out what was going on, discovered a small riverside camp of people who fell well outside what I understood to be the realm of human variation. The person I had started investigating with then claimed to be a god, or if I preferred, a vastly powerful and intelligent alien entity, and offered to do something to prove it to me. I remembered that I had once established for myself a standard of evidence for exactly this sort of question - the growth of a new, perfectly functional limb, in a way outside of present medical understanding... and in a few moments, my dream-self was the possesser of a nice, long tail. I had not been expecting that to happen, and noticed I was extremely confused, and deliberately raised my estimate of the probability that I really was talking to a god-like figure by some number of decibans. At the end of the dream, said deity-figure said that he would offer to split us off from his 'main project', on a few conditions - one of which was 'no more clues', since he had given us 'more than enough to figure out what's going on'... ... whereupon I questioned a few things, and immediately woke up.</p>\n<p>I don't recall having a dream of anything like that sort before - and I dream in understandable narrative plots so often that I sometimes dream sequels. So I'm curious; is this a normal sort of thing that happens to LessWrongians?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GPQ4tq2fSTGvZyEP3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -1, "extendedScore": null, "score": 9.095860892266037e-07, "legacy": true, "legacyId": "16394", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T16:13:13.741Z", "modifiedAt": null, "url": null, "title": "Sneaky Strategies for TDT", "slug": "sneaky-strategies-for-tdt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:49.357Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drnickbone", "createdAt": "2012-01-20T17:19:55.216Z", "isAdmin": false, "displayName": "drnickbone"}, "userId": "GgwHTM3agaskLi9cx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ku8PxdQKdCY3z8E3Q/sneaky-strategies-for-tdt", "pageUrlRelative": "/posts/ku8PxdQKdCY3z8E3Q/sneaky-strategies-for-tdt", "linkUrl": "https://www.lesswrong.com/posts/ku8PxdQKdCY3z8E3Q/sneaky-strategies-for-tdt", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sneaky%20Strategies%20for%20TDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASneaky%20Strategies%20for%20TDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fku8PxdQKdCY3z8E3Q%2Fsneaky-strategies-for-tdt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sneaky%20Strategies%20for%20TDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fku8PxdQKdCY3z8E3Q%2Fsneaky-strategies-for-tdt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fku8PxdQKdCY3z8E3Q%2Fsneaky-strategies-for-tdt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1683, "htmlBody": "<p>My previous article on <a title=\"problematic problems\" href=\"/r/lesswrong/lw/cl2/problematic_problems_for_tdt/\">problematic problems</a> attracted quite a lot of feedback and comment. One of&nbsp;the questions it posed (2) was whether TDT should do something other than the initial analysis suggests.</p>\n<p>I've had a couple of&nbsp;ideas on that, partly in response to the comments. I'm posting some follow-up thoughts in the hope that they might help clarify the original problems. Basically, it seems there are some sneaky things that TDT could <em>try</em> to do, but mostly they are not robust to slightly different variants of the problems. Interestingly, some of these variants look even \"fairer\", since they contain no mentions of TDT anywhere in the problem statement.</p>\n<p>An alternative approach could be that TDT resolves to never let itself be out-performed by any other decision theory, because of evolutionary considerations as discussed <a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies/\">here</a>. Even if that requires a large sacrifice of immediate utility (e.g. two-boxing and taking $1000 along with CDT, rather than one-boxing and taking $1 million, but with CDT getting $1,001,000.) I don't currently know what to think about that, except that it makes my head spin; it also sounds like a rather Unfriendly form of AI.</p>\n<p>&nbsp;</p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><strong style=\"font-weight: bold;\">Problem 1</strong>: Omega (who experience has shown is always truthful) presents the usual two boxes A&nbsp;and B and announces the following. \"Before you entered the room, I ran a simulation of this problem as presented to an agent running TDT. I won't tell you what the agent decided, but I will tell you that if the agent two-boxed then I put nothing in Box B, whereas if the agent one-boxed then I put $1 million in Box B. Regardless of how the simulated agent decided, I put $1000 in Box A. Now please choose your box or boxes.\"</span></p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">Consider the following sneaky strategy S, which TDT agents might try to adopt.&nbsp;</span></p>\n<p>\"Use every piece of evidence at your disposal to try to decide whether you are the simulated agent. If you can prove you are not the simulated agent, then 2-box; otherwise 1-box\".</p>\n<p>We could imagine Omega making this strategy very easy for agents, by revealing the <em><strong>full source-code</strong></em> of the simulated agent \"C-sim\" as well as the full source code of the actual choosing agent \"C-act\". By \"full source code\" here, we have to include not just code describing how the agent executes TDT, but also its exact computational state on entry to the room (or simulated room), including a database of existing beliefs and proven theorems, preferences, values etc. To be fully precise, it should also describe any computational constraints that the agent faces, including time-out after t steps, or an upper bound on memory usage m, if they exist. The choosing agent then checks whether C-sim = C-act, and unless there is an exact match, it two-boxes. (If Omega is not kind enough to reveal C-act, a TDT agent may be able to generate it anyway via a quine-program construction.)</p>\n<p>There seem to be a few difficulties with strategy S:</p>\n<p>1. Each particular C-act will still under-perform on a matching problem. Though, arguably, it is an \"unfair\" problem (specifically tailored by Omega to get that C-act to shoot itself in the foot).</p>\n<p>2. It looks unattractive for different \"flavours\" of TDT agent to de-link themselves like this. Aren't we going to run into problems with the different flavours refusing to co-operate with each other on Prisoner's Dilemma, and only co-operating with exact clones? One answer is \"We don't know&nbsp;until we perform&nbsp;a formal analysis\". Another answer is \"No, the different flavours of TDT are all linked at the strategy level, since they all follow strategy S; they are just de-linked at the execution level, since they do something different based on their different epistemic circumstances.\" That's slightly more convincing, but I still feel a need for analysis. One concern is if there are problems where some TDT agents/utilities benefit from a fine-grained de-linking of execution and others don't; what then is the best overall TDT strategy?</p>\n<p>3. It doesn't solve all versions of the problem, since Omega doesn't have to reveal the exact C-sim code which it&nbsp;used. Omega could just say that it picked C-sim's code at random from a probability distribution across all full source-codes for TDT agents. (To&nbsp;model this as a formally-defined&nbsp;problem, we'd need to specify the distribution of course.) In such a case, no TDT agent&nbsp;can prove it is distinct from the simulation, so by strategy S, all TDT agents will one-box. And CDT will win against all flavours of TDT.</p>\n<p>It is still arguable that the problem is \"unfair\" to TDT as a whole, but now suppose that Omega samples its C-sim from a probability distribution across multiple types of agent source-code, with TDT agents just part of the population. There is thus some probability p_t &gt; 0 for the simulated agent being a TDT agent. If the difference in box values is big enough (roughly value_A / value_B&nbsp; &lt; p_t e.g. suppose that 1000/1000000 = 1/1000 &lt; p_t) then a TDT agent would still maximize expected winnings by 1-boxing. This doesn't seem particularly unfair to TDT, and yet CDT would still do better.</p>\n<p><br />An alternative strategy to S is what I'd informally call \"TDT uber alles\". It relies on long-range consequentialism, and perhaps \"utility trading\" as well (for TDT agents that don't inherently care about long-range consequences). A TDT agent might argue to itself \"If TDT beats CDT - and other theories - at each and every available opportunity, and at least matches them where it can't beat them, then TDT will come to dominate the agent space as quickly as possible, which will maximize my expected utility. So I'm not going to let CDT beat me here: I'll two-box\".</p>\n<p>&nbsp;</p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><strong style=\"font-weight: bold;\">Problem 2</strong>: Our ever-reliable Omega now presents ten boxes, numbered from 1 to 10, and announces the following. \"Exactly one of these boxes contains $1 million; the others contain nothing. You must take exactly one box to win the money; if you try to take more than one, then you won't be allowed to keep any winnings. Before you entered the room, I ran multiple simulations of this problem as presented to an agent running TDT, and determined the box which the agent was least likely to take. If there were several such boxes tied for equal-lowest probability, then I just selected one of them, the one labelled with the smallest number. I then placed $1 million in the selected box. Please choose your box.\"</span></p>\n<p>My original analysis gave TDT no more than 10% chance of winning the $1 million. However, here's something a bit cleverer. Suppose each particular TDT agent has a <a title=\"favourite number\" href=\"/r/lesswrong/lw/cl2/problematic_problems_for_tdt/6nsk\">favourite number</a> in the set {1, 2}&nbsp;constructed as a balanced function over TDT full source-codes. (One way I imagine&nbsp;doing this&nbsp;is by taking a SHA-256 hash of the full source-code, extracting the first bit, and adding 1.)&nbsp; Each agent chooses the box of their favourite number with probability 1. Since&nbsp;the simulated agent does so as well, by the rules of the game, the $1 million must be in one of boxes 1 or 2. And unless the full source-code of the simulated agent is known, it will not be possible for the choosing agent to tell whether Box 1 or 2 was picked by the sim, so that if the choosing agent picks their own favourite box, they have a 50% chance of winning. And CDT won't do any better.</p>\n<p>Omega could prevent this approach by the information it reveals. If it reveals the full source-code for C-sim (and in its simulation, presents this same source-code C-sim to C-sim itself) then TDT shouldn't try to execute the \"favourite number\" strategy. A better strategy is to pick each of the ten boxes with equal probability if finding that C-act = C-sim; or if finding that C-act differs from C-sim, then pick Box 1 with certainty.</p>\n<p>Or much as for Problem 1, Omega can vary the problem as follows:</p>\n<p>\"...Before you entered the room, I ran multiple simulations of this problem as presented to different randomly-selected TDT agents. I determined which box&nbsp;they were collectively least likely to take...\" (Again this needs a distribution to be specified to become formally precise.)</p>\n<p>There&nbsp;doesn't seem much&nbsp;that TDT agents can do about that, except to give a collective groan, and arrange that TDT collectively selects each of the ten boxes with equal probability. The simplest way to ensure that is for each TDT agent individually to select the boxes with equal probability (so each individual agent at least gets an equal chance at the prize). And any other agent just takes Box 1, laughing all the way to the bank.&nbsp;</p>\n<p>Consider a final variant as follows:</p>\n<p>\"...Before you entered the room, I ran multiple simulations of this problem as presented to different agents, sampled uniformly from different possible future universes according to their relative numbers, with the universes themselves sampled from my best projections of the future. I determined the box which the agents were least likely to take...\"</p>\n<p>If TDT uber alles is the future, then almost all the sampled agents will be TDT agents, so the problem is essentially as before. And now it doesn't look like Omega is being unfair at all (nothing discriminatory in the problem description<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">). But TDT is still stuck, and can get beaten by CDT in the present. </span></p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">One thought is that the TDT collective should vary the box probabilities very very slightly, so that Omega can tell which has the lowest probability, but regular CDT agents can't - in that case CDT also has only maximum 10% chance of winning. Possibly, the computationally-advanced members of the collective toss a logical coin (which only they and Omega can compute) to decide which box to de-weight; the less advanced members - ones who actually have to compete against rival decision theories - just pick at random. If CDT tries to simulate TDT instances, it will detect equal probabilities, pick Box 1 and most likely get it wrong...</span></p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><br /></span></p>\n<p><strong>Edit 2</strong>: I've clarified the alternative to the \"favourite number\" strategy if Omega reveals C-sim in Problem 2. We can actually get a range of different problems and strategies by slight variations here. See the comment below from lackofcheese, and my replies.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ku8PxdQKdCY3z8E3Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 9.096283523721452e-07, "legacy": true, "legacyId": "16355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3GyQXTy2WhYcaBgS2", "HT8jwNJ6vH7p9gaTT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T17:25:41.763Z", "modifiedAt": null, "url": null, "title": "Over-applying rationality: Indefinite lifespans", "slug": "over-applying-rationality-indefinite-lifespans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:52.661Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bart119", "createdAt": "2012-04-19T14:46:10.499Z", "isAdmin": false, "displayName": "Bart119"}, "userId": "JQyd5QFQyrRxdo9G4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9QW9TFzXCWK2k5BMx/over-applying-rationality-indefinite-lifespans", "pageUrlRelative": "/posts/9QW9TFzXCWK2k5BMx/over-applying-rationality-indefinite-lifespans", "linkUrl": "https://www.lesswrong.com/posts/9QW9TFzXCWK2k5BMx/over-applying-rationality-indefinite-lifespans", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Over-applying%20rationality%3A%20Indefinite%20lifespans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOver-applying%20rationality%3A%20Indefinite%20lifespans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9QW9TFzXCWK2k5BMx%2Fover-applying-rationality-indefinite-lifespans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Over-applying%20rationality%3A%20Indefinite%20lifespans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9QW9TFzXCWK2k5BMx%2Fover-applying-rationality-indefinite-lifespans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9QW9TFzXCWK2k5BMx%2Fover-applying-rationality-indefinite-lifespans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1165, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">UPDATE: One commenter said that arguments against the desirability of indefinite lifespans and their rebuttal had appeared before on LW and elsewhere. I am very interested in links to the best such discussions. If I'm going over old ground, a kind soul who can point me to the prior art would be much appreciated.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\"><br /></span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\">&nbsp;</p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">-----------</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\">&nbsp;</p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">I am very impressed with this site in its goal of outlining cognitive biases and seeing how they apply in everyday situations. When you're trying to decide how to spend money to alleviate human misery, it works. Yeah, it's better to save 50,000 people than 5,000. The two alternatives concern the same moral intuitions. When faced with a specific choice among alternatives, you may find that the tools of rationality will apply and tell you what to do, which might be contrary to what you would have done without such analysis.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">But when I see people trying to use Bayesian analysis for bigger questions beyond this, I think there is a substantial danger of being led astray by the method. When you can't find a clear way to analyze the situation and you are making low-confidence probability estimates of alternative futures and their utility, you'd do better to just put your rationality toolbox back on the shelf and decide the way you've always decided: gut feelings, intuition, doing what everyone else does, etc. </span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">Let's take as a case study the popular view on LW that living as long as possible is a good thing. First, within the range of currently common lifespans, it's a good thing to live a longer, healthier life; that is uncontroversial.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">But judging from the LW posts I've read, the prospect that science could reach a point where people could live indefinitely long is hailed as a great and noble goal. I think it would be terrible.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">First, let me distinguish an indefinite lifespan with true immortality. Is there anyone here who thinks true immortality is within reach? The sun will go red giant, making earth uninhabitable. If we hop from star to star, we get a little longer. But there's stuff like heat death and entropy and all. Not to mention the accumulation of small, mundane risks over a very long time. Eternity is one friggin' long time.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">If you don't have true immortality, you have a longer lifespan, and then you die. You still have to face the same profoundly settling issue. One wry formulation might be: whenever you do die, you're saving yourself the trouble of dying later. Different lifespans all end with the same unsettling matter of personal extinction. (Other thought: mortality is the most salient and immediate roadblock to finding a more satisfying meaning in life, but it's just the first one; if it was removed we'd find others beyond.) If you live 500 years instead of 100, you haven't achieved anything special. You haven't cheated death. You've just got an extra 400 years of living. The mundane stuff of eating, sleeping, thinking, seeing beautiful sunsets, chatting with friends, etc., and of course the less pleasant parts too.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">The ecological integrity of the world is already under severe strain. Perhaps with technological and political improvements we could improve how many people could live sustainably on earth by some constant factor, but it doesn&rsquo;t affect the current argument. Our population is limited. (You may think we're going to personally take off to colonize the stars. Let's assume for now it can't be done.)</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">Given a population limit, the effect of people living 500 or 50,000 years is that the available slots will soon be filled, and reproduction would have to be seriously curtailed. Children would be very rare.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">I think that no matter how healthy they are, a world full of people who are over 100 or 10,000 years old with very few children would be a place that 'just isn't right'.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">First, I estimate that the human mind isn't designed to live beyond 100 years (if that) and will tend to become unhappy. Such people think the same thoughts over and over. They get bored (a lot of people today get bored at 50). They still know they're going to die someday.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">Second, they live in a world without children. (One thing I've never seen in a LessWrong survey is the proportion who are parents -- given a highly educated group predominantly in their 20s, I would estimate it is very low).</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">And aside from their own personal boredom and personal lack of children in their lives, they know they live in a world where everyone else is in the same position. It's an ossifying world.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">Now let's put rationality into it. I imagine a Bayesian feeling comfortable and at home constructing an equation with two key constants being the number of people and the number of happy, productive years they get to live. Multiplication is in order. I'm not sure how the argument goes after that: Potential future lives that don't happen don't get to add to the utility (do they? Or at a discounted rate?) Even if they do, the utility of a new life has to be weighed against the lost utility of an existing person dying. We can see the equation coming out in favor of extending life as long as possible.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">The argument on the other side can also be framed in Bayesian terms. My estimation is that the utility of a large majority of these people who are over 200 years old is going to be very small. We can multiply their utility and conclude that the world will be a happier place with a mix of children, young people, and people croaking after 90 years of happy, productive life.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">I imagine a Bayesian frowning at this analysis. It seems imprecise. I could I suppose assign some sort of utility-reduction weight to those various factors and multiply them out, but it isn't really going to make the Bayesian very happy. It&rsquo;s not going to make me very happy either. I would rather just consider the situation as a whole and assign a low utility to the bulk of the population that's hundreds of years old rather than break it into parts.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">At one level, my argument with the pro-indefinite-lifespan faction is just a difference in what kind of a future world would be a happier place. We've plugged in our different assumptions and reached different conclusions.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">But to what extent does framing the problem as one of Bayesian analysis bias people to prefer the indefinite extension of individual lives? If your favorite tool is a hammer, things tend to look like nails. My conclusion feels more naturally framed if we ignore individual utilities and just say: a world full of people living indefinitely long would suck. Spelling it out in terms of utility just doesn't add anything.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">The practical implications are a separate question. Killing people when they get to be 90 is of course highly repugnant, as is asking them to kill themselves. But it might affect what sort of scientific research we fund and what drugs we approve, for starters.</span></p>\n<p style=\"margin: 0in; margin-bottom: .0001pt;\"><span style=\"font-family: Verdana, sans-serif;\">&nbsp;</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9QW9TFzXCWK2k5BMx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -12, "extendedScore": null, "score": 9.096605469006682e-07, "legacy": true, "legacyId": "16396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T18:50:01.407Z", "modifiedAt": null, "url": null, "title": "PSA: Learn to code", "slug": "psa-learn-to-code", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:26.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "John_Maxwell", "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zQgZtMT6Tms6KTgGW/psa-learn-to-code", "pageUrlRelative": "/posts/zQgZtMT6Tms6KTgGW/psa-learn-to-code", "linkUrl": "https://www.lesswrong.com/posts/zQgZtMT6Tms6KTgGW/psa-learn-to-code", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PSA%3A%20Learn%20to%20code&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APSA%3A%20Learn%20to%20code%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQgZtMT6Tms6KTgGW%2Fpsa-learn-to-code%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PSA%3A%20Learn%20to%20code%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQgZtMT6Tms6KTgGW%2Fpsa-learn-to-code", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQgZtMT6Tms6KTgGW%2Fpsa-learn-to-code", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 885, "htmlBody": "<p>Presumably you read Less Wrong because you&#x27;re interested in thinking better.</p><p>If so, you might be interested in another opportunity to improve the quality of your thinking: learn to code.</p><p>Like nothing else, coding forces you to identify flaws in your thinking. If your thinking is flawed, your program won&#x27;t work, except by accident. There&#x27;s no other discipline quite like this. If you&#x27;re a mathematician or physicist and you solve a problem wrong, your paper won&#x27;t tell you. Computer programmers have to measure their thinking against the gold standard of correctness <em>constantly</em>. The process of uncovering and fixing flaws in a program, usually called &quot;debugging&quot;, typically takes up the majority of the time spent on software projects.</p><p>But this is only the beginning. You&#x27;ve probably heard something like &quot;there are some problems that humans are good at and some problems that computers are good at&quot;. This is true. And once you learn to code, you&#x27;ll be able to exploit computers to solve the problems they are good at. Having a computer to write software with is like having a hi-tech mental exoskeleton that lets your mind run harder and jump higher. Want to know what the second most common letter for an English word to end in is? That&#x27;s a 15 line script. Tired of balancing chemical equations for your homework? Automate it.</p><p>Two more benefits that have less to do with thinking better:</p><ul><li><a href=\"http://www.bls.gov/ooh/Computer-and-Information-Technology/Software-developers.htm\">Employment</a>. You probably don&#x27;t need a computer science degree. I know of <em>two </em>Less Wrong users who learned to program <em>after college </em>and got jobs at Silicon Valley startups with just a project or two on their resume. (<a href=\"/user/MBlume\">MBlume</a> and <a href=\"/user/FrankAdamek\">FrankAdamek</a>.)  See <a href=\"/lw/di2/advice_on_getting_a_software_job/\">Advice on Getting a Software Job</a> by Tom McCabe for more on this possibility.</li><li>Productivity software. Writing your own is much nicer than using stuff made by other people in my experience. The reason there are so many to-do list applications is because everyone&#x27;s needs are different. If you use the terminal as your interface, it doesn&#x27;t take much effort to write this stuff; you&#x27;ll spend most of your time figuring out what you want it to do. (Terminal + cron on Linux with JSON log files has worked great for my needs.)</li></ul><p>Having enough coding knowledge to be dangerous may take persistence. If you tried and failed in the past, you probably either got stuck and gave up because there was no one to help you, or you just didn&#x27;t keep at it.</p><p>I&#x27;ve take two different introductory programming classes now to meet college requirements. The students in both seemed substantially less intelligent to me than Less Wrong users, and most were successful in learning to program. So based on the fact that you are reading this, I am pretty darn sure you have the necessary level of mental ability.</p><p><strong>Starting Out</strong></p><p>I recommend trying one of <a href=\"/lw/cpz/computer_science_and_programming_links_and/6p1n\">these</a> interactive tutorials <em>right now</em> to get a quick feel for what programming is like.</p><p>After you do that, here are some freely available materials for studying programming:</p><ul><li><a href=\"http://learnpythonthehardway.org/\">Learn Python the Hard Way</a>. I like Zed&#x27;s philosophy of having you type a lot of code, and apparently I&#x27;m not the only one. (<a href=\"http://learncodethehardway.org/\">Other books</a> in the Hard Way series.)</li><li><a href=\"http://eloquentjavascript.net/\">Eloquent JavaScript</a>. No installation needed for this one, and the exercises are nicely interspersed with the text.</li><li><a href=\"http://www.greenteapress.com/thinkpython/thinkpython.html\">Think Python</a>. More of a computer science focus. (&quot;Computer science&quot; refers to more abstract, less applied aspects of programming.)</li><li><a href=\"http://www.codecademy.com/\">Codecademy</a> (uses JavaScript). Makes use of gamification-type incentives. Just don&#x27;t lose sight of the fact that programming can be fun without them.</li><li><a href=\"http://hackety.com/\">Hackety Hack</a> (uses Ruby). Might be especially good for younger folks.</li><li><a href=\"http://www.htdp.org/\">How to Design Programs</a>. This book uses an elegant, quirky, somewhat impractical language called Scheme, and emphasizes a disciplined approach to programming. Maybe that will appeal to you. <a href=\"http://mitpress.mit.edu/sicp/full-text/book/book.html\">Structure and Interpretation of Computer Programs</a> is a tougher, more computer science heavy book that also uses Scheme. You should probably have a good understanding of programming with recursive functions before tackling it.</li></ul><p><a href=\"/lw/cnp/what_is_the_best_programming_language/\">Here&#x27;s</a> a discussion on Less Wrong about what the best programming language to start with is.</p><p>If you&#x27;re having a hard time getting something up and running, that&#x27;s a system administration challenge, not a programming one. Everyone hates system administration I think, except maybe system administrators. Keep calm, put your error message into Google, get help on a relevant IRC channel, etc.</p><p>Once you&#x27;ve got the basics, a good way to proceed is to decide on something you want to write and try to write it. If you don&#x27;t know how to get started, start making Google searches. Soon you&#x27;ll figure out the sort of libraries/frameworks people use to write your kind of program.</p><p>At first you may just be aping what others do. For example, if you want to learn something called &quot;bleh&quot;, searching on Google for &quot;bleh tutorial&quot; is a great way to start. Finding a working program and modifying it to see out how it changes is another good option. Soon you&#x27;ll graduate to appropriating sample code from documentation. As you write more code and see more of the software landscape, you&#x27;ll be better prepared to craft original approaches to writing software.<br/>   <br/>See also: <a href=\"/lw/f2/on_the_fence_major_in_cs/\">On the Fence? Major in CS</a>, <a href=\"http://norvig.com/21-days.html\">Teach Yourself Programming in 10 Years</a>, <a href=\"/lw/cpz/computer_science_and_programming_links_and/\">Computer Science and Programming: Links and Resources</a>.<br/> <br/> </p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 3, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zQgZtMT6Tms6KTgGW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 45, "extendedScore": null, "score": 0.000123, "legacy": true, "legacyId": "16397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["cKyMoRotH7yGhTPun", "4vXLzG4Ydxo4gqnzb", "bv36xQt9FNyotpqEy", "5sKx9BXRzrfHp2SL8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T19:10:00.792Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow meetup, creating community", "slug": "meetup-moscow-meetup-creating-community", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/68t5sTrj4qq2RNEXe/meetup-moscow-meetup-creating-community", "pageUrlRelative": "/posts/68t5sTrj4qq2RNEXe/meetup-moscow-meetup-creating-community", "linkUrl": "https://www.lesswrong.com/posts/68t5sTrj4qq2RNEXe/meetup-moscow-meetup-creating-community", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%20meetup%2C%20creating%20community&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%20meetup%2C%20creating%20community%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68t5sTrj4qq2RNEXe%2Fmeetup-moscow-meetup-creating-community%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%20meetup%2C%20creating%20community%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68t5sTrj4qq2RNEXe%2Fmeetup-moscow-meetup-creating-community", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68t5sTrj4qq2RNEXe%2Fmeetup-moscow-meetup-creating-community", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/aj'>Moscow meetup, creating community</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 May 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Topics will be:</p>\n\n<ul>\n<li><p>Creating Russian-speaking LessWrong community.</p></li>\n<li><p>Building lesswrong.ru website.</p></li>\n</ul>\n\n<p>We are also looking for Russian-speaking LessWrong readers to participate in discussions on the Russian forum, so please contact me, if you are the one!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/aj'>Moscow meetup, creating community</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "68t5sTrj4qq2RNEXe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 9.097068948500613e-07, "legacy": true, "legacyId": "16398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow_meetup__creating_community\">Discussion article for the meetup : <a href=\"/meetups/aj\">Moscow meetup, creating community</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 May 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Topics will be:</p>\n\n<ul>\n<li><p>Creating Russian-speaking LessWrong community.</p></li>\n<li><p>Building lesswrong.ru website.</p></li>\n</ul>\n\n<p>We are also looking for Russian-speaking LessWrong readers to participate in discussions on the Russian forum, so please contact me, if you are the one!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow_meetup__creating_community1\">Discussion article for the meetup : <a href=\"/meetups/aj\">Moscow meetup, creating community</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow meetup, creating community", "anchor": "Discussion_article_for_the_meetup___Moscow_meetup__creating_community", "level": 1}, {"title": "Discussion article for the meetup : Moscow meetup, creating community", "anchor": "Discussion_article_for_the_meetup___Moscow_meetup__creating_community1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T20:53:27.955Z", "modifiedAt": null, "url": null, "title": "A Scholarly AI Risk Wiki", "slug": "a-scholarly-ai-risk-wiki", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mYuMdmMmGM7fFj382/a-scholarly-ai-risk-wiki", "pageUrlRelative": "/posts/mYuMdmMmGM7fFj382/a-scholarly-ai-risk-wiki", "linkUrl": "https://www.lesswrong.com/posts/mYuMdmMmGM7fFj382/a-scholarly-ai-risk-wiki", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Scholarly%20AI%20Risk%20Wiki&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Scholarly%20AI%20Risk%20Wiki%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmYuMdmMmGM7fFj382%2Fa-scholarly-ai-risk-wiki%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Scholarly%20AI%20Risk%20Wiki%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmYuMdmMmGM7fFj382%2Fa-scholarly-ai-risk-wiki", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmYuMdmMmGM7fFj382%2Fa-scholarly-ai-risk-wiki", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1493, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>One large project proposal currently undergoing cost-benefit analysis at the Singularity Institute is a <strong>scholarly AI risk wiki</strong>. Below I will summarize the project proposal, because:</p>\n<ul>\n<li>I would like feedback from the community on it, and</li>\n<li>I would like to provide <em>just one example</em> of the kind of x-risk reduction that can be purchased with donations to the Singularity Institute.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>The Idea</h4>\n<p>Think <a href=\"http://www.scholarpedia.org/article/Main_Page\">Scholarpedia</a>:</p>\n<ul>\n<li>Open-access scholarly articles written at roughly the \"Scientific American\" level of difficulty.</li>\n<li>Runs on MediaWiki, but articles can only be created and edited by carefully selected authors, and curated by experts in the domain relevant to each article. (The editors would be SI researchers at first, and most of the authors and contributors would be staff researchers, research associates, or \"remote researchers\" from SI.)</li>\n</ul>\n<p>But the scholarly AI risk wiki would differ from Scholarpedia in these respects:</p>\n<ul>\n<li>Is focused on the subject of AI risk and related subjects.</li>\n<li>No formal peer review system. The articles would, however, be continuously revised in response to comments from experts in the relevant fields, many of whom already work in the x-risk field or are knowledgeable participants on LessWrong and in the SIAI/FHI/etc. communities.</li>\n<li>Articles will be written for a broader educated audience, not just for domain experts. (Many articles on Scholarpedia aren't <em>actually</em>&nbsp;written at the Scientific American level, despite that stated intent.)</li>\n<li>A built-in citations and references system, <a href=\"http://openwetware.org/wiki/Wikiomics:Biblio\">Biblio</a> (perhaps with the <a href=\"http://nmrwiki.org/wiki/index.php?title=Help:Biblio_extension\">BibTeX addition</a>).</li>\n</ul>\n<p><strong>Example articles</strong>: Eliezer Yudkowsky, Nick Bostrom, Ben Goertzel, Carl Shulman, Artificial General Intelligence, Decision Theory, Bayesian Decision Theory, Evidential Decision Theory, Causal Decision Theory, Timeless Decision Theory, Counterfactual Mugging, Existential Risk, Expected Utility, Expected Value, Utility, Friendly AI, Intelligence Explosion, AGI Sputnik Moment, Optimization Process, Optimization Power, Metaethics, Tool AI, Oracle AI, Unfriendly AI, Complexity of Value, Fragility of Value, Church-Turing Thesis, Nanny AI, Whole Brain Emulation, AIXI, Orthogonality Thesis, Instrumental Convergence Thesis, Biological Cognitive Enhancement, Nanotechnology, Recursive Self-Improvement, Intelligence, AI Takeoff, AI Boxing, Coherent Extrapolated Volition, Coherent Aggregated Volition, Reflective Decision Theory, Value Learning, Logical Uncertainty, Technological Development, Technological Forecasting, Emulation Argument for Human-Level AI, Evolutionary Argument for Human-Level AI, Extensibility Argument for Greater-Than-Human Intelligence, Anvil Problem, Optimality Notions, Universal Intelligence, Differential Intellectual Progress, Brain-Computer Interfaces, Malthusian Scenarios, Seed AI, Singleton, Superintelligence, Pascal's Mugging, Moore's Law, Superorganism, Infinities in Ethics, Economic Consequences of AI and Whole Brain Emulation, Creating Friendly AI, Cognitive Bias, Great Filter, Observation Selection Effects, Astronomical Waste, AI Arms Races, Normative and Moral Uncertainty, The Simulation Hypothesis, The Simulation Argument, Information Hazards, Optimal Philanthropy, Neuromorphic AI, Hazards from Large-Scale Computation, AGI Skepticism, Machine Ethics, Event Horizon Thesis, Acceleration Thesis, Singularitarianism, Subgoal Stomp, Wireheading, Ontological Crisis, Moral Divergence, Utility Indifference, Personhood Predicates, Consequentialism, Technological Revolutions, Prediction Markets, Global Catastrophic Risks, Paperclip Maximizer, Coherent Blended Volition, Fun Theory, Game Theory, The Singularity, History of AI Risk Thought, Utility Extraction, Reinforcement Learning, Machine Learning, Probability Theory, Prior Probability, Preferences, Regulation and AI Risk, Godel Machine, Lifespan Dilemma, AI Advantages, Algorithmic Complexity, Human-AGI Integration and Trade, AGI Chaining, Value Extrapolation, 5 and 10 Problem.</p>\n<p><strong>Most of these articles would contain previously unpublished research</strong> (not published even in blog posts or comments), because most of the AI risk research that has been done has never been written up in any form but sits in the brains and Google docs of people like Yudkowsky, Bostrom, Shulman, and Armstrong.</p>\n<p>&nbsp;</p>\n<h4>Benefits</h4>\n<p>More than a year ago, I <a href=\"/lw/4r1/how_siai_could_publish_in_mainstream_cognitive/\">argued</a> that SI would benefit from publishing short, clear, scholarly articles on AI risk. More recently, Nick Beckstead <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k2x\">expressed</a> the point this way:</p>\n<blockquote>Most extant presentations of SIAI's views leave much to be desired in terms of clarity, completeness, concision, accessibility, and credibility signals.</blockquote>\n<p>Chris Hallquist <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k30\">added</a>:</p>\n<blockquote>I've been trying to write something about Eliezer's debate with Robin Hanson, but the problem I keep running up against is that Eliezer's points are not clearly articulated at all. Even making my best educated guesses about what's supposed to go in the gaps in his arguments, I still ended up with very little.</blockquote>\n<p>Of course, SI has long <em>known</em>&nbsp;it could benefit from clearer presentations of its views, but the cost was too high to implement it. Scholarly authors of <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>'s skill and productivity are extremely rare, and almost none of them care about AI risk. But now, let's be clear about what a scholarly AI risk wiki could accomplish:</p>\n<ul>\n<li><strong>Provide a clearer argument for caring about AI risk</strong>. Journal-published articles like <a href=\"http://www.consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a> can be clear and scholarly, but the linear format is not ideal for analyzing such a complex thing as AI risk. Even a 65-page article like Chalmers (2010) can't hope to address even the tiniest fraction of the relevant evidence and arguments. Nor can it hope to respond to the tiniest fraction of all the objections that are \"obvious\" to <em>some</em>&nbsp;of its readers. What we need is a <em>modular</em>&nbsp;presentation of the evidence and the arguments, so that those who accept physicalism, near-term AI, and the orthogonality thesis can jump right to the sections on why various AI boxing methods may not work, while those who aren't sure what to think of AI timelines can jump to those articles, and those who accept most of the concern for AI risk but think there's no reason to assert humane values over arbitrary machine values can jump to the article on <em>that</em>&nbsp;subject. (Note that I don't presume all the analysis that would go into building an AI risk wiki would end up clearly recommending SI's current, very specific positions on AI risk, but I'm pretty sure it would clearly recommend <em>some</em>&nbsp;considerable concern for AI risk.)</li>\n<li><strong>Provide a clearer picture of our AI risk situation</strong>. Without clear presentations of most of the relevant factors, it is <em>very costly</em>&nbsp;for interested parties to develop a clear picture of our AI risk situation.<em>&nbsp;</em>If you wanted to get roughly as clear a picture of our AI risk situation as can be had today, you'd have to (1) read several books, hundreds of articles and blog posts, and the archives of SI's decision theory mailing list and several forums, (2) analyze them in detail to try to fill in all the missing steps in the reasoning presented in these sources, and (3) have dozens of hours of conversation with the leading experts in the field (Yudkowsky, Bostrom, Shulman, Armstrong, etc.). With a scholarly AI risk wiki, a decently clear picture of our AI risk situation will be much cheaper to acquire. Indeed, it will almost certainly clarify the picture of our situation even for the leading experts in the field.</li>\n<li><strong>Make it easier to do AI risk research</strong>. A researcher hoping to do AI risk research is in much the same position as the interested reader hoping to gain a clearer picture of our AI risk situation. Most of the relevant material is scattered across hundreds of books, articles, blog posts, forum comments, mailing list messages, and personal conversations. And <em>those</em>&nbsp;presentations of the ideas leave \"much to be desired in terms of clarity, completeness, concision, accessibility...\" This makes it hard to do research, in big-picture conceptual ways, but also in small, annoying ways. What paper can you cite on Thing X and Thing Y? When the extant scholarly literature base is small, you can't cite the sources that other people have dug up already. You have to do all that digging yourself.</li>\n</ul>\n<p>There are some benefits to the <em>wiki </em>structure in particular:</p>\n<ul>\n<li>Some wiki articles can largely be ripped/paraphrased from existing papers like <a href=\"http://www.consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a> and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser &amp; Salamon (2012)</a>.</li>\n<li>Many wiki articles can be adapted to become journal articles, if they are seen as having much value. Probably, 1-3 wiki articles could be developed, then adapted and combined into a journal article and published, and then the original wiki article(s) could be published on the wiki (while citing the now-published journal article).</li>\n<li>It's not an all-or-nothing project. Some value is gained by having <em>some</em>&nbsp;articles on the wiki, more value is gained by having <em>more</em>&nbsp;articles on the wiki.</li>\n<li>There are robust programs and plugins for managing this kind of project (MediaWiki, Biblio, etc.)</li>\n<li>Dozens or hundreds of people can contribute, though they will all be selected by editors. (SI's army of part-time remote researchers is already more than a dozen strong, each with different skills and areas of domain expertise.)</li>\n</ul>\n<p>&nbsp;</p>\n<h4>Costs</h4>\n<p>This would be a large project, and has significant costs. I'm still estimating the costs, but here are some ballpark numbers for a scholarly AI risk wiki containing all the example articles above:</p>\n<ul>\n<li>1,920 hours of SI staff time (80 hrs/week for 24 months). This comes out to about $48,000, depending on who is putting in these hours.</li>\n<li>$384,000 paid to remote researchers and writers ($16,000/mo for 24 months; our remote researchers generally work part-time, and are relatively inexpensive).</li>\n<li>$30,000 for wiki design, development, hosting costs</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZWmB62xB6uLyRuAtX": 1, "NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mYuMdmMmGM7fFj382", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 28, "extendedScore": null, "score": 9.097528628442843e-07, "legacy": true, "legacyId": "16399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>One large project proposal currently undergoing cost-benefit analysis at the Singularity Institute is a <strong>scholarly AI risk wiki</strong>. Below I will summarize the project proposal, because:</p>\n<ul>\n<li>I would like feedback from the community on it, and</li>\n<li>I would like to provide <em>just one example</em> of the kind of x-risk reduction that can be purchased with donations to the Singularity Institute.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"The_Idea\">The Idea</h4>\n<p>Think <a href=\"http://www.scholarpedia.org/article/Main_Page\">Scholarpedia</a>:</p>\n<ul>\n<li>Open-access scholarly articles written at roughly the \"Scientific American\" level of difficulty.</li>\n<li>Runs on MediaWiki, but articles can only be created and edited by carefully selected authors, and curated by experts in the domain relevant to each article. (The editors would be SI researchers at first, and most of the authors and contributors would be staff researchers, research associates, or \"remote researchers\" from SI.)</li>\n</ul>\n<p>But the scholarly AI risk wiki would differ from Scholarpedia in these respects:</p>\n<ul>\n<li>Is focused on the subject of AI risk and related subjects.</li>\n<li>No formal peer review system. The articles would, however, be continuously revised in response to comments from experts in the relevant fields, many of whom already work in the x-risk field or are knowledgeable participants on LessWrong and in the SIAI/FHI/etc. communities.</li>\n<li>Articles will be written for a broader educated audience, not just for domain experts. (Many articles on Scholarpedia aren't <em>actually</em>&nbsp;written at the Scientific American level, despite that stated intent.)</li>\n<li>A built-in citations and references system, <a href=\"http://openwetware.org/wiki/Wikiomics:Biblio\">Biblio</a> (perhaps with the <a href=\"http://nmrwiki.org/wiki/index.php?title=Help:Biblio_extension\">BibTeX addition</a>).</li>\n</ul>\n<p><strong>Example articles</strong>: Eliezer Yudkowsky, Nick Bostrom, Ben Goertzel, Carl Shulman, Artificial General Intelligence, Decision Theory, Bayesian Decision Theory, Evidential Decision Theory, Causal Decision Theory, Timeless Decision Theory, Counterfactual Mugging, Existential Risk, Expected Utility, Expected Value, Utility, Friendly AI, Intelligence Explosion, AGI Sputnik Moment, Optimization Process, Optimization Power, Metaethics, Tool AI, Oracle AI, Unfriendly AI, Complexity of Value, Fragility of Value, Church-Turing Thesis, Nanny AI, Whole Brain Emulation, AIXI, Orthogonality Thesis, Instrumental Convergence Thesis, Biological Cognitive Enhancement, Nanotechnology, Recursive Self-Improvement, Intelligence, AI Takeoff, AI Boxing, Coherent Extrapolated Volition, Coherent Aggregated Volition, Reflective Decision Theory, Value Learning, Logical Uncertainty, Technological Development, Technological Forecasting, Emulation Argument for Human-Level AI, Evolutionary Argument for Human-Level AI, Extensibility Argument for Greater-Than-Human Intelligence, Anvil Problem, Optimality Notions, Universal Intelligence, Differential Intellectual Progress, Brain-Computer Interfaces, Malthusian Scenarios, Seed AI, Singleton, Superintelligence, Pascal's Mugging, Moore's Law, Superorganism, Infinities in Ethics, Economic Consequences of AI and Whole Brain Emulation, Creating Friendly AI, Cognitive Bias, Great Filter, Observation Selection Effects, Astronomical Waste, AI Arms Races, Normative and Moral Uncertainty, The Simulation Hypothesis, The Simulation Argument, Information Hazards, Optimal Philanthropy, Neuromorphic AI, Hazards from Large-Scale Computation, AGI Skepticism, Machine Ethics, Event Horizon Thesis, Acceleration Thesis, Singularitarianism, Subgoal Stomp, Wireheading, Ontological Crisis, Moral Divergence, Utility Indifference, Personhood Predicates, Consequentialism, Technological Revolutions, Prediction Markets, Global Catastrophic Risks, Paperclip Maximizer, Coherent Blended Volition, Fun Theory, Game Theory, The Singularity, History of AI Risk Thought, Utility Extraction, Reinforcement Learning, Machine Learning, Probability Theory, Prior Probability, Preferences, Regulation and AI Risk, Godel Machine, Lifespan Dilemma, AI Advantages, Algorithmic Complexity, Human-AGI Integration and Trade, AGI Chaining, Value Extrapolation, 5 and 10 Problem.</p>\n<p><strong>Most of these articles would contain previously unpublished research</strong> (not published even in blog posts or comments), because most of the AI risk research that has been done has never been written up in any form but sits in the brains and Google docs of people like Yudkowsky, Bostrom, Shulman, and Armstrong.</p>\n<p>&nbsp;</p>\n<h4 id=\"Benefits\">Benefits</h4>\n<p>More than a year ago, I <a href=\"/lw/4r1/how_siai_could_publish_in_mainstream_cognitive/\">argued</a> that SI would benefit from publishing short, clear, scholarly articles on AI risk. More recently, Nick Beckstead <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k2x\">expressed</a> the point this way:</p>\n<blockquote>Most extant presentations of SIAI's views leave much to be desired in terms of clarity, completeness, concision, accessibility, and credibility signals.</blockquote>\n<p>Chris Hallquist <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k30\">added</a>:</p>\n<blockquote>I've been trying to write something about Eliezer's debate with Robin Hanson, but the problem I keep running up against is that Eliezer's points are not clearly articulated at all. Even making my best educated guesses about what's supposed to go in the gaps in his arguments, I still ended up with very little.</blockquote>\n<p>Of course, SI has long <em>known</em>&nbsp;it could benefit from clearer presentations of its views, but the cost was too high to implement it. Scholarly authors of <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>'s skill and productivity are extremely rare, and almost none of them care about AI risk. But now, let's be clear about what a scholarly AI risk wiki could accomplish:</p>\n<ul>\n<li><strong>Provide a clearer argument for caring about AI risk</strong>. Journal-published articles like <a href=\"http://www.consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a> can be clear and scholarly, but the linear format is not ideal for analyzing such a complex thing as AI risk. Even a 65-page article like Chalmers (2010) can't hope to address even the tiniest fraction of the relevant evidence and arguments. Nor can it hope to respond to the tiniest fraction of all the objections that are \"obvious\" to <em>some</em>&nbsp;of its readers. What we need is a <em>modular</em>&nbsp;presentation of the evidence and the arguments, so that those who accept physicalism, near-term AI, and the orthogonality thesis can jump right to the sections on why various AI boxing methods may not work, while those who aren't sure what to think of AI timelines can jump to those articles, and those who accept most of the concern for AI risk but think there's no reason to assert humane values over arbitrary machine values can jump to the article on <em>that</em>&nbsp;subject. (Note that I don't presume all the analysis that would go into building an AI risk wiki would end up clearly recommending SI's current, very specific positions on AI risk, but I'm pretty sure it would clearly recommend <em>some</em>&nbsp;considerable concern for AI risk.)</li>\n<li><strong>Provide a clearer picture of our AI risk situation</strong>. Without clear presentations of most of the relevant factors, it is <em>very costly</em>&nbsp;for interested parties to develop a clear picture of our AI risk situation.<em>&nbsp;</em>If you wanted to get roughly as clear a picture of our AI risk situation as can be had today, you'd have to (1) read several books, hundreds of articles and blog posts, and the archives of SI's decision theory mailing list and several forums, (2) analyze them in detail to try to fill in all the missing steps in the reasoning presented in these sources, and (3) have dozens of hours of conversation with the leading experts in the field (Yudkowsky, Bostrom, Shulman, Armstrong, etc.). With a scholarly AI risk wiki, a decently clear picture of our AI risk situation will be much cheaper to acquire. Indeed, it will almost certainly clarify the picture of our situation even for the leading experts in the field.</li>\n<li><strong>Make it easier to do AI risk research</strong>. A researcher hoping to do AI risk research is in much the same position as the interested reader hoping to gain a clearer picture of our AI risk situation. Most of the relevant material is scattered across hundreds of books, articles, blog posts, forum comments, mailing list messages, and personal conversations. And <em>those</em>&nbsp;presentations of the ideas leave \"much to be desired in terms of clarity, completeness, concision, accessibility...\" This makes it hard to do research, in big-picture conceptual ways, but also in small, annoying ways. What paper can you cite on Thing X and Thing Y? When the extant scholarly literature base is small, you can't cite the sources that other people have dug up already. You have to do all that digging yourself.</li>\n</ul>\n<p>There are some benefits to the <em>wiki </em>structure in particular:</p>\n<ul>\n<li>Some wiki articles can largely be ripped/paraphrased from existing papers like <a href=\"http://www.consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a> and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Muehlhauser &amp; Salamon (2012)</a>.</li>\n<li>Many wiki articles can be adapted to become journal articles, if they are seen as having much value. Probably, 1-3 wiki articles could be developed, then adapted and combined into a journal article and published, and then the original wiki article(s) could be published on the wiki (while citing the now-published journal article).</li>\n<li>It's not an all-or-nothing project. Some value is gained by having <em>some</em>&nbsp;articles on the wiki, more value is gained by having <em>more</em>&nbsp;articles on the wiki.</li>\n<li>There are robust programs and plugins for managing this kind of project (MediaWiki, Biblio, etc.)</li>\n<li>Dozens or hundreds of people can contribute, though they will all be selected by editors. (SI's army of part-time remote researchers is already more than a dozen strong, each with different skills and areas of domain expertise.)</li>\n</ul>\n<p>&nbsp;</p>\n<h4 id=\"Costs\">Costs</h4>\n<p>This would be a large project, and has significant costs. I'm still estimating the costs, but here are some ballpark numbers for a scholarly AI risk wiki containing all the example articles above:</p>\n<ul>\n<li>1,920 hours of SI staff time (80 hrs/week for 24 months). This comes out to about $48,000, depending on who is putting in these hours.</li>\n<li>$384,000 paid to remote researchers and writers ($16,000/mo for 24 months; our remote researchers generally work part-time, and are relatively inexpensive).</li>\n<li>$30,000 for wiki design, development, hosting costs</li>\n</ul>\n<p>&nbsp;</p>", "sections": [{"title": "The Idea", "anchor": "The_Idea", "level": 1}, {"title": "Benefits", "anchor": "Benefits", "level": 1}, {"title": "Costs", "anchor": "Costs", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "57 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "4oWXnodxAu4WgHnrd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-25T21:40:53.666Z", "modifiedAt": null, "url": null, "title": "When what is rational is not what is \"right\"", "slug": "when-what-is-rational-is-not-what-is-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:53.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beberly37", "createdAt": "2012-05-23T15:39:45.044Z", "isAdmin": false, "displayName": "beberly37"}, "userId": "A3jR7csHrpM8QBtxb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8H28QRsEGFoNKer4J/when-what-is-rational-is-not-what-is-right", "pageUrlRelative": "/posts/8H28QRsEGFoNKer4J/when-what-is-rational-is-not-what-is-right", "linkUrl": "https://www.lesswrong.com/posts/8H28QRsEGFoNKer4J/when-what-is-rational-is-not-what-is-right", "postedAtFormatted": "Friday, May 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20what%20is%20rational%20is%20not%20what%20is%20%22right%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20what%20is%20rational%20is%20not%20what%20is%20%22right%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8H28QRsEGFoNKer4J%2Fwhen-what-is-rational-is-not-what-is-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20what%20is%20rational%20is%20not%20what%20is%20%22right%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8H28QRsEGFoNKer4J%2Fwhen-what-is-rational-is-not-what-is-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8H28QRsEGFoNKer4J%2Fwhen-what-is-rational-is-not-what-is-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 438, "htmlBody": "<p>A little background: &nbsp;I have an above average commute to work and make use of the time by listening to public radio. &nbsp;I have been doing this for just over a year without \"doing my part\" and contributing. &nbsp;The primary justification of this has been that my commute path and times have me listening to three different public radio stations. &nbsp;I never could decide which station to pledge; which needed it more, which I liked best, which had the least annoying pledge breaks.</p>\n<p>The other day, during a pledge break, they played a promo by Ira Glass which went something like:</p>\n<p><em>I'm going to say something that has never happened in a pledge break before. &nbsp;We don't need your money. You do not have to call. &nbsp;There is no evidence to back that up. &nbsp;Every year we say you have to pledge and give your money or we will go away, but year after year, we are still here, even though you didn't pledge.</em></p>\n<p><em>You should call because its the right thing to do. &nbsp;You like public radio, enough to listen to a pledge break, so you should pledge, not because it is logical but because it is right...</em></p>\n<p>This struck a note with me. &nbsp;Perhaps because of my recent attention here at LW (does that count as focus bias?). &nbsp;It&nbsp;brought two LW&nbsp;relevant&nbsp;questions to mind.</p>\n<p>If pledging public radio is the right thing to do, but all of the evidence suggests I personally do not have to pledge, what rational algorithm achieves that outcome? &nbsp;It is not like you can make a 'lives saved per dollar' figure for NPR, it is either there or not. &nbsp;I guess in a really poorly funded station, one might be able to come up with a figure for minutes of&nbsp;programming&nbsp;per dollar. &nbsp;Does doing the \"right thing\" simply produce a warm feeling? &nbsp;Or is it more like I should pledge because everyone should pledge, similar to &nbsp;\"I should tell the truth so no one lies to me\"?</p>\n<p>&nbsp;</p>\n<p>Would pledging public radio make a good metric for the friendliness of an AI? &nbsp;Obviously not an&nbsp;unchangeable&nbsp;line of code that says \"pledge NPR\", but an AI that decides becoming a member of KQED is a good thing to do. &nbsp;I'm sure there are plenty of other situations that are similar like donating to open source software that you use or paying to park in the state forest parking lot instead of parking on the street and walking in for free. &nbsp;It might seem silly, but an AI that chooses to become a member of the local public radio will probably also choose not killing everyone over some increase in another utility function. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8H28QRsEGFoNKer4J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -19, "extendedScore": null, "score": -4e-05, "legacy": true, "legacyId": "16400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T00:58:05.309Z", "modifiedAt": null, "url": null, "title": "What is the best programming language?", "slug": "what-is-the-best-programming-language", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:12.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4vXLzG4Ydxo4gqnzb/what-is-the-best-programming-language", "pageUrlRelative": "/posts/4vXLzG4Ydxo4gqnzb/what-is-the-best-programming-language", "linkUrl": "https://www.lesswrong.com/posts/4vXLzG4Ydxo4gqnzb/what-is-the-best-programming-language", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20best%20programming%20language%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20best%20programming%20language%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vXLzG4Ydxo4gqnzb%2Fwhat-is-the-best-programming-language%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20best%20programming%20language%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vXLzG4Ydxo4gqnzb%2Fwhat-is-the-best-programming-language", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vXLzG4Ydxo4gqnzb%2Fwhat-is-the-best-programming-language", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 324, "htmlBody": "<p>Learning to program in a given language requires a non-trivial amount of time. This seems to be agreed upon as a good use of LessWrongers' time.</p>\n<p>Each language may be more useful than others for particular purposes. However, like e.g. the choice of donation to a particular charity, we shouldn't expect the trade-offs of focusing on one versus another not to exist.</p>\n<p>Suppose I know nothing about programming... And I want to make a choice about what language to pick up beyond merely what sounds cool at the time. In short I would want to spend my <a href=\"/lw/ui/use_the_try_harder_luke/\">five minutes</a> on the problem before <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">jumping</a> to a solution.</p>\n<p>As an example of the dilemma, if I spend my time learning Scheme or Lisp, I will gain a particular kind of skill. It won't be a very directly marketable one, but it could (in theory) make me a better programmer. \"Code as lists\" is a powerful perspective -- and Eric S. Raymond recommends learning Lisp for this reason.</p>\n<p>Forth (or any similar concatenative language) presents a different yet similarly powerful perspective, one which encourages extreme factorization and use of small well-considered definitions of words for frequently reused concepts.</p>\n<p>Python encourages object oriented thinking and explicit declaration. Ruby is object oriented and complexity-hiding to the point of being almost magical.</p>\n<p>C teaches functions and varying abstraction levels. Javascript is more about the high level abstractions.</p>\n<p>If a newbie programmer focuses on any of these they will come out of it a different kind of programmer. If a competent programmer avoids one of these things they will avoid different kinds of costs as well as different kinds of benefits.</p>\n<p>Is it better to focus on one path, avoiding contamination from others?</p>\n<p>Is it better to explore several simultaneously, to make sure you don't miss the best parts?</p>\n<p>Which one results in converting time to dollars the most quickly?</p>\n<p>Which one most reliably converts you to a higher value programmer over a longer period of time?</p>\n<p>What other caveats are there?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4vXLzG4Ydxo4gqnzb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 4, "extendedScore": null, "score": 9.098615743713309e-07, "legacy": true, "legacyId": "16405", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fhEPnveFhb9tmd7Pe", "uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T04:12:03.811Z", "modifiedAt": null, "url": null, "title": "Share Your Checklists!", "slug": "share-your-checklists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:43.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XKXsJAFnnBLeqfPiY/share-your-checklists", "pageUrlRelative": "/posts/XKXsJAFnnBLeqfPiY/share-your-checklists", "linkUrl": "https://www.lesswrong.com/posts/XKXsJAFnnBLeqfPiY/share-your-checklists", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Share%20Your%20Checklists!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShare%20Your%20Checklists!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKXsJAFnnBLeqfPiY%2Fshare-your-checklists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Share%20Your%20Checklists!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKXsJAFnnBLeqfPiY%2Fshare-your-checklists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKXsJAFnnBLeqfPiY%2Fshare-your-checklists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p><a href=\"/lw/19/checklists/\">Checklists</a> <a href=\"/lw/8vm/the_rationalists_checklist/\">are</a> <a href=\"http://www.amazon.com/Checklist-Manifesto-How-Things-Right/dp/0805091742\">powerful</a>, and I don't use them enough. You probably don't, either.</p>\n<p>Below are some of my own checklists. Please share your own!</p>\n<p>&nbsp;</p>\n<p><strong>I don't know how to do X</strong>.</p>\n<ol>\n<li>Check <a href=\"http://www.ehow.com/\">eHow</a>, <a href=\"https://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=how+to+do+x\">Google</a>.</li>\n<li>Skim-read the <em>For Dummies</em>&nbsp;book on the subject.</li>\n<li>Check my social network for somebody who knows how to do X, ask the expert how to do X.</li>\n</ol>\n<p><br /> <strong>I don't understand X</strong>.</p>\n<ol>\n<li>Check <a href=\"http://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>, <a href=\"http://betterexplained.com/\">BetterExplained</a>, <a href=\"http://www.wisegeek.com/\">WiseGeek</a>.</li>\n<li>Read the relevant chapter(s) in a recent textbook, or find a recent review article. (See <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">here</a>.)</li>\n<li>Check my social network for someone who understands X, ask for a tutorial. Offer to buy them coffee or lunch if necessary.</li>\n</ol>\n<p><br /> <strong>I feel mentally exhausted but can't afford to sleep right now</strong>.</p>\n<ol>\n<li>Take a shower.</li>\n<li>Watch 10 minutes of <a href=\"http://www.wimp.com/\">wimp.com</a>, <a href=\"http://www.youtube.com/results?search_query=funny+cat&amp;oq=funny+cat&amp;aq=f&amp;aqi=g10&amp;aql=&amp;gs_l=youtube.3..0l10.720.1608.0.1726.9.8.0.0.0.0.128.601.7j1.8.0...0.0.Ftzt_KE9ZVM\">cats on YouTube</a>, <a href=\"http://www.youtube.com/show/ignreviews/feed\">IGN video reviews</a>, or <a href=\"http://trailers.apple.com/\">movie trailers</a>.</li>\n<li>Go for a walk and listen to awesome music on <a href=\"http://www.amazon.com/gp/product/B000ULAP4U/\">high-quality headphones</a>.</li>\n</ol>\n<p><br /> <strong>I don't want to get out of bed, but I should.</strong></p>\n<ol>\n<li>Imagine how good a hot shower will feel, then try again to get out of bed.</li>\n<li>Set my phone alarm to go off in 5 minutes, then slide it across the floor to the other side of the room.</li>\n</ol>\n<p><br /> <strong>I'm procrastinating on task X</strong>.</p>\n<ol>\n<li>Give the task to someone else. (Usually, this isn't possible, because I've always delegated away as much as possible.)</li>\n<li>Think about which part of the <a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">procrastination equation</a> is likely causing me the most trouble, and use one of the techniques aimed at tackling that specific problem that has worked best for me in the past.</li>\n<li>Procrastinate on task X by doing a different task that is slightly less urgent/important but still productive. (See <a href=\"http://www.structuredprocrastination.com/\">structured procrastination</a>.)</li>\n</ol>\n<p><br /> <strong>I'm about to send an email / post a comment of some significance</strong>.</p>\n<ol>\n<li>Is there criticism in the email or comment? Use the <a href=\"http://www.rightattitudes.com/2008/02/20/sandwich-feedback-technique/\">sandwich technique</a>.</li>\n<li>Emulate my reader(s) and predict what reaction they will have. If it's not the reaction I am aiming for with this communication, restructure the communication.</li>\n</ol>\n<p>(I don't do these ones <em>nearly</em> enough! D'oh!)</p>\n<p><br /> <strong>I feel sad about not doing a better job at X</strong>.</p>\n<ol>\n<li>Figure out something I can do better with regard to X, simulate in my head the steps required to execute that improvement, and if feasible then execute the improvement.</li>\n<li>Think about all the things I'm doing pretty well despite running on fucked-up ape-brain software and hardware.</li>\n</ol>\n<p><br /> <strong>I'm about to make a decision of some significance</strong>.</p>\n<ol>\n<li><a href=\"/lw/b4f/sotw_check_consequentialism/\">Check consequentialism</a>.</li>\n<li><a href=\"/lw/85x/value_of_information_four_examples/\">Check VoI</a>. Can I improve my decision by purchasing some piece of information relatively cheaply? (This includes running checks against various biases that may be at play, performing a more formal cost-benefit analysis, etc.)</li>\n<li>Sanity-check the decision with a couple people who have good decision-making skills and possess much of the relevant information.</li>\n</ol>\n<p><br /> I could go on, but... what are yours? (Now is also a good opportunity to <em>make</em>&nbsp;some checklists for yourself, based on what you think tends to work for you.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2oWPnnnzMbiAxWfbs": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XKXsJAFnnBLeqfPiY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 46, "extendedScore": null, "score": 9.099477942451871e-07, "legacy": true, "legacyId": "16407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HnzB46zsL8ehdpLcd", "tLR9YZHiNoDE2Czjh", "37sHjeisS9uJufi4u", "Ty2tjPwv8uyPK9vrz", "xypbWhzEEw4ZsRK9i", "vADtvr9iDeYsCDfxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T05:43:45.960Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Timeless Identity", "slug": "seq-rerun-timeless-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ct5vLMqGRjSDqL3Gv/seq-rerun-timeless-identity", "pageUrlRelative": "/posts/Ct5vLMqGRjSDqL3Gv/seq-rerun-timeless-identity", "linkUrl": "https://www.lesswrong.com/posts/Ct5vLMqGRjSDqL3Gv/seq-rerun-timeless-identity", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Timeless%20Identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Timeless%20Identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCt5vLMqGRjSDqL3Gv%2Fseq-rerun-timeless-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Timeless%20Identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCt5vLMqGRjSDqL3Gv%2Fseq-rerun-timeless-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCt5vLMqGRjSDqL3Gv%2Fseq-rerun-timeless-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>Today's post, <a href=\"/lw/qx/timeless_identity/\">Timeless Identity</a> was originally published on 03 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How can you be the same person tomorrow as today, in the river that never flows, when not a drop of water is shared between one time and another? Having used physics to completely trash all naive theories of identity, we reassemble a conception of persons and experiences from what is left. With a surprising practical application...</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cmq/seq_rerun_principles_of_disagreement/\">Principles of Disagreement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ct5vLMqGRjSDqL3Gv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.099885600415886e-07, "legacy": true, "legacyId": "16411", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["924arDrTu3QRHFA5r", "Mgav9g2dkffkpm62T", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T07:47:09.998Z", "modifiedAt": null, "url": null, "title": "Meetup : Third Buenos Aires Meetup: 2 June 2012 4:00PM", "slug": "meetup-third-buenos-aires-meetup-2-june-2012-4-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XLznemaq4yKRTMC7Q/meetup-third-buenos-aires-meetup-2-june-2012-4-00pm", "pageUrlRelative": "/posts/XLznemaq4yKRTMC7Q/meetup-third-buenos-aires-meetup-2-june-2012-4-00pm", "linkUrl": "https://www.lesswrong.com/posts/XLznemaq4yKRTMC7Q/meetup-third-buenos-aires-meetup-2-june-2012-4-00pm", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Third%20Buenos%20Aires%20Meetup%3A%202%20June%202012%204%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Third%20Buenos%20Aires%20Meetup%3A%202%20June%202012%204%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXLznemaq4yKRTMC7Q%2Fmeetup-third-buenos-aires-meetup-2-june-2012-4-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Third%20Buenos%20Aires%20Meetup%3A%202%20June%202012%204%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXLznemaq4yKRTMC7Q%2Fmeetup-third-buenos-aires-meetup-2-june-2012-4-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXLznemaq4yKRTMC7Q%2Fmeetup-third-buenos-aires-meetup-2-june-2012-4-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ak'>Third Buenos Aires Meetup: 2 June 2012 4:00PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 June 2012 04:00:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Florida 1, Ciudad de Buenos Aires</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be our third Buenos Aires LessWrong meetup. Our previous meetup was a success; see <a href=\"http://lesswrong.com/lw/a5w/meetup_buenos_aires_meetup_saturday_february_25th/\">here</a> for details. If you read and like this blog and live in BA or just happen to be visiting the city, do join us. It will be a great opportunity to meet like-minded folks in the area. We will meet at the Starbucks Coffee in Florida and Rivadavia, and will be sitting upstairs; a copy of Greg Egan's Diaspora will be displayed at our table.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ak'>Third Buenos Aires Meetup: 2 June 2012 4:00PM</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XLznemaq4yKRTMC7Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.100434220770079e-07, "legacy": true, "legacyId": "16419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Third_Buenos_Aires_Meetup__2_June_2012_4_00PM\">Discussion article for the meetup : <a href=\"/meetups/ak\">Third Buenos Aires Meetup: 2 June 2012 4:00PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 June 2012 04:00:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Florida 1, Ciudad de Buenos Aires</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be our third Buenos Aires LessWrong meetup. Our previous meetup was a success; see <a href=\"http://lesswrong.com/lw/a5w/meetup_buenos_aires_meetup_saturday_february_25th/\">here</a> for details. If you read and like this blog and live in BA or just happen to be visiting the city, do join us. It will be a great opportunity to meet like-minded folks in the area. We will meet at the Starbucks Coffee in Florida and Rivadavia, and will be sitting upstairs; a copy of Greg Egan's Diaspora will be displayed at our table.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Third_Buenos_Aires_Meetup__2_June_2012_4_00PM1\">Discussion article for the meetup : <a href=\"/meetups/ak\">Third Buenos Aires Meetup: 2 June 2012 4:00PM</a></h2>", "sections": [{"title": "Discussion article for the meetup : Third Buenos Aires Meetup: 2 June 2012 4:00PM", "anchor": "Discussion_article_for_the_meetup___Third_Buenos_Aires_Meetup__2_June_2012_4_00PM", "level": 1}, {"title": "Discussion article for the meetup : Third Buenos Aires Meetup: 2 June 2012 4:00PM", "anchor": "Discussion_article_for_the_meetup___Third_Buenos_Aires_Meetup__2_June_2012_4_00PM1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4jphBJSQkzs4wz7QQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T17:45:53.901Z", "modifiedAt": null, "url": null, "title": "Meetup : Dallas/Fort Worth Metro Area Meetup, 5/27", "slug": "meetup-dallas-fort-worth-metro-area-meetup-5-27", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jumandtonic", "createdAt": "2012-04-26T03:09:35.594Z", "isAdmin": false, "displayName": "jumandtonic"}, "userId": "2EmJ3AN5jKXHnvYpP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kyMQ43ysXexKD8DGH/meetup-dallas-fort-worth-metro-area-meetup-5-27", "pageUrlRelative": "/posts/kyMQ43ysXexKD8DGH/meetup-dallas-fort-worth-metro-area-meetup-5-27", "linkUrl": "https://www.lesswrong.com/posts/kyMQ43ysXexKD8DGH/meetup-dallas-fort-worth-metro-area-meetup-5-27", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Dallas%2FFort%20Worth%20Metro%20Area%20Meetup%2C%205%2F27&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Dallas%2FFort%20Worth%20Metro%20Area%20Meetup%2C%205%2F27%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkyMQ43ysXexKD8DGH%2Fmeetup-dallas-fort-worth-metro-area-meetup-5-27%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Dallas%2FFort%20Worth%20Metro%20Area%20Meetup%2C%205%2F27%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkyMQ43ysXexKD8DGH%2Fmeetup-dallas-fort-worth-metro-area-meetup-5-27", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkyMQ43ysXexKD8DGH%2Fmeetup-dallas-fort-worth-metro-area-meetup-5-27", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/al'>Dallas/Fort Worth Metro Area Meetup, 5/27</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Folks, the weekly meeting for the DFW LessWrong group is still on for the Sunday on Memorial Day Weekend (5/27).</p>\n\n<p>Same time, same place as usual.  America's Best Coffee in Arlington from 1 to 3 PM.  Last week, we competed for real estate with a religious meetup group; I'd say avoid religious topics to avoid confrontation, if possible.  We want to be good stewards of this coffee shop so we get invited back.</p>\n\n<p>Come on by if you are free and in the area.  Also, have a great Memorial Day Weekend.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/al'>Dallas/Fort Worth Metro Area Meetup, 5/27</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kyMQ43ysXexKD8DGH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.103096927947233e-07, "legacy": true, "legacyId": "16427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Dallas_Fort_Worth_Metro_Area_Meetup__5_27\">Discussion article for the meetup : <a href=\"/meetups/al\">Dallas/Fort Worth Metro Area Meetup, 5/27</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Folks, the weekly meeting for the DFW LessWrong group is still on for the Sunday on Memorial Day Weekend (5/27).</p>\n\n<p>Same time, same place as usual.  America's Best Coffee in Arlington from 1 to 3 PM.  Last week, we competed for real estate with a religious meetup group; I'd say avoid religious topics to avoid confrontation, if possible.  We want to be good stewards of this coffee shop so we get invited back.</p>\n\n<p>Come on by if you are free and in the area.  Also, have a great Memorial Day Weekend.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Dallas_Fort_Worth_Metro_Area_Meetup__5_271\">Discussion article for the meetup : <a href=\"/meetups/al\">Dallas/Fort Worth Metro Area Meetup, 5/27</a></h2>", "sections": [{"title": "Discussion article for the meetup : Dallas/Fort Worth Metro Area Meetup, 5/27", "anchor": "Discussion_article_for_the_meetup___Dallas_Fort_Worth_Metro_Area_Meetup__5_27", "level": 1}, {"title": "Discussion article for the meetup : Dallas/Fort Worth Metro Area Meetup, 5/27", "anchor": "Discussion_article_for_the_meetup___Dallas_Fort_Worth_Metro_Area_Meetup__5_271", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T21:18:30.111Z", "modifiedAt": null, "url": null, "title": "A novel approach to axiomatic decision theory", "slug": "a-novel-approach-to-axiomatic-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EqCraJS4mQPxp3MX3/a-novel-approach-to-axiomatic-decision-theory", "pageUrlRelative": "/posts/EqCraJS4mQPxp3MX3/a-novel-approach-to-axiomatic-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/EqCraJS4mQPxp3MX3/a-novel-approach-to-axiomatic-decision-theory", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20novel%20approach%20to%20axiomatic%20decision%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20novel%20approach%20to%20axiomatic%20decision%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqCraJS4mQPxp3MX3%2Fa-novel-approach-to-axiomatic-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20novel%20approach%20to%20axiomatic%20decision%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqCraJS4mQPxp3MX3%2Fa-novel-approach-to-axiomatic-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqCraJS4mQPxp3MX3%2Fa-novel-approach-to-axiomatic-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 519, "htmlBody": "<p>In the standard approach to axiomatic Bayesian decision theory, an agent (a decision maker) doesn't prefer Act #1 to Act #2&nbsp;<em>because</em>&nbsp;the expected utility of Act #1 exceeds that of Act #2. Instead, the agent states its preferences over a set of risky acts, and if these stated preferences are consistent with a certain set of axioms (e.g. the VNM axioms, or the Savage axioms), it can be proven that the agent's decisions can be described <em>as if</em>&nbsp;the agent were assigning probabilities and utilities to outcomes and then maximizing expected utility. (Let's call this the <em>ex post</em>&nbsp;approach.)</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Peterson-From-Outcomes-to-Acts-a-non-standard-axiomatization-of-the-expected-utility-principle.pdf\">Peterson (2004)</a>&nbsp;introduces a different approach, which he calls the <em>ex ante</em>&nbsp;approach. In many ways, this approach is more intuitive. The agent assigns probabilities and utilities directly to <em>outcomes</em> (not acts), and these assignments are used to generate preferences over acts. Using this approach, Peterson claims to have shown that the principle of expected utility maximization can be derived from just <em>four</em>&nbsp;axioms.</p>\n<p>As <a href=\"http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/\">Peterson (2009:75,77)</a> explains:</p>\n<blockquote>\n<p>The aim of the axiomatization [in the <em>ex ante</em>&nbsp;approach] is to show that the utility of an act equals the expected utility of its outcomes.</p>\n<p>...The axioms... entail that the utility of an act equals the expected utility of its outcomes. Or, put in slightly different words, the act that has the highest utility (is most attractive) will also have the highest expected utility, and vice versa. This appears to be a strong reason for letting the expected utility principle guide one's choices in decision under risk.</p>\n</blockquote>\n<p><a href=\"http://books.google.com/books?id=PyJ_4KYwxNwC&amp;lpg=PA29&amp;dq=A%20Philosophical%20Assessment%20of%20Decision%20Theory&amp;pg=PA405#v=onepage&amp;q&amp;f=false\">Jensen (2012:428)</a> calls the <em>ex ante</em>&nbsp;approach \"controversial,\" but I can't find any actual published <em>rebuttals</em>&nbsp;to Peterson (2004), so maybe Jensen just means that Peterson's result is \"new and not yet percolated to the broad community.\"</p>\n<p><a href=\"http://www.amazon.com/Non-Bayesian-Decision-Theory-Beliefs-Desires/dp/9048179572/\">Peterson (2008)</a> explores the <em>ex ante</em>&nbsp;approach in more detail, under the unfortunate title of \"non-Bayesian decision theory.\" (No, Peterson doesn't reject Bayesianism.) <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Cozic-Review-of-Non-Bayesian-Decision-Theory.pdf\">Cozic (2011)</a> is a review of Peterson (2008) that may offer the quickest entry point into the subject of <em>ex ante</em>&nbsp;axiomatic decision theory.</p>\n<p>Peterson (2009:210) illustrates the controversy nicely:</p>\n<blockquote>\n<p>...even if [this] discussion may appear a bit theoretical... the controversy over [<em>ex post</em>&nbsp;and <em>ex ante</em>&nbsp;approaches] is likely to have important practical implications. For example, a forty-year-old woman seeking advice about whether to, say, divorce her husband, is likely to get very different answers from the [two approaches]. The [<em>ex post</em>&nbsp;approach] will advise the woman to first figure out what her preferences are over a very large set of risky acts, including the one she is thinking about performing, and then just make sure that all preferences are consistent with certain structural requirements. Then, as long as none of the structural requirements is violated, the woman is free to do whatever she likes, no matter what her beliefs and desires actually are... The [<em>ex ante</em>&nbsp;approach] will [instead] advise the woman to first assign numerical utilities and probabilities to her desires and beliefs, and then aggregate them into a decision by applying the principle of maximizing expected utility.</p>\n</blockquote>\n<p>I'm not a decision theory expert, so I'd be very curious to hear what LW's decision theorists think of the axiomatization in Peterson (2004) &mdash; whether it works, and how significant it is.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EqCraJS4mQPxp3MX3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 9.104042759784157e-07, "legacy": true, "legacyId": "16429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T21:25:31.019Z", "modifiedAt": null, "url": null, "title": "Posts I'd Like To Write (Includes Poll)", "slug": "posts-i-d-like-to-write-includes-poll", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X2qk7paWE9sH7MLZp/posts-i-d-like-to-write-includes-poll", "pageUrlRelative": "/posts/X2qk7paWE9sH7MLZp/posts-i-d-like-to-write-includes-poll", "linkUrl": "https://www.lesswrong.com/posts/X2qk7paWE9sH7MLZp/posts-i-d-like-to-write-includes-poll", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Posts%20I'd%20Like%20To%20Write%20(Includes%20Poll)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APosts%20I'd%20Like%20To%20Write%20(Includes%20Poll)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2qk7paWE9sH7MLZp%2Fposts-i-d-like-to-write-includes-poll%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Posts%20I'd%20Like%20To%20Write%20(Includes%20Poll)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2qk7paWE9sH7MLZp%2Fposts-i-d-like-to-write-includes-poll", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2qk7paWE9sH7MLZp%2Fposts-i-d-like-to-write-includes-poll", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1442, "htmlBody": "<p><strong>Summary:</strong> <em>There are a bunch of posts I want to write; I'd like your help prioritizing them, and if you feel like writing one of them, that would be awesome too!<br /></em></p>\n<p>I haven't been writing up as many of my ideas for Less Wrong as I'd like; I have excuses, but so does everyone. So I'm listing out my backlog, both for my own motivation and for feedback/help. At the end, there's a link to a poll on which ones you'd like to see. Comments would also be helpful, and if you're interested in writing up one of the ideas from the third section yourself, say so!</p>\n<p>(The idea was inspired by <a href=\"/lw/85d/11_less_wrong_articles_i_probably_will_never_have/\">lukeprog's request</a> for post-writing help, and I think someone else did this a while ago as well.)</p>\n<h2>Posts I'm Going To Write (Barring Disaster)</h2>\n<p>These are posts that I currently have unfinished drafts of.</p>\n<p><strong>Decision Theories: A Semi-Formal Analysis, Part IV and Part V:</strong> Part IV concerns <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem\">bargaining problems</a> and introduces the tactic of playing chicken with the inference process; Part V discusses the benefits of UDT and perhaps wraps up the sequence. Part IV has been delayed by more than a month, partly by real life, and partly because bargaining problems are really difficult and the approach I was trying turned out not to work. I believe I have a fix now, but that's no guarantee; if it turns out to be flawed, then Part IV will mainly consist of \"bargaining problems are <em>hard</em>, you guys\".</p>\n<h2>Posts I Really Want To Write</h2>\n<p>These are posts that I feel I've already put substantial original work into, but I haven't written a draft. If anyone else wants to write on the topic, I'd welcome that, but I'd probably still write up my views on it later (unless the other post covers all the bases that I'd wanted to discuss, most of which aren't obvious from the capsule descriptions below).</p>\n<p><strong>An Error Theory of Qualia:</strong> My <a href=\"/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/\">sequence last summer</a> didn't turn out as well as I'd hoped, but I still think it's the right approach to a physically reductionist account of qualia (and that mere bullet-biting isn't going to suffice), so I'd like to try again and see if I can find ways to simplify and test my theory. (In essence, I'm proposing that what we experience as qualia are something akin to error messages, caused when we try and consciously introspect on something that introspection can't usefully break down. It's rather like the modern understanding of <a href=\"http://en.wikipedia.org/wiki/D%C3%A9j%C3%A0_vu\">d&eacute;j&agrave; vu</a>.)</p>\n<p><strong>Weak Solutions in Metaethics:</strong> I've been mulling over a certain approach to metaethics, which differs from <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Eliezer's sequence</a> and <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">lukeprog's sequence</a> (although the conclusions may turn out to be close). In mathematics, there's a concept of a <a href=\"http://en.wikipedia.org/wiki/Weak_solution\">weak solution</a> to a differential equation: a function that has the most important properties but isn't actually differentiable enough times to \"count\" in the original formulation. Sometimes these weak solutions can lead to \"genuine\" solutions, and other times it turns out that the weak solution is all you really need. The analogy is that there are a bunch of conditions humans want our ethical theories to satisfy (things like consistency, comprehensivity, universality, objectivity, and practical approximability), and that something which demonstrably had all these properties would be a \"strong\" solution. But the failure of moral philosophers to find a strong solution doesn't have to spell doom for metaethics; we can focus instead on the question of what sorts of weak solutions we can establish.</p>\n<h2>Posts I'd Really Love To See</h2>\n<p>And then we get to ideas that I'd like to write Less Wrong posts on, but that I haven't really developed beyond the kernels below. If any of these strike your fancy, you have my atheist's blessing to flesh them out. (Let me know in the comments if you want to publicly commit to doing so.)</p>\n<p><strong>Living with Rationality:</strong> Several people in real life criticize Less Wrong-style rationality on the grounds that \"you couldn't really benefit by living your life by Bayesian utility maximization, you have to go with intuition instead\". I think that's a strawman attack, but none of the defenses on Less Wrong seem to answer this directly. What I'd like to see described is how it works to actually improve one's life via rationality (which I've seen in my own life), and how it differs from the <a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Straw Vulcan</a> stereotype of decisionmaking. (That is, I usually apply conscious deliberation on the level of choosing habits rather than individual acts; I don't take out a calculator when deciding who to sit next to on a bus; I leave room for the kind of uncertainty described as \"my conscious model of the situation is vastly incomplete\", etc.)</p>\n<p><strong>An Explanation of the Born Probabilities in MWI:</strong> This topic might be even better suited to an actual physicist than to a know-it-all mathematician, but I don't see why the <a href=\"http://en.wikipedia.org/wiki/Born_rule\">Born probabilities</a> should be regarded as <a href=\"/lw/py/the_born_probabilities/\">mysterious</a> <em>at all</em> within the Many-Worlds interpretation. The universe is naturally defined as a Hilbert space, and the evolution of the wavefunction has a basic L^2 conservation law. If you're going to ask \"how big\" a chunk of the wavefunction is (which is the right way to compute the relative probabilities of being an observer that sees such-and-such), the only sane answer is going to be the L^2 norm (i.e. the Born probabilities).</p>\n<p><strong>Are Mutual Funds To Blame For Stock Bubbles?</strong> My opinion about the incentives behind the financial crisis, in a nutshell: Financial institutions caused the latest crash by speculating in ways that were good for their quarterly returns but involved themselves in way too much risk. The executives were incentivized to act in that short-sighted way because the investors wanted short-term returns and were willing to turn a blind eye to that kind of risk. But that's a crazy preference for most investors (I expect it had seriously negative value), so why weren't investors smarter (i.e. why didn't they flee from any company that wasn't clearly prioritizing longer-term expected value)? Well, there's one large chunk of investors with precisely those incentives: the 20% of the stock market that's composed of mutual funds. I'd like to test this theory and think about realistic ways to apply it to public policy. (It goes without saying that I think Less Wrong readers should, at minimum, invest in index funds rather than mutual funds.)</p>\n<p><strong>Strategies for Trustworthiness with the Singularity:</strong> I want to develop <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/56re\">this comment</a> into an article. Generally speaking, the usual methods of making the <a href=\"http://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem\">principal-agent problem</a> work out aren't available; the possible payoffs are too enormous when we're discussing rapidly accelerating technological progress. I'm wondering if there's any way of setting up a Singularity-affecting organization so that it will be transparent to the organization's backers that the organization is doing precisely what it claims. I'd like to know in general, but there's also an obvious application; I think highly of the idealism of SIAI's people, but trusting people on their signaled idealism in the face of large incentives turns out to backfire in politics pretty regularly, so I'd like a better structure than that if possible.</p>\n<p><strong>On Adding Up To Normality:</strong> People have a strange block about certain concepts, like the existence of a deity or of contracausal free will, where it seems to them that the instant they stopped believing in it, everything else in their life would fall apart or be robbed of meaning, or they'd suddenly incur an obligation that horrifies them (like raw hedonism or total fatalism). That instinct is like being on an airplane, having someone explain to you that <a href=\"http://xkcd.com/803/\">your current understanding of aerodynamic lift is wrong</a>, and then suddenly becoming terrified that the plane will plummet out of the sky now that there's no longer the kind of lift you expected. (That is, it's a fascinating example of the <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\">Mind Projection Fallacy</a>.) So I want a general elucidation of <a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">Egan's Law</a> to point people to.</p>\n<p><strong>The Subtle Difference Between Meta-Uncertainty and Uncertainty:</strong> If you're discussing a single toss of a coin, then you should treat it the same (for decision purposes) whether you know that it's a coin designed to land heads 3/4 of the time, or whether you know there's a 50% chance it's a fair coin and a 50% chance it's a two-headed coin. Metauncertainty and uncertainty are indistinguishable in that sense. Where they differ is in <em>how you update on new evidence</em>, or how you'd make bets about three upcoming flips taken together, etc. This is a worthwhile topic that seems to confuse the hell out of newcomers to Bayesianism.</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><strong>(Originally, this was a link to a poll on these post ideas)</strong></span></p>\n<p>Thanks for your feedback!</p>\n<h3>UPDATE:</h3>\n<p>Thanks to everyone who gave me feedback; results are in <a href=\"/lw/cnl/posts_id_like_to_write_includes_poll/6re6\">this comment</a>!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X2qk7paWE9sH7MLZp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 9.104073971577262e-07, "legacy": true, "legacyId": "16401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Summary:</strong> <em>There are a bunch of posts I want to write; I'd like your help prioritizing them, and if you feel like writing one of them, that would be awesome too!<br></em></p>\n<p>I haven't been writing up as many of my ideas for Less Wrong as I'd like; I have excuses, but so does everyone. So I'm listing out my backlog, both for my own motivation and for feedback/help. At the end, there's a link to a poll on which ones you'd like to see. Comments would also be helpful, and if you're interested in writing up one of the ideas from the third section yourself, say so!</p>\n<p>(The idea was inspired by <a href=\"/lw/85d/11_less_wrong_articles_i_probably_will_never_have/\">lukeprog's request</a> for post-writing help, and I think someone else did this a while ago as well.)</p>\n<h2 id=\"Posts_I_m_Going_To_Write__Barring_Disaster_\">Posts I'm Going To Write (Barring Disaster)</h2>\n<p>These are posts that I currently have unfinished drafts of.</p>\n<p><strong>Decision Theories: A Semi-Formal Analysis, Part IV and Part V:</strong> Part IV concerns <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem\">bargaining problems</a> and introduces the tactic of playing chicken with the inference process; Part V discusses the benefits of UDT and perhaps wraps up the sequence. Part IV has been delayed by more than a month, partly by real life, and partly because bargaining problems are really difficult and the approach I was trying turned out not to work. I believe I have a fix now, but that's no guarantee; if it turns out to be flawed, then Part IV will mainly consist of \"bargaining problems are <em>hard</em>, you guys\".</p>\n<h2 id=\"Posts_I_Really_Want_To_Write\">Posts I Really Want To Write</h2>\n<p>These are posts that I feel I've already put substantial original work into, but I haven't written a draft. If anyone else wants to write on the topic, I'd welcome that, but I'd probably still write up my views on it later (unless the other post covers all the bases that I'd wanted to discuss, most of which aren't obvious from the capsule descriptions below).</p>\n<p><strong>An Error Theory of Qualia:</strong> My <a href=\"/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/\">sequence last summer</a> didn't turn out as well as I'd hoped, but I still think it's the right approach to a physically reductionist account of qualia (and that mere bullet-biting isn't going to suffice), so I'd like to try again and see if I can find ways to simplify and test my theory. (In essence, I'm proposing that what we experience as qualia are something akin to error messages, caused when we try and consciously introspect on something that introspection can't usefully break down. It's rather like the modern understanding of <a href=\"http://en.wikipedia.org/wiki/D%C3%A9j%C3%A0_vu\">d\u00e9j\u00e0 vu</a>.)</p>\n<p><strong>Weak Solutions in Metaethics:</strong> I've been mulling over a certain approach to metaethics, which differs from <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Eliezer's sequence</a> and <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">lukeprog's sequence</a> (although the conclusions may turn out to be close). In mathematics, there's a concept of a <a href=\"http://en.wikipedia.org/wiki/Weak_solution\">weak solution</a> to a differential equation: a function that has the most important properties but isn't actually differentiable enough times to \"count\" in the original formulation. Sometimes these weak solutions can lead to \"genuine\" solutions, and other times it turns out that the weak solution is all you really need. The analogy is that there are a bunch of conditions humans want our ethical theories to satisfy (things like consistency, comprehensivity, universality, objectivity, and practical approximability), and that something which demonstrably had all these properties would be a \"strong\" solution. But the failure of moral philosophers to find a strong solution doesn't have to spell doom for metaethics; we can focus instead on the question of what sorts of weak solutions we can establish.</p>\n<h2 id=\"Posts_I_d_Really_Love_To_See\">Posts I'd Really Love To See</h2>\n<p>And then we get to ideas that I'd like to write Less Wrong posts on, but that I haven't really developed beyond the kernels below. If any of these strike your fancy, you have my atheist's blessing to flesh them out. (Let me know in the comments if you want to publicly commit to doing so.)</p>\n<p><strong>Living with Rationality:</strong> Several people in real life criticize Less Wrong-style rationality on the grounds that \"you couldn't really benefit by living your life by Bayesian utility maximization, you have to go with intuition instead\". I think that's a strawman attack, but none of the defenses on Less Wrong seem to answer this directly. What I'd like to see described is how it works to actually improve one's life via rationality (which I've seen in my own life), and how it differs from the <a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Straw Vulcan</a> stereotype of decisionmaking. (That is, I usually apply conscious deliberation on the level of choosing habits rather than individual acts; I don't take out a calculator when deciding who to sit next to on a bus; I leave room for the kind of uncertainty described as \"my conscious model of the situation is vastly incomplete\", etc.)</p>\n<p><strong>An Explanation of the Born Probabilities in MWI:</strong> This topic might be even better suited to an actual physicist than to a know-it-all mathematician, but I don't see why the <a href=\"http://en.wikipedia.org/wiki/Born_rule\">Born probabilities</a> should be regarded as <a href=\"/lw/py/the_born_probabilities/\">mysterious</a> <em>at all</em> within the Many-Worlds interpretation. The universe is naturally defined as a Hilbert space, and the evolution of the wavefunction has a basic L^2 conservation law. If you're going to ask \"how big\" a chunk of the wavefunction is (which is the right way to compute the relative probabilities of being an observer that sees such-and-such), the only sane answer is going to be the L^2 norm (i.e. the Born probabilities).</p>\n<p><strong>Are Mutual Funds To Blame For Stock Bubbles?</strong> My opinion about the incentives behind the financial crisis, in a nutshell: Financial institutions caused the latest crash by speculating in ways that were good for their quarterly returns but involved themselves in way too much risk. The executives were incentivized to act in that short-sighted way because the investors wanted short-term returns and were willing to turn a blind eye to that kind of risk. But that's a crazy preference for most investors (I expect it had seriously negative value), so why weren't investors smarter (i.e. why didn't they flee from any company that wasn't clearly prioritizing longer-term expected value)? Well, there's one large chunk of investors with precisely those incentives: the 20% of the stock market that's composed of mutual funds. I'd like to test this theory and think about realistic ways to apply it to public policy. (It goes without saying that I think Less Wrong readers should, at minimum, invest in index funds rather than mutual funds.)</p>\n<p><strong>Strategies for Trustworthiness with the Singularity:</strong> I want to develop <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/56re\">this comment</a> into an article. Generally speaking, the usual methods of making the <a href=\"http://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem\">principal-agent problem</a> work out aren't available; the possible payoffs are too enormous when we're discussing rapidly accelerating technological progress. I'm wondering if there's any way of setting up a Singularity-affecting organization so that it will be transparent to the organization's backers that the organization is doing precisely what it claims. I'd like to know in general, but there's also an obvious application; I think highly of the idealism of SIAI's people, but trusting people on their signaled idealism in the face of large incentives turns out to backfire in politics pretty regularly, so I'd like a better structure than that if possible.</p>\n<p><strong>On Adding Up To Normality:</strong> People have a strange block about certain concepts, like the existence of a deity or of contracausal free will, where it seems to them that the instant they stopped believing in it, everything else in their life would fall apart or be robbed of meaning, or they'd suddenly incur an obligation that horrifies them (like raw hedonism or total fatalism). That instinct is like being on an airplane, having someone explain to you that <a href=\"http://xkcd.com/803/\">your current understanding of aerodynamic lift is wrong</a>, and then suddenly becoming terrified that the plane will plummet out of the sky now that there's no longer the kind of lift you expected. (That is, it's a fascinating example of the <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\">Mind Projection Fallacy</a>.) So I want a general elucidation of <a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">Egan's Law</a> to point people to.</p>\n<p><strong>The Subtle Difference Between Meta-Uncertainty and Uncertainty:</strong> If you're discussing a single toss of a coin, then you should treat it the same (for decision purposes) whether you know that it's a coin designed to land heads 3/4 of the time, or whether you know there's a 50% chance it's a fair coin and a 50% chance it's a two-headed coin. Metauncertainty and uncertainty are indistinguishable in that sense. Where they differ is in <em>how you update on new evidence</em>, or how you'd make bets about three upcoming flips taken together, etc. This is a worthwhile topic that seems to confuse the hell out of newcomers to Bayesianism.</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #000000;\"><strong>(Originally, this was a link to a poll on these post ideas)</strong></span></p>\n<p>Thanks for your feedback!</p>\n<h3 id=\"UPDATE_\">UPDATE:</h3>\n<p>Thanks to everyone who gave me feedback; results are in <a href=\"/lw/cnl/posts_id_like_to_write_includes_poll/6re6\">this comment</a>!</p>", "sections": [{"title": "Posts I'm Going To Write (Barring Disaster)", "anchor": "Posts_I_m_Going_To_Write__Barring_Disaster_", "level": 1}, {"title": "Posts I Really Want To Write", "anchor": "Posts_I_Really_Want_To_Write", "level": 1}, {"title": "Posts I'd Really Love To See", "anchor": "Posts_I_d_Really_Love_To_See", "level": 1}, {"title": "UPDATE:", "anchor": "UPDATE_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qs96DvwTCCdyRvM5E", "3wYjyQ839MDsZ6E3L", "zuJmtSqt3TsnBTYyu", "3ZKvf9u2XEWddGZmS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-26T22:51:53.783Z", "modifiedAt": null, "url": null, "title": "2 Anthropic Questions", "slug": "2-anthropic-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abramdemski", "createdAt": "2009-03-12T06:07:25.510Z", "isAdmin": false, "displayName": "abramdemski"}, "userId": "Q7NW4XaWQmfPfdcFj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rFmJ8ENWxbHcKk6tb/2-anthropic-questions", "pageUrlRelative": "/posts/rFmJ8ENWxbHcKk6tb/2-anthropic-questions", "linkUrl": "https://www.lesswrong.com/posts/rFmJ8ENWxbHcKk6tb/2-anthropic-questions", "postedAtFormatted": "Saturday, May 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202%20Anthropic%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2%20Anthropic%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrFmJ8ENWxbHcKk6tb%2F2-anthropic-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2%20Anthropic%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrFmJ8ENWxbHcKk6tb%2F2-anthropic-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrFmJ8ENWxbHcKk6tb%2F2-anthropic-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 724, "htmlBody": "<p>I have just finished reading the section on anthropic bias in Nassim Taleb's book, <em>The Black Swan</em>. In general, the book is interesting to compare to the sort of things I read on Less Wrong; its message is largely very similar, except less Bayesian (and therefore less formal-- at times slightly anti-formal, arguing against misleading math).</p>\n<p>Two points concerning anthropic weirdness.</p>\n<p>First:</p>\n<p>If we win the lottery, should we really conclude that we live in a holodeck (or some such)? From <a href=\"/lw/7w/reallife_anthropic_weirdness/#more\">real-life anthropic weirdness</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Pity those poor folk who&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">actually win the lottery!</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp; If the hypothesis \"this world is a holodeck\" is normatively assigned a calibrated confidence well above 10</span><sup style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; \">-8</sup><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">, the lottery winner now has&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">incommunicable</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;good reason to believe they are in a holodeck.&nbsp; (I.e. to believe that the universe is such that&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">most</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;conscious observers observe ridiculously improbable positive events.)</span></p>\n</blockquote>\n<p>It seems to me that the right way of approaching the question is: before buying the lottery ticket, what belief-forming strategy would we prefer ourselves to have? (Ignore the issue of why we buy the ticket, of course.) Or, slightly different: what advice would you give to other people (for example, if you're writing a book on rationality that might be widely read)?</p>\n<p>\"Common sense\" says that it would be quite silly to start believing some strange theory, just because I win the lottery. However, Bayes says that if we assign greater than 10<sup>-8</sup>&nbsp;prior probability to \"strange\" explanations of getting a winning lottery ticket, then we should prefer them. In fact, we may want to buy a lottery ticket to test those theories! (This would be a very sensible test, which would strongly tend to give the right result.)</p>\n<p>However, as a society, we would not want lottery-winners to go crazy. Therefore, we would <em>not </em>want to give the advice \"if you win, you should massively update your probabilities\".</p>\n<p>(This is similar to the idea that we <em>might </em>be persuaded to defect in Prisoner's Dilemma if we are maximizing our personal utility, but if we are giving advice about rationality to other people, we should advise them that cooperating is the optimal strategy. In a somewhat unjustified leap, I suppose we should take the advice we would give to others in such matters. But I suppose that position is already widely accepted here.)</p>\n<p>On the other hand, if we were in a position to give advice to people who might really be living in a simulation, it would suddenly be good advice!</p>\n<p>&nbsp;</p>\n<p>Second:</p>\n<p>Taleb discusses an interesting example of anthropic bias:</p>\n<blockquote>\n<p>Apply this reasoning to the following question: Why didn't the bubonic plague kill more people? People will supply quantities of cosmetic explanations involving theories about the intensity of the plague and \"scientific models\" of epidemics. Now, try the weakened causality argument that I have just emphasized in this chapter: had the bubonic plague killed more people, the observers (us) would not be here to observe. So it may not neccessarily be the property of diseases to spare us humans.</p>\n</blockquote>\n<p>You'll have to read the chapter if you want to know exactly what \"argument\" is being discussed, but the general point is (hopefully) clear from this passage. If an event was a necessary prerequisite for our existence, then we should not take our survival of that event as evidence for a high probability of survival of such events. If we remember surviving a car crash, we should not take that to increase our estimates for surviving a car crash. (Instead, we should look at other car crashes.)</p>\n<p>This conclusion is somewhat troubling (as Taleb admits). It means that the past is fundamentally different from the future! The past will be a relatively \"safe\" place, where every event has led to our survival. The future is alien and unforgiving. As is said in the story <a href=\"/lw/14h/the_hero_with_a_thousand_chances/\">The Hero With A Thousand Chances</a>:</p>\n<blockquote>\n<p><span style=\"color: #222222; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); line-height: 19px; text-align: justify; font-family: Arial, Helvetica, sans-serif;\">\"The Counter-Force isn't going to help you this time.&nbsp; No hero's luck.&nbsp; Nothing but creativity and any scraps of&nbsp;</span><em style=\"color: #222222; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); line-height: 19px; text-align: justify; font-family: Arial, Helvetica, sans-serif;\">real</em><span style=\"color: #222222; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); line-height: 19px; text-align: justify; font-family: Arial, Helvetica, sans-serif;\">&nbsp;luck - and true random chance is as liable to hurt&nbsp;</span><em style=\"color: #222222; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); line-height: 19px; text-align: justify; font-family: Arial, Helvetica, sans-serif;\">you&nbsp;</em><span style=\"color: #222222; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); line-height: 19px; text-align: justify; font-family: Arial, Helvetica, sans-serif;\">as the Dust.&nbsp; Even if you do survive this time, the Counter-Force won't help you next time either.&nbsp; Or the time after that.&nbsp; What you remember happening before - will not happen for you ever again.\"</span></p>\n</blockquote>\n<p>Now, Taleb is saying that we are that hero. Scary, right?</p>\n<p>On the other hand, it seems reasonable to be skeptical of a view which presents difficulties generalizing from the past to the future. So. Any opinions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rFmJ8ENWxbHcKk6tb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 15, "extendedScore": null, "score": 9.104458308761526e-07, "legacy": true, "legacyId": "16430", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EKu66pFKDHFYPaZ6q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-27T01:49:25.444Z", "modifiedAt": null, "url": null, "title": "Expertise and advice", "slug": "expertise-and-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HBKJuqKWLyfRsWTco/expertise-and-advice", "pageUrlRelative": "/posts/HBKJuqKWLyfRsWTco/expertise-and-advice", "linkUrl": "https://www.lesswrong.com/posts/HBKJuqKWLyfRsWTco/expertise-and-advice", "postedAtFormatted": "Sunday, May 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Expertise%20and%20advice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExpertise%20and%20advice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBKJuqKWLyfRsWTco%2Fexpertise-and-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Expertise%20and%20advice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBKJuqKWLyfRsWTco%2Fexpertise-and-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBKJuqKWLyfRsWTco%2Fexpertise-and-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 309, "htmlBody": "<p>In his <a href=\"http://www.succeedsocially.com/changingadvice\">essay</a> \"How&nbsp;The Type Of Advice Someone Can Give&nbsp;Can Change Over Time\", the author of SucceedSocially.com writes:</p>\n<blockquote>\n<p>...as someone starts to become proficient in a field, they can start to take all the basic little steps for granted. With time they may even start to forget what it was like to be a beginner, or lose touch with how it felt to not have certain abilities. I've noticed this happening myself.</p>\n<p>...</p>\n<p>Once someone internalizes the basics and moves into more advanced levels of skill, I've noticed they can want to focus on the bigger picture. They'll want to figure out a handful of profound, succinct principles that tie all the advice about a field together.</p>\n<p>...</p>\n<p>I think beginners often need that drilled down, specific information though. Sometimes they need to learn some tactics as well before talk of larger strategies can really sink in. I also think if an advice giver gets too broad and abstract, his ideas can become vague to the point of being unhelpful.</p>\n</blockquote>\n<p>If you're trying to learn something, it's natural to try to find the best people in the field to learn from. And if you're thinking of sharing your thoughts on something, it's natural to think twice unless you have a proven track record of accomplishments in that area.</p>\n<div>A better model of advice dissemination might focus on the derivative of a person's abilities. For example, if you're an amateur at something and you stumble on something that really seems to help, write it up even if you haven't reached the peak of expertise yet. And if you're looking for advice, don't be afraid to read writing by other amateurs--they are the ones who are at your stage and trying to improve themselves.</div>\n<div>Of course, experts who do a lot of hands-on work helping amateurs are probably the best.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EdDGrAxYcrXnKkDca": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HBKJuqKWLyfRsWTco", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 25, "extendedScore": null, "score": 9.10524829197568e-07, "legacy": true, "legacyId": "16432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-27T05:00:38.350Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Why Quantum?", "slug": "seq-rerun-why-quantum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DRFNpGndcCpRRE4JL/seq-rerun-why-quantum", "pageUrlRelative": "/posts/DRFNpGndcCpRRE4JL/seq-rerun-why-quantum", "linkUrl": "https://www.lesswrong.com/posts/DRFNpGndcCpRRE4JL/seq-rerun-why-quantum", "postedAtFormatted": "Sunday, May 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Why%20Quantum%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Why%20Quantum%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDRFNpGndcCpRRE4JL%2Fseq-rerun-why-quantum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Why%20Quantum%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDRFNpGndcCpRRE4JL%2Fseq-rerun-why-quantum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDRFNpGndcCpRRE4JL%2Fseq-rerun-why-quantum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p>Today's post, <a href=\"/lw/qy/why_quantum/\">Why Quantum?</a> was originally published on 04 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Why do a series on quantum mechanics? Some of the many morals that are best illustrated by the tale of quantum mechanics and its misinterpretation.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cnv/seq_rerun_timeless_identity/\">Timeless Identity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DRFNpGndcCpRRE4JL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.106099318731325e-07, "legacy": true, "legacyId": "16435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gDL9NDEXPxYpDf4vz", "Ct5vLMqGRjSDqL3Gv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-27T06:41:56.183Z", "modifiedAt": null, "url": null, "title": "Funding Good Research", "slug": "funding-good-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qmiaJKkBNk2gGTkuG/funding-good-research", "pageUrlRelative": "/posts/qmiaJKkBNk2gGTkuG/funding-good-research", "linkUrl": "https://www.lesswrong.com/posts/qmiaJKkBNk2gGTkuG/funding-good-research", "postedAtFormatted": "Sunday, May 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Funding%20Good%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFunding%20Good%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqmiaJKkBNk2gGTkuG%2Ffunding-good-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Funding%20Good%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqmiaJKkBNk2gGTkuG%2Ffunding-good-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqmiaJKkBNk2gGTkuG%2Ffunding-good-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 678, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>I recently explained that one major project undergoing cost-benefit analysis at the Singularity Institute is that of a <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a>. The proposal is exciting to many, but as Kaj Sotala points out:</p>\n<blockquote>\n<p>This idea sounds promising, but I find it hard to say anything about \"should this be funded\" without knowing what the alternative uses for the money are. Almost any use of money can be made to sound attractive with some effort, but the crucial question in budgeting is not \"would this be useful\" but \"would this be the most useful thing\".</p>\n</blockquote>\n<p>Indeed. So here is another thing that donations to SI could purchase: good research papers by skilled academics.</p>\n<p>&nbsp;</p>\n<p>Our recent grant of $20,000 to <a href=\"http://www.rachaelbriggs.net/\">Rachael Briggs</a>&nbsp;(for an introductory paper on <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>) provides an example of how this works:</p>\n<ol>\n<li>SI thinks of a paper it wants to exist but doesn't have the resources to write itself (e.g. a clearer presentation of TDT).</li>\n<li>SI looks for a few productive academics well-suited to write the paper we have in mind, and approaches them directly with the grant proposal. (Briggs is an excellent choice for the TDT paper because she is a good explainer and has had two of her past decision theory papers selected as among the 10 best papers of the year by <a href=\"http://www.philosophersannual.org/\">The Philosopher's Annual</a>.)</li>\n<li>Hopefully, one of these academics says \"yes.\" We award them the grant in return for a certain kind of paper published in one of a pre-specified set of journals. (In the case of the TDT grant to Rachael Briggs, we specified that the final paper must be published in one of the following journals:&nbsp;<em>Philosopher's Imprint</em>, <em>Philosophy and Phenomenological Research</em>, <em>Philosophical Quarterly</em>, <em>Philosophical Studies</em>, <em>Erkenntnis</em>, <em>Theoria</em>, <em>Australasian Journal of Philosophy</em>, <em>Nous</em>, <em>The Philosophical Review</em>, or <em>Theory and Decision</em>.)</li>\n<li>SI gives regular feedback on outline drafts and article drafts prepared by the article author.</li>\n<li>Paper gets submitted, revised, and published!</li>\n</ol>\n<p><br /> For example, SI could award grants for the following papers:</p>\n<ul>\n<li>\"<strong>Objections to CEV</strong>,\" by somebody like <a href=\"http://www.unl.edu/philosophy/people/faculty/sobel/sobel.shtml\">David Sobel</a> (his \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Full Information Accounts of Well-Being</a>\" remains the most significant unanswered attack on ideal-preference theories like CEV).</li>\n<li>\"<strong>Counterfactual Mugging</strong>,\" by somebody like <a href=\"http://www.rachaelbriggs.net/\">Rachael Briggs</a> (<a href=\"/lw/3l/counterfactual_mugging/\">here</a> is the original post by Vladimir Nesov).</li>\n<li>\"<strong>CEV as a Computational Meta-Ethics,</strong>\" by somebody like <a href=\"http://homepages.ipact.nl/~lokhorst/\">Gert-Jan Lokhorst</a> (see his paper \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf\">Computational Metaethics</a>\").</li>\n<li>\"<strong>Non-Bayesian Decision Theory and Normative Uncertainty,</strong>\" by somebody like <a href=\"http://www.martinpeterson.org/\">Martin Peterson</a> (the problem of normative uncertainty is a serious one, and <a href=\"http://www.amazon.com/Non-Bayesian-Decision-Theory-Beliefs-Desires/dp/9048179572/\">Peterson's approach</a> is a different line of approach than the one pursued by <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Nick Bostrom</a>, <a href=\"http://www.amirrorclear.net/academic/\">Toby Ord</a>, and <a href=\"http://www.practicalethics.ox.ac.uk/staff/staff/research_associates/will_crouch\">Will Crouch</a>, and also different from the one pursued by <a href=\"http://www.fil.lu.se/files/conference117.pdf\">Andrew Sepielli</a>).</li>\n<li>\"<strong>Methods for Long-Term Technological Forecasting,</strong>\" by somebody like <a href=\"http://www.santafe.edu/about/people/profile/B%C3%A9la%20Nagy\">Bela Nagy</a> (Nagy is the lead author on <a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\">one of the best papers in the field</a>)</li>\n<li>\"<strong>Convergence to Rational Economic Agency,</strong>\" by somebody like <a href=\"http://steveomohundro.com/\">Steve Omohundro</a> (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">Omohundro's 2007 paper</a> argues that advanced agents will converge toward the rational economic model of decision-making, if true this would make it easier to predict the convergent instrumental goals of advanced AIs, but his argument leaves much to be desired in persuasiveness as it is currently formulated).</li>\n<li>\"<strong>Value Learning,</strong>\" by somebody like <a href=\"http://www.ssec.wisc.edu/~billh/homepage1.html\">Bill Hibbard</a> (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey's 2011 paper</a> and <a href=\"http://arxiv.org/pdf/1111.3934v1.pdf\">Hibbard's 2012 paper</a> make interesting advances on this topic, but there is much more work to be done).</li>\n<li>\"<strong>Learning Preferences from Human Behavior,</strong>\" by somebody like <a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nielsen:Thomas_D=.html\">Thomas Nielsen</a> (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Nielsen's 2004 paper with Finn Jensen</a> described the first computationally tractable algorithms&nbsp;capable of learning a decision maker&rsquo;s utility function from potentially inconsistent behavior. Their&nbsp;solution was to interpret inconsistent choices as random deviations from an underlying &ldquo;true&rdquo;&nbsp;utility function. But the data from neuroeconomics <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">suggest</a> a different solution: interpret&nbsp;inconsistent choices as deviations from an underlying &ldquo;true&rdquo; utility function that are produced by&nbsp;non-model-based valuation systems in the brain, and use the latest neuroscientific research to&nbsp;predict when and to what extent model-based choices are being &ldquo;overruled&rdquo; by the&nbsp;non-model-based valuation systems).</li>\n</ul>\n<p>(These are only examples. I don't necessarily think these <em>particular</em>&nbsp;papers would be good investments.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1, "Z38PqJbRyfwCxKvvL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qmiaJKkBNk2gGTkuG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 38, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "16436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "mYuMdmMmGM7fFj382", "mg6jDEuQEjBGtibX7", "fa5o2tg9EfJE77jEQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-27T19:13:25.279Z", "modifiedAt": "2021-08-12T19:35:18.224Z", "url": null, "title": "The rational rationalist's guide to rationally using \"rational\" in rational post titles", "slug": "the-rational-rationalist-s-guide-to-rationally-using", "viewCount": null, "lastCommentedAt": "2013-03-23T18:38:34.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DFHhuAMexXAi8T6AY/the-rational-rationalist-s-guide-to-rationally-using", "pageUrlRelative": "/posts/DFHhuAMexXAi8T6AY/the-rational-rationalist-s-guide-to-rationally-using", "linkUrl": "https://www.lesswrong.com/posts/DFHhuAMexXAi8T6AY/the-rational-rationalist-s-guide-to-rationally-using", "postedAtFormatted": "Sunday, May 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20rational%20rationalist's%20guide%20to%20rationally%20using%20%22rational%22%20in%20rational%20post%20titles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20rational%20rationalist's%20guide%20to%20rationally%20using%20%22rational%22%20in%20rational%20post%20titles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDFHhuAMexXAi8T6AY%2Fthe-rational-rationalist-s-guide-to-rationally-using%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20rational%20rationalist's%20guide%20to%20rationally%20using%20%22rational%22%20in%20rational%20post%20titles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDFHhuAMexXAi8T6AY%2Fthe-rational-rationalist-s-guide-to-rationally-using", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDFHhuAMexXAi8T6AY%2Fthe-rational-rationalist-s-guide-to-rationally-using", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3, "htmlBody": "<ol>\n<li>Don't.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "Ng8Gice9KNkncxqcj": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DFHhuAMexXAi8T6AY", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 125, "baseScore": 125, "extendedScore": null, "score": 0.000263, "legacy": true, "legacyId": "16439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 125, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-05-27T19:13:25.279Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-28T13:07:03.247Z", "modifiedAt": null, "url": null, "title": "Analogy to the Heisenberg Uncertainty Principle for Powerful AI?", "slug": "analogy-to-the-heisenberg-uncertainty-principle-for-powerful", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "demented", "createdAt": "2011-09-24T09:17:51.977Z", "isAdmin": false, "displayName": "demented"}, "userId": "pfmw79DXFQbifMwdo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SeJ6J52fs9g3MsSeC/analogy-to-the-heisenberg-uncertainty-principle-for-powerful", "pageUrlRelative": "/posts/SeJ6J52fs9g3MsSeC/analogy-to-the-heisenberg-uncertainty-principle-for-powerful", "linkUrl": "https://www.lesswrong.com/posts/SeJ6J52fs9g3MsSeC/analogy-to-the-heisenberg-uncertainty-principle-for-powerful", "postedAtFormatted": "Monday, May 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Analogy%20to%20the%20Heisenberg%20Uncertainty%20Principle%20for%20Powerful%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnalogy%20to%20the%20Heisenberg%20Uncertainty%20Principle%20for%20Powerful%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSeJ6J52fs9g3MsSeC%2Fanalogy-to-the-heisenberg-uncertainty-principle-for-powerful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Analogy%20to%20the%20Heisenberg%20Uncertainty%20Principle%20for%20Powerful%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSeJ6J52fs9g3MsSeC%2Fanalogy-to-the-heisenberg-uncertainty-principle-for-powerful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSeJ6J52fs9g3MsSeC%2Fanalogy-to-the-heisenberg-uncertainty-principle-for-powerful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>What do you think? There might be a theoritical limitation to how much data an AI could collect without influencing the data itself and making its prediction redundant. Would this negate the idea of a 'God' AIand cause it to make suboptimal choices even with near limitless processing power?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SeJ6J52fs9g3MsSeC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -29, "extendedScore": null, "score": 9.114680926940017e-07, "legacy": true, "legacyId": "16459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-28T15:57:55.813Z", "modifiedAt": null, "url": null, "title": "SotW: Avoid Motivated Cognition", "slug": "sotw-avoid-motivated-cognition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:57.347Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/thfnus52hgHA32XfQ/sotw-avoid-motivated-cognition", "pageUrlRelative": "/posts/thfnus52hgHA32XfQ/sotw-avoid-motivated-cognition", "linkUrl": "https://www.lesswrong.com/posts/thfnus52hgHA32XfQ/sotw-avoid-motivated-cognition", "postedAtFormatted": "Monday, May 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SotW%3A%20Avoid%20Motivated%20Cognition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASotW%3A%20Avoid%20Motivated%20Cognition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fthfnus52hgHA32XfQ%2Fsotw-avoid-motivated-cognition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SotW%3A%20Avoid%20Motivated%20Cognition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fthfnus52hgHA32XfQ%2Fsotw-avoid-motivated-cognition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fthfnus52hgHA32XfQ%2Fsotw-avoid-motivated-cognition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2987, "htmlBody": "<p><em>(The <a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">Exercise Prize</a> series of <a href=\"/tag/exprize/\">posts</a> is the Center for Applied Rationality asking for help inventing exercises that can teach cognitive skills. &nbsp;The difficulty is&nbsp;</em><em>coming up with exercises interesting enough, with a high enough hedonic return, that people actually do them and remember them; this often involves standing up and performing actions, or interacting with other people, not just working alone with an exercise booklet and a pencil. &nbsp;</em><em>We offer prizes of $50 for any suggestion we decide to test, and $500 for any suggestion we decide to adopt. &nbsp;This prize also extends to LW meetup activities and good ideas for verifying that a skill has been acquired. &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">See here&nbsp;for details</a>.)</em></p>\n<hr />\n<p>The following awards have been made: &nbsp;$550 to <strong>Palladias</strong>, $550 to <strong>Stefie_K</strong>, $50 to <strong>lincolnquirk</strong>, and $50 to <strong>John_Maxwell_IV</strong>. &nbsp;See the bottom for details. &nbsp;If you've earned a prize, please PM <a href=\"/message/compose/?to=StephenCole\">StephenCole</a>&nbsp;to claim it. &nbsp;(If you strongly believe that one of your suggestions Really Would Have Worked, consider trying it at your local Less Wrong meetup. &nbsp;If it works there, send us some participant comments; this may make us update enough to test it.)</p>\n<hr />\n<blockquote>\n<p>Lucy and Marvin are walking down the street one day, when they pass a shop showing a large chocolate cake in the window.</p>\n<p>\"Hm,\" says Lucy, \"I think I'll buy and eat that chocolate cake.\"</p>\n<p>\"What, the whole thing?\" says Marvin. &nbsp;\"Now?\"</p>\n<p>\"Yes,\" says Lucy, \"I want to support the sugar industry.\"</p>\n<p>There is a slight pause.</p>\n<p>\"I don't suppose that your liking chocolate cake has anything to do with your decision?\" says Marvin.</p>\n<p>\"Well,\" says Lucy, \"I suppose it could have played a role in suggesting that I eat a whole chocolate cake, but the reason why I decided to do it was to support the sugar industry. &nbsp;Lots of people have jobs in the sugar industry, and they've been having some trouble lately.\"</p>\n</blockquote>\n<hr />\n<p>Motivated cognition is the way (all? most?) brains generate false landscapes of justification in the presence of attachments and flinches. &nbsp;It's not enough for the human brain to attach to the sunk cost of a PhD program, so that we are impelled in our actions to stay - no, that attachment can also go off and spin a justificational landscape to convince the other parts of ourselves, even the part that knows about consequentialism and the sunk cost fallacy, to stay in the PhD program.</p>\n<p>We're almost certain that the subject matter of \"motivated cognition\" isn't a single unit, probably more like 3 or 8 units. &nbsp;We're also highly uncertain of where to start teaching it. &nbsp;Where we start will probably end up being determined by where we get the best suggestions for exercises that can teach it - i.e., end up being determined by what we (the community) can figure out how to teach well.</p>\n<p>The cognitive patterns that we use to actually combat motivated cognition seem to break out along the following lines:</p>\n<ol>\n<li>Our conceptual understanding of 'motivated cognition', and why it's defective as a cognitive algorithm - the \"Bottom Line\" insight.</li>\n<li>Ways to reduce the strength of the rationalization impulse, or restore truth-seeking in the presence of motivation: e.g., Anna's \"Become Curious\" technique.</li>\n<li>Noticing the internal attachment or internal flinch, so that you can invoke the other skills; realizing when you're in a situation that makes you liable to rationalize.</li>\n<li>Realigning the internal parts that are trying to persuade each other: belief-alief or goal-urge reconciliation procedures.</li>\n</ol>\n<p>And also:</p>\n<ul>\n<li>Pattern recognition of the many styles of warped justification landscape that rationalization creates - being able to recognize \"motivated skepticism\" or \"rehearsing the evidence\" or \"motivated uncertainty\".</li>\n<li>Specific counters to rationalization styles, like \"Set betting odds\" as a counter to motivated uncertainty.</li>\n</ul>\n<p>Exercises to teach all of these are desired, but I'm setting apart the Rationalization Patterns into a separate SotW, since there are so many that I'm worried 1-4 won't get fair treatment otherwise. &nbsp;This SotW will focus on items 1-3 above; #4 seems like more of a separate unit.<a id=\"more\"></a></p>\n<hr />\n<p><strong>Conceptual understanding / insights / theoretical background:</strong></p>\n<p>The core reasons why rationalization doesn't work are given in <a href=\"/lw/js/the_bottom_line/\">The Bottom Line</a> and <a href=\"/lw/ju/rationalization/\">Rationalization</a>. &nbsp;The Bayesian analysis of selective search is given in <a href=\"/lw/jt/what_evidence_filtered_evidence/\">What Evidence Filtered Evidence?</a>&nbsp;and <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a>.</p>\n<p>For further discussion, see&nbsp;the entire&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\">Against Rationalization</a>&nbsp;sequence, also&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">The Meditation on Curiosity</a>&nbsp;(for the Litany of Tarski).</p>\n<p>Some key concepts (it'd be nice if some exercise taught a gut-level understanding thereof, although as always the goal is to t each skills rather than concepts):</p>\n<ul>\n<li>Once you write down the answer on the <a href=\"/lw/js/the_bottom_line/\">bottom line</a> of a piece of paper in pen, it's already right or already wrong, and won't change regardless of what clever arguments you write on the lines above.</li>\n<li>What <a href=\"/lw/ju/rationalization/\">determines your life outcome</a> isn't how cleverly you argue for the foregone conclusion - what determines life outcomes is the algorithm that chooses which side to argue for, <em>what you actually do.</em></li>\n<li>Rationality isn't something you can use to argue for your side; you can never say \"Please come up with <a href=\"/lw/jw/a_rational_argument/\">a rational argument</a> for X\"; the only chance for rationality to operate is when you're deciding which side to be on.</li>\n<li>Evidence that has <a href=\"/lw/jt/what_evidence_filtered_evidence/\">passed through a filter</a> takes on a different Bayesian import. &nbsp;<em>(Currently handled in the Bayes unit.)</em></li>\n<li>It is impossible for a rational agent to search for evidence to look at that will&nbsp;<a href=\"/lw/ii/conservation_of_expected_evidence/\">send their beliefs in a predetermined direction</a>.<em> &nbsp;(Currently handled in the Bayes unit.)</em></li>\n<li>There's such a thing as a <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Overly_Convenient_Excuses\">correct probability</a>&nbsp;to assign to an uncertain proposition, and this in turn determines the weight to lend that possibility in our actions. &nbsp;Even when things are uncertain, any cognition that makes you put too much action-weight on the wrong belief or choice is screwing you up.</li>\n<li>If you're selective about where you look for flaws, or how hard you look for flaws, <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">every new fallacy you learn how to detect makes you that much stupider</a>.</li>\n</ul>\n<p>(We&nbsp;<em>might</em>&nbsp;also need an exercise just for getting people to understand the&nbsp;<em>concept</em>&nbsp;of motivated cognition&nbsp;<em>at all</em>. &nbsp;<a href=\"/lw/ail/people_who_dont_rationalize_help_rationality/\">When Anna and Michael ran their first session on motivated cognition</a>, they found that while most participants immediately recognized the notion of 'rationalization' from examples like Lucy above, several people had&nbsp;<em>no idea what they were talking about</em>&nbsp;-&nbsp;they didn't see why anyone would ever want to use a technique like the&nbsp;<a href=\"/lw/jz/the_meditation_on_curiosity/\">Litany of Tarski</a>. &nbsp;Yes, we know you're skeptical, we also couldn't see how that could possibly be true&nbsp;<em>a priori</em>, but sometimes the evidence just punches you in the nose. &nbsp;After some investigation, it seems entirely possible that Alicorn has simply never rationalized, ever. &nbsp;Other cases (not Alicorn's) suggest that some people might have a very low&nbsp;<em>need for verbal justification;</em>&nbsp;even if they feel guilty about breaking their diet, they feel no urge to invent an elaborate excuse - they just break their diet. &nbsp;On the other hand, LW!Hermione failed to reproduce this experiment - she couldn't find anyone who didn't immediately recognize \"rationalization\" after 10 tries with her friends. &nbsp;We notice we are confused.)</p>\n<p>(The upshot is that part of the challenge of constructing a first unit on motivated cognition&nbsp;<em>may&nbsp;</em>be to \"Explain to some participants what the heck a 'rationalization' is, when they don't remember any internal experience of that\" or&nbsp;<em>might</em>&nbsp;even be&nbsp;\"Filter out attendees who don't rationalize in the first place, and have them do a different unit instead.\" &nbsp;Please don't be fascinated by this problem at the expense of the primary purpose of the unit, though; we're probably going to award at most 1 prize on this subtopic, and more likely 0, and there's an&nbsp;<a href=\"/lw/ail/people_who_dont_rationalize_help_rationality/\">existing thread</a>&nbsp;for further discussion.)</p>\n<hr />\n<p><strong>Countering the rationalization impulse / restoring truth-seeking:</strong></p>\n<p><em>The Tarski method:</em>&nbsp; This is the new name of what we were previously calling the Litany of Tarski: &nbsp;\"If the sky is blue, I want to believe the sky is blue; if the sky is not blue, I want to believe the sky is not blue; let me not become attached to beliefs I may not want.\"</p>\n<p><em>Example:</em>&nbsp;&nbsp;Suppose you walk outside on a fall day wearing a short-sleeved shirt, when you feel a slightly chill breath of air on your arms. &nbsp;You wonder if you should go back into the house and get a sweater. &nbsp;But that seems like work; and so your mind quickly notes that the Sun might come out soon and then you wouldn't need the sweater.</p>\n<p><span id=\"internal-source-marker_0.5286629283800721\"><span><em>Diagram:</em></span><br /> </span></p>\n<p dir=\"ltr\">\n<table border=\"0\">\n<colgroup><col width=\"*\"></col><col width=\"*\"></col><col width=\"*\"></col></colgroup> \n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><span>It stays cold enough to require a sweater</span></td>\n<td><span>It gets warm enough that no sweater is needed.</span></td>\n</tr>\n<tr>\n<td><span>You believe you need a sweater</span></td>\n<td><span>A warm walk in a toasty sweater.</span></td>\n<td><span>Your walk is ruined forever by the need to carry an extra sweater.</span></td>\n</tr>\n<tr>\n<td><span>You believe you don't need a sweater</span></td>\n<td><span>You are cold! &nbsp;Cold cold cold! &nbsp;Why didn't you get a sweater?</span></td>\n<td><span>Free and unencumbered, you stroll along as the warm Sun comes out overhead.</span></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>Visualizing all 4 quadrants of this binary proposition - the world is like A and I believe A, the world is like B and I believe A, etc. - should, in principle, emotionally confirm the truth of the proposition: &nbsp;\"If it will be cold, I want to believe it's cold; if it's not cold, I want to believe it's not cold; let me not become attached to beliefs I may not want.\"</p>\n<p>Eliezer and Anna, when using this method against the temptation to believe X, visualize only the quadrant \"The world is not like X and I believe X\" to remind themselves of the consequences; e.g. we would only visualize the \"You are cold!\" quadrant. &nbsp;Michael Smith (aka \"Val\", short for Valentine) says that after some practice on this technique as a kata, he was able to visualize all 4 quadrants quickly and that visualizing all 4 seemed to help.</p>\n<p>Val also used an upside-down W-diagram with the two worlds at the top and the four beliefs at the bottom, to emphasize the idea that <em>the world is there first,</em>&nbsp;and is fixed, and we have only a choice of what to believe within a fixed world, not a choice of which background world to live in. &nbsp;The Tarski Method embodies a \"Start from the world\" mental process in which you visualize the world being there first, and your belief coming afterward; a similar \"Start from the world\" rule is likewise emphasized in the Bayes unit, wherein one starts from a world and asks about the probability of the evidence, rather than starting from the evidence and trying to make it match up with a world.</p>\n<p>When we actually tested a unit based on asking people to draw Tarski squares, it didn't work very well - possibly because people didn't seem to understand what it was for, or when they would use it; possibly because it wasn't a group exercise. &nbsp;In any case, we already tried teaching this the obvious way (\"Go draw Tarski squares!\") and it didn't work. &nbsp;But it still seems worth teaching if someone can invent a better exercise, because it's something that multiple CfAR people actually use to counter the rationalization impulse / restore truthseeking in real life.</p>\n<p><em>Become Curious:</em>&nbsp; Detect non-curiosity and become curious. &nbsp;Anna's main alarm signal is when she notices that she's not <em>curious</em>&nbsp;in the middle of a conversation - that she doesn't have an impulse-to-find-out the answer - and then try to make herself curious about the subject of discussion. &nbsp;Besides visualizing the not-X-and-believe-X quadrant of the Tarski diagram, this is also something you may be able to do by brute introspection - remember the feeling of curiosity, and try to call it up. &nbsp;(This is probably in the top 3 most important things I learned from Anna. -- EY)</p>\n<p><em>Take Pride in Your Objectivity:</em>&nbsp; Julia teaches this as a primary counter in her Combat Reflexes unit (how to avoid instantly defending or attacking). &nbsp;Eliezer does this every time he admits he's wrong on the Internet - congratulates himself on being such a great rationalist, in order to apply counter-hedons to the flash of pain that would otherwise be associated.</p>\n<p><em>Visualize a Fixed Probability:</em>&nbsp; This is what Eliezer used as a child to stop being scared of the dark - he would deliberately visualize a murderer standing with a knife behind a door, then visualize his own <em>thoughts</em>&nbsp;having no effect on the <em>fixed probability</em>&nbsp;that any such murderer was actually present. &nbsp;In other words, the notion of a \"true probability\" that his thoughts couldn't affect, countered the fear of thoughts affecting reality. &nbsp;Visualizing there being a fixed frequency of worlds, or a lawful probability that a Bayesian agent would assign, can help in perceiving the futility of rationalization because you're trying to use arguments to move a lawful probability that is fixed. &nbsp;This is also part of the domain of <em>Lawful Uncertainty,&nbsp;</em>the notion that there are still rules which apply even when we're unsure (not presently part of any unit).</p>\n<p><em>Imagine the Revelation:</em>&nbsp; Anna imagines that the answer is about to be looked up on the Internet, that Omega is about to reveal the answer, etc., to check if her thoughts would change if she was potentially about to be embarrassed&nbsp;<em>right now.</em>&nbsp; This detects belief-alief divergence, but also provides truthseeking impulse.</p>\n<p><em>Knowing the Rules:</em>&nbsp; And finally, if you have sufficient mastery of probability theory or decision theory, you may have a procedure to follow which is lawful enough, and sufficiently well-understood, that rationalization can't influence it much without the mistake being blatant even to you. &nbsp;(In a sense, this is what most of Less Wrong is about - reducing the <em>amount</em>&nbsp;of self-honesty required by increasing the obviousness of mistakes.)</p>\n<hr />\n<p><strong>Noticing flinches and attachments, and raising them to conscious attention:</strong></p>\n<p>A trigger for use of curiosity-restoration or the Tarski Method: &nbsp;Noticing what it feels like for your mind to:</p>\n<ul>\n<li>Quickly glimpse a disliked argument before <a href=\"/lw/k0/singlethink/\">sweeping it under a mental rug</a> <em>(flinch)</em></li>\n<li>Glimpse a conclusion, find it unacceptable, quickly start generating arguments against it <em>(flinch)</em></li>\n<li>Be centered on a conclusion, automatically counter all arguments against it <em>(attachment)</em></li>\n<li>Instantly attack a new idea, instantly defend an old idea (this is the subject of Julia's <em>Combat Reflexes</em>&nbsp;unit)</li>\n</ul>\n<div>Learning to notice these events introspectively seems <strong>extremely important</strong>&nbsp;- we all use it heavily in daily practice - but we don't know how to teach that.</div>\n<div>Anna observes that Rejection Therapy is often a good time to observe oneself rationalizing, as apparently many participants reported that their mind started generating crazy reasons not to approach someone with a request.</div>\n<div>Anna also says that she's been self-rewarding each time she <em>notices</em>&nbsp;a flinch or attachment, i.e., she's trying to train her inner pigeon to notice (not, one hopes, training the flinching or attachment!) &nbsp;It's possible we could ask participants to self-reward each event of \"noticing the flinch or attachment\" while doing Rejection Therapy, but we still need other ideas.</div>\n<div>Along similar lines of internal behaviorism, Eliezer avoids rewarding himself <em>for</em>&nbsp;rationalizing by repeating the phrase \"Only congratulate yourself for actually changing a probability estimate or policy\" on any occasion where he <em>hasn't</em>&nbsp;changed his mind after argument - as opposed to e.g. feeling any sense of reward for having defeated an incoming argument; even if the incoming argument happens to be wrong, still, \"Only congratulate yourself for actually changing a probability estimate or policy.\"</div>\n<div>Another thing most of us do is name attachments or flinches out loud, in conversation, as we notice them, in order to reduce their strength, i.e. \"This is probably a complete post-facto rationalization, but...\" (Eliezer) or \"I may just be trying to avoid having my status reduced, but...\" (Anna). &nbsp;(Note: &nbsp;This requires enough trust that nearby people also know they're flawed themselves, that you don't feel embarrassed for confessing your own flaws in front of them. &nbsp;In other words, you have to tell embarrassing stories about your own failures of rationality, before other people will feel that they can do this around you.)</div>\n<p>Anna's anti-rationalization makes heavy use of noticing suspect situations where the outside view says she might rationalize - cases where her status is at stake, and so on - and specific keywords like \"I believe that\" or \"No, I really believe that\". &nbsp;She wants to try training people to notice likely contexts for rationalization, and to figure out keywords that might indicate rationalization in themselves. &nbsp;(Eliezer has never tried to train himself to notice keywords because he figures his brain will just train itself to avoid the trigger phrase; and he worries about likely-context training because he's seen failure modes where no amount of evidence or sound argument is enough to overcome the suspicion of rationalization once it's been invoked.)</p>\n<div>\"Look toward the painful thought instead of away from it\" is an important reflex to install to counter flinches, but would probably require some sort of hedonic support - like a strong, pre-existing pride in objectivity, or a social support group that applauds, or <em>something</em>&nbsp;to stop this from being pure negative reinforcement.</div>\n<hr />\n<p><strong>Awards for previous SotW suggestions:</strong></p>\n<p><strong>$550</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/bc3/sotw_be_specific/681c\">Palladias</a>&nbsp;</strong>for the Monday-Tuesday game, which has been tested ($50) and now adopted ($500) into the Be Specific unit (though it might be moved to some sort of Anticipation unit later on).</p>\n<p><strong>$550</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/bc3/sotw_be_specific/686a\">Stefie_K</a>&nbsp;</strong>for her suggestion to have the instructor pretend to be someone who really wants you to invest in their company, but is never specific; also $50 to daenrys for the&nbsp;&nbsp;\"More Specific!\" improv-game suggestion. &nbsp;In combination these inspired the&nbsp;Vague Consultant game (\"Hi, I'm a consultant, I'm here to improve your business processes!\" &nbsp;\"How?\" &nbsp;\"By consulting with stakeholders!\") which has now been adopted into the Be Specific unit.</p>\n<p><strong>$50</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/bc3/sotw_be_specific/67zb\">lincolnquirk</a>&nbsp;</strong>for the \"Channel Paul Graham\" game, which we tested. &nbsp;We all thought this would work - it was our highest-rated candidate suggestion - but it didn't get positive audience feedback. &nbsp;Congratulations to lincolnquirk on a good suggestion nonetheless.</p>\n<p>We haven't yet tested, but definitely intend to at least test, and are hence already awarding $50 to, the following idea:</p>\n<p><strong>$50</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/b4f/sotw_check_consequentialism/643p\">John Maxwell IV</a>&nbsp;</strong>for the Choose Your Own Adventure suggestion for the Consequentialism unit.</p>\n<p>To claim a prize, send a LessWrong private message (so we know it originates from the same LW user account) to <a href=\"/message/compose/?to=StephenCole\">StephenCole</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "ZzxvopS4BwLuQy42n": 2, "iP2X4jQNHMWHRNPne": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "thfnus52hgHA32XfQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 31, "extendedScore": null, "score": 9.115442820408029e-07, "legacy": true, "legacyId": "15104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(The <a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">Exercise Prize</a> series of <a href=\"/tag/exprize/\">posts</a> is the Center for Applied Rationality asking for help inventing exercises that can teach cognitive skills. &nbsp;The difficulty is&nbsp;</em><em>coming up with exercises interesting enough, with a high enough hedonic return, that people actually do them and remember them; this often involves standing up and performing actions, or interacting with other people, not just working alone with an exercise booklet and a pencil. &nbsp;</em><em>We offer prizes of $50 for any suggestion we decide to test, and $500 for any suggestion we decide to adopt. &nbsp;This prize also extends to LW meetup activities and good ideas for verifying that a skill has been acquired. &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">See here&nbsp;for details</a>.)</em></p>\n<hr>\n<p>The following awards have been made: &nbsp;$550 to <strong>Palladias</strong>, $550 to <strong>Stefie_K</strong>, $50 to <strong>lincolnquirk</strong>, and $50 to <strong>John_Maxwell_IV</strong>. &nbsp;See the bottom for details. &nbsp;If you've earned a prize, please PM <a href=\"/message/compose/?to=StephenCole\">StephenCole</a>&nbsp;to claim it. &nbsp;(If you strongly believe that one of your suggestions Really Would Have Worked, consider trying it at your local Less Wrong meetup. &nbsp;If it works there, send us some participant comments; this may make us update enough to test it.)</p>\n<hr>\n<blockquote>\n<p>Lucy and Marvin are walking down the street one day, when they pass a shop showing a large chocolate cake in the window.</p>\n<p>\"Hm,\" says Lucy, \"I think I'll buy and eat that chocolate cake.\"</p>\n<p>\"What, the whole thing?\" says Marvin. &nbsp;\"Now?\"</p>\n<p>\"Yes,\" says Lucy, \"I want to support the sugar industry.\"</p>\n<p>There is a slight pause.</p>\n<p>\"I don't suppose that your liking chocolate cake has anything to do with your decision?\" says Marvin.</p>\n<p>\"Well,\" says Lucy, \"I suppose it could have played a role in suggesting that I eat a whole chocolate cake, but the reason why I decided to do it was to support the sugar industry. &nbsp;Lots of people have jobs in the sugar industry, and they've been having some trouble lately.\"</p>\n</blockquote>\n<hr>\n<p>Motivated cognition is the way (all? most?) brains generate false landscapes of justification in the presence of attachments and flinches. &nbsp;It's not enough for the human brain to attach to the sunk cost of a PhD program, so that we are impelled in our actions to stay - no, that attachment can also go off and spin a justificational landscape to convince the other parts of ourselves, even the part that knows about consequentialism and the sunk cost fallacy, to stay in the PhD program.</p>\n<p>We're almost certain that the subject matter of \"motivated cognition\" isn't a single unit, probably more like 3 or 8 units. &nbsp;We're also highly uncertain of where to start teaching it. &nbsp;Where we start will probably end up being determined by where we get the best suggestions for exercises that can teach it - i.e., end up being determined by what we (the community) can figure out how to teach well.</p>\n<p>The cognitive patterns that we use to actually combat motivated cognition seem to break out along the following lines:</p>\n<ol>\n<li>Our conceptual understanding of 'motivated cognition', and why it's defective as a cognitive algorithm - the \"Bottom Line\" insight.</li>\n<li>Ways to reduce the strength of the rationalization impulse, or restore truth-seeking in the presence of motivation: e.g., Anna's \"Become Curious\" technique.</li>\n<li>Noticing the internal attachment or internal flinch, so that you can invoke the other skills; realizing when you're in a situation that makes you liable to rationalize.</li>\n<li>Realigning the internal parts that are trying to persuade each other: belief-alief or goal-urge reconciliation procedures.</li>\n</ol>\n<p>And also:</p>\n<ul>\n<li>Pattern recognition of the many styles of warped justification landscape that rationalization creates - being able to recognize \"motivated skepticism\" or \"rehearsing the evidence\" or \"motivated uncertainty\".</li>\n<li>Specific counters to rationalization styles, like \"Set betting odds\" as a counter to motivated uncertainty.</li>\n</ul>\n<p>Exercises to teach all of these are desired, but I'm setting apart the Rationalization Patterns into a separate SotW, since there are so many that I'm worried 1-4 won't get fair treatment otherwise. &nbsp;This SotW will focus on items 1-3 above; #4 seems like more of a separate unit.<a id=\"more\"></a></p>\n<hr>\n<p><strong id=\"Conceptual_understanding___insights___theoretical_background_\">Conceptual understanding / insights / theoretical background:</strong></p>\n<p>The core reasons why rationalization doesn't work are given in <a href=\"/lw/js/the_bottom_line/\">The Bottom Line</a> and <a href=\"/lw/ju/rationalization/\">Rationalization</a>. &nbsp;The Bayesian analysis of selective search is given in <a href=\"/lw/jt/what_evidence_filtered_evidence/\">What Evidence Filtered Evidence?</a>&nbsp;and <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a>.</p>\n<p>For further discussion, see&nbsp;the entire&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\">Against Rationalization</a>&nbsp;sequence, also&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">The Meditation on Curiosity</a>&nbsp;(for the Litany of Tarski).</p>\n<p>Some key concepts (it'd be nice if some exercise taught a gut-level understanding thereof, although as always the goal is to t each skills rather than concepts):</p>\n<ul>\n<li>Once you write down the answer on the <a href=\"/lw/js/the_bottom_line/\">bottom line</a> of a piece of paper in pen, it's already right or already wrong, and won't change regardless of what clever arguments you write on the lines above.</li>\n<li>What <a href=\"/lw/ju/rationalization/\">determines your life outcome</a> isn't how cleverly you argue for the foregone conclusion - what determines life outcomes is the algorithm that chooses which side to argue for, <em>what you actually do.</em></li>\n<li>Rationality isn't something you can use to argue for your side; you can never say \"Please come up with <a href=\"/lw/jw/a_rational_argument/\">a rational argument</a> for X\"; the only chance for rationality to operate is when you're deciding which side to be on.</li>\n<li>Evidence that has <a href=\"/lw/jt/what_evidence_filtered_evidence/\">passed through a filter</a> takes on a different Bayesian import. &nbsp;<em>(Currently handled in the Bayes unit.)</em></li>\n<li>It is impossible for a rational agent to search for evidence to look at that will&nbsp;<a href=\"/lw/ii/conservation_of_expected_evidence/\">send their beliefs in a predetermined direction</a>.<em> &nbsp;(Currently handled in the Bayes unit.)</em></li>\n<li>There's such a thing as a <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Overly_Convenient_Excuses\">correct probability</a>&nbsp;to assign to an uncertain proposition, and this in turn determines the weight to lend that possibility in our actions. &nbsp;Even when things are uncertain, any cognition that makes you put too much action-weight on the wrong belief or choice is screwing you up.</li>\n<li>If you're selective about where you look for flaws, or how hard you look for flaws, <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">every new fallacy you learn how to detect makes you that much stupider</a>.</li>\n</ul>\n<p>(We&nbsp;<em>might</em>&nbsp;also need an exercise just for getting people to understand the&nbsp;<em>concept</em>&nbsp;of motivated cognition&nbsp;<em>at all</em>. &nbsp;<a href=\"/lw/ail/people_who_dont_rationalize_help_rationality/\">When Anna and Michael ran their first session on motivated cognition</a>, they found that while most participants immediately recognized the notion of 'rationalization' from examples like Lucy above, several people had&nbsp;<em>no idea what they were talking about</em>&nbsp;-&nbsp;they didn't see why anyone would ever want to use a technique like the&nbsp;<a href=\"/lw/jz/the_meditation_on_curiosity/\">Litany of Tarski</a>. &nbsp;Yes, we know you're skeptical, we also couldn't see how that could possibly be true&nbsp;<em>a priori</em>, but sometimes the evidence just punches you in the nose. &nbsp;After some investigation, it seems entirely possible that Alicorn has simply never rationalized, ever. &nbsp;Other cases (not Alicorn's) suggest that some people might have a very low&nbsp;<em>need for verbal justification;</em>&nbsp;even if they feel guilty about breaking their diet, they feel no urge to invent an elaborate excuse - they just break their diet. &nbsp;On the other hand, LW!Hermione failed to reproduce this experiment - she couldn't find anyone who didn't immediately recognize \"rationalization\" after 10 tries with her friends. &nbsp;We notice we are confused.)</p>\n<p>(The upshot is that part of the challenge of constructing a first unit on motivated cognition&nbsp;<em>may&nbsp;</em>be to \"Explain to some participants what the heck a 'rationalization' is, when they don't remember any internal experience of that\" or&nbsp;<em>might</em>&nbsp;even be&nbsp;\"Filter out attendees who don't rationalize in the first place, and have them do a different unit instead.\" &nbsp;Please don't be fascinated by this problem at the expense of the primary purpose of the unit, though; we're probably going to award at most 1 prize on this subtopic, and more likely 0, and there's an&nbsp;<a href=\"/lw/ail/people_who_dont_rationalize_help_rationality/\">existing thread</a>&nbsp;for further discussion.)</p>\n<hr>\n<p><strong id=\"Countering_the_rationalization_impulse___restoring_truth_seeking_\">Countering the rationalization impulse / restoring truth-seeking:</strong></p>\n<p><em>The Tarski method:</em>&nbsp; This is the new name of what we were previously calling the Litany of Tarski: &nbsp;\"If the sky is blue, I want to believe the sky is blue; if the sky is not blue, I want to believe the sky is not blue; let me not become attached to beliefs I may not want.\"</p>\n<p><em>Example:</em>&nbsp;&nbsp;Suppose you walk outside on a fall day wearing a short-sleeved shirt, when you feel a slightly chill breath of air on your arms. &nbsp;You wonder if you should go back into the house and get a sweater. &nbsp;But that seems like work; and so your mind quickly notes that the Sun might come out soon and then you wouldn't need the sweater.</p>\n<p><span id=\"internal-source-marker_0.5286629283800721\"><span><em>Diagram:</em></span><br> </span></p>\n<p dir=\"ltr\">\n</p><table border=\"0\">\n<colgroup><col width=\"*\"><col width=\"*\"><col width=\"*\"></colgroup> \n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td><span>It stays cold enough to require a sweater</span></td>\n<td><span>It gets warm enough that no sweater is needed.</span></td>\n</tr>\n<tr>\n<td><span>You believe you need a sweater</span></td>\n<td><span>A warm walk in a toasty sweater.</span></td>\n<td><span>Your walk is ruined forever by the need to carry an extra sweater.</span></td>\n</tr>\n<tr>\n<td><span>You believe you don't need a sweater</span></td>\n<td><span>You are cold! &nbsp;Cold cold cold! &nbsp;Why didn't you get a sweater?</span></td>\n<td><span>Free and unencumbered, you stroll along as the warm Sun comes out overhead.</span></td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>Visualizing all 4 quadrants of this binary proposition - the world is like A and I believe A, the world is like B and I believe A, etc. - should, in principle, emotionally confirm the truth of the proposition: &nbsp;\"If it will be cold, I want to believe it's cold; if it's not cold, I want to believe it's not cold; let me not become attached to beliefs I may not want.\"</p>\n<p>Eliezer and Anna, when using this method against the temptation to believe X, visualize only the quadrant \"The world is not like X and I believe X\" to remind themselves of the consequences; e.g. we would only visualize the \"You are cold!\" quadrant. &nbsp;Michael Smith (aka \"Val\", short for Valentine) says that after some practice on this technique as a kata, he was able to visualize all 4 quadrants quickly and that visualizing all 4 seemed to help.</p>\n<p>Val also used an upside-down W-diagram with the two worlds at the top and the four beliefs at the bottom, to emphasize the idea that <em>the world is there first,</em>&nbsp;and is fixed, and we have only a choice of what to believe within a fixed world, not a choice of which background world to live in. &nbsp;The Tarski Method embodies a \"Start from the world\" mental process in which you visualize the world being there first, and your belief coming afterward; a similar \"Start from the world\" rule is likewise emphasized in the Bayes unit, wherein one starts from a world and asks about the probability of the evidence, rather than starting from the evidence and trying to make it match up with a world.</p>\n<p>When we actually tested a unit based on asking people to draw Tarski squares, it didn't work very well - possibly because people didn't seem to understand what it was for, or when they would use it; possibly because it wasn't a group exercise. &nbsp;In any case, we already tried teaching this the obvious way (\"Go draw Tarski squares!\") and it didn't work. &nbsp;But it still seems worth teaching if someone can invent a better exercise, because it's something that multiple CfAR people actually use to counter the rationalization impulse / restore truthseeking in real life.</p>\n<p><em>Become Curious:</em>&nbsp; Detect non-curiosity and become curious. &nbsp;Anna's main alarm signal is when she notices that she's not <em>curious</em>&nbsp;in the middle of a conversation - that she doesn't have an impulse-to-find-out the answer - and then try to make herself curious about the subject of discussion. &nbsp;Besides visualizing the not-X-and-believe-X quadrant of the Tarski diagram, this is also something you may be able to do by brute introspection - remember the feeling of curiosity, and try to call it up. &nbsp;(This is probably in the top 3 most important things I learned from Anna. -- EY)</p>\n<p><em>Take Pride in Your Objectivity:</em>&nbsp; Julia teaches this as a primary counter in her Combat Reflexes unit (how to avoid instantly defending or attacking). &nbsp;Eliezer does this every time he admits he's wrong on the Internet - congratulates himself on being such a great rationalist, in order to apply counter-hedons to the flash of pain that would otherwise be associated.</p>\n<p><em>Visualize a Fixed Probability:</em>&nbsp; This is what Eliezer used as a child to stop being scared of the dark - he would deliberately visualize a murderer standing with a knife behind a door, then visualize his own <em>thoughts</em>&nbsp;having no effect on the <em>fixed probability</em>&nbsp;that any such murderer was actually present. &nbsp;In other words, the notion of a \"true probability\" that his thoughts couldn't affect, countered the fear of thoughts affecting reality. &nbsp;Visualizing there being a fixed frequency of worlds, or a lawful probability that a Bayesian agent would assign, can help in perceiving the futility of rationalization because you're trying to use arguments to move a lawful probability that is fixed. &nbsp;This is also part of the domain of <em>Lawful Uncertainty,&nbsp;</em>the notion that there are still rules which apply even when we're unsure (not presently part of any unit).</p>\n<p><em>Imagine the Revelation:</em>&nbsp; Anna imagines that the answer is about to be looked up on the Internet, that Omega is about to reveal the answer, etc., to check if her thoughts would change if she was potentially about to be embarrassed&nbsp;<em>right now.</em>&nbsp; This detects belief-alief divergence, but also provides truthseeking impulse.</p>\n<p><em>Knowing the Rules:</em>&nbsp; And finally, if you have sufficient mastery of probability theory or decision theory, you may have a procedure to follow which is lawful enough, and sufficiently well-understood, that rationalization can't influence it much without the mistake being blatant even to you. &nbsp;(In a sense, this is what most of Less Wrong is about - reducing the <em>amount</em>&nbsp;of self-honesty required by increasing the obviousness of mistakes.)</p>\n<hr>\n<p><strong id=\"Noticing_flinches_and_attachments__and_raising_them_to_conscious_attention_\">Noticing flinches and attachments, and raising them to conscious attention:</strong></p>\n<p>A trigger for use of curiosity-restoration or the Tarski Method: &nbsp;Noticing what it feels like for your mind to:</p>\n<ul>\n<li>Quickly glimpse a disliked argument before <a href=\"/lw/k0/singlethink/\">sweeping it under a mental rug</a> <em>(flinch)</em></li>\n<li>Glimpse a conclusion, find it unacceptable, quickly start generating arguments against it <em>(flinch)</em></li>\n<li>Be centered on a conclusion, automatically counter all arguments against it <em>(attachment)</em></li>\n<li>Instantly attack a new idea, instantly defend an old idea (this is the subject of Julia's <em>Combat Reflexes</em>&nbsp;unit)</li>\n</ul>\n<div>Learning to notice these events introspectively seems <strong>extremely important</strong>&nbsp;- we all use it heavily in daily practice - but we don't know how to teach that.</div>\n<div>Anna observes that Rejection Therapy is often a good time to observe oneself rationalizing, as apparently many participants reported that their mind started generating crazy reasons not to approach someone with a request.</div>\n<div>Anna also says that she's been self-rewarding each time she <em>notices</em>&nbsp;a flinch or attachment, i.e., she's trying to train her inner pigeon to notice (not, one hopes, training the flinching or attachment!) &nbsp;It's possible we could ask participants to self-reward each event of \"noticing the flinch or attachment\" while doing Rejection Therapy, but we still need other ideas.</div>\n<div>Along similar lines of internal behaviorism, Eliezer avoids rewarding himself <em>for</em>&nbsp;rationalizing by repeating the phrase \"Only congratulate yourself for actually changing a probability estimate or policy\" on any occasion where he <em>hasn't</em>&nbsp;changed his mind after argument - as opposed to e.g. feeling any sense of reward for having defeated an incoming argument; even if the incoming argument happens to be wrong, still, \"Only congratulate yourself for actually changing a probability estimate or policy.\"</div>\n<div>Another thing most of us do is name attachments or flinches out loud, in conversation, as we notice them, in order to reduce their strength, i.e. \"This is probably a complete post-facto rationalization, but...\" (Eliezer) or \"I may just be trying to avoid having my status reduced, but...\" (Anna). &nbsp;(Note: &nbsp;This requires enough trust that nearby people also know they're flawed themselves, that you don't feel embarrassed for confessing your own flaws in front of them. &nbsp;In other words, you have to tell embarrassing stories about your own failures of rationality, before other people will feel that they can do this around you.)</div>\n<p>Anna's anti-rationalization makes heavy use of noticing suspect situations where the outside view says she might rationalize - cases where her status is at stake, and so on - and specific keywords like \"I believe that\" or \"No, I really believe that\". &nbsp;She wants to try training people to notice likely contexts for rationalization, and to figure out keywords that might indicate rationalization in themselves. &nbsp;(Eliezer has never tried to train himself to notice keywords because he figures his brain will just train itself to avoid the trigger phrase; and he worries about likely-context training because he's seen failure modes where no amount of evidence or sound argument is enough to overcome the suspicion of rationalization once it's been invoked.)</p>\n<div>\"Look toward the painful thought instead of away from it\" is an important reflex to install to counter flinches, but would probably require some sort of hedonic support - like a strong, pre-existing pride in objectivity, or a social support group that applauds, or <em>something</em>&nbsp;to stop this from being pure negative reinforcement.</div>\n<hr>\n<p><strong id=\"Awards_for_previous_SotW_suggestions_\">Awards for previous SotW suggestions:</strong></p>\n<p><strong>$550</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/bc3/sotw_be_specific/681c\">Palladias</a>&nbsp;</strong>for the Monday-Tuesday game, which has been tested ($50) and now adopted ($500) into the Be Specific unit (though it might be moved to some sort of Anticipation unit later on).</p>\n<p><strong>$550</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/bc3/sotw_be_specific/686a\">Stefie_K</a>&nbsp;</strong>for her suggestion to have the instructor pretend to be someone who really wants you to invest in their company, but is never specific; also $50 to daenrys for the&nbsp;&nbsp;\"More Specific!\" improv-game suggestion. &nbsp;In combination these inspired the&nbsp;Vague Consultant game (\"Hi, I'm a consultant, I'm here to improve your business processes!\" &nbsp;\"How?\" &nbsp;\"By consulting with stakeholders!\") which has now been adopted into the Be Specific unit.</p>\n<p><strong>$50</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/bc3/sotw_be_specific/67zb\">lincolnquirk</a>&nbsp;</strong>for the \"Channel Paul Graham\" game, which we tested. &nbsp;We all thought this would work - it was our highest-rated candidate suggestion - but it didn't get positive audience feedback. &nbsp;Congratulations to lincolnquirk on a good suggestion nonetheless.</p>\n<p>We haven't yet tested, but definitely intend to at least test, and are hence already awarding $50 to, the following idea:</p>\n<p><strong>$50</strong>&nbsp;to&nbsp;<strong><a href=\"/lw/b4f/sotw_check_consequentialism/643p\">John Maxwell IV</a>&nbsp;</strong>for the Choose Your Own Adventure suggestion for the Consequentialism unit.</p>\n<p>To claim a prize, send a LessWrong private message (so we know it originates from the same LW user account) to <a href=\"/message/compose/?to=StephenCole\">StephenCole</a>.</p>", "sections": [{"title": "Conceptual understanding / insights / theoretical background:", "anchor": "Conceptual_understanding___insights___theoretical_background_", "level": 1}, {"title": "Countering the rationalization impulse / restoring truth-seeking:", "anchor": "Countering_the_rationalization_impulse___restoring_truth_seeking_", "level": 1}, {"title": "Noticing flinches and attachments, and raising them to conscious attention:", "anchor": "Noticing_flinches_and_attachments__and_raising_them_to_conscious_attention_", "level": 1}, {"title": "Awards for previous SotW suggestions:", "anchor": "Awards_for_previous_SotW_suggestions_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "77 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34XxbRFe54FycoCDw", "SFZoEBpLo9frSJGkc", "kJiPnaQPiy4p9Eqki", "jiBFC7DcCrZjGmZnJ", "9f5EXt8KNNxTAihtZ", "AdYdLP2sRqPMoe8fb", "hEFrm3nZMFiW2czvb", "3nZMgRTfFEfHp34Gb", "CahCppKy9HuXe3j2i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-28T19:03:19.664Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Living in Many Worlds", "slug": "seq-rerun-living-in-many-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QPCNurCRfrYf434yJ/seq-rerun-living-in-many-worlds", "pageUrlRelative": "/posts/QPCNurCRfrYf434yJ/seq-rerun-living-in-many-worlds", "linkUrl": "https://www.lesswrong.com/posts/QPCNurCRfrYf434yJ/seq-rerun-living-in-many-worlds", "postedAtFormatted": "Monday, May 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Living%20in%20Many%20Worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Living%20in%20Many%20Worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPCNurCRfrYf434yJ%2Fseq-rerun-living-in-many-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Living%20in%20Many%20Worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPCNurCRfrYf434yJ%2Fseq-rerun-living-in-many-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPCNurCRfrYf434yJ%2Fseq-rerun-living-in-many-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 225, "htmlBody": "<p>Today's post, <a href=\"/lw/qz/living_in_many_worlds/\">Living in Many Worlds</a> was originally published on 05 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The many worlds of quantum mechanics are not some strange, alien universe into which you have been thrust. They are where you have always lived. Egan's Law: \"It all adds up to normality.\" Then why care about quantum physics at all? Because there's still the question of what adds up to normality, and the answer to this question turns out to be, \"Quantum physics.\" If you're thinking of building any strange philosophies around many-worlds, you probably shouldn't - that's not what it's for.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/coj/seq_rerun_why_quantum/\">Why Quantum?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QPCNurCRfrYf434yJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.116269588674809e-07, "legacy": true, "legacyId": "16460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qcYCAxYZT4Xp9iMZY", "DRFNpGndcCpRRE4JL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-28T21:25:58.312Z", "modifiedAt": null, "url": null, "title": "Meetup : Big Berkeley meetup", "slug": "meetup-big-berkeley-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KxH4nwup89KjbaAGG/meetup-big-berkeley-meetup", "pageUrlRelative": "/posts/KxH4nwup89KjbaAGG/meetup-big-berkeley-meetup", "linkUrl": "https://www.lesswrong.com/posts/KxH4nwup89KjbaAGG/meetup-big-berkeley-meetup", "postedAtFormatted": "Monday, May 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Big%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Big%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxH4nwup89KjbaAGG%2Fmeetup-big-berkeley-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Big%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxH4nwup89KjbaAGG%2Fmeetup-big-berkeley-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxH4nwup89KjbaAGG%2Fmeetup-big-berkeley-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/am\">Big Berkeley meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">30 May 2012 07:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2128 oxford street, berkeley, ca</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The next Big Berkeley meetup is this Wednesday. Let's meet at 7pm in the Starbucks at 2128 Oxford St. as usual, and then decide where to go from there. I hear that there's considerable demand for a board games night; <strong>EDIT: There are definitely places to play games, like Games of Berkeley; or, better yet, Biryani House. So expect to play games! Bring your favorite game.</strong>&nbsp;As a backup I will \"bring\" some rationality exercises/games that Eliezer presented at this months minicamp, including Rationalist Taboo.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/am\">Big Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KxH4nwup89KjbaAGG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 9.116905791206161e-07, "legacy": true, "legacyId": "16461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/am\">Big Berkeley meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">30 May 2012 07:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2128 oxford street, berkeley, ca</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The next Big Berkeley meetup is this Wednesday. Let's meet at 7pm in the Starbucks at 2128 Oxford St. as usual, and then decide where to go from there. I hear that there's considerable demand for a board games night; <strong>EDIT: There are definitely places to play games, like Games of Berkeley; or, better yet, Biryani House. So expect to play games! Bring your favorite game.</strong>&nbsp;As a backup I will \"bring\" some rationality exercises/games that Eliezer presented at this months minicamp, including Rationalist Taboo.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/am\">Big Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Big Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Big Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T02:50:49.168Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Thou Art Physics", "slug": "seq-rerun-thou-art-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EB8QXYdBiKDre6sBa/seq-rerun-thou-art-physics", "pageUrlRelative": "/posts/EB8QXYdBiKDre6sBa/seq-rerun-thou-art-physics", "linkUrl": "https://www.lesswrong.com/posts/EB8QXYdBiKDre6sBa/seq-rerun-thou-art-physics", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Thou%20Art%20Physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Thou%20Art%20Physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEB8QXYdBiKDre6sBa%2Fseq-rerun-thou-art-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Thou%20Art%20Physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEB8QXYdBiKDre6sBa%2Fseq-rerun-thou-art-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEB8QXYdBiKDre6sBa%2Fseq-rerun-thou-art-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/r0/thou_art_physics/\">Thou Art Physics</a> was originally published on 06 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If the laws of physics control everything we do, then how can our choices be meaningful? Because <em>you are</em> physics. You aren't <em>competing </em>with physics for control of the universe, you are <em>within </em>physics. Anything you control is <em>necessarily </em>controlled by physics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cp8/seq_rerun_living_in_many_worlds/\">Living in Many Worlds</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EB8QXYdBiKDre6sBa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.118354926505549e-07, "legacy": true, "legacyId": "16466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NEeW7eSXThPz7o4Ne", "QPCNurCRfrYf434yJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T04:10:25.364Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 5/28/12", "slug": "group-rationality-diary-5-28-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RpeHoK89Ra3hBR26r/group-rationality-diary-5-28-12", "pageUrlRelative": "/posts/RpeHoK89Ra3hBR26r/group-rationality-diary-5-28-12", "linkUrl": "https://www.lesswrong.com/posts/RpeHoK89Ra3hBR26r/group-rationality-diary-5-28-12", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%205%2F28%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%205%2F28%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpeHoK89Ra3hBR26r%2Fgroup-rationality-diary-5-28-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%205%2F28%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpeHoK89Ra3hBR26r%2Fgroup-rationality-diary-5-28-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRpeHoK89Ra3hBR26r%2Fgroup-rationality-diary-5-28-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for the week of May 28th. &nbsp;It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Thanks to everyone who contributes!</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">(</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Previously:&nbsp;</span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"/lw/cew/group_rationality_diary_51412/\">5/14/12</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">,&nbsp;</span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"/r/discussion/lw/ckm/group_rationality_diary_52112/\">5/21/12</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">)</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RpeHoK89Ra3hBR26r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.118710096431517e-07, "legacy": true, "legacyId": "16468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JA2MnwunfZnFXb3by", "aFRC3J3TNmKMf2p4T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T10:24:57.640Z", "modifiedAt": null, "url": null, "title": "[LINK] Wireheading in SMBC", "slug": "link-wireheading-in-smbc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.662Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "hRzKAvARt4HspRBWA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fDmN7vy6uweJ5qSi2/link-wireheading-in-smbc", "pageUrlRelative": "/posts/fDmN7vy6uweJ5qSi2/link-wireheading-in-smbc", "linkUrl": "https://www.lesswrong.com/posts/fDmN7vy6uweJ5qSi2/link-wireheading-in-smbc", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Wireheading%20in%20SMBC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Wireheading%20in%20SMBC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfDmN7vy6uweJ5qSi2%2Flink-wireheading-in-smbc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Wireheading%20in%20SMBC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfDmN7vy6uweJ5qSi2%2Flink-wireheading-in-smbc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfDmN7vy6uweJ5qSi2%2Flink-wireheading-in-smbc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10, "htmlBody": "<p>http://www.smbc-comics.com/index.php?db=comics&amp;id=2625#comic</p>\n<p>(The last panel also illustrates Hanson's near vs far distinction.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fDmN7vy6uweJ5qSi2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 8, "extendedScore": null, "score": 9.120381516676822e-07, "legacy": true, "legacyId": "16486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T13:17:53.156Z", "modifiedAt": null, "url": null, "title": "Computer Science and Programming: Links and Resources", "slug": "computer-science-and-programming-links-and-resources", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:33.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5sKx9BXRzrfHp2SL8/computer-science-and-programming-links-and-resources", "pageUrlRelative": "/posts/5sKx9BXRzrfHp2SL8/computer-science-and-programming-links-and-resources", "linkUrl": "https://www.lesswrong.com/posts/5sKx9BXRzrfHp2SL8/computer-science-and-programming-links-and-resources", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Computer%20Science%20and%20Programming%3A%20Links%20and%20Resources&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComputer%20Science%20and%20Programming%3A%20Links%20and%20Resources%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5sKx9BXRzrfHp2SL8%2Fcomputer-science-and-programming-links-and-resources%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Computer%20Science%20and%20Programming%3A%20Links%20and%20Resources%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5sKx9BXRzrfHp2SL8%2Fcomputer-science-and-programming-links-and-resources", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5sKx9BXRzrfHp2SL8%2Fcomputer-science-and-programming-links-and-resources", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4173, "htmlBody": "<p><span style=\"color: #ff0000;\">Updated Version @ LW Wiki:</span> <a href=\"http://wiki.lesswrong.com/wiki/Programming_resources\">wiki.lesswrong.com/wiki/Programming_resources</a></p>\n<p><strong>Contents</strong></p>\n<table style=\"border: thin solid #808080; width: 440px; padding: 5px;\" border=\"0\">\n<tbody>\n<tr>\n<td>\n<ul>\n<li><a href=\"#basics\">How Computers Work</a></li>\n<li><a href=\"#programming\">An Overview of Computer Programming</a></li>\n<li><a href=\"#cs\">Computer Science and Computation</a></li>\n<li><a href=\"#resources\">Supplementary Resources: Mathematics and Algorithms</a></li>\n<li><a href=\"#practice\">Practice</a></li>\n<li><a href=\"#python\">Python</a></li>\n<li><a href=\"#haskell\">Haskell</a></li>\n<li><a href=\"#lisp\">Common Lisp</a></li>\n<li><a href=\"#r\">R</a></li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h3><strong><a name=\"basics\">How Computers Work</a></strong></h3>\n<p>1. <a href=\"http://www.charlespetzold.com/code/\">CODE The Hidden Language of Computer Hardware and Software</a></p>\n<blockquote>\n<p>The book intends to show a layman the basic mechanical principles of how computers work, instead of merely summarizing how the different parts relate. He starts with basic principles of language and logic and then demonstrates how they can be embodied by electrical circuits, and these principles give him an opening to describe in principle how computers work mechanically without requiring very much technical knowledge. Although it is not possible in a medium sized book for layman to describe the entire technical summary of a computer, he describes how and why it is possible that elaborate electronics can act in the ways computers do. In the introduction, he contrasts his own work with those books which \"include pictures of trains full of 1s and 0s.\"</p>\n</blockquote>\n<p>2. <a href=\"http://www1.idc.ac.il/tecs/\">The Elements of Computing Systems: Building a Modern Computer from First Principles</a></p>\n<blockquote>\n<p>Indeed, the best way to understand how computers work is to build one from scratch, and this textbook leads students through twelve chapters and projects that gradually build a basic hardware platform and a modern software hierarchy from the ground up. In the process, the students gain hands-on knowledge of hardware architecture, operating systems, programming languages, compilers, data structures, algorithms, and software engineering. Using this constructive approach, the book exposes a significant body of computer science knowledge and demonstrates how theoretical and applied techniques taught in other courses fit into the overall picture.</p>\n</blockquote>\n<p>3. <a href=\"http://homepage.mac.com/randyhyde/webster.cs.ucr.edu/www.writegreatcode.com/\">The Write Great Code Series (A Solid Foundation in Software Engineering for Programmers)</a></p>\n<p>Write Great Code Volume I: Understanding the Machine</p>\n<blockquote>\n<p>This, the first of four volumes, teaches important concepts of machine organization in a language-independent fashion, giving programmers what they need to know to write great code in any language, without the usual overhead of learning assembly language to master this topic. The Write Great Code series will help programmers make wiser choices with respect to programming statements and data types when writing software.</p>\n</blockquote>\n<p>Write Great Code Volume II: Thinking Low-Level, Writing High-Level</p>\n<blockquote>\n<p>...a good question to ask might be \"Is there some way to write high-level language code to help the compiler produce high-quality machine code?\" The answer to this question is \"yes\" and Write Great Code, Volume II, will teach you how to write such high-level code. This volume in the Write Great Code series describes how compilers translate statements into machine code so that you can choose appropriate high-level programming language statements to produce executable code that is almost as good as hand-optimized assembly code.</p>\n</blockquote>\n<p>4. <a href=\"http://www.amazon.com/Haskell-Logic-Maths-Programming-Computing/dp/0954300696/\">The Art of Assembly Language Programming</a></p>\n<blockquote>\n<p>Assembly is a low-level programming language that's one step above a computer's native machine language. Although assembly language is commonly used for writing device drivers, emulators, and video games, many programmers find its somewhat unfriendly syntax intimidating to learn and use.<br /><br />Since 1996, Randall Hyde's The Art of Assembly Language has provided a comprehensive, plain-English, and patient introduction to assembly for non-assembly programmers. Hyde's primary teaching tool, High Level Assembler (or HLA), incorporates many of the features found in high-level languages (like C, C++, and Java) to help you quickly grasp basic assembly concepts. HLA lets you write true low-level code while enjoying the benefits of high-level language programming.</p>\n</blockquote>\n<p>5. <a href=\"http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming\">The Art of Computer Programming</a></p>\n<blockquote>\n<p>This work is not about computer programming in the narrow sense, but about the algorithms and methods which lie at the heart of most computer systems.</p>\n<p>At the end of 1999, these books were named among the best twelve physical-science monographs of the century by American Scientist, along with: Dirac on quantum mechanics, Einstein on relativity, Mandelbrot on fractals, Pauling on the chemical bond, Russell and Whitehead on foundations of mathematics, von Neumann and Morgenstern on game theory, Wiener on cybernetics, Woodward and Hoffmann on orbital symmetry, Feynman on quantum electrodynamics, Smith on the search for structure, and Einstein's collected papers.</p>\n</blockquote>\n<h3><strong><a name=\"programming\">An Overview of Computer Programming</a></strong></h3>\n<p>1. <a href=\"http://www.amazon.com/Seven-Languages-Weeks-Programming-Programmers/dp/193435659X\">Seven Languages in Seven Weeks: A Pragmatic Guide to Learning Programming Languages</a></p>\n<blockquote>\n<p>Ruby, Io, Prolog, Scala, Erlang, Clojure, Haskell. With Seven Languages in Seven Weeks, by Bruce A. Tate, you'll go beyond the syntax-and beyond the 20-minute tutorial you'll find someplace online. This book has an audacious goal: to present a meaningful exploration of seven languages within a single book. Rather than serve as a complete reference or installation guide, Seven Languages hits what's essential and unique about each language. Moreover, this approach will help teach you how to grok new languages.<br /><br />For each language, you'll solve a nontrivial problem, using techniques that show off the language's most important features. As the book proceeds, you'll discover the strengths and weaknesses of the languages, while dissecting the process of learning languages quickly--for example, finding the typing and programming models, decision structures, and how you interact with them.</p>\n</blockquote>\n<p>2. <a href=\"http://www.amazon.com/Programming-Language-Pragmatics-Third-Michael/dp/0123745144/\">Programming Language Pragmatics</a></p>\n<blockquote>\n<p>The ubiquity of computers in everyday life in the 21st century justifies the centrality of programming languages to computer science education.&nbsp; Programming languages is the area that connects the theoretical foundations of computer science, the source of problem-solving algorithms, to modern computer architectures on which the corresponding programs produce solutions.&nbsp; Given the speed with which computing technology advances in this post-Internet era, a computing textbook must present a structure for organizing information about a subject, not just the facts of the subject itself.&nbsp; In this book, Michael Scott broadly and comprehensively presents the key concepts of programming languages and their implementation, in a manner appropriate for computer science majors.&nbsp;</p>\n</blockquote>\n<p>3. <a href=\"http://www.amazon.com/Introduction-Functional-Programming-Calculus-Mathematics/dp/0486478831/\">An Introduction to Functional Programming Through Lambda Calculus</a></p>\n<blockquote>\n<p>This well-respected text offers an accessible introduction to functional programming concepts and techniques for students of mathematics and computer science. The treatment is as nontechnical as possible, assuming no prior knowledge of mathematics or functional programming. Numerous exercises appear throughout the text, and all problems feature complete solutions.</p>\n</blockquote>\n<p>4. <a href=\"http://www.htdp.org/\">How to Design Programs</a> (An Introduction to Computing and Programming)</p>\n<blockquote>\n<p>This introduction to programming places computer science in the core of a liberal arts education. Unlike other introductory books, it focuses on the program design process. This approach fosters a variety of skills--critical reading, analytical thinking, creative synthesis, and attention to detail--that are important for everyone, not just future computer programmers.The book exposes readers to two fundamentally new ideas. First, it presents program design guidelines that show the reader how to analyze a problem statement; how to formulate concise goals; how to make up examples; how to develop an outline of the solution, based on the analysis; how to finish the program; and how to test. Each step produces a well-defined intermediate product. Second, the book comes with a novel programming environment, the first one explicitly designed for beginners.</p>\n</blockquote>\n<p>5. <a href=\"http://mitpress.mit.edu/sicp/\">Structure and Interpretation of Computer Programs</a></p>\n<blockquote>\n<p>Using a dialect of the Lisp programming language known as Scheme, the book explains core computer science concepts, including abstraction, recursion, interpreters and metalinguistic abstraction, and teaches modular programming.<br /><br />The program also introduces a practical implementation of the register machine concept, defining and developing an assembler for such a construct, which is used as a virtual machine for the implementation of interpreters and compilers in the book, and as a testbed for illustrating the implementation and effect of modifications to the evaluation mechanism. Working Scheme systems based on the design described in this book are quite common student projects.</p>\n</blockquote>\n<h3><strong><a name=\"cs\">Computer Science and Computation</a><br /></strong></h3>\n<p>1. <a href=\"http://www.theannotatedturing.com/\">The Annotated Turing: A Guided Tour Through Alan Turing's Historic Paper on Computability and the Turing Machine</a></p>\n<blockquote>\n<p>Mathematician Alan Turing invented an imaginary computer known as the Turing Machine; in an age before computers, he explored the concept of what it meant to be computable, creating the field of computability theory in the process, a foundation of present-day computer programming.<br /><br />The book expands Turing&rsquo;s original 36-page paper with additional background chapters and extensive annotations; the author elaborates on and clarifies many of Turing&rsquo;s statements, making the original difficult-to-read document accessible to present day programmers, computer science majors, math geeks, and others.</p>\n</blockquote>\n<p>2. <a href=\"http://www.amazon.com/New-Turing-Omnibus-Turning-Excursions/dp/0716782715/ref=la_B000APOGWI_1_2?ie=UTF8&amp;qid=1338291432&amp;sr=1-2\">New Turing Omnibus</a> (New Turning Omnibus : 66 Excursions in Computer Science)</p>\n<blockquote>\n<p>This text provides a broad introduction to the realm of computers. Updated and expanded, \"The New Turing Omnibus\" offers 66 concise articles on the major points of interest in computer science theory, technology and applications. New for this edition are: updated information on algorithms, detecting primes, noncomputable functions, and self-replicating computers - plus completely new sections on the Mandelbrot set, genetic algorithms, the Newton-Raphson Method, neural networks that learn, DOS systems for personal computers, and computer viruses.</p>\n</blockquote>\n<p>3. <a href=\"http://www.udacity.com/\">Udacity</a></p>\n<blockquote>\n<p>Udacity is a private educational organization founded by Sebastian Thrun, David Stavens, and Mike Sokolsky, with the stated goal of democratizing education<br /><br />It is the outgrowth of free computer science classes offered in 2011 through Stanford University. As of May 2012 Udacity has six active courses.<br /><br />The first two courses ever launched on Udacity both started on 20th February, 2012, entitled \"CS 101: Building a Search Engine\", taught by Dave Evans, from the University of Virginia, and \"CS 373: Programming a Robotic Car\" taught by Thrun. Both courses use Python.</p>\n</blockquote>\n<p>4. <a href=\"https://www.ai-class.com/\">Introduction to Artificial Intelligence</a></p>\n<blockquote>\n<p>A bold experiment in distributed education, \"Introduction to Artificial Intelligence\" will be offered free and online to students worldwide from October 10th to December 18th 2011. The course will include feedback on progress and a statement of accomplishment. Taught by Sebastian Thrun and Peter Norvig, the curriculum draws from that used in Stanford's introductory Artificial Intelligence course. The instructors will offer similar materials, assignments, and exams.<br /><br />Artificial Intelligence is the science of making computer software that reasons about the world around it. Humanoid robots, Google Goggles, self-driving cars, even software that suggests music you might like to hear are all examples of AI. In this class, you will learn how to create this software from two of the leaders in the field. Class begins October 10.</p>\n</blockquote>\n<h3><strong><a name=\"resources\">Supplementary Resources: Mathematics and Algorithms</a></strong></h3>\n<p>1. <a href=\"http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025\">Concrete Mathematics: A Foundation for Computer Science</a></p>\n<blockquote>\n<p>This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline.</p>\n</blockquote>\n<p>2. <a href=\"http://algs4.cs.princeton.edu/home/\">Algorithms</a></p>\n<blockquote>\n<p>The textbook Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne surveys the most important algorithms and data structures in use today.</p>\n</blockquote>\n<p>3. <a href=\"http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/\">Introduction to Algorithms</a></p>\n<blockquote>\n<p>Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. The book covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers. Each chapter is relatively self-contained and can be used as a unit of study. The algorithms are described in English and in a pseudocode designed to be readable by anyone who has done a little programming. The explanations have been kept elementary without sacrificing depth of coverage or mathematical rigor.</p>\n</blockquote>\n<h3><a name=\"practice\">Practice</a></h3>\n<p>1. <a href=\"http://projecteuler.net/\">Project Euler</a></p>\n<blockquote>\n<p>Project Euler is a series of challenging mathematical/computer programming problems that will require more than just mathematical insights to solve. Although mathematics will help you arrive at elegant and efficient methods, the use of a computer and programming skills will be required to solve most problems.</p>\n</blockquote>\n<p>2. <a href=\"http://www.pythonchallenge.com/\">The Python Challenge</a></p>\n<blockquote>\n<p>Python Challenge is a game in which each level can be solved by a bit of (Python) programming.</p>\n</blockquote>\n<p>3. <a href=\"http://www.codechef.com\">CodeChef Programming Competition</a></p>\n<blockquote>\n<p>CodeChef is a global programming community. We host contests, trainings and events for programmers around the world. Our goal is to provide a platform for programmers everywhere to meet, compete, and have fun.</p>\n</blockquote>\n<p>4. Write your own programs.</p>\n<h3><a name=\"python\">Python</a></h3>\n<p><a href=\"http://code.google.com/p/pyscripter/\">pyscripter</a></p>\n<p>An open-source Python Integrated Development Environment (IDE)</p>\n<p><a href=\"http://www.khanacademy.org/#computer-science\">Khan Academy</a></p>\n<p>Introduction to programming and computer science (using Python)</p>\n<p>1. <a href=\"http://inventwithpython.com/\">Invent Your Own Computer Games with Python</a></p>\n<blockquote>\n<p>&ldquo;Invent Your Own Computer Games with Python&rdquo; is a free book (as in, open source) and a free eBook (as in, no cost to download) that teaches you how to program in the Python programming language. Each chapter gives you the complete source code for a new game, and then teaches the programming concepts from the example.<br /><br />&ldquo;Invent with Python&rdquo; was written to be understandable by kids as young as 10 to 12 years old, although it is great for anyone of any age who has never programmed before.</p>\n</blockquote>\n<p>2. <a href=\"http://learnpythonthehardway.org/\">Learn Python The Hard Way</a></p>\n<blockquote>\n<p>Have you always wanted to learn how to code but never thought you could? Are you looking to build a foundation for more complex coding? Do you want to challenge your brain in a new way? Then Learn Python the Hard Way is the book for you.</p>\n</blockquote>\n<p>3. <a href=\"http://www.greenteapress.com/thinkpython/\">Python for Software Design: How to Think Like a Computer Scientist</a></p>\n<blockquote>\n<p>Think Python is an introduction to Python programming for beginners. It starts with basic concepts of programming, and is carefully designed to define all terms when they are first used and to develop each new concept in a logical progression. Larger pieces, like recursion and object-oriented programming are divided into a sequence of smaller steps and introduced over the course of several chapters.</p>\n</blockquote>\n<p>4. <a href=\"http://www.amazon.com/Python-Programming-Introduction-Computer-Science/dp/1590282418/\">Python Programming: An Introduction to Computer Science</a></p>\n<blockquote>\n<p>This book is suitable for use in a university-level first course in computing (CS1), as well as the increasingly popular course known as CS0. It is difficult for many students to master basic concepts in computer science and programming. A large portion of the confusion can be blamed on the complexity of the tools and materials that are traditionally used to teach CS1 and CS2. This textbook was written with a single overarching goal: to present the core concepts of computer science as simply as possible without being simplistic.</p>\n</blockquote>\n<p>5. <a href=\"http://www.amazon.com/Practical-Programming-Introduction-Pragmatic-Programmers/dp/1934356271/\">Practical Programming: An Introduction to Computer Science Using Python</a></p>\n<blockquote>\n<p>Computers are used in every part of science from ecology to particle physics. This introduction to computer science continually reinforces those ties by using real-world science problems as examples. Anyone who has taken a high school science class will be able to follow along as the book introduces the basics of programming, then goes on to show readers how to work with databases, download data from the web automatically, build graphical interfaces, and most importantly, how to think like a professional programmer.</p>\n</blockquote>\n<p>6. <a href=\"http://www.amazon.com/Quick-Python-Book-Second/dp/193518220X/\">The Quick Python Book</a></p>\n<blockquote>\n<p>The Quick Python Book, Second Edition, is a clear, concise introduction to Python 3, aimed at programmers new to Python. This updated edition includes all the changes in Python 3, itself a significant shift from earlier versions of Python.</p>\n<p>The book begins with basic but useful programs that teach the core features of syntax, control flow, and data structures. It then moves to larger applications involving code management, object-oriented programming, web development, and converting code from earlier versions of Python.</p>\n</blockquote>\n<h3><a name=\"haskell\">Haskell</a></h3>\n<p><a href=\"http://hackage.haskell.org/platform/\">The Haskell Platform</a></p>\n<p>The Haskell Platform is the easiest way to get started with programming Haskell. It comes with all you need to get up and running. Think of it as \"Haskell: batteries included\".</p>\n<p>1. <a href=\"http://www.haskell.org/haskellwiki/Haskell_in_5_steps\">Haskell in 5 steps</a></p>\n<blockquote>\n<p>This page will help you get started as quickly as possible.</p>\n</blockquote>\n<p>2. <a href=\"http://www.haskell.org/haskellwiki/Learn_Haskell_in_10_minutes\">Learn Haskell in 10 minutes</a></p>\n<p>3. <a href=\"http://www.haskell.org/haskellwiki/A_brief_introduction_to_Haskell\">A brief introduction to Haskell</a></p>\n<p>4. <a href=\"http://www.amazon.com/Programming-Haskell-Graham-Hutton/dp/0521692695/\">Programming in Haskell</a></p>\n<blockquote>\n<p>Haskell is one of the leading languages for teaching functional programming, enabling students to write simpler and cleaner code, and to learn how to structure and reason about programs. This introduction is ideal for beginners: it requires no previous programming experience and all concepts are explained from first principles via carefully chosen examples. Each chapter includes exercises that range from the straightforward to extended projects, plus suggestions for further reading on more advanced topics. The author is a leading Haskell researcher and instructor, well-known for his teaching skills. The presentation is clear and simple, and benefits from having been refined and class-tested over several years. The result is a text that can be used with courses, or for self-learning. Features include freely accessible Powerpoint slides for each chapter, solutions to exercises and examination questions (with solutions) available to instructors, and a downloadable code that's fully compliant with the latest Haskell release.</p>\n</blockquote>\n<p>5. <a href=\"http://learnyouahaskell.com/\">Learn You a Haskell for Great Good!</a></p>\n<blockquote>\n<p>Learn You a Haskell, the funkiest way to learn Haskell, which is the best functional programming language around. You may have heard of it. This guide is meant for people who have programmed already, but have yet to try functional programming.</p>\n</blockquote>\n<p>6. <a href=\"http://book.realworldhaskell.org/\">Real World Haskell</a></p>\n<blockquote>\n<p>This easy-to-use, fast-moving tutorial introduces you to functional programming with Haskell. You'll learn how to use Haskell in a variety of practical ways, from short scripts to large and demanding applications. Real World Haskell takes you through the basics of functional programming at a brisk pace, and then helps you increase your understanding of Haskell in real-world issues like I/O, performance, dealing with data, concurrency, and more as you move through each chapter.</p>\n</blockquote>\n<p>7. <a href=\"http://fldit-www.cs.uni-dortmund.de/~peter/PS07/HR.pdf\">The Haskell Road to Logic, Maths and Programming</a></p>\n<blockquote>\n<p>The textbook by Doets and van Eijck puts the Haskell programming language systematically to work for presenting a major piece of logic and mathematics. The reader is taken through chapters on basic logic, proof recipes, sets and lists, relations and functions, recursion and co-recursion, the number systems, polynomials and power series, ending with Cantor's infinities. The book uses Haskell for the executable and strongly typed manifestation of various mathematical notions at the level of declarative programming. The book adopts a systematic but relaxed mathematical style (definition, example, exercise, ...); the text is very pleasant to read due to a small amount of anecdotal information, and due to the fact that definitions are fluently integrated in the running text. An important goal of the book is to get the reader acquainted with reasoning about programs.&nbsp;</p>\n</blockquote>\n<h3><a name=\"lisp\">Common Lisp</a></h3>\n<p>1. <a href=\"http://www.amazon.com/Land-Lisp-Learn-Program-Game/dp/1593272812/\">Land of Lisp: Learn to Program in Lisp, One Game at a Time!</a></p>\n<blockquote>\n<p>Lisp has been hailed as the world's most powerful programming language, but its cryptic syntax and academic reputation can be enough to scare off even experienced programmers. Those dark days are finally over&mdash;<em>Land of Lisp</em> brings the power of functional programming to the people!</p>\n<p>With his brilliantly quirky comics and out-of-this-world games, longtime Lisper Conrad Barski teaches you the mysteries of Common Lisp. You'll start with the basics, like list manipulation, I/O, and recursion, then move on to more complex topics like macros, higher order programming, and domain-specific languages. Then, when your brain overheats, you can kick back with an action-packed comic book interlude!</p>\n</blockquote>\n<p>2. <a href=\"http://www.gigamonkeys.com/book/\">Practical Common Lisp</a></p>\n<blockquote>\n<p><em>Practical Common Lisp</em> presents a thorough introduction to Common Lisp, providing you with an overall understanding of the language features and how they work. Over a third of the book is devoted to practical examples such as the core of a spam filter and a web application for browsing MP3s and streaming them via the Shoutcast protocol to any standard MP3 client software (e.g., iTunes, XMMS, or WinAmp). In other \"practical\" chapters, author Peter Seibel demonstrates how to build a simple but flexible in-memory database, how to parse binary files, and how to build a unit test framework in 26 lines of code.</p>\n</blockquote>\n<p>3. <a href=\"http://www.amazon.com/ANSI-Common-LISP-Paul-Graham/dp/0133708756/\">ANSI Common LISP</a></p>\n<blockquote>\n<p>Teaching users new and more powerful ways of thinking about programs, this two-in-one text contains a tutorial&mdash;full of examples&mdash;that explains all the essential concepts of Lisp programming, <em>plus</em> an up-to-date summary of ANSI Common Lisp, listing every operator in the language. Informative and fun, it gives users everything they need to start writing programs in Lisp both efficiently and effectively, and highlights such innovative Lisp features as automatic memory management, manifest typing, closures, and more. Dividing material into two parts, the tutorial half of the book covers subject-by-subject the essential core of Common Lisp, and sums up lessons of preceding chapters in two examples of real applications: a backward-chainer, and an embedded language for object-oriented programming. Consisting of three appendices, the summary half of the book gives source code for a selection of widely used Common Lisp operators, with definitions that offer a comprehensive explanation of the language and provide a rich source of real examples; summarizes some differences between ANSI Common Lisp and Common Lisp as it was originally defined in 1984; and contains a concise description of every function, macro, and special operator in ANSI Common Lisp. The book concludes with a section of notes containing clarifications, references, and additional code.</p>\n</blockquote>\n<p>4. <a href=\"http://www.amazon.com/Paradigms-Artificial-Intelligence-Programming-Studies/dp/1558601910/\">Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp</a></p>\n<blockquote>\n<p><em>Paradigms of AI Programming</em> is the first text to teach advanced Common Lisp techniques in the context of building major AI systems. By reconstructing authentic, complex AI programs using state-of-the-art Common Lisp, the book teaches students and professionals how to build and debug robust practical programs, while demonstrating superior programming style and important AI concepts. The author strongly emphasizes the practical performance issues involved in writing real working programs of significant size. Chapters on troubleshooting and efficiency are included, along with a discussion of the fundamentals of object-oriented programming and a description of the main CLOS functions. This volume is an excellent text for a course on AI programming, a useful supplement for general AI courses and an indispensable reference for the professional programmer.</p>\n</blockquote>\n<p>5. <a href=\"http://letoverlambda.com/\">Let Over Lambda</a></p>\n<blockquote>\n<p>Let Over Lambda is one of the most hardcore computer programming books out there. Starting with the fundamentals, it describes the most advanced features of the most advanced language: COMMON LISP. The point of this book is to expose you to ideas that you might otherwise never be exposed to.</p>\n</blockquote>\n<p>6. <a href=\"http://www.michaelnielsen.org/ddi/lisp-as-the-maxwells-equations-of-software/\">Lisp as the Maxwell&rsquo;s equations of software</a></p>\n<blockquote>\n<p>These are Maxwell&rsquo;s equations. Just four compact equations. With a little work it&rsquo;s easy to understand the basic elements of the equations &ndash; what all the symbols mean, how we can compute all the relevant quantities, and so on. But while it&rsquo;s easy to understand the elements of the equations, understanding all their consequences is another matter. Inside these equations is all of electromagnetism &ndash; everything from antennas to motors to circuits. If you think you understand the consequences of these four equations, then you may leave the room now, and you can come back and ace the exam at the end of semester.</p>\n</blockquote>\n<h3><a name=\"r\">R</a></h3>\n<p><a href=\"http://rstudio.org/\">RStudio</a></p>\n<p>RStudio&trade; is a free and open source integrated development environment (IDE) for R. You can run it on your desktop (Windows, Mac, or Linux) or even over the web using RStudio Server.</p>\n<p>1. <a href=\"http://www.stat.berkeley.edu/share/rvideos/R_Videos/R_Videos.html\">R Videos</a></p>\n<p>2. <a href=\"http://www.twotorials.com/\">R Tutorials</a></p>\n<p>3. <a href=\"http://pairach.com/2012/02/26/r-tutorials-from-universities-around-the-world/\">R Tutorials from Universities Around the World</a></p>\n<blockquote>\n<p>Here is a list of FREE R tutorials hosted in official website of universities around the world.</p>\n</blockquote>\n<p>4. <a href=\"http://www.r-bloggers.com/\">R-bloggers</a></p>\n<blockquote>\n<p>Here you will find daily news and tutorials about R, contributed by over 300 bloggers.</p>\n</blockquote>\n<p>5. <a href=\"http://nostarch.com/artofr.htm\">The Art of R Programming: A Tour of Statistical Software Design</a></p>\n<blockquote>\n<p>R is the world's most popular language for developing statistical software: Archaeologists use it to track the spread of ancient civilizations, drug companies use it to discover which medications are safe and effective, and actuaries use it to assess financial risks and keep economies running smoothly.<br /><br />The Art of R Programming takes you on a guided tour of software development with R, from basic types and data structures to advanced topics like closures, recursion, and anonymous functions. No statistical knowledge is required, and your programming skills can range from hobbyist to pro.<br /><br />Along the way, you'll learn about functional and object-oriented programming, running mathematical simulations, and rearranging complex data into simpler, more useful formats.</p>\n</blockquote>\n<p>6. <a href=\"http://pluto.huji.ac.il/~msby/StatThink/index.html\">Introduction to Statistical Thinking (With R, Without Calculus)</a></p>\n<blockquote>\n<p>The target audience for this book is college students who are required to learn statistics, students with little background in mathematics and often no motivation to learn more.</p>\n</blockquote>\n<p>7. <a href=\"http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/\">Doing Bayesian Data Analysis: A Tutorial with R and BUGS </a></p>\n<blockquote>\n<p>There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis, A Tutorial Introduction with R and BUGS provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. The text delivers comprehensive coverage of all scenarios addressed by non-Bayesian textbooks--t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis).<br /><br />This book is intended for first year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Prerequisite is knowledge of algebra and basic calculus. Free software now includes programs in JAGS, which runs on Macintosh, Linux, and Windows.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5sKx9BXRzrfHp2SL8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 42, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "16487", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"color: #ff0000;\">Updated Version @ LW Wiki:</span> <a href=\"http://wiki.lesswrong.com/wiki/Programming_resources\">wiki.lesswrong.com/wiki/Programming_resources</a></p>\n<p><strong id=\"Contents\">Contents</strong></p>\n<table style=\"border: thin solid #808080; width: 440px; padding: 5px;\" border=\"0\">\n<tbody>\n<tr>\n<td>\n<ul>\n<li><a href=\"#basics\">How Computers Work</a></li>\n<li><a href=\"#programming\">An Overview of Computer Programming</a></li>\n<li><a href=\"#cs\">Computer Science and Computation</a></li>\n<li><a href=\"#resources\">Supplementary Resources: Mathematics and Algorithms</a></li>\n<li><a href=\"#practice\">Practice</a></li>\n<li><a href=\"#python\">Python</a></li>\n<li><a href=\"#haskell\">Haskell</a></li>\n<li><a href=\"#lisp\">Common Lisp</a></li>\n<li><a href=\"#r\">R</a></li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h3 id=\"How_Computers_Work\"><strong><a name=\"basics\">How Computers Work</a></strong></h3>\n<p>1. <a href=\"http://www.charlespetzold.com/code/\">CODE The Hidden Language of Computer Hardware and Software</a></p>\n<blockquote>\n<p>The book intends to show a layman the basic mechanical principles of how computers work, instead of merely summarizing how the different parts relate. He starts with basic principles of language and logic and then demonstrates how they can be embodied by electrical circuits, and these principles give him an opening to describe in principle how computers work mechanically without requiring very much technical knowledge. Although it is not possible in a medium sized book for layman to describe the entire technical summary of a computer, he describes how and why it is possible that elaborate electronics can act in the ways computers do. In the introduction, he contrasts his own work with those books which \"include pictures of trains full of 1s and 0s.\"</p>\n</blockquote>\n<p>2. <a href=\"http://www1.idc.ac.il/tecs/\">The Elements of Computing Systems: Building a Modern Computer from First Principles</a></p>\n<blockquote>\n<p>Indeed, the best way to understand how computers work is to build one from scratch, and this textbook leads students through twelve chapters and projects that gradually build a basic hardware platform and a modern software hierarchy from the ground up. In the process, the students gain hands-on knowledge of hardware architecture, operating systems, programming languages, compilers, data structures, algorithms, and software engineering. Using this constructive approach, the book exposes a significant body of computer science knowledge and demonstrates how theoretical and applied techniques taught in other courses fit into the overall picture.</p>\n</blockquote>\n<p>3. <a href=\"http://homepage.mac.com/randyhyde/webster.cs.ucr.edu/www.writegreatcode.com/\">The Write Great Code Series (A Solid Foundation in Software Engineering for Programmers)</a></p>\n<p>Write Great Code Volume I: Understanding the Machine</p>\n<blockquote>\n<p>This, the first of four volumes, teaches important concepts of machine organization in a language-independent fashion, giving programmers what they need to know to write great code in any language, without the usual overhead of learning assembly language to master this topic. The Write Great Code series will help programmers make wiser choices with respect to programming statements and data types when writing software.</p>\n</blockquote>\n<p>Write Great Code Volume II: Thinking Low-Level, Writing High-Level</p>\n<blockquote>\n<p>...a good question to ask might be \"Is there some way to write high-level language code to help the compiler produce high-quality machine code?\" The answer to this question is \"yes\" and Write Great Code, Volume II, will teach you how to write such high-level code. This volume in the Write Great Code series describes how compilers translate statements into machine code so that you can choose appropriate high-level programming language statements to produce executable code that is almost as good as hand-optimized assembly code.</p>\n</blockquote>\n<p>4. <a href=\"http://www.amazon.com/Haskell-Logic-Maths-Programming-Computing/dp/0954300696/\">The Art of Assembly Language Programming</a></p>\n<blockquote>\n<p>Assembly is a low-level programming language that's one step above a computer's native machine language. Although assembly language is commonly used for writing device drivers, emulators, and video games, many programmers find its somewhat unfriendly syntax intimidating to learn and use.<br><br>Since 1996, Randall Hyde's The Art of Assembly Language has provided a comprehensive, plain-English, and patient introduction to assembly for non-assembly programmers. Hyde's primary teaching tool, High Level Assembler (or HLA), incorporates many of the features found in high-level languages (like C, C++, and Java) to help you quickly grasp basic assembly concepts. HLA lets you write true low-level code while enjoying the benefits of high-level language programming.</p>\n</blockquote>\n<p>5. <a href=\"http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming\">The Art of Computer Programming</a></p>\n<blockquote>\n<p>This work is not about computer programming in the narrow sense, but about the algorithms and methods which lie at the heart of most computer systems.</p>\n<p>At the end of 1999, these books were named among the best twelve physical-science monographs of the century by American Scientist, along with: Dirac on quantum mechanics, Einstein on relativity, Mandelbrot on fractals, Pauling on the chemical bond, Russell and Whitehead on foundations of mathematics, von Neumann and Morgenstern on game theory, Wiener on cybernetics, Woodward and Hoffmann on orbital symmetry, Feynman on quantum electrodynamics, Smith on the search for structure, and Einstein's collected papers.</p>\n</blockquote>\n<h3 id=\"An_Overview_of_Computer_Programming\"><strong><a name=\"programming\">An Overview of Computer Programming</a></strong></h3>\n<p>1. <a href=\"http://www.amazon.com/Seven-Languages-Weeks-Programming-Programmers/dp/193435659X\">Seven Languages in Seven Weeks: A Pragmatic Guide to Learning Programming Languages</a></p>\n<blockquote>\n<p>Ruby, Io, Prolog, Scala, Erlang, Clojure, Haskell. With Seven Languages in Seven Weeks, by Bruce A. Tate, you'll go beyond the syntax-and beyond the 20-minute tutorial you'll find someplace online. This book has an audacious goal: to present a meaningful exploration of seven languages within a single book. Rather than serve as a complete reference or installation guide, Seven Languages hits what's essential and unique about each language. Moreover, this approach will help teach you how to grok new languages.<br><br>For each language, you'll solve a nontrivial problem, using techniques that show off the language's most important features. As the book proceeds, you'll discover the strengths and weaknesses of the languages, while dissecting the process of learning languages quickly--for example, finding the typing and programming models, decision structures, and how you interact with them.</p>\n</blockquote>\n<p>2. <a href=\"http://www.amazon.com/Programming-Language-Pragmatics-Third-Michael/dp/0123745144/\">Programming Language Pragmatics</a></p>\n<blockquote>\n<p>The ubiquity of computers in everyday life in the 21st century justifies the centrality of programming languages to computer science education.&nbsp; Programming languages is the area that connects the theoretical foundations of computer science, the source of problem-solving algorithms, to modern computer architectures on which the corresponding programs produce solutions.&nbsp; Given the speed with which computing technology advances in this post-Internet era, a computing textbook must present a structure for organizing information about a subject, not just the facts of the subject itself.&nbsp; In this book, Michael Scott broadly and comprehensively presents the key concepts of programming languages and their implementation, in a manner appropriate for computer science majors.&nbsp;</p>\n</blockquote>\n<p>3. <a href=\"http://www.amazon.com/Introduction-Functional-Programming-Calculus-Mathematics/dp/0486478831/\">An Introduction to Functional Programming Through Lambda Calculus</a></p>\n<blockquote>\n<p>This well-respected text offers an accessible introduction to functional programming concepts and techniques for students of mathematics and computer science. The treatment is as nontechnical as possible, assuming no prior knowledge of mathematics or functional programming. Numerous exercises appear throughout the text, and all problems feature complete solutions.</p>\n</blockquote>\n<p>4. <a href=\"http://www.htdp.org/\">How to Design Programs</a> (An Introduction to Computing and Programming)</p>\n<blockquote>\n<p>This introduction to programming places computer science in the core of a liberal arts education. Unlike other introductory books, it focuses on the program design process. This approach fosters a variety of skills--critical reading, analytical thinking, creative synthesis, and attention to detail--that are important for everyone, not just future computer programmers.The book exposes readers to two fundamentally new ideas. First, it presents program design guidelines that show the reader how to analyze a problem statement; how to formulate concise goals; how to make up examples; how to develop an outline of the solution, based on the analysis; how to finish the program; and how to test. Each step produces a well-defined intermediate product. Second, the book comes with a novel programming environment, the first one explicitly designed for beginners.</p>\n</blockquote>\n<p>5. <a href=\"http://mitpress.mit.edu/sicp/\">Structure and Interpretation of Computer Programs</a></p>\n<blockquote>\n<p>Using a dialect of the Lisp programming language known as Scheme, the book explains core computer science concepts, including abstraction, recursion, interpreters and metalinguistic abstraction, and teaches modular programming.<br><br>The program also introduces a practical implementation of the register machine concept, defining and developing an assembler for such a construct, which is used as a virtual machine for the implementation of interpreters and compilers in the book, and as a testbed for illustrating the implementation and effect of modifications to the evaluation mechanism. Working Scheme systems based on the design described in this book are quite common student projects.</p>\n</blockquote>\n<h3 id=\"Computer_Science_and_Computation\"><strong><a name=\"cs\">Computer Science and Computation</a><br></strong></h3>\n<p>1. <a href=\"http://www.theannotatedturing.com/\">The Annotated Turing: A Guided Tour Through Alan Turing's Historic Paper on Computability and the Turing Machine</a></p>\n<blockquote>\n<p>Mathematician Alan Turing invented an imaginary computer known as the Turing Machine; in an age before computers, he explored the concept of what it meant to be computable, creating the field of computability theory in the process, a foundation of present-day computer programming.<br><br>The book expands Turing\u2019s original 36-page paper with additional background chapters and extensive annotations; the author elaborates on and clarifies many of Turing\u2019s statements, making the original difficult-to-read document accessible to present day programmers, computer science majors, math geeks, and others.</p>\n</blockquote>\n<p>2. <a href=\"http://www.amazon.com/New-Turing-Omnibus-Turning-Excursions/dp/0716782715/ref=la_B000APOGWI_1_2?ie=UTF8&amp;qid=1338291432&amp;sr=1-2\">New Turing Omnibus</a> (New Turning Omnibus : 66 Excursions in Computer Science)</p>\n<blockquote>\n<p>This text provides a broad introduction to the realm of computers. Updated and expanded, \"The New Turing Omnibus\" offers 66 concise articles on the major points of interest in computer science theory, technology and applications. New for this edition are: updated information on algorithms, detecting primes, noncomputable functions, and self-replicating computers - plus completely new sections on the Mandelbrot set, genetic algorithms, the Newton-Raphson Method, neural networks that learn, DOS systems for personal computers, and computer viruses.</p>\n</blockquote>\n<p>3. <a href=\"http://www.udacity.com/\">Udacity</a></p>\n<blockquote>\n<p>Udacity is a private educational organization founded by Sebastian Thrun, David Stavens, and Mike Sokolsky, with the stated goal of democratizing education<br><br>It is the outgrowth of free computer science classes offered in 2011 through Stanford University. As of May 2012 Udacity has six active courses.<br><br>The first two courses ever launched on Udacity both started on 20th February, 2012, entitled \"CS 101: Building a Search Engine\", taught by Dave Evans, from the University of Virginia, and \"CS 373: Programming a Robotic Car\" taught by Thrun. Both courses use Python.</p>\n</blockquote>\n<p>4. <a href=\"https://www.ai-class.com/\">Introduction to Artificial Intelligence</a></p>\n<blockquote>\n<p>A bold experiment in distributed education, \"Introduction to Artificial Intelligence\" will be offered free and online to students worldwide from October 10th to December 18th 2011. The course will include feedback on progress and a statement of accomplishment. Taught by Sebastian Thrun and Peter Norvig, the curriculum draws from that used in Stanford's introductory Artificial Intelligence course. The instructors will offer similar materials, assignments, and exams.<br><br>Artificial Intelligence is the science of making computer software that reasons about the world around it. Humanoid robots, Google Goggles, self-driving cars, even software that suggests music you might like to hear are all examples of AI. In this class, you will learn how to create this software from two of the leaders in the field. Class begins October 10.</p>\n</blockquote>\n<h3 id=\"Supplementary_Resources__Mathematics_and_Algorithms\"><strong><a name=\"resources\">Supplementary Resources: Mathematics and Algorithms</a></strong></h3>\n<p>1. <a href=\"http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025\">Concrete Mathematics: A Foundation for Computer Science</a></p>\n<blockquote>\n<p>This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline.</p>\n</blockquote>\n<p>2. <a href=\"http://algs4.cs.princeton.edu/home/\">Algorithms</a></p>\n<blockquote>\n<p>The textbook Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne surveys the most important algorithms and data structures in use today.</p>\n</blockquote>\n<p>3. <a href=\"http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/\">Introduction to Algorithms</a></p>\n<blockquote>\n<p>Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. The book covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers. Each chapter is relatively self-contained and can be used as a unit of study. The algorithms are described in English and in a pseudocode designed to be readable by anyone who has done a little programming. The explanations have been kept elementary without sacrificing depth of coverage or mathematical rigor.</p>\n</blockquote>\n<h3 id=\"Practice\"><a name=\"practice\">Practice</a></h3>\n<p>1. <a href=\"http://projecteuler.net/\">Project Euler</a></p>\n<blockquote>\n<p>Project Euler is a series of challenging mathematical/computer programming problems that will require more than just mathematical insights to solve. Although mathematics will help you arrive at elegant and efficient methods, the use of a computer and programming skills will be required to solve most problems.</p>\n</blockquote>\n<p>2. <a href=\"http://www.pythonchallenge.com/\">The Python Challenge</a></p>\n<blockquote>\n<p>Python Challenge is a game in which each level can be solved by a bit of (Python) programming.</p>\n</blockquote>\n<p>3. <a href=\"http://www.codechef.com\">CodeChef Programming Competition</a></p>\n<blockquote>\n<p>CodeChef is a global programming community. We host contests, trainings and events for programmers around the world. Our goal is to provide a platform for programmers everywhere to meet, compete, and have fun.</p>\n</blockquote>\n<p>4. Write your own programs.</p>\n<h3 id=\"Python\"><a name=\"python\">Python</a></h3>\n<p><a href=\"http://code.google.com/p/pyscripter/\">pyscripter</a></p>\n<p>An open-source Python Integrated Development Environment (IDE)</p>\n<p><a href=\"http://www.khanacademy.org/#computer-science\">Khan Academy</a></p>\n<p>Introduction to programming and computer science (using Python)</p>\n<p>1. <a href=\"http://inventwithpython.com/\">Invent Your Own Computer Games with Python</a></p>\n<blockquote>\n<p>\u201cInvent Your Own Computer Games with Python\u201d is a free book (as in, open source) and a free eBook (as in, no cost to download) that teaches you how to program in the Python programming language. Each chapter gives you the complete source code for a new game, and then teaches the programming concepts from the example.<br><br>\u201cInvent with Python\u201d was written to be understandable by kids as young as 10 to 12 years old, although it is great for anyone of any age who has never programmed before.</p>\n</blockquote>\n<p>2. <a href=\"http://learnpythonthehardway.org/\">Learn Python The Hard Way</a></p>\n<blockquote>\n<p>Have you always wanted to learn how to code but never thought you could? Are you looking to build a foundation for more complex coding? Do you want to challenge your brain in a new way? Then Learn Python the Hard Way is the book for you.</p>\n</blockquote>\n<p>3. <a href=\"http://www.greenteapress.com/thinkpython/\">Python for Software Design: How to Think Like a Computer Scientist</a></p>\n<blockquote>\n<p>Think Python is an introduction to Python programming for beginners. It starts with basic concepts of programming, and is carefully designed to define all terms when they are first used and to develop each new concept in a logical progression. Larger pieces, like recursion and object-oriented programming are divided into a sequence of smaller steps and introduced over the course of several chapters.</p>\n</blockquote>\n<p>4. <a href=\"http://www.amazon.com/Python-Programming-Introduction-Computer-Science/dp/1590282418/\">Python Programming: An Introduction to Computer Science</a></p>\n<blockquote>\n<p>This book is suitable for use in a university-level first course in computing (CS1), as well as the increasingly popular course known as CS0. It is difficult for many students to master basic concepts in computer science and programming. A large portion of the confusion can be blamed on the complexity of the tools and materials that are traditionally used to teach CS1 and CS2. This textbook was written with a single overarching goal: to present the core concepts of computer science as simply as possible without being simplistic.</p>\n</blockquote>\n<p>5. <a href=\"http://www.amazon.com/Practical-Programming-Introduction-Pragmatic-Programmers/dp/1934356271/\">Practical Programming: An Introduction to Computer Science Using Python</a></p>\n<blockquote>\n<p>Computers are used in every part of science from ecology to particle physics. This introduction to computer science continually reinforces those ties by using real-world science problems as examples. Anyone who has taken a high school science class will be able to follow along as the book introduces the basics of programming, then goes on to show readers how to work with databases, download data from the web automatically, build graphical interfaces, and most importantly, how to think like a professional programmer.</p>\n</blockquote>\n<p>6. <a href=\"http://www.amazon.com/Quick-Python-Book-Second/dp/193518220X/\">The Quick Python Book</a></p>\n<blockquote>\n<p>The Quick Python Book, Second Edition, is a clear, concise introduction to Python 3, aimed at programmers new to Python. This updated edition includes all the changes in Python 3, itself a significant shift from earlier versions of Python.</p>\n<p>The book begins with basic but useful programs that teach the core features of syntax, control flow, and data structures. It then moves to larger applications involving code management, object-oriented programming, web development, and converting code from earlier versions of Python.</p>\n</blockquote>\n<h3 id=\"Haskell\"><a name=\"haskell\">Haskell</a></h3>\n<p><a href=\"http://hackage.haskell.org/platform/\">The Haskell Platform</a></p>\n<p>The Haskell Platform is the easiest way to get started with programming Haskell. It comes with all you need to get up and running. Think of it as \"Haskell: batteries included\".</p>\n<p>1. <a href=\"http://www.haskell.org/haskellwiki/Haskell_in_5_steps\">Haskell in 5 steps</a></p>\n<blockquote>\n<p>This page will help you get started as quickly as possible.</p>\n</blockquote>\n<p>2. <a href=\"http://www.haskell.org/haskellwiki/Learn_Haskell_in_10_minutes\">Learn Haskell in 10 minutes</a></p>\n<p>3. <a href=\"http://www.haskell.org/haskellwiki/A_brief_introduction_to_Haskell\">A brief introduction to Haskell</a></p>\n<p>4. <a href=\"http://www.amazon.com/Programming-Haskell-Graham-Hutton/dp/0521692695/\">Programming in Haskell</a></p>\n<blockquote>\n<p>Haskell is one of the leading languages for teaching functional programming, enabling students to write simpler and cleaner code, and to learn how to structure and reason about programs. This introduction is ideal for beginners: it requires no previous programming experience and all concepts are explained from first principles via carefully chosen examples. Each chapter includes exercises that range from the straightforward to extended projects, plus suggestions for further reading on more advanced topics. The author is a leading Haskell researcher and instructor, well-known for his teaching skills. The presentation is clear and simple, and benefits from having been refined and class-tested over several years. The result is a text that can be used with courses, or for self-learning. Features include freely accessible Powerpoint slides for each chapter, solutions to exercises and examination questions (with solutions) available to instructors, and a downloadable code that's fully compliant with the latest Haskell release.</p>\n</blockquote>\n<p>5. <a href=\"http://learnyouahaskell.com/\">Learn You a Haskell for Great Good!</a></p>\n<blockquote>\n<p>Learn You a Haskell, the funkiest way to learn Haskell, which is the best functional programming language around. You may have heard of it. This guide is meant for people who have programmed already, but have yet to try functional programming.</p>\n</blockquote>\n<p>6. <a href=\"http://book.realworldhaskell.org/\">Real World Haskell</a></p>\n<blockquote>\n<p>This easy-to-use, fast-moving tutorial introduces you to functional programming with Haskell. You'll learn how to use Haskell in a variety of practical ways, from short scripts to large and demanding applications. Real World Haskell takes you through the basics of functional programming at a brisk pace, and then helps you increase your understanding of Haskell in real-world issues like I/O, performance, dealing with data, concurrency, and more as you move through each chapter.</p>\n</blockquote>\n<p>7. <a href=\"http://fldit-www.cs.uni-dortmund.de/~peter/PS07/HR.pdf\">The Haskell Road to Logic, Maths and Programming</a></p>\n<blockquote>\n<p>The textbook by Doets and van Eijck puts the Haskell programming language systematically to work for presenting a major piece of logic and mathematics. The reader is taken through chapters on basic logic, proof recipes, sets and lists, relations and functions, recursion and co-recursion, the number systems, polynomials and power series, ending with Cantor's infinities. The book uses Haskell for the executable and strongly typed manifestation of various mathematical notions at the level of declarative programming. The book adopts a systematic but relaxed mathematical style (definition, example, exercise, ...); the text is very pleasant to read due to a small amount of anecdotal information, and due to the fact that definitions are fluently integrated in the running text. An important goal of the book is to get the reader acquainted with reasoning about programs.&nbsp;</p>\n</blockquote>\n<h3 id=\"Common_Lisp\"><a name=\"lisp\">Common Lisp</a></h3>\n<p>1. <a href=\"http://www.amazon.com/Land-Lisp-Learn-Program-Game/dp/1593272812/\">Land of Lisp: Learn to Program in Lisp, One Game at a Time!</a></p>\n<blockquote>\n<p>Lisp has been hailed as the world's most powerful programming language, but its cryptic syntax and academic reputation can be enough to scare off even experienced programmers. Those dark days are finally over\u2014<em>Land of Lisp</em> brings the power of functional programming to the people!</p>\n<p>With his brilliantly quirky comics and out-of-this-world games, longtime Lisper Conrad Barski teaches you the mysteries of Common Lisp. You'll start with the basics, like list manipulation, I/O, and recursion, then move on to more complex topics like macros, higher order programming, and domain-specific languages. Then, when your brain overheats, you can kick back with an action-packed comic book interlude!</p>\n</blockquote>\n<p>2. <a href=\"http://www.gigamonkeys.com/book/\">Practical Common Lisp</a></p>\n<blockquote>\n<p><em>Practical Common Lisp</em> presents a thorough introduction to Common Lisp, providing you with an overall understanding of the language features and how they work. Over a third of the book is devoted to practical examples such as the core of a spam filter and a web application for browsing MP3s and streaming them via the Shoutcast protocol to any standard MP3 client software (e.g., iTunes, XMMS, or WinAmp). In other \"practical\" chapters, author Peter Seibel demonstrates how to build a simple but flexible in-memory database, how to parse binary files, and how to build a unit test framework in 26 lines of code.</p>\n</blockquote>\n<p>3. <a href=\"http://www.amazon.com/ANSI-Common-LISP-Paul-Graham/dp/0133708756/\">ANSI Common LISP</a></p>\n<blockquote>\n<p>Teaching users new and more powerful ways of thinking about programs, this two-in-one text contains a tutorial\u2014full of examples\u2014that explains all the essential concepts of Lisp programming, <em>plus</em> an up-to-date summary of ANSI Common Lisp, listing every operator in the language. Informative and fun, it gives users everything they need to start writing programs in Lisp both efficiently and effectively, and highlights such innovative Lisp features as automatic memory management, manifest typing, closures, and more. Dividing material into two parts, the tutorial half of the book covers subject-by-subject the essential core of Common Lisp, and sums up lessons of preceding chapters in two examples of real applications: a backward-chainer, and an embedded language for object-oriented programming. Consisting of three appendices, the summary half of the book gives source code for a selection of widely used Common Lisp operators, with definitions that offer a comprehensive explanation of the language and provide a rich source of real examples; summarizes some differences between ANSI Common Lisp and Common Lisp as it was originally defined in 1984; and contains a concise description of every function, macro, and special operator in ANSI Common Lisp. The book concludes with a section of notes containing clarifications, references, and additional code.</p>\n</blockquote>\n<p>4. <a href=\"http://www.amazon.com/Paradigms-Artificial-Intelligence-Programming-Studies/dp/1558601910/\">Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp</a></p>\n<blockquote>\n<p><em>Paradigms of AI Programming</em> is the first text to teach advanced Common Lisp techniques in the context of building major AI systems. By reconstructing authentic, complex AI programs using state-of-the-art Common Lisp, the book teaches students and professionals how to build and debug robust practical programs, while demonstrating superior programming style and important AI concepts. The author strongly emphasizes the practical performance issues involved in writing real working programs of significant size. Chapters on troubleshooting and efficiency are included, along with a discussion of the fundamentals of object-oriented programming and a description of the main CLOS functions. This volume is an excellent text for a course on AI programming, a useful supplement for general AI courses and an indispensable reference for the professional programmer.</p>\n</blockquote>\n<p>5. <a href=\"http://letoverlambda.com/\">Let Over Lambda</a></p>\n<blockquote>\n<p>Let Over Lambda is one of the most hardcore computer programming books out there. Starting with the fundamentals, it describes the most advanced features of the most advanced language: COMMON LISP. The point of this book is to expose you to ideas that you might otherwise never be exposed to.</p>\n</blockquote>\n<p>6. <a href=\"http://www.michaelnielsen.org/ddi/lisp-as-the-maxwells-equations-of-software/\">Lisp as the Maxwell\u2019s equations of software</a></p>\n<blockquote>\n<p>These are Maxwell\u2019s equations. Just four compact equations. With a little work it\u2019s easy to understand the basic elements of the equations \u2013 what all the symbols mean, how we can compute all the relevant quantities, and so on. But while it\u2019s easy to understand the elements of the equations, understanding all their consequences is another matter. Inside these equations is all of electromagnetism \u2013 everything from antennas to motors to circuits. If you think you understand the consequences of these four equations, then you may leave the room now, and you can come back and ace the exam at the end of semester.</p>\n</blockquote>\n<h3 id=\"R\"><a name=\"r\">R</a></h3>\n<p><a href=\"http://rstudio.org/\">RStudio</a></p>\n<p>RStudio\u2122 is a free and open source integrated development environment (IDE) for R. You can run it on your desktop (Windows, Mac, or Linux) or even over the web using RStudio Server.</p>\n<p>1. <a href=\"http://www.stat.berkeley.edu/share/rvideos/R_Videos/R_Videos.html\">R Videos</a></p>\n<p>2. <a href=\"http://www.twotorials.com/\">R Tutorials</a></p>\n<p>3. <a href=\"http://pairach.com/2012/02/26/r-tutorials-from-universities-around-the-world/\">R Tutorials from Universities Around the World</a></p>\n<blockquote>\n<p>Here is a list of FREE R tutorials hosted in official website of universities around the world.</p>\n</blockquote>\n<p>4. <a href=\"http://www.r-bloggers.com/\">R-bloggers</a></p>\n<blockquote>\n<p>Here you will find daily news and tutorials about R, contributed by over 300 bloggers.</p>\n</blockquote>\n<p>5. <a href=\"http://nostarch.com/artofr.htm\">The Art of R Programming: A Tour of Statistical Software Design</a></p>\n<blockquote>\n<p>R is the world's most popular language for developing statistical software: Archaeologists use it to track the spread of ancient civilizations, drug companies use it to discover which medications are safe and effective, and actuaries use it to assess financial risks and keep economies running smoothly.<br><br>The Art of R Programming takes you on a guided tour of software development with R, from basic types and data structures to advanced topics like closures, recursion, and anonymous functions. No statistical knowledge is required, and your programming skills can range from hobbyist to pro.<br><br>Along the way, you'll learn about functional and object-oriented programming, running mathematical simulations, and rearranging complex data into simpler, more useful formats.</p>\n</blockquote>\n<p>6. <a href=\"http://pluto.huji.ac.il/~msby/StatThink/index.html\">Introduction to Statistical Thinking (With R, Without Calculus)</a></p>\n<blockquote>\n<p>The target audience for this book is college students who are required to learn statistics, students with little background in mathematics and often no motivation to learn more.</p>\n</blockquote>\n<p>7. <a href=\"http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/\">Doing Bayesian Data Analysis: A Tutorial with R and BUGS </a></p>\n<blockquote>\n<p>There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis, A Tutorial Introduction with R and BUGS provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. The text delivers comprehensive coverage of all scenarios addressed by non-Bayesian textbooks--t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis).<br><br>This book is intended for first year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Prerequisite is knowledge of algebra and basic calculus. Free software now includes programs in JAGS, which runs on Macintosh, Linux, and Windows.</p>\n</blockquote>", "sections": [{"title": "Contents", "anchor": "Contents", "level": 2}, {"title": "How Computers Work", "anchor": "How_Computers_Work", "level": 1}, {"title": "An Overview of Computer Programming", "anchor": "An_Overview_of_Computer_Programming", "level": 1}, {"title": "Computer Science and Computation", "anchor": "Computer_Science_and_Computation", "level": 1}, {"title": "Supplementary Resources: Mathematics and Algorithms", "anchor": "Supplementary_Resources__Mathematics_and_Algorithms", "level": 1}, {"title": "Practice", "anchor": "Practice", "level": 1}, {"title": "Python", "anchor": "Python", "level": 1}, {"title": "Haskell", "anchor": "Haskell", "level": 1}, {"title": "Common Lisp", "anchor": "Common_Lisp", "level": 1}, {"title": "R", "anchor": "R", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T15:41:37.964Z", "modifiedAt": null, "url": null, "title": "Problematic Problems for TDT", "slug": "problematic-problems-for-tdt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:03.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drnickbone", "createdAt": "2012-01-20T17:19:55.216Z", "isAdmin": false, "displayName": "drnickbone"}, "userId": "GgwHTM3agaskLi9cx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3GyQXTy2WhYcaBgS2/problematic-problems-for-tdt", "pageUrlRelative": "/posts/3GyQXTy2WhYcaBgS2/problematic-problems-for-tdt", "linkUrl": "https://www.lesswrong.com/posts/3GyQXTy2WhYcaBgS2/problematic-problems-for-tdt", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Problematic%20Problems%20for%20TDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProblematic%20Problems%20for%20TDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3GyQXTy2WhYcaBgS2%2Fproblematic-problems-for-tdt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Problematic%20Problems%20for%20TDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3GyQXTy2WhYcaBgS2%2Fproblematic-problems-for-tdt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3GyQXTy2WhYcaBgS2%2Fproblematic-problems-for-tdt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1069, "htmlBody": "<p>A key goal of Less Wrong's \"advanced\" <a title=\"decision theories\" href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theories</a> (like <a title=\"TDT\" href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>, <a title=\"UDT\" href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a> and <a title=\"ADT\" href=\"http://wiki.lesswrong.com/wiki/Ambient_decision_theory\">ADT</a>) is that they should out-perform&nbsp;standard decision theories (such as&nbsp;<a title=\"CDT\" href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\">CDT</a>) in contexts where another agent has access to the decider's code, or can otherwise predict the decider's behaviour. In particular, agents who run these theories will one-box on Newcomb's problem, and so generally make more money than&nbsp;agents which two-box. Slightly surprisingly, they may well continue to one-box even if the boxes are transparent, and even if the predictor Omega makes occasional errors (a problem due to <a title=\"Good and Real\" href=\"http://mitpress.mit.edu/catalog/item/default.asp?tid=10902&amp;ttype=2\">Gary Drescher</a>, which <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">Eliezer&nbsp;has described </a>as equivalent&nbsp;to \"<a title=\"counterfactual mugging\" href=\"/lw/3l/counterfactual_mugging/\">counterfactual mugging</a>\"). M<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">ore generally, these agents&nbsp;behave like a CDT agent will wish it had pre-committed itself to behaving before being faced with the problem.</span></p>\n<p>However, I've recently thought of a class of Omega problems where TDT (and related theories) appears to under-perform compared to CDT. Importantly, these are problems which are \"fair\" - at least as fair as the original Newcomb problem - because the reward is a function of the agent's actual choices in the problem (namely which box or boxes get picked) and independent of the method that the agent uses to choose, or of its choices on any other problems. This contrasts with clearly \"unfair\" problems like the following:</p>\n<p><strong>Discrimination</strong>: Omega presents the usual two boxes. Box A always contains $1000. Box B contains nothing if Omega detects that the agent is running TDT; otherwise it contains $1 million.</p>\n<p>&nbsp;</p>\n<p>So what are some&nbsp;<em>fair</em> \"problematic problems\"?</p>\n<p><strong>Problem 1</strong>: Omega (who experience has shown is always truthful) presents the usual two boxes A&nbsp;and B and announces the following. \"Before you entered the room, I ran a simulation of this problem as presented to an agent running TDT. I won't tell you what the agent decided, but I will tell you that if the agent two-boxed then I put nothing in Box B, whereas if the agent one-boxed then I put $1 million in Box B. Regardless of how the simulated agent decided, I put $1000 in Box A. Now please choose your box or boxes.\"</p>\n<p><strong><em>Analysis</em></strong>: Any agent who is themselves running TDT will reason as in the<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">&nbsp;standard Newcomb problem. They'll prove that their decision is linked to the simulated agent's, so that if they two-box they'll only win $1000, whereas if they one-box they will win $1 million. So the agent will choose to one-box and win $1 million.</span></p>\n<p>However, any CDT agent can just take both boxes and win $1001000. In fact, any other agent who is <em>not</em> running TDT (e.g. an <a title=\"EDT\" href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">EDT</a> agent) will be able to re-construct the chain of logic and reason that the simulation one-boxed and so box B contains the $1 million. So any other agent can safely two-box as well.&nbsp;</p>\n<p>Note that we&nbsp;<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">can modify the contents of Box A so that it contains anything up to $1 million; the CDT agent (or EDT agent) can in principle win up to twice as much as the TDT agent.</span></p>\n<p>&nbsp;</p>\n<p><strong>Problem 2</strong>: Our ever-reliable Omega now presents ten boxes, numbered from 1 to 10, and announces the following. \"Exactly one of these boxes contains $1 million; the others contain nothing. You must take exactly one box to win the money; if you try to take more than one, then you won't be allowed to keep any winnings. Before you entered the room, I ran multiple simulations of this problem as presented to an agent running TDT, and determined the box which the agent was least likely to take. If there were several such boxes tied for equal-lowest probability, then I just selected one of them, the one labelled with the smallest number. I then placed $1 million in the selected box. Please choose your box.\"</p>\n<p><strong><em>Analysis</em></strong>: A TDT agent will reason that whatever it does, it cannot have more than 10% chance of winning the $1 million. In fact, the TDT agent's best reply is to pick each box with equal probability; after Omega calculates this, it will place the $1 million under box number 1 and the TDT agent has exactly 10% chance of winning it. <br />&nbsp;<br />But any non-TDT agent (e.g. CDT or EDT) can reason this through as well, and just pick box number 1, so winning $1 million.&nbsp;<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">By increasing the number of boxes, we can ensure that TDT has arbitrarily low chance of winning, compared to CDT which always wins.</span></p>\n<p><br /><em><strong>Some questions:</strong></em></p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">1. Have these or similar problems already been discovered by TDT (or UDT) theorists, and if so, is there a known solution? I had a search on Less Wrong but couldn't find anything obviously like them.</span></p>\n<p>2.&nbsp;<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">Is the analysis correct, or is there some subtle reason why a TDT (or UDT) agent would choose differently from described?</span></p>\n<p>3. If a TDT agent believed (or had reason to believe) that Omega was going to present it with such problems, then wouldn't it want to self-modify to CDT? But this seems paradoxical, since the whole idea of a TDT agent is that it doesn't have to self-modify.</p>\n<p>4. Might such problems show that there cannot be a single TDT algorithm (or family of provably-linked TDT algorithms) so that when Omega says it is simulating a TDT agent, it is quite ambiguous what it is doing? (This objection would go away if Omega revealed the source-code of its simulated agent, and the source-code of the choosing agent; each particular version of TDT would then be out-performed on a specific matching problem.)</p>\n<p>5. Are these really \"fair\" problems? Is there some intelligible sense in which they are not fair, but Newcomb's problem is fair? It certainly looks like Omega may be \"rewarding irrationality\" (i.e. giving greater gains to someone who runs an inferior decision theory), but that's exactly the argument that CDT theorists use about Newcomb.</p>\n<p>6. Finally, is it more likely that Omegas - or things like them - will present agents with Newcomb and Prisoner's Dilemma problems (on which TDT succeeds)&nbsp;rather than problematic problems (on which it fails)?</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> I tweaked the explanation of Box A's contents in Problem 1, since this was causing some confusion. The idea is that, as in the usual Newcomb problem, Box A always contains $1000. Note that Box B depends on what the simulated agent chooses; it doesn't depend on Omega predicting what the actual deciding agent chooses (so Omega doesn't put less money in any box just because it sees that the actual decider is running TDT).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3GyQXTy2WhYcaBgS2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 60, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "16310", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 301, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["c3wWnvgzdbRhNnNbQ", "mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T18:00:02.945Z", "modifiedAt": null, "url": null, "title": "Review: Selfish Reasons to Have More Kids", "slug": "review-selfish-reasons-to-have-more-kids", "viewCount": null, "lastCommentedAt": "2019-10-03T06:04:28.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nzsHQzsvwLw6g4pyE/review-selfish-reasons-to-have-more-kids", "pageUrlRelative": "/posts/nzsHQzsvwLw6g4pyE/review-selfish-reasons-to-have-more-kids", "linkUrl": "https://www.lesswrong.com/posts/nzsHQzsvwLw6g4pyE/review-selfish-reasons-to-have-more-kids", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Review%3A%20Selfish%20Reasons%20to%20Have%20More%20Kids&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReview%3A%20Selfish%20Reasons%20to%20Have%20More%20Kids%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnzsHQzsvwLw6g4pyE%2Freview-selfish-reasons-to-have-more-kids%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Review%3A%20Selfish%20Reasons%20to%20Have%20More%20Kids%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnzsHQzsvwLw6g4pyE%2Freview-selfish-reasons-to-have-more-kids", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnzsHQzsvwLw6g4pyE%2Freview-selfish-reasons-to-have-more-kids", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1135, "htmlBody": "<p>This is a review of Bryan Caplan&rsquo;s book <a href=\"http://www.amazon.com/Selfish-Reasons-Have-More-ebook/dp/B004OA64Q6/ref=kinw_dp_ke?ie=UTF8&amp;m=AG56TWVU5XWC2\">Selfish Reasons to Have More Kids</a>. Co-written with Walid.</p>\n<h1>Summary</h1>\n<p>Adoption studies indicate that differences in parenting styles have mostly small impacts on long term life outcomes of children, such as happiness, income, intelligence, health, etc.. This means that parents can put less effort into parenting without hurting their children&rsquo;s futures. If you think kids are neat, then you should consider having more.&nbsp;</p>\n<h1>Review</h1>\n<p><em>Note: We think this is a pretty useful book, and it has changed our minds on how many children we want to have, though neither one us has any children yet. Also, neither of us are experts on twin or adoption studies.</em></p>\n<p>Caplan argues that parents drastically overestimate their ability to improve the adult lives of their children. His argument is driven by adoption studies, which suggest that there is very little that parents can do beyond techniques employed by the average parent that would get them better results with their children. Specifically, the following areas are identified as areas where differences in parenting don&rsquo;t seem to matter:</p>\n<ul>\n<li>No effect on life expectancy, overall health (as measured by the presence/absence of particular health problems and self reported health), height, weight or dental health.</li>\n<li>No effect on intelligence.</li>\n<li>No effect on various measures of personality: conscientiousness, agreeableness or openness (not certain about extroversion or neuroticism).</li>\n<li>Little or no effect on marriage, marriage satisfaction, divorce, or child bearing.</li>\n</ul>\n<p>But that is not to say that styles outside of the average do not matter at all -- there are a few areas where parenting differences do seem to have an effect:</p>\n<ul>\n<li>A small effect on adult drinking, smoking and drug problems.</li>\n<li>A small effect on educational attainment, but no effect on grades in school or on income.</li>\n<li>A large effect on political and religious labels, such as whether you call yourself democrat or republican or Christian or Muslim but small effects on actual political and religious attitudes or behavior.</li>\n<li>A moderate effect on when girls start having sex (but not boys), but no effect on teen pregnancy or adult sexual behaviors.&nbsp;</li>\n<li>Possibly a small effect on sexual orientation.</li>\n<li>A moderate effect on how children remember and perceive their parents.</li>\n</ul>\n<p>So how do adoption studies lead to these conclusions?</p>\n<p><a href=\"http://encyclopedia.adoption.com/entry/adoption-studies/25/1.html\">Adoption studies</a> (If you have a link to a better overview or discussion of adoption studies, we'd appreciate it) help find out the influence of parenting differences on adult outcomes by comparing adoptees to their adopting family. If adoptees systematically tend to be more like their adopting family than like other adoptees along some measure (say religiosity or income), that implies that parenting differences affect that measure.</p>\n<p>When an adoption study finds that parenting does not affect outcome X, it does not mean that parenting cannot affect it, just that the parenting styles in the data set did not affect it.</p>\n<p>The evidence Caplan talks about is primarily long run life outcomes. Shorter run life outcomes often do show larger effects from parenting, but these effects diminish as the time horizon increases.</p>\n<h2>If parenting doesn&rsquo;t matter, what does?</h2>\n<p>Caplan references <a href=\"http://en.wikipedia.org/wiki/Twin_study\">twin studies</a> in showing that genetics have relatively big effects on all the measures previously mentioned. This explains why we see strong correlations between parents&rsquo; traits and children's&rsquo; traits. He specifically uses it to call out attributes that we would commonly ascribe to parenting, but may actually have a much larger genetic component.</p>\n<h2>Implications</h2>\n<p>Once Caplan has argued for the stylized fact that parenting has only small effects on major life outcomes, he explores some of its implications.&nbsp;</p>\n<h3>Don&rsquo;t be a tiger parent</h3>\n<p>One big implication is that you should put less effort into trying to make your kids into great adults and more effort into making your and your kids&rsquo; lives more fun right now.</p>\n<p>For example, parents probably spend too much energy convincing their children to eat their vegetables and learn the piano, given that it won&rsquo;t affect whether they will eat healthy as adults or be more intelligent. No one likes fighting. If you want your kid to learn the violin so they&rsquo;ll have fun right now, it may very well be worth it, but don&rsquo;t do it because you think it will increase their future income or intelligence. If neither you nor your child likes doing an activity, consider whether you can stop doing it.</p>\n<p>Adoption studies provide good evidence that most activities don&rsquo;t have a much of a long term effect on your children, so you need good evidence to start thinking that an activity will be good for your kids future. The odds are against it.</p>\n<h3>Have more kids</h3>\n<p>Focusing more on making your and your children&rsquo;s lives more fun means that overall, having kids should be more attractive. If having another kid no longer means fighting about finishing their broccoli every night, maybe it&rsquo;s not such a bad idea. On the margin, you should consider having more kids. If you were planning to have zero kids, consider having one. If you were planning to have 3 kids, consider 4, etc.</p>\n<h2>Other Topics</h2>\n<p>In much of the rest of the book Caplan gives common sense advice for making parenting easier for the parents. A couple of these, such as the Ferber method for dealing with infant sleep problems, are empirically based.</p>\n<p>Here are some other topics Caplan discusses in his book:</p>\n<ul>\n<li>Happiness research on parenting. Caplan argues that although being a parent seems to make people less happy, the effect is small (Ch 1).</li>\n<li>Child safety statistics. Children are many times safer than in decades past (Ch 4).</li>\n<li>Many of the benefits of having children come later in life (e.g. having people who will come and visit you, etc.), which makes it psychologically easy to ignore these benefits (Ch 5).</li>\n<li>The externalities of children. He argues that on net, extra people have large positive externalities (Ch 6), so you shouldn&rsquo;t feel guilty for having more children.</li>\n</ul>\n<h2>What parts should I read?</h2>\n<p>We wholeheartedly recommend reading the first 5 chapters (121 pages) of Selfish Reasons To Have More Kids as these have the most useful parts of the book; the rest of the book is less valuable.&nbsp;</p>\n<h2>Criticisms of Selfish Reason To Have More Kids</h2>\n<p>There are a number of criticisms relevant to Caplan&rsquo;s arugments. For example:</p>\n<ul>\n<li>Nisbett <a href=\"http://blogs.wsj.com/ideas-market/2011/04/15/differing-views-of-twin-studies/\">claims</a> that heredity is much less important for IQ than thought (see also counterclaims posted below).</li>\n<li>Will Wilkinson claims (<a href=\"http://blogs.wsj.com/ideas-market/2011/04/15/differing-views-of-twin-studies/\">one</a>, <a href=\"http://blogs.wsj.com/ideas-market/2011/04/15/differing-views-of-twin-studies/\">two</a>) that the cost of parenting plays a small role in people's family size decisions, thus it's not a very strong reason to have more kids.</li>\n<li>Jason Collins likes the book but would like it to discuss the research on non-shared environment (i.e. that not explained by genetic or parenting differences, such as peer effects) (<a href=\"http://www.gnxp.com/wp/2011/05/09/caplans-selfish-reasons-to-have-more-kids/\">link</a>). &nbsp;</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 1, "Q55STnFh6gbSezRuR": 1, "b7ZSAGimsbzrLR5CR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nzsHQzsvwLw6g4pyE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 33, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "16437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is a review of Bryan Caplan\u2019s book <a href=\"http://www.amazon.com/Selfish-Reasons-Have-More-ebook/dp/B004OA64Q6/ref=kinw_dp_ke?ie=UTF8&amp;m=AG56TWVU5XWC2\">Selfish Reasons to Have More Kids</a>. Co-written with Walid.</p>\n<h1 id=\"Summary\">Summary</h1>\n<p>Adoption studies indicate that differences in parenting styles have mostly small impacts on long term life outcomes of children, such as happiness, income, intelligence, health, etc.. This means that parents can put less effort into parenting without hurting their children\u2019s futures. If you think kids are neat, then you should consider having more.&nbsp;</p>\n<h1 id=\"Review\">Review</h1>\n<p><em>Note: We think this is a pretty useful book, and it has changed our minds on how many children we want to have, though neither one us has any children yet. Also, neither of us are experts on twin or adoption studies.</em></p>\n<p>Caplan argues that parents drastically overestimate their ability to improve the adult lives of their children. His argument is driven by adoption studies, which suggest that there is very little that parents can do beyond techniques employed by the average parent that would get them better results with their children. Specifically, the following areas are identified as areas where differences in parenting don\u2019t seem to matter:</p>\n<ul>\n<li>No effect on life expectancy, overall health (as measured by the presence/absence of particular health problems and self reported health), height, weight or dental health.</li>\n<li>No effect on intelligence.</li>\n<li>No effect on various measures of personality: conscientiousness, agreeableness or openness (not certain about extroversion or neuroticism).</li>\n<li>Little or no effect on marriage, marriage satisfaction, divorce, or child bearing.</li>\n</ul>\n<p>But that is not to say that styles outside of the average do not matter at all -- there are a few areas where parenting differences do seem to have an effect:</p>\n<ul>\n<li>A small effect on adult drinking, smoking and drug problems.</li>\n<li>A small effect on educational attainment, but no effect on grades in school or on income.</li>\n<li>A large effect on political and religious labels, such as whether you call yourself democrat or republican or Christian or Muslim but small effects on actual political and religious attitudes or behavior.</li>\n<li>A moderate effect on when girls start having sex (but not boys), but no effect on teen pregnancy or adult sexual behaviors.&nbsp;</li>\n<li>Possibly a small effect on sexual orientation.</li>\n<li>A moderate effect on how children remember and perceive their parents.</li>\n</ul>\n<p>So how do adoption studies lead to these conclusions?</p>\n<p><a href=\"http://encyclopedia.adoption.com/entry/adoption-studies/25/1.html\">Adoption studies</a> (If you have a link to a better overview or discussion of adoption studies, we'd appreciate it) help find out the influence of parenting differences on adult outcomes by comparing adoptees to their adopting family. If adoptees systematically tend to be more like their adopting family than like other adoptees along some measure (say religiosity or income), that implies that parenting differences affect that measure.</p>\n<p>When an adoption study finds that parenting does not affect outcome X, it does not mean that parenting cannot affect it, just that the parenting styles in the data set did not affect it.</p>\n<p>The evidence Caplan talks about is primarily long run life outcomes. Shorter run life outcomes often do show larger effects from parenting, but these effects diminish as the time horizon increases.</p>\n<h2 id=\"If_parenting_doesn_t_matter__what_does_\">If parenting doesn\u2019t matter, what does?</h2>\n<p>Caplan references <a href=\"http://en.wikipedia.org/wiki/Twin_study\">twin studies</a> in showing that genetics have relatively big effects on all the measures previously mentioned. This explains why we see strong correlations between parents\u2019 traits and children's\u2019 traits. He specifically uses it to call out attributes that we would commonly ascribe to parenting, but may actually have a much larger genetic component.</p>\n<h2 id=\"Implications\">Implications</h2>\n<p>Once Caplan has argued for the stylized fact that parenting has only small effects on major life outcomes, he explores some of its implications.&nbsp;</p>\n<h3 id=\"Don_t_be_a_tiger_parent\">Don\u2019t be a tiger parent</h3>\n<p>One big implication is that you should put less effort into trying to make your kids into great adults and more effort into making your and your kids\u2019 lives more fun right now.</p>\n<p>For example, parents probably spend too much energy convincing their children to eat their vegetables and learn the piano, given that it won\u2019t affect whether they will eat healthy as adults or be more intelligent. No one likes fighting. If you want your kid to learn the violin so they\u2019ll have fun right now, it may very well be worth it, but don\u2019t do it because you think it will increase their future income or intelligence. If neither you nor your child likes doing an activity, consider whether you can stop doing it.</p>\n<p>Adoption studies provide good evidence that most activities don\u2019t have a much of a long term effect on your children, so you need good evidence to start thinking that an activity will be good for your kids future. The odds are against it.</p>\n<h3 id=\"Have_more_kids\">Have more kids</h3>\n<p>Focusing more on making your and your children\u2019s lives more fun means that overall, having kids should be more attractive. If having another kid no longer means fighting about finishing their broccoli every night, maybe it\u2019s not such a bad idea. On the margin, you should consider having more kids. If you were planning to have zero kids, consider having one. If you were planning to have 3 kids, consider 4, etc.</p>\n<h2 id=\"Other_Topics\">Other Topics</h2>\n<p>In much of the rest of the book Caplan gives common sense advice for making parenting easier for the parents. A couple of these, such as the Ferber method for dealing with infant sleep problems, are empirically based.</p>\n<p>Here are some other topics Caplan discusses in his book:</p>\n<ul>\n<li>Happiness research on parenting. Caplan argues that although being a parent seems to make people less happy, the effect is small (Ch 1).</li>\n<li>Child safety statistics. Children are many times safer than in decades past (Ch 4).</li>\n<li>Many of the benefits of having children come later in life (e.g. having people who will come and visit you, etc.), which makes it psychologically easy to ignore these benefits (Ch 5).</li>\n<li>The externalities of children. He argues that on net, extra people have large positive externalities (Ch 6), so you shouldn\u2019t feel guilty for having more children.</li>\n</ul>\n<h2 id=\"What_parts_should_I_read_\">What parts should I read?</h2>\n<p>We wholeheartedly recommend reading the first 5 chapters (121 pages) of Selfish Reasons To Have More Kids as these have the most useful parts of the book; the rest of the book is less valuable.&nbsp;</p>\n<h2 id=\"Criticisms_of_Selfish_Reason_To_Have_More_Kids\">Criticisms of Selfish Reason To Have More Kids</h2>\n<p>There are a number of criticisms relevant to Caplan\u2019s arugments. For example:</p>\n<ul>\n<li>Nisbett <a href=\"http://blogs.wsj.com/ideas-market/2011/04/15/differing-views-of-twin-studies/\">claims</a> that heredity is much less important for IQ than thought (see also counterclaims posted below).</li>\n<li>Will Wilkinson claims (<a href=\"http://blogs.wsj.com/ideas-market/2011/04/15/differing-views-of-twin-studies/\">one</a>, <a href=\"http://blogs.wsj.com/ideas-market/2011/04/15/differing-views-of-twin-studies/\">two</a>) that the cost of parenting plays a small role in people's family size decisions, thus it's not a very strong reason to have more kids.</li>\n<li>Jason Collins likes the book but would like it to discuss the research on non-shared environment (i.e. that not explained by genetic or parenting differences, such as peer effects) (<a href=\"http://www.gnxp.com/wp/2011/05/09/caplans-selfish-reasons-to-have-more-kids/\">link</a>). &nbsp;</li>\n</ul>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Review", "anchor": "Review", "level": 1}, {"title": "If parenting doesn\u2019t matter, what does?", "anchor": "If_parenting_doesn_t_matter__what_does_", "level": 2}, {"title": "Implications", "anchor": "Implications", "level": 2}, {"title": "Don\u2019t be a tiger parent", "anchor": "Don_t_be_a_tiger_parent", "level": 3}, {"title": "Have more kids", "anchor": "Have_more_kids", "level": 3}, {"title": "Other Topics", "anchor": "Other_Topics", "level": 2}, {"title": "What parts should I read?", "anchor": "What_parts_should_I_read_", "level": 2}, {"title": "Criticisms of Selfish Reason To Have More Kids", "anchor": "Criticisms_of_Selfish_Reason_To_Have_More_Kids", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "256 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 260, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-29T20:40:00.077Z", "modifiedAt": null, "url": null, "title": "Mory Buckman: a case study of modeling yourself as multiple agents", "slug": "mory-buckman-a-case-study-of-modeling-yourself-as-multiple", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.669Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5BycMQfF6xHggydDh/mory-buckman-a-case-study-of-modeling-yourself-as-multiple", "pageUrlRelative": "/posts/5BycMQfF6xHggydDh/mory-buckman-a-case-study-of-modeling-yourself-as-multiple", "linkUrl": "https://www.lesswrong.com/posts/5BycMQfF6xHggydDh/mory-buckman-a-case-study-of-modeling-yourself-as-multiple", "postedAtFormatted": "Tuesday, May 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mory%20Buckman%3A%20a%20case%20study%20of%20modeling%20yourself%20as%20multiple%20agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMory%20Buckman%3A%20a%20case%20study%20of%20modeling%20yourself%20as%20multiple%20agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BycMQfF6xHggydDh%2Fmory-buckman-a-case-study-of-modeling-yourself-as-multiple%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mory%20Buckman%3A%20a%20case%20study%20of%20modeling%20yourself%20as%20multiple%20agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BycMQfF6xHggydDh%2Fmory-buckman-a-case-study-of-modeling-yourself-as-multiple", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BycMQfF6xHggydDh%2Fmory-buckman-a-case-study-of-modeling-yourself-as-multiple", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>Hat tip to <a href=\"/user/cata\">cata</a>.</p>\n<p><a href=\"http://www.thebuckmans.com/mory/\">Mory Buckman</a> is <a href=\"http://www.thebuckmans.com/mory/#a2203110\">eight people</a>: the explorer, the worker, the gamer, the musician, the programmer, the thinker, the addict, and the person. (He's also not neurotypical.) It looks like a fascinating case study of Hansonian \"model yourself as multiple agents and make deals between them,\" especially because he's been doing it for over a year, he writes quite a bit of notes, scores himself, and has regular conferences between the personalities that are posted to the blog. I haven't read enough to provide any intelligent commentary, but wanted to raise it to the attention interested in that sort of modeling.</p>\n<p>He wrote <a href=\"http://adventure.gamism.org/\">Gamer Mom</a>, an adventure game about convincing your family to share an experience with you, which is a design masterpiece (but also fairly depressing, so don't get too involved unless you're willing to take an emotional hit).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5BycMQfF6xHggydDh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 9.123127390613355e-07, "legacy": true, "legacyId": "16489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T00:38:51.839Z", "modifiedAt": null, "url": null, "title": "A Protocol for Optimizing Affection", "slug": "a-protocol-for-optimizing-affection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:25.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hRvAeLQmbQtuhhjTH/a-protocol-for-optimizing-affection", "pageUrlRelative": "/posts/hRvAeLQmbQtuhhjTH/a-protocol-for-optimizing-affection", "linkUrl": "https://www.lesswrong.com/posts/hRvAeLQmbQtuhhjTH/a-protocol-for-optimizing-affection", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Protocol%20for%20Optimizing%20Affection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Protocol%20for%20Optimizing%20Affection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhRvAeLQmbQtuhhjTH%2Fa-protocol-for-optimizing-affection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Protocol%20for%20Optimizing%20Affection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhRvAeLQmbQtuhhjTH%2Fa-protocol-for-optimizing-affection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhRvAeLQmbQtuhhjTH%2Fa-protocol-for-optimizing-affection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1159, "htmlBody": "<p>If Eliezer's art of solving confusing questions is the <a href=\"/lw/c4/go_forth_and_create_the_art/\">basic punch of&nbsp;rationality</a>, and fighting akrasia and becoming personally effective is the basic&nbsp;front kick, I would like to master the loving hug. Here is a simple protocol to&nbsp;help us build stronger relationships and stronger communities:</p>\n<p>In the spirit of <a href=\"http://wiki.lesswrong.com/wiki/Crocker's_rules\">Crocker's rules</a>, I give you Nyan's rules: I&nbsp;hereby declare that you are allowed to love me. I will not judge you or hate you&nbsp;or stop talking to you. I will recieve and return your affection happily and&nbsp;gently let you know if you push my limits.</p>\n<p>What's this all about? Here is the story:</p>\n<p>I have strong feels of love and friendship for some of you that I met at&nbsp;minicamp, and some of you that I know from my meetup. On reflection, I see&nbsp;that I want to be deeply in (reciprocal) love with as many people as possible. I&nbsp;look forward to a future when I am smart enough to be in wonderful friendly love&nbsp;with all N billion of us.</p>\n<p>I don't just want more feels, I want to be able to express them, too. I want&nbsp;to be able to tell you all that I love you and hold your hands and hug and&nbsp;cuddle and generally be nice without anyone feeling awkward or creeped out or&nbsp;conflicted.</p>\n<p><a href=\"/lw/bq0/be_happier/#interpersonal\">Happiness research</a> and personal experience suggests that more affection and&nbsp;closer relationships are generally a good thing. Mammals seem to like curling up&nbsp;together. Unwelcome affection is no good tho; the utility of affection seems&nbsp;to drop off past some point where people start to feel uncomfortable or unsafe.&nbsp;I think if we tried, we would find that there is tremendous value in finding&nbsp;the right level of affection in our relationships. The problem at this point is how&nbsp;<em>quickly</em>&nbsp;the utility of affection drops off, and how unwilling people are to&nbsp;be explicit about their preferences here.</p>\n<p>Currently, I feel like if I tell my friend that I love him or try to hold his hand, and he is not interested, this at best creates an awkward situation,&nbsp;and at worst irrevocably damages the friendship. It is a violation of fun&nbsp;theory to have a misstep that is this expensive. The usual method prescribed to&nbsp;deal with this is to be able to work up the curve slowly and get a feel for when&nbsp;you are reaching the limit. The location of the optimum also moves up, so&nbsp;<a href=\"https://en.wikipedia.org/wiki/Rapport\">building rapport</a>&nbsp;like this is a pretty important skill. IMO, tho, it is too&nbsp;expensive to do things this way if we can avoid it.</p>\n<p>We should be able to find and operate at the optimal level of affection with&nbsp;minimal cost. In the current social dynamic with my current skill level, even&nbsp;probing for information is so scary that I don't bother to play the game.</p>\n<p>Many percieved social risks are imaginary, but if this one is, no one&nbsp;is being explicit about its non-existance, so it still scares me. If it scares&nbsp;me, it probably scares others. There may even be people who want to be more&nbsp;affectionate with me, and aren't able to work up enough courage to try. That&nbsp;makes me really sad.</p>\n<p>This is all made worse by love being mixed up with romance. Romance brings a&nbsp;whole other bag of grenades to the love party. If, in some case, full-on&nbsp;romance is uncomfortable or inconvienient for someone, that doesn't mean the&nbsp;optimal level of affection is none. We can probably still have hugs and cuddles.&nbsp;Note that this is just a consequence of the optimal-affection idea.</p>\n<p>So there are two things we need to do, I think, to create a better social&nbsp;dynamic <a href=\"/lw/xb/free_to_optimize/\">in which we can optimize</a> affection and relationships faster and better.&nbsp;We need to be more comfortable with being explicit about what we are comfortable&nbsp;with, and we need to try to flatten the tail of our affection-&gt;utility curve so&nbsp;that overstepping comfort limits is not such a disaster. This means not&nbsp;punishing people for overstepping the bounds the first time, just gently nudging&nbsp;them back to your comfort zone.</p>\n<p>At minicamp, there were a couple moments where a few of us semi-deliberately&nbsp;made these changes. IMO, the result was huge; we probed each other's comfort&nbsp;boundaries and built loving relationships very quickly, and all came out of it&nbsp;happier. At least that's what it felt like to me. This is one of the sources of&nbsp;strong feels of love and friendship that I mentioned above. This post is an&nbsp;attempt to formalize what happened there into a useful protocol.</p>\n<p>Human social dynamics is one of the most complex systems in the known universe.&nbsp;Hacking it naively is bound to hit some pitfall or other. Even so, it is <em>our</em>&nbsp;system, and we are rationalists; I think we can <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">do better</a> here.</p>\n<p>The naive approach is to do like radical honesty and start expressing love&nbsp;honestly when you feel it. Even if this were explicitly endorsed and enforced by&nbsp;the group (good luck overcoming <em>that</em>&nbsp;momentum), it still has two big issues:&nbsp;It requires way too much courage, and punishes people who are not comfortable&nbsp;with saying they are uncomfortable. This is the same sort of thing, except&nbsp;worse, that sinks radical honesty. Forcing the new rules on people who are not&nbsp;ready is <em>bad</em>.</p>\n<p>The solution, I think is the same as the solution to these problems for radical&nbsp;honesty: transform the intervention from a something&nbsp;<em>forced</em>&nbsp;on people who are&nbsp;not ready to an opt-in protocol where people who are ready <em>invite</em>&nbsp;others to&nbsp;initiate interactions under the new system. Radical honesty becomes Crocker's&nbsp;rules, really awkward affection becomes Nyan's rules (or something).</p>\n<p>(if anyone has a better name...)</p>\n<p>So here are the rules:</p>\n<ol style=\"position: static; z-index: auto;\">\n<li>I want to optimize the level of affection between us; I probably want&nbsp;more of your love.</li>\n<li>To make it easier for you, I will give you feedback about what I feel&nbsp;comfortable with. I am ready to do this and you don't have to worry that I&nbsp;am secretly uncomfortable.</li>\n<li>To make it safer for you, I won't punish you or hate you for going over my&nbsp;limits. I still expect you to respect them, but you can expect me to warn&nbsp;you before blowing up. (don't keep testing me tho).</li>\n<li>If you reach out to me, please be comfortable with being open about your own&nbsp;limits. You may be suprised at how much I love you back.</li>\n</ol>\n<p>What does this get us? If this works as well as I think it should, it will&nbsp;become a major piece of the group rationality puzzle. Rationalists <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">should</a> be able&nbsp;to <a href=\"/lw/5f/bayesians_vs_barbarians/\">build strong emotional relationships</a> faster and better than any Dark Side&nbsp;cult. Is this going to work? I think it is at least worth testing.</p>\n<p>I feel so much love just waiting for an opportunity to come out. There are many&nbsp;people I would love to be more open and affectionate with, but don't want to&nbsp;risk making them uncomfortable or ruining a friendship. I can't force this on&nbsp;them; all I can do is do for others what I would like them to do for me.</p>\n<p>So if you like, try this out at your meetups. Lets see if it works.&nbsp;It seems safe enough, so I'll be the first to awkwardly stick my neck out and&nbsp;say it:</p>\n<p>It is safe to express love or be affectionate with me, really, I won't bite.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 2, "PAugtjFrmvbepCNej": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hRvAeLQmbQtuhhjTH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 39, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "16490", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 114, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aFEsqd6ofwnkNqaXo", "EZ8GniEPSechjDYP9", "DoLQN5ryZ9XkZjq5h", "KsHmn6iJAEr9bACQW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T02:31:35.817Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Timeless Control", "slug": "seq-rerun-timeless-control", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MMmpQtbLa3yhkjHkb/seq-rerun-timeless-control", "pageUrlRelative": "/posts/MMmpQtbLa3yhkjHkb/seq-rerun-timeless-control", "linkUrl": "https://www.lesswrong.com/posts/MMmpQtbLa3yhkjHkb/seq-rerun-timeless-control", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Timeless%20Control&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Timeless%20Control%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMmpQtbLa3yhkjHkb%2Fseq-rerun-timeless-control%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Timeless%20Control%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMmpQtbLa3yhkjHkb%2Fseq-rerun-timeless-control", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMmpQtbLa3yhkjHkb%2Fseq-rerun-timeless-control", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>Today's post, <a href=\"/lw/r1/timeless_control/\">Timeless Control</a> was originally published on 07 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>We throw away \"time\" but retain causality, and with it, the concepts \"control\" and \"decide\". To talk of something as having been \"always determined\" is mixing up a timeless and a timeful conclusion, with paradoxical results. When you take a perspective outside time, you have to be careful not to let your old, timeful intuitions run wild in the absence of their subject matter.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cpe/seq_rerun_thou_art_physics/\">Thou Art Physics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MMmpQtbLa3yhkjHkb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.124697760336589e-07, "legacy": true, "legacyId": "16496", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YYLmZFEGKsjCKQZut", "EB8QXYdBiKDre6sBa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T10:59:43.437Z", "modifiedAt": null, "url": null, "title": "[Link] Reason: the God that fails, but we keep socially promoting\u2026.", "slug": "link-reason-the-god-that-fails-but-we-keep-socially", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/55vDmhF8vZykersix/link-reason-the-god-that-fails-but-we-keep-socially", "pageUrlRelative": "/posts/55vDmhF8vZykersix/link-reason-the-god-that-fails-but-we-keep-socially", "linkUrl": "https://www.lesswrong.com/posts/55vDmhF8vZykersix/link-reason-the-god-that-fails-but-we-keep-socially", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Reason%3A%20the%20God%20that%20fails%2C%20but%20we%20keep%20socially%20promoting%E2%80%A6.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Reason%3A%20the%20God%20that%20fails%2C%20but%20we%20keep%20socially%20promoting%E2%80%A6.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F55vDmhF8vZykersix%2Flink-reason-the-god-that-fails-but-we-keep-socially%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Reason%3A%20the%20God%20that%20fails%2C%20but%20we%20keep%20socially%20promoting%E2%80%A6.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F55vDmhF8vZykersix%2Flink-reason-the-god-that-fails-but-we-keep-socially", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F55vDmhF8vZykersix%2Flink-reason-the-god-that-fails-but-we-keep-socially", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1071, "htmlBody": "<p>An interesting <a href=\"http://blogs.discovermagazine.com/gnxp/2012/05/reason-the-god-that-fails-but-we-keep-socially-promoting/\">blog post</a> by Razib Khan, who many here probably know from his <a href=\"http://blogs.discovermagazine.com/gnxp/\">Gene Expression blog</a>, the <a href=\"http://www.gnxp.com/\">old gnxp site</a> or perhaps from his <a href=\"/lw/1qu/bhtv_eliezer_yudkowsky_razib_khan/\">BHTV debate with Eliezer</a>.</p>\n<blockquote>\n<p>One point which I&rsquo;ve made on this weblog several times is that <strong>on a whole range of issues and behaviors people simply follow the consensus of their self-identified group.</strong> This group conformity probably has deep evolutionary origins. It is often much cognitively &ldquo;cheaper&rdquo; to simply utilize a heuristic &ldquo;do what my peers do&rdquo; than reason from first principles. The &ldquo;wisdom of the crowds&rdquo; and &ldquo;irrational herds&rdquo; both arise from this dynamic, positive and negative manifestations. The interesting point is that from a proximate (game-theoretic rational actor) and ultimate (evolutionary fitness) perspective ditching reason is often quite reasonable (in fact, it may be the only feasible option if you want to &ldquo;understand,&rdquo; for example, celestial mechanics).</p>\n<p><br /> If you&rsquo;re faced with a complex environment or set of issues &ldquo;re-inventing the wheel&rdquo; is often both laborious and impossible. Laborious because our individual general intelligence is simply not that sharp. Impossible because most of us are too stupid to do something like invent calculus. Many people can learn the rules for obtaining derivatives and integrals, but far fewer can come up with the <a href=\"http://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus\">fundamental theorem of calculus</a>. Similarly, in the 18th century engineers who utilized Newtonian mechanics for practical purposes were not capable of coming up with Newtonian mechanics themselves. I&rsquo;m using these two examples because calculus and mechanics are generally consider &ldquo;high level&rdquo; cognitive tasks, but even they at the root illustrate the principle of collective wisdom and group conformity. Calculus and mechanics is included in the curriculum not because all of the individuals who decide the curriculum understand these two topics in detail, <strong>but because individuals whom they trust and believe are worthy of emulation and deference, as well as past empirical history, tell them that this is the &ldquo;reasonable&rdquo; way to go.</strong> (science and engineering have the neat property is that you don&rsquo;t just trust people, you trust concrete <em>results</em>!)</p>\n<p>This sort of behavior is even more evident in political and social viewpoints. Recently there have been signs of <a href=\"http://fivethirtyeight.blogs.nytimes.com/2012/05/25/signs-of-shift-among-african-americans-on-same-sex-marriage/\">shifts in African American</a> attitudes toward same-sex marriage, and a more general trend in that direction across the population. Is this because individuals are sitting in their armchair and reflecting on justice? Of course people will enter into evidence the experience of knowing gay people, and the empathy which that generates, but are you willing to bet that these public policy shifts are primarily and independent driven by simply these sorts of dynamics? (i.e., run a regression and trying predict the change in attitude by the number of people coming out of the closet over time) Similarly,people like <a href=\"http://en.wikipedia.org/wiki/Chris_Mooney_%28journalist%29\">Chris Mooney</a> have documented the shift among the Republican grassroots in issues like climate change which seem to have moved very rapidly likely due to elite cues, rather than a deep analysis of the evidence.</p>\n<p>But let&rsquo;s look at something less controversial, at least on this weblog. <strong>Most people who accept evolution really don&rsquo;t understand how it works, nor are they very conversant in the reasons for why evolutionary process is compelling.</strong> The vast majority of the 50 percent of Americans who accept evolution have not read Charles Darwin, nor could they tell you what the neo-Darwinian Synthesis is. They have not read <a href=\"http://www.talkorigins.org/\">Talk Origins</a>, or <a href=\"http://www.amazon.com/exec/obidos/ASIN/B002ZNJWJU/geneexpressio-20\">Why Evolution is True</a>. So why do they accept evolution? Because evolution, like Newtonian mechanics, is part of established science, and educated people tend to accept established science. <strong>But that&rsquo;s conditional.</strong> If you look in the <em>General Social Survey</em> you notice a weird trend: the correlation between education and acceptance of evolution holds for those who are not Biblical literalists, but not for those who are Biblical literalists! Why? Because well educated Biblical literalists accept a different set of authorities on this issue. In their own knowledge ecology the &ldquo;well-informed&rdquo; perspective might actually be that evolution is a disputed area in science.</p>\n<p>At this point everything is straightforward, more or less. But I want to push this further: <strong>most biologists do not understand evolution as a phenomenon, though they may be able to recall the basic evidence for evolution.</strong> If you are working in molecular biology, medical research, neuroscience, etc., there isn&rsquo;t a deep need to understand evolutionary biology on a day to day basis on the bench (I would argue the rise of -omics is changing this some, but many labs have one or two -omics people to handle that aspect). The high rates of acceptance of evolution among researchers in these fields has less to do with reason, and more to do with the ecology of ideas which they inhabit. Evolutionary biologists in their own turn accept the basic structural outlines of how axons and dendrites are essential in the proper function of the brain without understanding all the details about action potentials and such. They assume that neuroscientists understand their domain.</p>\n<p>So far I&rsquo;ve been talking about opinions and beliefs that are held by contemporaries. <strong>The basic model is that you offload the task of reasoning about issues which you are not familiar with, or do not understand in detail, to the collective with which you identify, and give weight to specialists if they exist within that collective.</strong> I would submit that to some extent the same occurs across time as well. Why do we do X and not Y? Because in the <em>past</em> our collective unit did X, not Y. How persuasive this sort of argument is all things equal probably smokes out to some extent where you are on the conservative-liberal spectrum. Traditional conservatives argue that the past has wisdom through its organic evolution, and the trial and error of customs and traditions. This is a general tendency, applicable both to Confucius and Edmund Burke. Liberal utopians, whether Mozi or the partisans of the French Revolution, don&rsquo;t put so much stock in the past, which they may perceive to be the font of injustice rather than wisdom. Instead, they rely on their reason in the here and now, more or less, to &ldquo;solve&rdquo; the problems which they believe are amenable to decomposition via their rational faculties.</p>\n<p>...</p>\n</blockquote>\n<p>I recommend following the <a href=\"http://blogs.discovermagazine.com/gnxp/2012/05/reason-the-god-that-fails-but-we-keep-socially-promoting/\">link</a> and reading the rest of it there, not only does interestingness continue, the comment section there is usually worth reading since he vigorously moderates it.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "55vDmhF8vZykersix", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 22, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "16516", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4gP3692vXuauKGjMR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T15:28:27.801Z", "modifiedAt": null, "url": null, "title": "When None Dare Urge Restraint, pt. 2", "slug": "when-none-dare-urge-restraint-pt-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.877Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jay_Schweikert", "createdAt": "2011-04-29T03:53:40.789Z", "isAdmin": false, "displayName": "Jay_Schweikert"}, "userId": "6niJdEi2bPMT4Bd5t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f5BocZMNkG2bLBLnx/when-none-dare-urge-restraint-pt-2", "pageUrlRelative": "/posts/f5BocZMNkG2bLBLnx/when-none-dare-urge-restraint-pt-2", "linkUrl": "https://www.lesswrong.com/posts/f5BocZMNkG2bLBLnx/when-none-dare-urge-restraint-pt-2", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20None%20Dare%20Urge%20Restraint%2C%20pt.%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20None%20Dare%20Urge%20Restraint%2C%20pt.%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff5BocZMNkG2bLBLnx%2Fwhen-none-dare-urge-restraint-pt-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20None%20Dare%20Urge%20Restraint%2C%20pt.%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff5BocZMNkG2bLBLnx%2Fwhen-none-dare-urge-restraint-pt-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff5BocZMNkG2bLBLnx%2Fwhen-none-dare-urge-restraint-pt-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 753, "htmlBody": "<p>In the original <a href=\"/lw/ls/when_none_dare_urge_restraint/\">When None Dare Urge Restraint</a> post, Eliezer discusses the dangers of the \"spiral of hate\" that can develop when saying negative things about the Hated Enemy trumps saying accurate things. Specifically, he uses the example of how the 9/11 hijackers were widely criticized as \"cowards,\" even though this vice in particular was surely not on their list. Over this past Memorial Day weekend, however, it seems like the exact mirror-image problem played out in nearly textbook form.</p>\n<p>The trouble began when MSNBC host <a href=\"http://www.youtube.com/watch?v=57Tv2d4O3Ns\">Chris Hayes noted</a>* that he was uncomfortable with how people use the word \"hero\" to describe those who die in war -- in particular, because he thinks this sort of automatic valor attributed to the war dead makes it easier to justify future wars. And as you might expect, <a href=\"http://www.newser.com/story/146906/msnbc-host-sparks-outrage-by-doubting-use-of-hero.html\">people went crazy in response</a>, calling Hayes's comments \"reprehensible and disgusting,\" something that \"commie grad students would say,\" and that old chestnut, apparently offered without a hint of irony, \"unAmerican.\" If you watch the video, you can tell that Hayes himself is really struggling to make the point, and by the end he definitely knew he was going to get in trouble, as he started backpedaling with a \"but maybe I'm wrong about that.\" And of course, <a href=\"http://www.huffingtonpost.com/2012/05/28/chris-hayes-uncomfortable-soldiers-heroes_n_1550643.html\">he apologized the very next day</a>, basically stating that it was improper to have \"opine[d] about the people who fight our wars, having never dodged a bullet or guarded a post or walked a mile in their boots.\"</p>\n<p>This whole episode struck me as particularly frightening, mostly because Hayes <em>wasn't even offering a criticism</em>. Soldiers in the American military are, of course, an untouchable target, and I would hardly expect any attack on&nbsp;soldiers to be well received, no matter how grounded. But what genuinely surprised me in this case was that Hayes was merely saying \"let's not <em>automatically</em> apply the single most valorizing word we have, because that might cause future wars, and thus future war deaths.\" But apparently anything less than maximum praise was not only incorrect, but offensive.</p>\n<p>Of course, there's <a href=\"/lw/gw/politics_is_the_mindkiller\">no shortage of rationality failures in political discourse</a>, and I'm obviously not intending this post as a political statement about any particular war, policy, candidate, etc. But I think this example is worth mentioning, for two main reasons. First, it's just such a textbook example of the exact sort of problem discussed in Eliezer's original post, in a purer form than I can recall seeing since 9/11 itself. I don't imagine many LW members need convincing in this regard, but I do think there's value in being mindful of this sort of problem on the national stage, even if we're not going to start arguing politics ourselves.</p>\n<p>But second, I think this episode says something not just about nationalism, but about how people approach death more generally. Of course, we're all familiar with afterlifism/\"they're-in-a-better-place\"-style rationalizations of death, but labeling a death as \"heroic\" can be a similar sort of rationalization.&nbsp;If a death is \"heroic,\" then there's at least some kind of silver lining, some sense of justification, if only partial justification. The movie might not be happy, but it can still go on, and there's at least a chance to play inspiring music. So there's an obvious temptation to label death as \"heroic\" as much as possible -- I'm reminded of how people tried to call the 9/11 victims \"heroes,\" apparently because they had the great courage to work in buildings that were targeted in a terrorist attack.</p>\n<p>If a death is <em>just</em>&nbsp;a tragedy, however, you're left with a more painful situation. You have to acknowledge that <em>yes, really</em>, <a href=\"/lw/uk/beyond_the_reach_of_god/\">the world isn't fair</a>, and <em>yes, really</em>, thousands of people -- even the Good Guy's soldiers! -- might be dying for <em>no good reason at all</em>. And even for those who <a href=\"/lw/i4/belief_in_belief/\">don't really believe</a> in an afterlife, facing death on such a large scale without the \"heroic\" modifier might just be too painful. The obvious problem, of course -- and Hayes's original point -- is that this sort of death-anesthetic makes it all too easy to numb yourself to more death. If you really care about the problem, you have to <em>face</em> the sheer tragedy of it. Sometimes, all you can say is \"<a href=\"http://yudkowsky.net/other/yehuda\">we shall have to work faster</a>.\" And I think that lesson's as appropriate on Memorial Day as any other.</p>\n<p>*I apologize that this clip is inserted into a rather low-brow attack video. At the time of posting it was the only link on Youtube I could find, and I wanted something accessible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f5BocZMNkG2bLBLnx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 84, "extendedScore": null, "score": 0.000178, "legacy": true, "legacyId": "16469", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Tw9cLvzSKrkGjNHW3", "9weLK2AJ9JEt2Tt8f", "sYgv4eYH82JEsTD34", "CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T20:08:18.333Z", "modifiedAt": null, "url": null, "title": "[Link] A superintelligent solution to the Fermi paradox", "slug": "link-a-superintelligent-solution-to-the-fermi-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:57.147Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ipetuoP7Wrr3GZghq/link-a-superintelligent-solution-to-the-fermi-paradox", "pageUrlRelative": "/posts/ipetuoP7Wrr3GZghq/link-a-superintelligent-solution-to-the-fermi-paradox", "linkUrl": "https://www.lesswrong.com/posts/ipetuoP7Wrr3GZghq/link-a-superintelligent-solution-to-the-fermi-paradox", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20A%20superintelligent%20solution%20to%20the%20Fermi%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20A%20superintelligent%20solution%20to%20the%20Fermi%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipetuoP7Wrr3GZghq%2Flink-a-superintelligent-solution-to-the-fermi-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20A%20superintelligent%20solution%20to%20the%20Fermi%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipetuoP7Wrr3GZghq%2Flink-a-superintelligent-solution-to-the-fermi-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipetuoP7Wrr3GZghq%2Flink-a-superintelligent-solution-to-the-fermi-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p><a href=\"http://computationaltheology.blogspot.com/2012/05/superintelligent-solution-to-fermi.html\">Here.</a></p>\n<p>Long story short, it's an attempt to justify the <a href=\"http://en.wikipedia.org/wiki/Planetarium_hypothesis\">planetarium hypothesis</a>&nbsp;as a solution to the Fermi paradox. The first half is a discussion of how it and things like it are relevant to the intended purview of the blog, and the second half is the meat of the post. You'll probably want to just eat the meat, which I think is relevant to the interests of many LessWrong folk.</p>\n<p>The blog is <em>Computational Theology. </em>It's new. I'll be the primary poster, but others are sought. I'll likely introduce the blog and more completely describe it in its own discussion post when more posts are up, hopefully including a few from people besides me, and when the archive will give a more informative indication of what to expect from the blog. Despite theism's suspect reputation here at LessWrong I suspect many of the future posts will be of interest to this audience anyway, especially for those of you who take interest in discussion of the singularity. The blog will even occasionally touch on rationality proper. So you might want to store the fact of the blog's existence somewhere deep in the back of your head. A link to the blog's main page can be found on <a href=\"/user/Will_Newsome\">my LessWrong user page</a>&nbsp;if you forget the url.</p>\n<p>I'd appreciate it if comments about the substance of the post were made on the blog post itself, but if you want to discuss the content here on LessWrong then that's okay too. Any meta-level comments about presentation, typos, or the post's relevance to LessWrong, should probably be put as comments on this discussion post. Thanks all!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ipetuoP7Wrr3GZghq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": -2, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "16519", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T20:31:41.775Z", "modifiedAt": null, "url": null, "title": "Avoid inflationary use of terms", "slug": "avoid-inflationary-use-of-terms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:59.737Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LqfnkpmK2EBcwbAwA/avoid-inflationary-use-of-terms", "pageUrlRelative": "/posts/LqfnkpmK2EBcwbAwA/avoid-inflationary-use-of-terms", "linkUrl": "https://www.lesswrong.com/posts/LqfnkpmK2EBcwbAwA/avoid-inflationary-use-of-terms", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Avoid%20inflationary%20use%20of%20terms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAvoid%20inflationary%20use%20of%20terms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqfnkpmK2EBcwbAwA%2Favoid-inflationary-use-of-terms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Avoid%20inflationary%20use%20of%20terms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqfnkpmK2EBcwbAwA%2Favoid-inflationary-use-of-terms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqfnkpmK2EBcwbAwA%2Favoid-inflationary-use-of-terms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 852, "htmlBody": "<p>Inflationary terms! You see them everywhere. And for those who actually know and care about the subject matter they can be very frustrating. These terms are notorious for being used in contexts where:</p>\n<ol>\n<li>They are only loosely applicable at best.</li>\n<li>There exists a better word that is more specific.</li>\n<li>The topic has a <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">far</a> bias.</li>\n</ol>\n<p>Some examples:</p>\n<ul>\n<li>Rational</li>\n<li>Evolution</li>\n<li>Singularity</li>\n<li>Emergent</li>\n<li>Nanotech</li>\n<li>Cryogenics</li>\n<li>Faith</li>\n</ul>\n<p>The problem is <em>not</em> that these words are meaningless in their original form, <em>nor</em> that you shouldn't ever use them. The problem is that they often get used in stupid ways that make them much less meaningful. By that I mean, less useful for keeping a focus on the topic and understanding what the person is really talking about.</p>\n<p>For example, terms like Nanotech (or worse, \"Nanobot\") <em>do apply</em> in a certain <em>loose</em> sense to several kinds of chemistry and biological innovations that are currently in vogue. Nonetheless, each time the term is used to refer to these things it makes it much harder to know if you are referring to <a href=\"http://en.wikipedia.org/wiki/Eric_Drexler\">Drexlerian</a> <a href=\"http://en.wikipedia.org/wiki/Mechanosynthesis\">Mechanosynthesis</a>. Hint: If you get your grant money by convincing someone you are working on one thing whereas you are really working on something completely different, that's fraud.</p>\n<p>Similarly, Cryogenics is the science of keeping things really cold. And of course <em>Cryonics</em> is a form of that. But saying \"Cryogenics\" when you really mean exactly Cryonics is an incredibly harmful practice which actual Cryonicists generally avoid. Most people who work in Cryogenics have nothing to do with Cryonics, and this kind of confusion in popular culture has <a href=\"http://www.cryogenicsociety.org/cryonics/\">apparently</a> engendered animosity towards Cryonics among Cryogenics specialists.</p>\n<p>Recently I fell prey to something like this with respect to the term \"Rational\". I wanted to know in general terms what the <a href=\"/r/discussion/lw/cnp/what_is_the_best_programming_language/\">best programming language</a> for a newbie would be and why. I wanted some in depth analysis, from a group I trust to do so. (And I wasn't disappointed -- we have some very knowledgeable programmers whose opinions were most helpful to me.) However the reaction of some lesswrongers to the title I initially chose for the post was distinctly negative. The title was \"Most rational programming language?\"</p>\n<p>After thinking about it for a while I realized what the problem was: This way of using the term, despite being more or less valid, makes the term less meaningful in the long run. And I <em>don't</em> want to be the person who makes Rational a less meaningful word. Nobody here wants that to happen. Thus it would have been better to use a term such as \"Best\" or \"Most optimal\" instead.</p>\n<p>Another example that comes to mind is when people (usually outsiders) refer to Transhumanism, Bayeseanism, the Singularity, or even skepticism, as a \"Faith\" or \"Belief\". Well yeah, trivially, if you are willing to stretch that word to its broadest possible meaning you can feel free to apply it to such as us. But... for crying out loud! What meaning does the word have if Faith is something absolutely everyone has? We're really referring to something like \"Confidence\" here.</p>\n<p>Then there's Evolution. Is Transhumanism really about <a href=\"http://www.amazon.com/Radical-Evolution-Promise-Enhancing-Bodies/dp/0385509650\">the next stage in human Evolution</a>? Perhaps in a certain <em>loose</em> sense it is -- but let's not lose sight of the mutilation of the language (and consequent noise-to-signal increase) that occurs when you say such a thing. Human Evolution is an <em>existing scientific specialty</em> with absolutely zilch to do with cybernetic body modification or genetic engineering, and everything to do with the effects of natural selection and mutation on the development of humans in the past.</p>\n<p>Co-opting terms isn't <em>always</em> bad. If you are brand-new to a topic, seeing an analogy to something with which you are already familiar may reduce the inferential distance and help you click the idea in your brain. But this gets more hazardous the closer the terms actually are in meaning. Distant terms are safer&nbsp; -- when I say \"Avoid <em>inflationary</em> use of terms\" you can instantly see that I'm definitely not talking about money, nor rubber objects with compressed air inside of them, but about words and phrases.</p>\n<p>On the other hand with such things as Rational versus Optimal, we're taking two surface-level-similar words and blurring them in such a way that one cannot meaningfully talk about either without accidentally importing baggage from the other. Rational is more suitable for use in <em>contrast</em> with clear examples of irrationality -- cognitive biases, for example, or <a href=\"https://rational.org/\">drug addiction</a>, and is a rather unabashedly idealistic term. Optimal on the other hand doesn't so much require specific contrast because pretty much everything is suboptimal <em>by default</em> to some degree or another -- optimizing is understood as an ongoing and very relativistic process.</p>\n<p>To sum up: Avoid making words cheaper and less effective for their specialized tasks. Don't use them for things where a better and more appropriate term exists. As your brain gets used to an idea, be prepared to discard old terms you have co-opted from other domains that were really just useful placeholders to get you started. Specialized jargon exists for a reason!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"p8nXWqwPH7mPSZf6p": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LqfnkpmK2EBcwbAwA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 89, "baseScore": 98, "extendedScore": null, "score": 0.000223, "legacy": true, "legacyId": "16440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4vXLzG4Ydxo4gqnzb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T20:41:00.167Z", "modifiedAt": null, "url": null, "title": "Learning with Audiobooks", "slug": "learning-with-audiobooks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.536Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jabberslythe", "createdAt": "2011-05-16T22:21:09.850Z", "isAdmin": false, "displayName": "Jabberslythe"}, "userId": "cKRinn5dDewj4wFc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TCYvS9QZgWyBj3he2/learning-with-audiobooks", "pageUrlRelative": "/posts/TCYvS9QZgWyBj3he2/learning-with-audiobooks", "linkUrl": "https://www.lesswrong.com/posts/TCYvS9QZgWyBj3he2/learning-with-audiobooks", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learning%20with%20Audiobooks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearning%20with%20Audiobooks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCYvS9QZgWyBj3he2%2Flearning-with-audiobooks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learning%20with%20Audiobooks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCYvS9QZgWyBj3he2%2Flearning-with-audiobooks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCYvS9QZgWyBj3he2%2Flearning-with-audiobooks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">I use Audiobooks, recorded lectures and podcasts to learn to a massive extent and I find them extremely useful. I haven&rsquo;t heard them discussed on here much, so I thought I would broach the subject. Here on lesswrong in terms on scholarship&nbsp;<a style=\"color: #8a8a8b;\" title=\"textbooks are widely favored\" href=\"http://lesswrong.com/lw/5me/scholarship_how_to_do_it_efficiently/\" target=\"_self\">textbooks are widely favored</a>&nbsp;and textbooks on math are especially encouraged. I&rsquo;ve listened to some textbooks that worked well in audio and even if you can&rsquo;t learn math very well with them, there are plenty of other useful things to learn as well.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;One barrier to using audiobooks that a lot of people have described to me is that they can read much faster than they can get through audiobooks. It&rsquo;s pretty easy to find applications that speed up audio, though, so this&nbsp;doesn't&nbsp;seem like a great reason. Another barrier is that it is harder to find audio sources of things, again, clever use of the internet can still find you most things.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;The two main benefits that I would expect most people to receiving when listening to audio rather than reading are:</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;<span style=\"white-space: pre;\"> </span>(1) That you can accomplish some other manual task while listening to an audiobook.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;<span style=\"white-space: pre;\"> </span>&nbsp;I work at a manual job so I have more time available to listen than the average person, but I think that most people underestimate the amount of time that they could spend at activities that you can listen to audiobooks while doing. Some examples are commuting, cleaning, making things, sports, shopping, shaving and falling asleep. Doubling you productivity is just not something that can be overlooked.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><span style=\"white-space: pre;\"> </span>(2) That listening to audiobooks is less effortful than reading.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;<span style=\"white-space: pre;\"> </span>This has definitely been my experience and I would be interested in hearing what other people&rsquo;s experiences are to see if I really am typical in this respect. I love reading, but it&rsquo;s the sort of activity that I have to take breaks from and whereas with audiobooks I can actually just happily listen to them with all of my waking hours. I remember being read to as a kid and it&rsquo;s just like that.&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TCYvS9QZgWyBj3he2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 9.129566528217176e-07, "legacy": true, "legacyId": "16520", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T20:59:06.788Z", "modifiedAt": null, "url": null, "title": "List of underrated risks?", "slug": "list-of-underrated-risks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yttrium", "createdAt": "2011-11-26T23:30:29.876Z", "isAdmin": false, "displayName": "yttrium"}, "userId": "BBzE4abSkhSfkQ89D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bX2N9rFZvFbBt7jhT/list-of-underrated-risks", "pageUrlRelative": "/posts/bX2N9rFZvFbBt7jhT/list-of-underrated-risks", "linkUrl": "https://www.lesswrong.com/posts/bX2N9rFZvFbBt7jhT/list-of-underrated-risks", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20of%20underrated%20risks%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20of%20underrated%20risks%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbX2N9rFZvFbBt7jhT%2Flist-of-underrated-risks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20of%20underrated%20risks%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbX2N9rFZvFbBt7jhT%2Flist-of-underrated-risks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbX2N9rFZvFbBt7jhT%2Flist-of-underrated-risks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p>\n<p>As everyone here knows, it would be a stupid idea to switch from airplanes to cars out of&nbsp;safety/terrorism concerns: Cars are a much more risky means of transportation than&nbsp;airplanes. But what other major risks are there that many people systematically undervalue&nbsp;or are not even consciously aware of?</p>\n<p>The same can be asked for chances.</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bX2N9rFZvFbBt7jhT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 9.129647505600836e-07, "legacy": true, "legacyId": "16521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-30T22:24:38.149Z", "modifiedAt": null, "url": null, "title": "One possible issue with radically increased lifespan", "slug": "one-possible-issue-with-radically-increased-lifespan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.981Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spectral_Dragon", "createdAt": "2012-01-14T14:24:07.407Z", "isAdmin": false, "displayName": "Spectral_Dragon"}, "userId": "JeoxZ45Aada9AwZXN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pAHurAu3cBjBHQvdx/one-possible-issue-with-radically-increased-lifespan", "pageUrlRelative": "/posts/pAHurAu3cBjBHQvdx/one-possible-issue-with-radically-increased-lifespan", "linkUrl": "https://www.lesswrong.com/posts/pAHurAu3cBjBHQvdx/one-possible-issue-with-radically-increased-lifespan", "postedAtFormatted": "Wednesday, May 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20possible%20issue%20with%20radically%20increased%20lifespan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20possible%20issue%20with%20radically%20increased%20lifespan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAHurAu3cBjBHQvdx%2Fone-possible-issue-with-radically-increased-lifespan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20possible%20issue%20with%20radically%20increased%20lifespan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAHurAu3cBjBHQvdx%2Fone-possible-issue-with-radically-increased-lifespan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAHurAu3cBjBHQvdx%2Fone-possible-issue-with-radically-increased-lifespan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>I might need a better title (It has now been updated), but here goes, anyway:</p>\n<p>I've been considering this for a while now. Suppose we reach a point where we can live for centuries, maybe even millenia, then how do we balance? Even assuming we're as efficient as possible, there's a limit for how much resources we can have, meaning an artificial limit at the amount of people that could exist at any given moment even if we explore what we can of the galaxy and use any avaliable resource. There would have to be roughly the same rate of births and deaths in a stable population.</p>\n<p>How would this be achieved? Somehow limiting lifespan, or children, assuming it's available to a majority? Or would this lead to a genespliced, technologically augmented and essentially immortal elite that the poor, unaugmented ones would have no chance of measuring up to? I'm sorry if this has already been considered, I'm very uneducated on the topic. If it has, could someone maybe link an analysis of the topic of lifespans and the like?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pAHurAu3cBjBHQvdx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 13, "extendedScore": null, "score": 9.130029923895829e-07, "legacy": true, "legacyId": "16522", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T00:31:57.865Z", "modifiedAt": null, "url": null, "title": "Rational Toothpaste: A Case Study", "slug": "rational-toothpaste-a-case-study", "viewCount": null, "lastCommentedAt": "2021-02-28T14:05:46.074Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NHuLAS3oKZWr2X9hP/rational-toothpaste-a-case-study", "pageUrlRelative": "/posts/NHuLAS3oKZWr2X9hP/rational-toothpaste-a-case-study", "linkUrl": "https://www.lesswrong.com/posts/NHuLAS3oKZWr2X9hP/rational-toothpaste-a-case-study", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Toothpaste%3A%20A%20Case%20Study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Toothpaste%3A%20A%20Case%20Study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHuLAS3oKZWr2X9hP%2Frational-toothpaste-a-case-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Toothpaste%3A%20A%20Case%20Study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHuLAS3oKZWr2X9hP%2Frational-toothpaste-a-case-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHuLAS3oKZWr2X9hP%2Frational-toothpaste-a-case-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1717, "htmlBody": "<p><em>Inspired by <a href=\"/r/discussion/lw/cnp/what_is_the_best_programming_language/6oan\">Konkvistador's comment</a></em></p>\n<p>Posts titled \"Rational <em>___</em>-ing\" or \"A Rational Approach to ___<em>_</em>\" induce groans among a <a href=\"/r/discussion/lw/con/the_rational_rationalists_guide_to_rationally/\">sizeable</a> <a href=\"/lw/ayi/suggestions_on_tech_devicegear_purchasing/62c9\">contingent</a> here, myself included. However, <a href=\"/lw/coo/avoid_inflationary_use_of_terms/\">inflationary use</a> of \"rational\" and its transformation into an applause light is only one part of the problem. These posts tend to revolve around specific answers, rather than the process of how to find answers. I claim a post on \"rational toothpaste buying\" could be on-topic and useful, if correctly written to illustrate determining goals, assessing tradeoffs, and implementing the final conclusions. A post detailing the pros and cons of various toothpaste brands is for a dentistry or personal hygiene forum; a post about algorithms for how to determine the best brands or whether to do so at all is for a rationality forum. This post is my shot at showing what this would look like.<a id=\"more\"></a></p>\n<hr />\n<p>&nbsp;</p>\n<p>At one point or another, we've all asked ourselves, \"what is the most rational toothpaste?\" After all, despite the length of the sequences, I've yet to see Eliezer's endorsed personal hygiene products. What is an aspiring rationalist to do?</p>\n<p>Step one is to throw out the question entirely. The most rational toothpaste does not exist, nor does the best toothpaste nor the optimal toothpaste. These adjectives are only applicable relative to particular goals, constraints, and contexts. Avoid the mistake of assuming optimality is a trait inherent to toothpaste, rather than a <a href=\"/lw/oi/mind_projection_fallacy/\">joint function of the toothpaste and who is using it</a>. Similarly, the best programming language, the best footwear, the best way to write, and the best job are all under-specified.&nbsp;</p>\n<p>Even before determining what you are looking for in toothpaste, take one more step back. Is optimizing your toothpaste worth the time and attention? First, there is the issue of whether improved dental care is worth it, and then, whether better toothpaste is the best means of improving your teeth.</p>\n<p>While recognizing \"optimal\" varies across individuals, goals might be aligned closely enough that something can be identified as approximately optimal. The search costs of finding the perfect solution could outweigh going with an approximate solution. Toothpaste seems like a product where users have essentially the same needs or fall into a small number of categories, unlike the best place to reside, which depends on a large number of individual factors. As a result, toothpaste is probably already well optimized for you and picking anything up off the shelf of a supermarket should do fine, but a product you use everyday still deserves a few minutes of deliberate analysis.</p>\n<p>One basic algorithm for tackling these issue:</p>\n<ul>\n<li>What do you actually want to accomplish? Two approaches for determining goals: 1. (Bottom-up) List all the goals your current actions or the first proposed solution might fulfill. 2. (Top-down) List your basic values, major goals, mid-level goals, etc until you reach the relevant scope.</li>\n<li>How much are you actually willing to spend in time and experimentation costs for improvements to these goals? Quickly estimate the value of information.</li>\n<li>Generate actions that might suit each goal. Focus on quantity.</li>\n<li>Gather information. Is there published research on the topic? Who might have good advice? Are there quick experiments that can be run?</li>\n<li>Filter actions and form a plan.</li>\n<li>Are you satisfied with implementing the conclusion reached? If you feel a hang-up, try optimizing specifically for that.<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup></li>\n</ul>\n<p>Following a bottom-up approach, why do I use toothpaste at all? Toothpaste can decrease risk of cavities, whiten teeth, improve bad breath, or make brushing more pleasant. A change I make could be relevant for at least five years. Beyond that point, I discount the future enough not to worry about it, with the chances of my conclusions becoming irrelevant or out of date included in the discount factor.</p>\n<p>How much am I willing to spend for improvements in these areas? Remember that resources are fungible and can be converted into one another, so this represents the total value of time, attentions, and explicit money spent. For myself:</p>\n<ul>\n<li>Avoiding a cavity is worth about $300, including the cost of a filling, time, discomfort, etc. I tend to have one and a half cavities a year, so a 1% reduction in cavities over five years is worth <code>1.5 x 5 x $300 x .01 = $22.5</code>. I expect I might be able to uncover something with up to a 5% improvement, so more info would be worth around $120.</li>\n<li>Whiter teeth and better breath are harder to quantify. For noticeable improvements on either front, I'll say I'd pay $10 a month. Over five years, that would be $600. Additional information about aesthetic improvements probably only has a 5-10% chance of finding something that has a noticeable effect and causing me to go through with it, so I should be willing to invest about $50 now.</li>\n<li>If a change made brushing more pleasant and ensured I did it regularly, this would multiply the benefits I'm getting from improved health or aesthetics, but I expect this to be too small to account for.</li>\n</ul>\n<p>WIth a five-minute estimate, I've learned I could be willing to spend up to $170 total for improved information about dental care. How should I cash that out? Valuing my time at $20 per hour, I could justify up to eight hours of research for a 5% reduction in cavities and a 5% chance of aesthetic improvement. Diminishing returns to researching a topic like this likely set in quickly though, so no need to go overboard.</p>\n<p>What possible actions might I take to improve these goals? Here are the results of a few minutes generating as many options as possible.</p>\n<ul>\n<li>For health, better toothpaste, more frequent dentist visits, switching to an electric toothbrush, different means of flossing, regular mouth wash, topical fluoride treatments, investing in a commitment device to ensure always brushing and flossing, eating fewer sugary foods.</li>\n<li>For aesthetics, in addition to the above, whitening treatments by dentist or store-bought, drinking less coffee and tea, buy a tongue scraper, use gum or breath fresheners more often, use a whitening toothpaste.</li>\n</ul>\n<p>How can I gather information about the relative effectiveness of these solutions? Since dentistry is fairly well studied, some quick searches on Google Scholar would be my best starting point. A search for \"evidence based dentistry\" turned up <a href=\"http://ebd.ada.org/Default.aspx\">this site</a> with multiple meta-studies. Other actions, like buying a tongue cleaner for $5 are just worth trying to see if they have an effect.</p>\n<h2>Fluoride</h2>\n<p>In adolescents, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20091655\">fluoride in toothpaste</a> can reduce cavities by 23% at 1000 parts per million and 36% at 2500 ppm. The effects for adults are unclear. Since the typical US toothpaste has about 1000 ppm, a high fluoride toothpaste could be worth $2 more per tube for a 10% reduction (~4 tubes/year x 5 years x $2 = $40). Type of fluoride might also matter, with stannous fluoride about <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/8593194\">30% more effective</a> at reducing gingivitis than sodium fluoride, and <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21280424\">reducing bad breath</a>. Mouthwashes with fluoride have too little to make a difference.</p>\n<p>At my last visit to the dentist, the hygenist recommended a topical fluoride varnish. I wondered whether it was simply an up-sell, an easy way to tack $35 on my bill. For this price, a treatment at each visit should reduce the risk over cavities in a year by about 10%. Hard to find good conclusions, but benefit and cost probably cancel out.</p>\n<h3 id=\"whitening-products\">Whitening Products</h3>\n<p>For amount of stain reduction with whitening toothpastes, multiple studies show statistically significant improvements with special toothpastes, although the level of practical significance is unclear. In <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1600-051x.2004.00611.x/abstract\">one study</a>, a water rinse control was equally efficacious as one commercial whitening paste. Other commercial whitening products have some evidence in their favor, although likely suffer from <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17054282\">publication bias</a>. Hydrogen peroxide treatments might be more effective, although have increased risk of tooth sensitivity. A <a href=\"http://www.sciencedirect.com/science/article/pii/S0300571209000086\">meta-analysis</a> of whitening strips conducted over four years at one dental school showed lightness and yellowness improved each by 2 points over two weeks on a scale where participants varied up to 10 points at the baseline. Approximately 20% of the participants experienced irritation or sensitivity. At around $35 for a two-week treatment, experimentation might be worthwhile since if results aren't obvious, it's not worthwhile.</p>\n<h3 id=\"electric-toothbrushes\">Electric toothbrushes</h3>\n<p>Using an electric toothbrush could be <a href=\"http://ukpmc.ac.uk/abstract/MED/2778601/reload=0;jsessionid=9kzCWSD6QsjD6XKZKlV7.0\">as beneficial</a> for gingivitis and sensitivity over a manual toothbrush as regular toothbrushing is over baseline. Meta-studies <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15001285\">(1)</a> <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15846633\">(2)</a> identify this as specific to rotation-oscillation action, with improvements of 20% in gingivitis and plaque over manual. Rotating electric toothbrushes sell for <a href=\"http://www.amazon.com/Oral-B-Professional-1000-Electric-Toothbrush/dp/B003UKM9CO/ref=sr_1_3?ie=UTF8&amp;qid=1338420404&amp;sr=8-3\">$50 and up</a>, with replacement heads around $7, so purchasing one looks straightforwardly worthwhile.</p>\n<h3 id=\"tongue-cleaning\">Tongue cleaning</h3>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20961381\">Some positive effect</a>. Over toothbrushing along, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17004573\">statistically significant and small</a>. As noted above, for $5 and a small amount of effort each day as part of a routine, experimentation of my own is probably worthwhile.</p>\n<p>&nbsp;</p>\n<p>In conclusion, even though I didn't expect changing toothpastes to make a difference, finding a high fluoride toothpaste might be worthwhile. I've heard about the advantages of electric toothbrushes before, but never seriously considered it. I plan on buying one soon. My flossing previously improved substantially with a flosser using detachable heads over floss alone. The tongue cleaner and whitening strips are low priority experiments, established as plausible.</p>\n<p>Do I feel resistance to implementing the new plan? I'm actually rather relieved. Instead staring at the wall of toothpaste in the supermarket, unsure of what to choose like I often do, I feel like the issue is settled. Going through analysis like this often leaves me excited or relieved. Common pitfalls I find myself in are trying to reason without meaningful information or not acting on conclusions due to lack of motivation. Reasoning without good information is what I try to do when standing in the toothpaste aisle. Each box has its little claims, but I don't know how to process it well, and end up frustrated by the abundance of choice. Fruitless conclusions aren't an issue here, but are common with bigger decisions. Hidden emotions around an issue can mean that an optimal solution on paper isn't feasible. Investigating small resistances and optimizing specifically for that concern is a means of implementing <a href=\"http://en.wikipedia.org/wiki/Pareto_efficiency\">Pareto improvements</a> for all parts of yourself.</p>\n<p>&nbsp;</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\">\n<p>Thanks for Anna Salamon for pointing out the benefits of this step to me as part of her fungibility algorithm. <a class=\"footnoteBackLink\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 6, "XqykXFKL9t38pbSEm": 1, "eamWQNQ2dPYWEwhqr": 2, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NHuLAS3oKZWr2X9hP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 85, "baseScore": 123, "extendedScore": null, "score": 0.000255, "legacy": true, "legacyId": "16523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 124, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Inspired by <a href=\"/r/discussion/lw/cnp/what_is_the_best_programming_language/6oan\">Konkvistador's comment</a></em></p>\n<p>Posts titled \"Rational <em>___</em>-ing\" or \"A Rational Approach to ___<em>_</em>\" induce groans among a <a href=\"/r/discussion/lw/con/the_rational_rationalists_guide_to_rationally/\">sizeable</a> <a href=\"/lw/ayi/suggestions_on_tech_devicegear_purchasing/62c9\">contingent</a> here, myself included. However, <a href=\"/lw/coo/avoid_inflationary_use_of_terms/\">inflationary use</a> of \"rational\" and its transformation into an applause light is only one part of the problem. These posts tend to revolve around specific answers, rather than the process of how to find answers. I claim a post on \"rational toothpaste buying\" could be on-topic and useful, if correctly written to illustrate determining goals, assessing tradeoffs, and implementing the final conclusions. A post detailing the pros and cons of various toothpaste brands is for a dentistry or personal hygiene forum; a post about algorithms for how to determine the best brands or whether to do so at all is for a rationality forum. This post is my shot at showing what this would look like.<a id=\"more\"></a></p>\n<hr>\n<p>&nbsp;</p>\n<p>At one point or another, we've all asked ourselves, \"what is the most rational toothpaste?\" After all, despite the length of the sequences, I've yet to see Eliezer's endorsed personal hygiene products. What is an aspiring rationalist to do?</p>\n<p>Step one is to throw out the question entirely. The most rational toothpaste does not exist, nor does the best toothpaste nor the optimal toothpaste. These adjectives are only applicable relative to particular goals, constraints, and contexts. Avoid the mistake of assuming optimality is a trait inherent to toothpaste, rather than a <a href=\"/lw/oi/mind_projection_fallacy/\">joint function of the toothpaste and who is using it</a>. Similarly, the best programming language, the best footwear, the best way to write, and the best job are all under-specified.&nbsp;</p>\n<p>Even before determining what you are looking for in toothpaste, take one more step back. Is optimizing your toothpaste worth the time and attention? First, there is the issue of whether improved dental care is worth it, and then, whether better toothpaste is the best means of improving your teeth.</p>\n<p>While recognizing \"optimal\" varies across individuals, goals might be aligned closely enough that something can be identified as approximately optimal. The search costs of finding the perfect solution could outweigh going with an approximate solution. Toothpaste seems like a product where users have essentially the same needs or fall into a small number of categories, unlike the best place to reside, which depends on a large number of individual factors. As a result, toothpaste is probably already well optimized for you and picking anything up off the shelf of a supermarket should do fine, but a product you use everyday still deserves a few minutes of deliberate analysis.</p>\n<p>One basic algorithm for tackling these issue:</p>\n<ul>\n<li>What do you actually want to accomplish? Two approaches for determining goals: 1. (Bottom-up) List all the goals your current actions or the first proposed solution might fulfill. 2. (Top-down) List your basic values, major goals, mid-level goals, etc until you reach the relevant scope.</li>\n<li>How much are you actually willing to spend in time and experimentation costs for improvements to these goals? Quickly estimate the value of information.</li>\n<li>Generate actions that might suit each goal. Focus on quantity.</li>\n<li>Gather information. Is there published research on the topic? Who might have good advice? Are there quick experiments that can be run?</li>\n<li>Filter actions and form a plan.</li>\n<li>Are you satisfied with implementing the conclusion reached? If you feel a hang-up, try optimizing specifically for that.<sup><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\">1</a></sup></li>\n</ul>\n<p>Following a bottom-up approach, why do I use toothpaste at all? Toothpaste can decrease risk of cavities, whiten teeth, improve bad breath, or make brushing more pleasant. A change I make could be relevant for at least five years. Beyond that point, I discount the future enough not to worry about it, with the chances of my conclusions becoming irrelevant or out of date included in the discount factor.</p>\n<p>How much am I willing to spend for improvements in these areas? Remember that resources are fungible and can be converted into one another, so this represents the total value of time, attentions, and explicit money spent. For myself:</p>\n<ul>\n<li>Avoiding a cavity is worth about $300, including the cost of a filling, time, discomfort, etc. I tend to have one and a half cavities a year, so a 1% reduction in cavities over five years is worth <code>1.5 x 5 x $300 x .01 = $22.5</code>. I expect I might be able to uncover something with up to a 5% improvement, so more info would be worth around $120.</li>\n<li>Whiter teeth and better breath are harder to quantify. For noticeable improvements on either front, I'll say I'd pay $10 a month. Over five years, that would be $600. Additional information about aesthetic improvements probably only has a 5-10% chance of finding something that has a noticeable effect and causing me to go through with it, so I should be willing to invest about $50 now.</li>\n<li>If a change made brushing more pleasant and ensured I did it regularly, this would multiply the benefits I'm getting from improved health or aesthetics, but I expect this to be too small to account for.</li>\n</ul>\n<p>WIth a five-minute estimate, I've learned I could be willing to spend up to $170 total for improved information about dental care. How should I cash that out? Valuing my time at $20 per hour, I could justify up to eight hours of research for a 5% reduction in cavities and a 5% chance of aesthetic improvement. Diminishing returns to researching a topic like this likely set in quickly though, so no need to go overboard.</p>\n<p>What possible actions might I take to improve these goals? Here are the results of a few minutes generating as many options as possible.</p>\n<ul>\n<li>For health, better toothpaste, more frequent dentist visits, switching to an electric toothbrush, different means of flossing, regular mouth wash, topical fluoride treatments, investing in a commitment device to ensure always brushing and flossing, eating fewer sugary foods.</li>\n<li>For aesthetics, in addition to the above, whitening treatments by dentist or store-bought, drinking less coffee and tea, buy a tongue scraper, use gum or breath fresheners more often, use a whitening toothpaste.</li>\n</ul>\n<p>How can I gather information about the relative effectiveness of these solutions? Since dentistry is fairly well studied, some quick searches on Google Scholar would be my best starting point. A search for \"evidence based dentistry\" turned up <a href=\"http://ebd.ada.org/Default.aspx\">this site</a> with multiple meta-studies. Other actions, like buying a tongue cleaner for $5 are just worth trying to see if they have an effect.</p>\n<h2 id=\"Fluoride\">Fluoride</h2>\n<p>In adolescents, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20091655\">fluoride in toothpaste</a> can reduce cavities by 23% at 1000 parts per million and 36% at 2500 ppm. The effects for adults are unclear. Since the typical US toothpaste has about 1000 ppm, a high fluoride toothpaste could be worth $2 more per tube for a 10% reduction (~4 tubes/year x 5 years x $2 = $40). Type of fluoride might also matter, with stannous fluoride about <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/8593194\">30% more effective</a> at reducing gingivitis than sodium fluoride, and <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21280424\">reducing bad breath</a>. Mouthwashes with fluoride have too little to make a difference.</p>\n<p>At my last visit to the dentist, the hygenist recommended a topical fluoride varnish. I wondered whether it was simply an up-sell, an easy way to tack $35 on my bill. For this price, a treatment at each visit should reduce the risk over cavities in a year by about 10%. Hard to find good conclusions, but benefit and cost probably cancel out.</p>\n<h3 id=\"Whitening_Products\">Whitening Products</h3>\n<p>For amount of stain reduction with whitening toothpastes, multiple studies show statistically significant improvements with special toothpastes, although the level of practical significance is unclear. In <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1600-051x.2004.00611.x/abstract\">one study</a>, a water rinse control was equally efficacious as one commercial whitening paste. Other commercial whitening products have some evidence in their favor, although likely suffer from <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17054282\">publication bias</a>. Hydrogen peroxide treatments might be more effective, although have increased risk of tooth sensitivity. A <a href=\"http://www.sciencedirect.com/science/article/pii/S0300571209000086\">meta-analysis</a> of whitening strips conducted over four years at one dental school showed lightness and yellowness improved each by 2 points over two weeks on a scale where participants varied up to 10 points at the baseline. Approximately 20% of the participants experienced irritation or sensitivity. At around $35 for a two-week treatment, experimentation might be worthwhile since if results aren't obvious, it's not worthwhile.</p>\n<h3 id=\"Electric_toothbrushes\">Electric toothbrushes</h3>\n<p>Using an electric toothbrush could be <a href=\"http://ukpmc.ac.uk/abstract/MED/2778601/reload=0;jsessionid=9kzCWSD6QsjD6XKZKlV7.0\">as beneficial</a> for gingivitis and sensitivity over a manual toothbrush as regular toothbrushing is over baseline. Meta-studies <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15001285\">(1)</a> <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15846633\">(2)</a> identify this as specific to rotation-oscillation action, with improvements of 20% in gingivitis and plaque over manual. Rotating electric toothbrushes sell for <a href=\"http://www.amazon.com/Oral-B-Professional-1000-Electric-Toothbrush/dp/B003UKM9CO/ref=sr_1_3?ie=UTF8&amp;qid=1338420404&amp;sr=8-3\">$50 and up</a>, with replacement heads around $7, so purchasing one looks straightforwardly worthwhile.</p>\n<h3 id=\"Tongue_cleaning\">Tongue cleaning</h3>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20961381\">Some positive effect</a>. Over toothbrushing along, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17004573\">statistically significant and small</a>. As noted above, for $5 and a small amount of effort each day as part of a routine, experimentation of my own is probably worthwhile.</p>\n<p>&nbsp;</p>\n<p>In conclusion, even though I didn't expect changing toothpastes to make a difference, finding a high fluoride toothpaste might be worthwhile. I've heard about the advantages of electric toothbrushes before, but never seriously considered it. I plan on buying one soon. My flossing previously improved substantially with a flosser using detachable heads over floss alone. The tongue cleaner and whitening strips are low priority experiments, established as plausible.</p>\n<p>Do I feel resistance to implementing the new plan? I'm actually rather relieved. Instead staring at the wall of toothpaste in the supermarket, unsure of what to choose like I often do, I feel like the issue is settled. Going through analysis like this often leaves me excited or relieved. Common pitfalls I find myself in are trying to reason without meaningful information or not acting on conclusions due to lack of motivation. Reasoning without good information is what I try to do when standing in the toothpaste aisle. Each box has its little claims, but I don't know how to process it well, and end up frustrated by the abundance of choice. Fruitless conclusions aren't an issue here, but are common with bigger decisions. Hidden emotions around an issue can mean that an optimal solution on paper isn't feasible. Investigating small resistances and optimizing specifically for that concern is a means of implementing <a href=\"http://en.wikipedia.org/wiki/Pareto_efficiency\">Pareto improvements</a> for all parts of yourself.</p>\n<p>&nbsp;</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn1\">\n<p>Thanks for Anna Salamon for pointing out the benefits of this step to me as part of her fungibility algorithm. <a class=\"footnoteBackLink\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Fluoride", "anchor": "Fluoride", "level": 1}, {"title": "Whitening Products", "anchor": "Whitening_Products", "level": 2}, {"title": "Electric toothbrushes", "anchor": "Electric_toothbrushes", "level": 2}, {"title": "Tongue cleaning", "anchor": "Tongue_cleaning", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "60 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DFHhuAMexXAi8T6AY", "LqfnkpmK2EBcwbAwA", "ZTRiSNmeGQK8AkdN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T00:46:19.709Z", "modifiedAt": null, "url": null, "title": "Short Primers on Crucial Topics", "slug": "short-primers-on-crucial-topics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LFfizpc5YupK7pWNv/short-primers-on-crucial-topics", "pageUrlRelative": "/posts/LFfizpc5YupK7pWNv/short-primers-on-crucial-topics", "linkUrl": "https://www.lesswrong.com/posts/LFfizpc5YupK7pWNv/short-primers-on-crucial-topics", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Short%20Primers%20on%20Crucial%20Topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShort%20Primers%20on%20Crucial%20Topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLFfizpc5YupK7pWNv%2Fshort-primers-on-crucial-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Short%20Primers%20on%20Crucial%20Topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLFfizpc5YupK7pWNv%2Fshort-primers-on-crucial-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLFfizpc5YupK7pWNv%2Fshort-primers-on-crucial-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 356, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>Here's <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">another</a> <a href=\"/r/discussion/lw/cok/funding_good_research/\">way</a> we might purchase existential risk reduction: the production of <strong>short primers on crucial topics</strong>.</p>\n<p>Resources like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>&nbsp;and <a href=\"http://nickbostrom.com/\">NickBostrom.com</a> have been incredibly effective at gathering and creating a community engaged in x-risk reduction (either through direct action or, perhaps&nbsp;<a href=\"https://vimeo.com/32787159\">more importantly</a>, through donations), but most people who could make a difference probably won't take the time to read The Sequences or academic papers.</p>\n<p>One solution? <em>Short primers on crucial topics</em>.</p>\n<p><a href=\"http://facingthesingularity.com/\"><em>Facing the Singularity</em></a> is one example. I'm waiting for some work from remote researchers before I write the last chapter, but once it's complete we'll produce a PDF version and a Kindle version. Already, several people (including <a href=\"http://en.wikipedia.org/wiki/Jaan_Tallinn\">Jaan Tallinn</a>) use it as a standard introduction they send to AI risk newbies.</p>\n<p><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/ExistentialRisk.png\" alt=\"\" width=\"300\" height=\"480\" />Similar documents (say, 10 pages in length) could be produced for topics like Existential Risk, AI Risk, Friendly AI, Optimal Philanthropy, and Rationality. These would be concise, fun to read, and emotionally engaging, while also being accurate and thoroughly hyperlinked/referenced to fuller explanations of each section and major idea (on LessWrong, in academic papers, etc.).</p>\n<p>These could even be printed and left lying around wherever we think is most important: say, at the top math, computer science, and formal philosophy departments in the English-speaking world.</p>\n<p>The major difficulty in executing such a project would be in finding good writers with the relevant knowledge. Eliezer, Yvain, and myself might qualify, but right now the three of us are otherwise occupied. The time investment of the primary author(s) could be minimized by outsourcing as much of the work as possible to SI's team of remote researchers, writers, and editors.</p>\n<p>Estimated cost <em>per primer</em>:</p>\n<ul>\n<li>80 hours from primary author. (Well, if it's <em>me</em>. I've put about 60 hours into the <em>writing</em>&nbsp;of&nbsp;<em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>&nbsp;so far, which is of similar length to the proposed primers but I'm adding some padding to the estimate.)</li>\n<li>$4,000 on remote research. (Tracking down statistics and references, etc.)</li>\n<li>$1000 on book design, Kindle version production, etc.</li>\n</ul>\n<div>Translations to other languages could also be produced, for an estimated cost of $2,000 per translation (this includes checks and improvements by multiple translators).</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QPt5ECwTCAg63mbNu": 1, "EdDGrAxYcrXnKkDca": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LFfizpc5YupK7pWNv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 35, "extendedScore": null, "score": 9.130663571244916e-07, "legacy": true, "legacyId": "16524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "mYuMdmMmGM7fFj382", "qmiaJKkBNk2gGTkuG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T01:11:36.774Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Against Devil's Advocacy", "slug": "seq-rerun-against-devil-s-advocacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NLQmKwDqoCEgHQjd3/seq-rerun-against-devil-s-advocacy", "pageUrlRelative": "/posts/NLQmKwDqoCEgHQjd3/seq-rerun-against-devil-s-advocacy", "linkUrl": "https://www.lesswrong.com/posts/NLQmKwDqoCEgHQjd3/seq-rerun-against-devil-s-advocacy", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Against%20Devil's%20Advocacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Against%20Devil's%20Advocacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLQmKwDqoCEgHQjd3%2Fseq-rerun-against-devil-s-advocacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Against%20Devil's%20Advocacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLQmKwDqoCEgHQjd3%2Fseq-rerun-against-devil-s-advocacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLQmKwDqoCEgHQjd3%2Fseq-rerun-against-devil-s-advocacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>Today's post, <a href=\"/lw/r3/against_devils_advocacy/\">Against Devil's Advocacy</a> was originally published on 09 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Against_Devil.27s_Advocacy\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Playing Devil's Advocate is occasionally helpful, but much less so than it appears. Ultimately, you should only be able to create plausible arguments for things that are actually plausible.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cq8/seq_rerun_timeless_control/\">Timeless Control</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NLQmKwDqoCEgHQjd3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 9.130776650737401e-07, "legacy": true, "legacyId": "16525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PxN5iwS2CTCYi4oAP", "MMmpQtbLa3yhkjHkb", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T06:56:44.278Z", "modifiedAt": null, "url": null, "title": "Only say 'rational' when you can't eliminate the word", "slug": "only-say-rational-when-you-can-t-eliminate-the-word", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:39.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hN8Ld8YdqFsui2xgc/only-say-rational-when-you-can-t-eliminate-the-word", "pageUrlRelative": "/posts/hN8Ld8YdqFsui2xgc/only-say-rational-when-you-can-t-eliminate-the-word", "linkUrl": "https://www.lesswrong.com/posts/hN8Ld8YdqFsui2xgc/only-say-rational-when-you-can-t-eliminate-the-word", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Only%20say%20'rational'%20when%20you%20can't%20eliminate%20the%20word&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnly%20say%20'rational'%20when%20you%20can't%20eliminate%20the%20word%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN8Ld8YdqFsui2xgc%2Fonly-say-rational-when-you-can-t-eliminate-the-word%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Only%20say%20'rational'%20when%20you%20can't%20eliminate%20the%20word%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN8Ld8YdqFsui2xgc%2Fonly-say-rational-when-you-can-t-eliminate-the-word", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN8Ld8YdqFsui2xgc%2Fonly-say-rational-when-you-can-t-eliminate-the-word", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 389, "htmlBody": "<p>Almost all instances of the word \"true\" can be eliminated from the sentences in which they appear by applying Tarski's formula. &nbsp;For example, if you say, \"I believe the sky is blue, and that's true!\" then this can be rephrased as the statement, \"I believe the sky is blue, and the sky is blue.\" &nbsp;For every \"The sentence 'X' is true\" you can just say X and convey the same information about what you believe - just talk about the territory the map allegedly corresponds to, instead of talking about the map.</p>\n<p>When <em>can't</em>&nbsp;you eliminate the word \"true\"? &nbsp;When you're <em>generalizing</em>&nbsp;over map-territory correspondences, e.g., \"True theories are more likely to make correct experimental predictions.\" &nbsp;There's no way to take the word 'true' out of that sentence because it's talking about a feature of map-territory correspondences in general.</p>\n<p>Similarly, you can eliminate the sentence 'rational' from almost any sentence in which it appears. &nbsp;\"It's rational to believe the sky is blue\", \"It's true that the sky is blue\", and \"The sky is blue\", all convey exactly the same information about what color you think the sky is - no more, no less.</p>\n<p>When <em>can't</em>&nbsp;you eliminate the word \"rational\" from a sentence?</p>\n<p>When you're <em>generalizing</em>&nbsp;over <em>cognitive algorithms&nbsp;</em>for producing map-territory correspondences (epistemic rationality) or steering the future where you want it to go (instrumental rationality). &nbsp;So while you can eliminate the word 'rational' from \"It's rational to believe the sky is blue\", you can't eliminate the concept 'rational' from the sentence \"It's epistemically rational to increase belief in hypotheses that make successful experimental predictions.\" &nbsp;You can Taboo the word, of course, but then the sentence just becomes, \"To increase map-territory correspondences, follow the cognitive algorithm of increasing belief in hypotheses that make successful experimental predictions.\" &nbsp;You can eliminate the word, but you can't eliminate the <em>concept</em>&nbsp;without changing the meaning of the sentence, because the primary subject of discussion is, in fact, <em>general&nbsp;cognitive algorithms</em> with the property of producing map-territory correspondences.</p>\n<p><strong>The word 'rational' should never be used on any occasion except when it is necessary, i.e., when we are discussing cognitive algorithms as algorithms.</strong></p>\n<p>If you want to talk about how to buy a great car by <em>applying </em>rationality, but you're primarily&nbsp;<em>talking about</em>&nbsp;the car rather than <em>considering the question</em>&nbsp;of which cognitive algorithms are best,&nbsp;then title your post Optimal Car-Buying, not Rational Car-Buying.</p>\n<p>Thank you for observing all safety precautions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "FtT2T9bRbECCGYxrL": 3, "7mTviCYysGmLqiHai": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hN8Ld8YdqFsui2xgc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": 86, "extendedScore": null, "score": 0.000183, "legacy": true, "legacyId": "16537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 86, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T16:41:35.630Z", "modifiedAt": null, "url": null, "title": "Interest in a Coventry/Birmingham meetup?", "slug": "interest-in-a-coventry-birmingham-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/okX8vmYGuqBsL6cCy/interest-in-a-coventry-birmingham-meetup", "pageUrlRelative": "/posts/okX8vmYGuqBsL6cCy/interest-in-a-coventry-birmingham-meetup", "linkUrl": "https://www.lesswrong.com/posts/okX8vmYGuqBsL6cCy/interest-in-a-coventry-birmingham-meetup", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interest%20in%20a%20Coventry%2FBirmingham%20meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterest%20in%20a%20Coventry%2FBirmingham%20meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokX8vmYGuqBsL6cCy%2Finterest-in-a-coventry-birmingham-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interest%20in%20a%20Coventry%2FBirmingham%20meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokX8vmYGuqBsL6cCy%2Finterest-in-a-coventry-birmingham-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokX8vmYGuqBsL6cCy%2Finterest-in-a-coventry-birmingham-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>I'm in Coventry myself (at Warwick uni), and now that I've finished exams I'd like to see if there's sufficient interest for a meetup in the area.</p>\n<p>(I'm actually only around for another six weeks or so, but even if we only manage to meet once or twice, that seems worth doing.)</p>\n<p>If there's interest, I'm happy to organise; I'd default to a saturday afternoon thing, but can probably work with any time. (And I don't know Birmingham, so if we go there, suggestions for where to meet would be useful.)</p>\n<p>(There was an <a href=\"/lw/5u6/any_lwers_in_uk_west_midlands/\">attempt</a> a while back which didn't take off, but it can't hurt to try again. I'll message those two posters in case they don't see this.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "okX8vmYGuqBsL6cCy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "16555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dWte6vN4rRDheBphx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T17:26:55.597Z", "modifiedAt": null, "url": null, "title": "Low Hanging Fruit- Basic bedroom decorating", "slug": "low-hanging-fruit-basic-bedroom-decorating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.775Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vr8yoKg7R52mTKDMm/low-hanging-fruit-basic-bedroom-decorating", "pageUrlRelative": "/posts/vr8yoKg7R52mTKDMm/low-hanging-fruit-basic-bedroom-decorating", "linkUrl": "https://www.lesswrong.com/posts/vr8yoKg7R52mTKDMm/low-hanging-fruit-basic-bedroom-decorating", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Low%20Hanging%20Fruit-%20Basic%20bedroom%20decorating&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALow%20Hanging%20Fruit-%20Basic%20bedroom%20decorating%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvr8yoKg7R52mTKDMm%2Flow-hanging-fruit-basic-bedroom-decorating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Low%20Hanging%20Fruit-%20Basic%20bedroom%20decorating%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvr8yoKg7R52mTKDMm%2Flow-hanging-fruit-basic-bedroom-decorating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvr8yoKg7R52mTKDMm%2Flow-hanging-fruit-basic-bedroom-decorating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1276, "htmlBody": "<p><strong>This is a weird topic for a discussion post. Why are you writing it?</strong></p>\n<p>In a recent post-mini-camp skills exchange thread, I jokingly mentioned that I could \"help make your room look Awesome!\" To my surprise, a couple people were actually interested. Note: I'm not an interior decorator, and what follows will be the extremely basic 101-level breakdown. Follow these instructions, and your room will look significantly nicer than the average 20-something male's (I'm assuming gender, based on the 92% male readership of this site), but not look like it was done by a professional or anything.</p>\n<p><strong>Why should I care about Making my Room Awesome?<br /></strong>(aka \"Why would you be able to put the word \"Rational\" in this post, should there not be edicts against it?\")</p>\n<p>1) It's Low-Hanging Fruit- If you haven't spent much time or effort on room decoration, then it will take very little money or effort to make significant improvements. $200 and a weekend will have your room looking super-awesome for the remainder of your stay at your current location.</p>\n<p>2) You probably spend more time there, than any other location- Your environment effects your mood, and your bedroom is likely to be the environment you spend the most time in. It is what you wake up in, setting the mood for the rest of the day. It also sets the mood (or lack thereof) for....</p>\n<p>3) Romantic Considerations- 'nuff said</p>\n<p><strong>Psh! It doesn't count if you don't Use Math.</strong></p>\n<p>How much does it cost?-- I recently Awesome-ized a room, and it cost about $250. Assuming you re-decorate every two years (either because you move, or you just want a change, or your stuff is getting old), then that ends up being about $10/month.</p>\n<p>What about time?-- Assuming it takes you about 2 weekends of part-time work, then that is about 30 hours of work. If you value your time at about $15/hour, then that's about $450 total. So it actually costs more in time than it does in money, but that's still only about $19/mo.</p>\n<p>Note: Ways to make the relative cost of time less-</p>\n<ul>\n<li>Enjoy it: decorating is fun, if you let it be!</li>\n<li>Use it as an excuse to ask for accompaniment from the romantic interest of choice: \"Gee! I'm about to go comforter shopping and have no clue what to look for! I sure could use help...Wanna come?\"</li>\n</ul>\n<p>So, if you add up monetary and time cost, you get $29/mo to have an awesome room.&nbsp;Considering the small return you get on apartment rental (a $450/month rental is not <em>that</em> much better than a $421/month rental), Awesome-izing your room is a much better investment than renting a slightly nicer place.</p>\n<p><strong>But I don't care about having a nice place anyways!</strong></p>\n<p><strong></strong>How much do you already value having a \"nice\" living space?--In my city, you can get a room that is safe and livable, but gross and/or not \"nice\" for about $250/month. If you are willing to spend $450/mo on rent, then you are willing to spend $200/mo on having a nice place. Assuming you live there for one year, that is<em>&nbsp;$2400 per year</em>. Note, that at this point in the discussion, your definition of \"nice\" probably&nbsp;<em>also</em>&nbsp;includes location, neighborhood, roommates or lack thereof, etc. Point being, that you are already spending a couple thousand dollars a year on having a living space that is more than just somewhere to sleep, so you can't rationalize that you just don't care about living somewhere nice, unless your ONLY consideration when apartment shopping was price and location.</p>\n<p>&nbsp;</p>\n<h2><strong>OK! Let's do this!</strong></h2>\n<p>(or...\"meh, I'm unconvinced. I think I'll hit the \"Back\" button now\")</p>\n<p>&nbsp;</p>\n<p><strong>1) Make sure your room is decently clean</strong>- We're not talking perfection here, but no amount of decorating can overcome having a gross room. Keeping your room clean would be it's own post, but generically: laundry off the floor and into a hamper, books on a shelf, no dirty dishes or food. Protip: Those plastic shoe organizers that hang on the back of doors are actually perfect for organizing all sorts of small non-shoe items (pencils, deodorant, scissors, lighters, ipod, etc)</p>\n<p><strong>2) Spend 5-10 minutes on the internet</strong>- Google \"bedroom decor\" or \"bed set\" or something similar, in order to get a basic idea of what's out there, what's popular, and what you like/dislike.&nbsp;</p>\n<p><strong>3) Buy a comforter set</strong>- In my experience, very few single males have even made it to this pretty basic step, so just buying a comforter set will put you ahead of the game. Beds-in-a-bag are the easiest way to go for the newly-initiated into the world of Awesome-Bedroom-Making. They are much cheaper than buying all the pieces separately, and do all the work of matching a set for you. They tend to come with a comforter, sham covers, throw pillows, and bed skirt. Valances (the top part of a curtain set) and sheets are also occasionally included.</p>\n<p>Protip- Make sure that when you buy your bed set that you've already planned out what you are going to do with your room to complement it.</p>\n<p>Protip- You'll be using this for sleeping comfortably, so make sure that it isn't scratchy, stiff, or otherwise generally uncomfortable. Don't buy it without running your hands across it first!</p>\n<p><strong>4) Paint</strong>- (Note- This is the step that makes the biggest difference, though may be less optimal for renters. IMO, if you intend on staying at a rented location for 2+ years, it is worth losing your deposit to paint, need be) Use your comforter to pull wall colors off of. You can just bring in one of the throw pillows to the paint store, and they will use their Super-Magic Technology to mix paint that matches.</p>\n<p>Protip: You want to paint at least one wall (generally the far wall) an \"accent color\" which is either darker or more vibrant than the other walls. This makes a HUGE difference.</p>\n<p><strong>5) Hang some wall decor</strong>- This does NOT mean unframed posters. Unframed posters signal \"college dorm\". If you would like to display your personality via geeky stuff (comic books, fantasy art, etc), this is acceptable, but frames should be utilized, or they should be mounted in some other way. Think of it as protection for your art.</p>\n<p>The wall decor part is pretty individualized, so if you make it this far and want specific help, feel free to message me. There are lots of really cheap, do-it-yourself options here too.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>(Note</strong>- I wrote more on where to get Awesome Art stuff in <a href=\"/lw/cru/low_hanging_fruit_basic_bedroom_decorating/6prs\">this comment</a>, below.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Lighting protip</strong>: Dimmer switches are pretty awesome. You can have your room be really bright for studying or board gaming, or you can have it be mid-level for nighttime web browsing or hanging out (when super-bright light would seem harsh), and even have it be dim mood-lighting for more intimate moments</p>\n<p><strong>Scent protip</strong>: Bad-smelling rooms are bad. Assuming you already understand laundry and throwing away old food (Febreze can cover a decent amount as well), the next step is actually making it smell GOOD. There are many options--if you consider candles too girly, and incense too hippy, then you probably want to go with either high-quality (Slatkin and Co is always good) wall-plug scents or concentrated sprays. You might even have to go into....Bath and Body Works (*gasp!*). If you want more masculine scents go for things with words like \"-wood\", \"sea-\", \"sage\" etc. \"Clean\" scents are also good, and not&nbsp;necessarily&nbsp;feminine.</p>\n<p>&nbsp;</p>\n<p><strong>Note for decorating other rooms</strong>: In the bedroom, it is easiest to base color/decor off the comforter, which is your largest piece. If you feel like decorating a non-bedroom room, then the easiest way to do it is to buy a piece of wall art (of a decent size) that you really like, and base color/scheme around that.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Tg9aFPFCPBHxGABRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vr8yoKg7R52mTKDMm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 27, "extendedScore": null, "score": 9.135138398565175e-07, "legacy": true, "legacyId": "16554", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"This_is_a_weird_topic_for_a_discussion_post__Why_are_you_writing_it_\">This is a weird topic for a discussion post. Why are you writing it?</strong></p>\n<p>In a recent post-mini-camp skills exchange thread, I jokingly mentioned that I could \"help make your room look Awesome!\" To my surprise, a couple people were actually interested. Note: I'm not an interior decorator, and what follows will be the extremely basic 101-level breakdown. Follow these instructions, and your room will look significantly nicer than the average 20-something male's (I'm assuming gender, based on the 92% male readership of this site), but not look like it was done by a professional or anything.</p>\n<p><strong>Why should I care about Making my Room Awesome?<br></strong>(aka \"Why would you be able to put the word \"Rational\" in this post, should there not be edicts against it?\")</p>\n<p>1) It's Low-Hanging Fruit- If you haven't spent much time or effort on room decoration, then it will take very little money or effort to make significant improvements. $200 and a weekend will have your room looking super-awesome for the remainder of your stay at your current location.</p>\n<p>2) You probably spend more time there, than any other location- Your environment effects your mood, and your bedroom is likely to be the environment you spend the most time in. It is what you wake up in, setting the mood for the rest of the day. It also sets the mood (or lack thereof) for....</p>\n<p>3) Romantic Considerations- 'nuff said</p>\n<p><strong id=\"Psh__It_doesn_t_count_if_you_don_t_Use_Math_\">Psh! It doesn't count if you don't Use Math.</strong></p>\n<p>How much does it cost?-- I recently Awesome-ized a room, and it cost about $250. Assuming you re-decorate every two years (either because you move, or you just want a change, or your stuff is getting old), then that ends up being about $10/month.</p>\n<p>What about time?-- Assuming it takes you about 2 weekends of part-time work, then that is about 30 hours of work. If you value your time at about $15/hour, then that's about $450 total. So it actually costs more in time than it does in money, but that's still only about $19/mo.</p>\n<p>Note: Ways to make the relative cost of time less-</p>\n<ul>\n<li>Enjoy it: decorating is fun, if you let it be!</li>\n<li>Use it as an excuse to ask for accompaniment from the romantic interest of choice: \"Gee! I'm about to go comforter shopping and have no clue what to look for! I sure could use help...Wanna come?\"</li>\n</ul>\n<p>So, if you add up monetary and time cost, you get $29/mo to have an awesome room.&nbsp;Considering the small return you get on apartment rental (a $450/month rental is not <em>that</em> much better than a $421/month rental), Awesome-izing your room is a much better investment than renting a slightly nicer place.</p>\n<p><strong id=\"But_I_don_t_care_about_having_a_nice_place_anyways_\">But I don't care about having a nice place anyways!</strong></p>\n<p><strong></strong>How much do you already value having a \"nice\" living space?--In my city, you can get a room that is safe and livable, but gross and/or not \"nice\" for about $250/month. If you are willing to spend $450/mo on rent, then you are willing to spend $200/mo on having a nice place. Assuming you live there for one year, that is<em>&nbsp;$2400 per year</em>. Note, that at this point in the discussion, your definition of \"nice\" probably&nbsp;<em>also</em>&nbsp;includes location, neighborhood, roommates or lack thereof, etc. Point being, that you are already spending a couple thousand dollars a year on having a living space that is more than just somewhere to sleep, so you can't rationalize that you just don't care about living somewhere nice, unless your ONLY consideration when apartment shopping was price and location.</p>\n<p>&nbsp;</p>\n<h2 id=\"OK__Let_s_do_this_\"><strong>OK! Let's do this!</strong></h2>\n<p>(or...\"meh, I'm unconvinced. I think I'll hit the \"Back\" button now\")</p>\n<p>&nbsp;</p>\n<p><strong>1) Make sure your room is decently clean</strong>- We're not talking perfection here, but no amount of decorating can overcome having a gross room. Keeping your room clean would be it's own post, but generically: laundry off the floor and into a hamper, books on a shelf, no dirty dishes or food. Protip: Those plastic shoe organizers that hang on the back of doors are actually perfect for organizing all sorts of small non-shoe items (pencils, deodorant, scissors, lighters, ipod, etc)</p>\n<p><strong>2) Spend 5-10 minutes on the internet</strong>- Google \"bedroom decor\" or \"bed set\" or something similar, in order to get a basic idea of what's out there, what's popular, and what you like/dislike.&nbsp;</p>\n<p><strong>3) Buy a comforter set</strong>- In my experience, very few single males have even made it to this pretty basic step, so just buying a comforter set will put you ahead of the game. Beds-in-a-bag are the easiest way to go for the newly-initiated into the world of Awesome-Bedroom-Making. They are much cheaper than buying all the pieces separately, and do all the work of matching a set for you. They tend to come with a comforter, sham covers, throw pillows, and bed skirt. Valances (the top part of a curtain set) and sheets are also occasionally included.</p>\n<p>Protip- Make sure that when you buy your bed set that you've already planned out what you are going to do with your room to complement it.</p>\n<p>Protip- You'll be using this for sleeping comfortably, so make sure that it isn't scratchy, stiff, or otherwise generally uncomfortable. Don't buy it without running your hands across it first!</p>\n<p><strong>4) Paint</strong>- (Note- This is the step that makes the biggest difference, though may be less optimal for renters. IMO, if you intend on staying at a rented location for 2+ years, it is worth losing your deposit to paint, need be) Use your comforter to pull wall colors off of. You can just bring in one of the throw pillows to the paint store, and they will use their Super-Magic Technology to mix paint that matches.</p>\n<p>Protip: You want to paint at least one wall (generally the far wall) an \"accent color\" which is either darker or more vibrant than the other walls. This makes a HUGE difference.</p>\n<p><strong>5) Hang some wall decor</strong>- This does NOT mean unframed posters. Unframed posters signal \"college dorm\". If you would like to display your personality via geeky stuff (comic books, fantasy art, etc), this is acceptable, but frames should be utilized, or they should be mounted in some other way. Think of it as protection for your art.</p>\n<p>The wall decor part is pretty individualized, so if you make it this far and want specific help, feel free to message me. There are lots of really cheap, do-it-yourself options here too.</p>\n<p><span style=\"white-space: pre;\"> </span><strong>(Note</strong>- I wrote more on where to get Awesome Art stuff in <a href=\"/lw/cru/low_hanging_fruit_basic_bedroom_decorating/6prs\">this comment</a>, below.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Lighting protip</strong>: Dimmer switches are pretty awesome. You can have your room be really bright for studying or board gaming, or you can have it be mid-level for nighttime web browsing or hanging out (when super-bright light would seem harsh), and even have it be dim mood-lighting for more intimate moments</p>\n<p><strong>Scent protip</strong>: Bad-smelling rooms are bad. Assuming you already understand laundry and throwing away old food (Febreze can cover a decent amount as well), the next step is actually making it smell GOOD. There are many options--if you consider candles too girly, and incense too hippy, then you probably want to go with either high-quality (Slatkin and Co is always good) wall-plug scents or concentrated sprays. You might even have to go into....Bath and Body Works (*gasp!*). If you want more masculine scents go for things with words like \"-wood\", \"sea-\", \"sage\" etc. \"Clean\" scents are also good, and not&nbsp;necessarily&nbsp;feminine.</p>\n<p>&nbsp;</p>\n<p><strong>Note for decorating other rooms</strong>: In the bedroom, it is easiest to base color/decor off the comforter, which is your largest piece. If you feel like decorating a non-bedroom room, then the easiest way to do it is to buy a piece of wall art (of a decent size) that you really like, and base color/scheme around that.</p>\n<p>&nbsp;</p>", "sections": [{"title": "This is a weird topic for a discussion post. Why are you writing it?", "anchor": "This_is_a_weird_topic_for_a_discussion_post__Why_are_you_writing_it_", "level": 2}, {"title": "Psh! It doesn't count if you don't Use Math.", "anchor": "Psh__It_doesn_t_count_if_you_don_t_Use_Math_", "level": 2}, {"title": "But I don't care about having a nice place anyways!", "anchor": "But_I_don_t_care_about_having_a_nice_place_anyways_", "level": 2}, {"title": "OK! Let's do this!", "anchor": "OK__Let_s_do_this_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "48 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T18:41:09.552Z", "modifiedAt": null, "url": null, "title": "central planning is intractable (polynomial, but n is large)", "slug": "central-planning-is-intractable-polynomial-but-n-is-large", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zAoLMDXQafjWecBLM/central-planning-is-intractable-polynomial-but-n-is-large", "pageUrlRelative": "/posts/zAoLMDXQafjWecBLM/central-planning-is-intractable-polynomial-but-n-is-large", "linkUrl": "https://www.lesswrong.com/posts/zAoLMDXQafjWecBLM/central-planning-is-intractable-polynomial-but-n-is-large", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20central%20planning%20is%20intractable%20(polynomial%2C%20but%20n%20is%20large)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Acentral%20planning%20is%20intractable%20(polynomial%2C%20but%20n%20is%20large)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzAoLMDXQafjWecBLM%2Fcentral-planning-is-intractable-polynomial-but-n-is-large%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=central%20planning%20is%20intractable%20(polynomial%2C%20but%20n%20is%20large)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzAoLMDXQafjWecBLM%2Fcentral-planning-is-intractable-polynomial-but-n-is-large", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzAoLMDXQafjWecBLM%2Fcentral-planning-is-intractable-polynomial-but-n-is-large", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p><a href=\"http://cscs.umich.edu/~crshalizi/weblog/918.html  \">Three Toed Sloth</a> has a nice exposition on the difficulties of optimizing an economy, including the best explanation of convex optimization ever:</p>\n<blockquote>\n<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 11px; line-height: 14px; text-align: left; background-color: #edeff4;\">If plan A calls for 10,000 diapers and 2,000 towels, and plan B calls for 2,000 diapers and 10,000 towels, we could do half of plan A and half of plan B, make 6,000 diapers and 6,000 towels, and not run up against the constraints.</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zAoLMDXQafjWecBLM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "16556", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-31T18:41:52.131Z", "modifiedAt": null, "url": null, "title": "Loebian cooperation, version 2", "slug": "loebian-cooperation-version-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B64CZks9sb3PmgxXK/loebian-cooperation-version-2", "pageUrlRelative": "/posts/B64CZks9sb3PmgxXK/loebian-cooperation-version-2", "linkUrl": "https://www.lesswrong.com/posts/B64CZks9sb3PmgxXK/loebian-cooperation-version-2", "postedAtFormatted": "Thursday, May 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Loebian%20cooperation%2C%20version%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALoebian%20cooperation%2C%20version%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB64CZks9sb3PmgxXK%2Floebian-cooperation-version-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Loebian%20cooperation%2C%20version%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB64CZks9sb3PmgxXK%2Floebian-cooperation-version-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB64CZks9sb3PmgxXK%2Floebian-cooperation-version-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>I'm writing up some math results developed on LW as a paper with the tentative title \"Self-referential decision algorithms\".&nbsp;Something interesting came up while I was cleaning up the <a href=\"/lw/2ip/ai_cooperation_in_practice\">Loebian cooperation</a> result.&nbsp;Namely, how do we say precisely that Loebian cooperation is stable under minor syntactic changes? After all, if we define a \"minor change\" to program A as a change that preserves A's behavior against any program B, then <a href=\"/lw/do/reformalizing_pd/\">quining cooperation</a> is just as stable under such \"minor changes\" by definition. Digging down this rabbit hole, I seem to have found a nice new reformulation of the whole thing.</p>\n<p>I will post some sections of my current draft in the comments to this post. Eventually this material is meant to become an academic paper (hopefully), so any comments on math mistakes, notation or tone would be much appreciated! And yeah, I have no clue about academic writing, so you're welcome to tell me that too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B64CZks9sb3PmgxXK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 22, "extendedScore": null, "score": 9.135475904374004e-07, "legacy": true, "legacyId": "16557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TNfx89dh5KkcKrvho", "5iK6rsa3MSrMhHQyf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T01:57:55.742Z", "modifiedAt": null, "url": null, "title": "Is this rule of thumb useful for gauging low probabilities?", "slug": "is-this-rule-of-thumb-useful-for-gauging-low-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:56.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yNLgHMnKud3sC29Ai/is-this-rule-of-thumb-useful-for-gauging-low-probabilities", "pageUrlRelative": "/posts/yNLgHMnKud3sC29Ai/is-this-rule-of-thumb-useful-for-gauging-low-probabilities", "linkUrl": "https://www.lesswrong.com/posts/yNLgHMnKud3sC29Ai/is-this-rule-of-thumb-useful-for-gauging-low-probabilities", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20this%20rule%20of%20thumb%20useful%20for%20gauging%20low%20probabilities%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20this%20rule%20of%20thumb%20useful%20for%20gauging%20low%20probabilities%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNLgHMnKud3sC29Ai%2Fis-this-rule-of-thumb-useful-for-gauging-low-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20this%20rule%20of%20thumb%20useful%20for%20gauging%20low%20probabilities%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNLgHMnKud3sC29Ai%2Fis-this-rule-of-thumb-useful-for-gauging-low-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNLgHMnKud3sC29Ai%2Fis-this-rule-of-thumb-useful-for-gauging-low-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<p>Does something like this seem to you to be a reasonable rule of thumb, for helping handle scope insensitivity to low probabilities?</p>\n<p>There's a roughly 30 to 35 out of a million chance that you will die on any given day; and so if I'm dealing with a probability of one in a million, then I 'should' spend 30 times as much time preparing for my imminent death within the next 24 hours as I do playing with the one-in-a-million shot. If it's not worth spending 30 seconds preparing for dying within the next day, then I should spend less than one second dealing with that one-in-a-million shot.</p>\n<p>Relatedly, can you think of a way to improve it, such as to make it more memorable? Are there any pre-existing references - not just to micromorts, but to comparing them to other probabilities - which I've missed?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yNLgHMnKud3sC29Ai", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 5, "extendedScore": null, "score": 9.137428265444326e-07, "legacy": true, "legacyId": "16564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T02:06:52.851Z", "modifiedAt": null, "url": null, "title": "Proposal for \"Open Problems in Friendly AI\"", "slug": "proposal-for-open-problems-in-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:39.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YiyAdz2DcmkR7Nz2H/proposal-for-open-problems-in-friendly-ai", "pageUrlRelative": "/posts/YiyAdz2DcmkR7Nz2H/proposal-for-open-problems-in-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/YiyAdz2DcmkR7Nz2H/proposal-for-open-problems-in-friendly-ai", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%20for%20%22Open%20Problems%20in%20Friendly%20AI%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%20for%20%22Open%20Problems%20in%20Friendly%20AI%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYiyAdz2DcmkR7Nz2H%2Fproposal-for-open-problems-in-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%20for%20%22Open%20Problems%20in%20Friendly%20AI%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYiyAdz2DcmkR7Nz2H%2Fproposal-for-open-problems-in-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYiyAdz2DcmkR7Nz2H%2Fproposal-for-open-problems-in-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 377, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Open-Problems-in-Friendly-Artificial-Intelligence.pdf\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Open-FAI-preview-screenshot.png\" alt=\"\" /></a><a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">One</a> <a href=\"/lw/cok/funding_good_research/\">more</a> <a href=\"/lw/cr0/short_primers_on_crucial_topics/\">project</a> SI is considering...</p>\n<p>When I was hired as an intern for SI in April 2011, one of my first proposals was that SI create a technical document called <em>Open Problems in Friendly Artificial Intelligence</em>. (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Open-Problems-in-Friendly-Artificial-Intelligence.pdf\">Here</a> is a preview of what the document would be like.)</p>\n<p>When someone becomes persuaded that Friendly AI is important, their first question is often: \"Okay, so what's the technical research agenda?\"</p>\n<p><a href=\"http://lukeprog.com/SaveTheWorld.html\">So You Want to Save the World</a> maps out some broad <em>categories of research questions</em>, but it doesn't explain what the <em>technical research agenda</em>&nbsp;is. In fact, SI hasn't yet explained much of the technical research agenda yet.</p>\n<p><em>Much</em>&nbsp;of the technical research agenda should be kept secret for the same reasons you might want to keep secret the DNA for a synthesized supervirus. But <em>some</em>&nbsp;of the Friendly AI technical research agenda is safe to explain so that a broad research community can contribute to it.</p>\n<p>This research agenda includes:</p>\n<ul>\n<li>Second-order logical version of Solomonoff induction.</li>\n<li>Non-Cartesian version of Solomonoff induction.</li>\n<li>Construing utility functions from psychologically realistic models of human decision processes.</li>\n<li>Formalizations of value extrapolation. (Like <a href=\"http://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">Christiano's attempt</a>.)</li>\n<li>Microeconomic models of self-improving systems (e.g. takeoff speeds).</li>\n<li>...and several others open problems.</li>\n</ul>\n<p>The goal would be to define the open problems as formally and precisely as possible. Some will be more formalizable than others, at this stage. (As a model for this kind of document, see Marcus Hutter's <a href=\"http://www.hutter1.net/ai/aixiopen.pdf\">Open Problems in Universal Induction and Intelligence</a>.)</p>\n<p>Nobody knows the open problems in Friendly AI research better than <a href=\"http://yudkowsky.net/\">Eliezer</a>, so it would probably be best to approach the project this way:</p>\n<ol>\n<li>Eliezer spends a month writing an \"Open Problems in Friendly AI\" sequence for Less Wrong.</li>\n<li>Luke organizes a (fairly large) research team for presenting these open problems with greater clarity and thoroughness, in the mainstream academic form.</li>\n<li>These researchers collaborate for several months to put together the document, involving Eliezer when necessary.</li>\n<li>SI publishes the final document, possibly in a journal.</li>\n</ol>\n<p>Estimated cost:</p>\n<ul>\n<li>2 months of Eliezer's time.</li>\n<li>150 hours of Luke's time.</li>\n<li>$40,000 for contributed hours from staff researchers, remote researchers, and perhaps domain experts (as consultants) from mainstream academia.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Pa2SdZsLFmqhs42Do": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YiyAdz2DcmkR7Nz2H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 33, "extendedScore": null, "score": 9.137468352865923e-07, "legacy": true, "legacyId": "16531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "mYuMdmMmGM7fFj382", "qmiaJKkBNk2gGTkuG", "LFfizpc5YupK7pWNv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T02:13:47.486Z", "modifiedAt": null, "url": null, "title": "Building the AI Risk Research Community", "slug": "building-the-ai-risk-research-community", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kkkCuCLiT8G9gDEJK/building-the-ai-risk-research-community", "pageUrlRelative": "/posts/kkkCuCLiT8G9gDEJK/building-the-ai-risk-research-community", "linkUrl": "https://www.lesswrong.com/posts/kkkCuCLiT8G9gDEJK/building-the-ai-risk-research-community", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20the%20AI%20Risk%20Research%20Community&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20the%20AI%20Risk%20Research%20Community%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkkkCuCLiT8G9gDEJK%2Fbuilding-the-ai-risk-research-community%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20the%20AI%20Risk%20Research%20Community%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkkkCuCLiT8G9gDEJK%2Fbuilding-the-ai-risk-research-community", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkkkCuCLiT8G9gDEJK%2Fbuilding-the-ai-risk-research-community", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 812, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p><a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">Yet</a> <a href=\"/r/discussion/lw/cok/funding_good_research/\">another</a> <a href=\"/lw/cr0/short_primers_on_crucial_topics/\">way</a> <a href=\"/lw/cr7/proposal_for_open_problems_in_friendly_ai/\">to</a> purchase reductions in AI risk may be to grow the AI risk research community.</p>\n<p>The AI risk research community is pretty small. It currently consists of:</p>\n<ul>\n<li>4-ish AI risk researchers at the Singularity Institute. (Eliezer is helping to launch CFAR before he goes back to AI risk research. The AI risk research done at SI right now is: about 40% of Carl, 25% of me, plus large and small fractions of various remote researchers, most significantly about 90% of&nbsp;<a href=\"http://www.xuenay.net/\">Kaj Sotala</a>.)</li>\n<li>4-ish AI risk researchers at the Future of Humanity Institute: Nick Bostrom, Anders Sandberg, Stuart Armstrong, Vincent Mueller. (This number might be wrong. It seems that Nick and Stuart are working basically full-time on AI risk right now, but I'm not sure about Anders and Vincent. Also, FHI should be hiring someone shortly with the <a href=\"http://data.ox.ac.uk/doc/vacancy/102060\">Tamas Research Fellowship</a> money. Finally, note that FHI has a broader mission than AI risk, so while they are focusing on AI risk while Nick works on his <em>Superintelligence</em>&nbsp;book, they will probably return to other subjects sometime thereafter.)</li>\n<li>0.6-ish AI risk researchers at <a href=\"http://www.leverageresearch.org/\">Leverage Research</a>, maybe?</li>\n<li>0.2-ish AI risk researchers at <a href=\"http://www.bmsis.org/gcri\">GCRi</a>, maybe?</li>\n<li>Nobody yet at <a href=\"http://cser.org/\">CSER</a>, but maybe 1-2 people in the relatively near future?</li>\n<li>Occasionally, something useful might come from mainstream <a href=\"http://en.wikipedia.org/wiki/Machine_ethics\">machine ethics</a>, but they mostly aren't focused on problems of machine superintelligence (yet).</li>\n<li>Small fractions of some people in the broader AI risk community, e.g. <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Ben Goertzel</a>,&nbsp;<a href=\"http://consc.net/papers/singreply.pdf\">David Chalmers</a>, and&nbsp;<a href=\"/user/Wei_Dai/submitted/\">Wei Dai</a>.</li>\n</ul>\n<p>Obviously, a larger AI risk research community could be more productive. (It could also grow to include more people but fail to do actually <em>useful</em>&nbsp;work, like so many academic disciplines. But there are ways to push such a small field in useful directions as it grows.)</p>\n<p>So, how would one grow the AI risk research community? Here are some methods:</p>\n<ol>\n<li>Make it easier for AI risk researchers to do their work, by providing a well-organized platform of work from which they can build. Nick's <em>Superintelligence</em>&nbsp;book will help with that. So would a <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a>. So do sites like <a href=\"http://www.existential-risk.org/\">Existential-Risk.org</a>, <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>, <a href=\"http://friendly-ai.com/\">Friendly-AI.com</a>, <a href=\"http://friendlyai.tumblr.com/\">Friendly AI Research</a>, and SI's <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>. So does my <a href=\"http://intelligence.org/upload/AI%20Risk%20Bibliography%202012.pdf\">AI Risk Bibliography 2012</a>, and so do \"basics\" or \"survey\" articles like <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a>, <a href=\"http://www.consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>, and&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>. So do helpful lists like <a href=\"https://docs.google.com/spreadsheet/pub?key=0AvoX2xCTgYnWdFhzanBvSllRaHROVW90eDctZVhPMFE&amp;output=html\">journals that may publish articles related to AI risk</a>. (If you don't think such things are useful, then you probably don't know what it's like to be a researcher trying to develop papers in the field. When I send my AI risk bibliography and my list of \"journals that may publish articles related to AI risk\" to AI risk researchers, I get back emails that say \"thank you\" with multiple exclamation points.)</li>\n<li>Run an annual conference for researchers, put out a call for papers, etc. This brings researchers together and creates a community. The <a href=\"http://agi-conf.org/\">AGI conference series</a> did this for AGI. The new <a href=\"http://agi-conf.org/2012/call-for-papers/\">AGI Impacts sessions</a> at AGI-12 could potentially be grown into an AGI Impacts conference series that would effectively be an AI Risk conference series.</li>\n<li><em>Maybe</em>&nbsp;launch a journal for AI risk papers. Like a conference, this can to some degree bring the community closer. It can also provide a place to publish articles that don't fit within the scope of any other existing journals. I say \"maybe\" on this one because it can be costly to run a journal <em>well</em>, and there are <a href=\"https://docs.google.com/spreadsheet/pub?key=0AvoX2xCTgYnWdFhzanBvSllRaHROVW90eDctZVhPMFE&amp;output=html\">plenty of journals</a> already that will publish papers on AI risk.</li>\n<li>Give out <a href=\"/r/discussion/lw/cok/funding_good_research/\">grants</a> for AI risk research.</li>\n</ol>\n<p>Here's just <em>one example</em> of what SI is <em>currently</em>&nbsp;doing to help grow the AI risk research community.</p>\n<p style=\"padding-left: 30px;\">Writing \"<strong>Responses to Catastrophic AGI Risk</strong>\": A journal-bound summary of the AI risk problem, and a taxonomy of the societal proposals (e.g. denial of the risk, no action, legal and economic controls, differential technological development) and AI design proposals (e.g. AI confinement, chaining, Oracle AI, FAI) that have been made.</p>\n<p style=\"padding-left: 30px;\"><em>Estimated final cost</em>: $5,000 for Kaj's time, $500 for other remote research, 30 hours of Luke's time.</p>\n<p>Now, here's a list of things SI <em>could</em>&nbsp;be doing to help grow the AI risk research community:</p>\n<ul>\n<li>Creating a&nbsp;<a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a>. Estimated cost:&nbsp;1,920 hours of SI staff time (over two years), $384,000 for remote researchers and writers, and $30,000 for wiki design, development, and hosting costs.</li>\n<li>Helping to grow the AGI Impacts sessions at AGI-12 into an AGI Impacts conference. (No cost estimate yet.)</li>\n<li>Writing&nbsp;<em><a href=\"/lw/cr7/proposal_for_open_problems_in_friendly_ai/\">Open Problems in Friendly AI</a></em>. Estimated cost: 2 months of Eliezer's time, 250 hours of Luke's time, $40,000 for internal and external researchers.</li>\n<li>Writing more \"basics\" and \"survey\" articles on AI risk topics.</li>\n<li>Giving out <a href=\"/r/discussion/lw/cok/funding_good_research/\">grants</a> for AI risk research.</li>\n</ul>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kkkCuCLiT8G9gDEJK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 9.137499299495253e-07, "legacy": true, "legacyId": "16530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "mYuMdmMmGM7fFj382", "qmiaJKkBNk2gGTkuG", "LFfizpc5YupK7pWNv", "YiyAdz2DcmkR7Nz2H"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T03:13:04.149Z", "modifiedAt": null, "url": null, "title": "How to Purchase AI Risk Reduction", "slug": "how-to-purchase-ai-risk-reduction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.625Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8LrrHdt8HSvZFKmRb/how-to-purchase-ai-risk-reduction", "pageUrlRelative": "/posts/8LrrHdt8HSvZFKmRb/how-to-purchase-ai-risk-reduction", "linkUrl": "https://www.lesswrong.com/posts/8LrrHdt8HSvZFKmRb/how-to-purchase-ai-risk-reduction", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Purchase%20AI%20Risk%20Reduction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Purchase%20AI%20Risk%20Reduction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8LrrHdt8HSvZFKmRb%2Fhow-to-purchase-ai-risk-reduction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Purchase%20AI%20Risk%20Reduction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8LrrHdt8HSvZFKmRb%2Fhow-to-purchase-ai-risk-reduction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8LrrHdt8HSvZFKmRb%2Fhow-to-purchase-ai-risk-reduction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>I'm writing a series of discussion posts on <strong>how to purchase AI risk reduction</strong> (through <a href=\"http://intelligence.org/donate/\">donations to the Singularity Institute</a>, anyway; other x-risk organizations will have to speak for themselves about their plans).</p>\n<p>Each post outlines a concrete proposal, with cost estimates:</p>\n<ul>\n<li><a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">A scholarly AI risk wiki</a></li>\n<li><a href=\"/r/discussion/lw/cok/funding_good_research/\">Funding good research</a></li>\n<li><a href=\"/lw/cr0/short_primers_on_crucial_topics/\">Short primers on crucial topics</a></li>\n<li><a href=\"/lw/cr7/proposal_for_open_problems_in_friendly_ai/\">\"Open Problems in Friendly AI\"</a></li>\n<li><a href=\"/r/discussion/lw/cr6/building_the_ai_risk_research_community/\">Building the AI risk research community</a></li>\n<li><a href=\"/r/discussion/lw/cs7/reaching_young_mathcompsci_talent/\">Reaching young math/compsci talent</a></li>\n<li><a href=\"/r/discussion/lw/cu6/raising_safetyconsciousness_among_agi_researchers/\">Raising safety-consciousness among AGI researchers</a></li>\n<li><a href=\"/r/discussion/lw/cua/strategic_research_on_ai_risk/\">Strategic research on AI risk</a></li>\n<li><a href=\"/r/discussion/lw/cv9/building_toward_a_friendly_ai_team/\">Building toward a Friendly AI team</a></li>\n</ul>\n<div>Also see John Maxwell's <a href=\"/lw/d34/brainstorming_additional_ideas_on_how_to_purchase/\">Brainstorming additional AI risk reduction ideas</a>.</div>\n<p>(For a quick primer on AI risk, see <em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>.)</p>\n<p><a href=\"http://facingthesingularity.com/\"><img style=\"float: left;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/FtS-medium.png\" alt=\"\" width=\"700\" height=\"293\" /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8LrrHdt8HSvZFKmRb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "16566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mYuMdmMmGM7fFj382", "qmiaJKkBNk2gGTkuG", "LFfizpc5YupK7pWNv", "YiyAdz2DcmkR7Nz2H", "kkkCuCLiT8G9gDEJK", "4dFzBkpjx6aHZpjqL", "6Lg8RWL9pEvoAeEvr", "D9xvHv3hZj56cbTEe", "p4Gd8pRcbnKo46hus", "8HPsRYKE2pYHtqRhw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T04:01:13.236Z", "modifiedAt": null, "url": null, "title": "Open Thread, June 1-15, 2012", "slug": "open-thread-june-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SgTpxJEKgtGu98pBA/open-thread-june-1-15-2012", "pageUrlRelative": "/posts/SgTpxJEKgtGu98pBA/open-thread-june-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/SgTpxJEKgtGu98pBA/open-thread-june-1-15-2012", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20June%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20June%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSgTpxJEKgtGu98pBA%2Fopen-thread-june-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20June%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSgTpxJEKgtGu98pBA%2Fopen-thread-june-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSgTpxJEKgtGu98pBA%2Fopen-thread-june-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>\n<div id=\"entry_t3_cg7\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>\n</div>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SgTpxJEKgtGu98pBA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.13798040907384e-07, "legacy": true, "legacyId": "16568", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 260, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T05:29:23.750Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Causality and Moral Responsibility", "slug": "seq-rerun-causality-and-moral-responsibility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fmoB2JZFoXDYfpxsr/seq-rerun-causality-and-moral-responsibility", "pageUrlRelative": "/posts/fmoB2JZFoXDYfpxsr/seq-rerun-causality-and-moral-responsibility", "linkUrl": "https://www.lesswrong.com/posts/fmoB2JZFoXDYfpxsr/seq-rerun-causality-and-moral-responsibility", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Causality%20and%20Moral%20Responsibility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Causality%20and%20Moral%20Responsibility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfmoB2JZFoXDYfpxsr%2Fseq-rerun-causality-and-moral-responsibility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Causality%20and%20Moral%20Responsibility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfmoB2JZFoXDYfpxsr%2Fseq-rerun-causality-and-moral-responsibility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfmoB2JZFoXDYfpxsr%2Fseq-rerun-causality-and-moral-responsibility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/ra/causality_and_moral_responsibility/\">Causality and Moral Responsibility</a> was originally published on 13 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Causality_and_Moral_Responsibility\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Knowing that you are a deterministic system does not make you any less responsible for the consequences of your actions. You still make your decisions; you do have psychological traits, and experiences, and goals. Determinism doesn't change any of that.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cr1/seq_rerun_against_devils_advocacy/\">Against Devil's Advocacy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fmoB2JZFoXDYfpxsr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.138375325747264e-07, "legacy": true, "legacyId": "16570", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FqJGfSrXphrcwpiZe", "NLQmKwDqoCEgHQjd3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T06:37:44.383Z", "modifiedAt": null, "url": null, "title": "Which cognitive biases should we trust in? ", "slug": "which-cognitive-biases-should-we-trust-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.209Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Andy_McKenzie", "createdAt": "2009-02-28T21:46:45.283Z", "isAdmin": false, "displayName": "Andy_McKenzie"}, "userId": "7PFnr3J3uCGSfjnZJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8arFF9SdstBqz7c8K/which-cognitive-biases-should-we-trust-in", "pageUrlRelative": "/posts/8arFF9SdstBqz7c8K/which-cognitive-biases-should-we-trust-in", "linkUrl": "https://www.lesswrong.com/posts/8arFF9SdstBqz7c8K/which-cognitive-biases-should-we-trust-in", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20cognitive%20biases%20should%20we%20trust%20in%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20cognitive%20biases%20should%20we%20trust%20in%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8arFF9SdstBqz7c8K%2Fwhich-cognitive-biases-should-we-trust-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20cognitive%20biases%20should%20we%20trust%20in%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8arFF9SdstBqz7c8K%2Fwhich-cognitive-biases-should-we-trust-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8arFF9SdstBqz7c8K%2Fwhich-cognitive-biases-should-we-trust-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 816, "htmlBody": "<p>There have been (at least) a couple of attempts on LW to make Anki flashcards from Wikipedia's famous <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">List of Cognitive Biases</a>, <a href=\"/lw/3px/anki_deck_for_biases_and_fallacies/\">here</a> and <a href=\"/lw/5t9/anki_flashcard_deck_cognitive_biases_and_related/\">here</a>. However, stylistically&nbsp;they are not my type of flashcard, with too much info in the \"answer\" section.&nbsp;</p>\n<p>Further, and more troublingly, I'm not sure whether all of the biases in the flashcards are real, generalizable effects; or, if they are real, whether they have effect sizes large enough to be worth the effort to learn &amp;&nbsp;disseminate. Psychology is an academic discipline with all of the <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">baggage that entails</a>.&nbsp;Psychology is also one of the least tangible sciences, which is not helpful.</p>\n<p>There are studies showing that Wikipedia is <a href=\"http://en.wikipedia.org/wiki/Reliability_of_Wikipedia\">no less reliable</a> than more conventional sources, but this is in aggregate, and it seems plausible (though difficult to detect without diligently checking sources) that the set of cognitive bias articles on Wikipedia has high variance in quality.</p>\n<p>We do have some knowledge of how many of them were made, in that LW user nerfhammer <a href=\"/lw/ap1/new_cognitive_bias_articles_on_wikipedia_update/\">wrote a bunch</a>. But, as far as I can tell, s/he didn't discuss how s/he selected biases to include. (Though, s/he is obviously quite knowledgable on the subject, see e.g. <a href=\"/lw/a8k/ambiguity_in_cognitive_bias_names_a_refresher/\">here</a>.)</p>\n<p>As the articles stand today, many (e.g., <a href=\"http://en.wikipedia.org/wiki/Pro-innovation_bias\">here</a>, <a href=\"http://en.wikipedia.org/wiki/Pseudocertainty_effect\">here</a>, <a href=\"http://en.wikipedia.org/wiki/Semmelweis_reflex\">here</a>, <a href=\"http://en.wikipedia.org/wiki/Rhyme_as_reason_effect\">here</a>, and <a href=\"http://en.wikipedia.org/wiki/Conservatism_(Bayesian)\">here</a>) only cite research from one study/lab. I do not want to come across as whining: the authors who wrote these on Wikipedia are awesome. But, as a consumer the lack of independent replication makes me nervous. I don't want to contribute to information cascades.&nbsp;</p>\n<p>Nevertheless, I do still want to make flashcards for at least some of these biases, because I am relatively sure that there are some strong, important, widespread biases out there.&nbsp;</p>\n<p>So, I am asking LW whether you all have any ideas about, on the meta level,&nbsp;</p>\n<p>1) how we should go about deciding/indexing which articles/biases capture legit effects worth knowing,</p>\n<p>and, on the object level,</p>\n<p>2) which of the biases/heuristics/fallacies are actually legit (like, a list).&nbsp;</p>\n<p>Here are some of my ideas. First, for how to decide:&nbsp;</p>\n<p>- Only include biases that are mentioned by prestigious sources like Kahneman in his new book.&nbsp;<strong>Upside</strong>: authoritative.&nbsp;<strong>Downside</strong>: potentially throwing out some good info and putting too much faith in one source.&nbsp;</p>\n<p>- Only include biases whose Wikipedia articles cite at least two primary articles that share none of the same authors.&nbsp;<strong>Upside</strong>: establishes some degree of consensus in the field.&nbsp;<strong>Downside</strong>: won't actually vet the articles for quality, and a presumably false assumption that the Wikipedia pages will reflect the state of knowledge in the field.&nbsp;</p>\n<p>- Search for the name of the bias (or any bold, alternative names on Wikipedia) on Google scholar, and only accept those with, say, &gt;30 citations. <strong>Upside</strong>: less of a sampling bias of what is included on Wikipedia, which is likely to be somewhat arbitrary. <strong>Downside</strong>: information cascades occur in academia too, and this method doesn't filter for actual experimental evidence (e.g., there could be lots of reviews discussing the idea). &nbsp;</p>\n<p>- Make some sort of a voting system where experts (surely some frequent this site) can weigh in on what they think of the primary evidence for a given bias.&nbsp;<strong>Upside: </strong>rather than counting articles, evaluates actual evidence for the bias.<strong>&nbsp;Downside: </strong>seems&nbsp;hard to get the scale (~ 8 - 12 + people voting) to make this useful.&nbsp;</p>\n<p>- Build some arbitrarily weighted rating scale that takes into account some or all of the above.&nbsp;<strong>Upside</strong>: meta.&nbsp;<strong>Downside</strong>: garbage in, garbage out, and the first three features seem highly correlated anyway.&nbsp;</p>\n<p>Second, for which biases to include. I'm just going off of which ones I have heard of and/or look legit on a fairly quick run through. Note that those annotated with a (?) are ones I am especially unsure about.&nbsp;</p>\n<p>- anchoring</p>\n<p>- availability</p>\n<p>- bandwagon effect</p>\n<p>- base rate neglect</p>\n<p>- choice-supportive bias</p>\n<p>- clustering illusion</p>\n<p>- confirmation bias</p>\n<p>- conjunction fallacy (is subadditivity a subset of this?)&nbsp;</p>\n<p>- conservatism (?)&nbsp;</p>\n<p>- context effect (aka state-dependent memory)&nbsp;</p>\n<p>- curse of knowledge (?)&nbsp;</p>\n<p>- contrast effect</p>\n<p>- decoy effect (aka&nbsp;independence of irrelevant alternatives)&nbsp;</p>\n<p>-&nbsp;Dunning&ndash;Kruger effect (?)&nbsp;</p>\n<p>- duration neglect</p>\n<p>- empathy gap</p>\n<p>- expectation bias</p>\n<p>- framing</p>\n<p>- gambler's fallacy</p>\n<p>- halo effect</p>\n<p>- hindsight bias</p>\n<p>- hyperbolic discounting&nbsp;</p>\n<p>- illusion of control</p>\n<p>- illusion of transparency</p>\n<p>- illusory correlation</p>\n<p>- illusory superiority</p>\n<p>- illusion of validity (?)&nbsp;</p>\n<p>- impact bias</p>\n<p>- information bias (? aka failure to consider value of information)</p>\n<p>- in-group bias (this is also clearly real, but I'm also <a href=\"http://andymckenzie.blogspot.com/2010/07/trade-off-5-loyalty-vs-universality.html\">not sure I'd call it a bias</a>)&nbsp;</p>\n<p>- escalation of commitment (aka sunk cost/loss aversion/endowment effect; note, <a href=\"http://www.gwern.net/Sunk%20cost\">contra Gwern</a>, that I do think this is a useful fallacy to know about, if overrated)</p>\n<p>- false consensus (related to projection bias)&nbsp;</p>\n<p>-&nbsp;Forer effect</p>\n<p>- fundamental attribution error (related to the just-world hypothesis)&nbsp;</p>\n<p>- familiarity principle (aka mere exposure effect)&nbsp;</p>\n<p>- moral licensing (aka moral credential)&nbsp;</p>\n<p>- negativity bias (seems controversial &amp; it's troubling that there is also a positivity bias)&nbsp;</p>\n<p>- normalcy bias (related to existential risk?)&nbsp;</p>\n<p>- omission bias</p>\n<p>- optimism bias (related to overconfidence)</p>\n<p>- outcome bias (aka moral luck)&nbsp;</p>\n<p>- outgroup homogeneity bias</p>\n<p>- peak-end rule</p>\n<p>- primacy</p>\n<p>- planning fallacy</p>\n<p>- reactance (aka contrarianism)&nbsp;</p>\n<p>- recency</p>\n<p>- representativeness</p>\n<p>- self-serving bias&nbsp;</p>\n<p>- social desirability bias</p>\n<p>- status quo bias</p>\n<p>Happy to hear any thoughts!&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb105": 2, "5f5c37ee1b5cdee568cfb1a0": 2, "5f5c37ee1b5cdee568cfb17d": 2, "5f5c37ee1b5cdee568cfb182": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8arFF9SdstBqz7c8K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 29, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "16575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NZY2eAALzTTuYzR2t", "BH9ysqhTmsF2WaftT", "XKfPvj9gtrskGuP3v", "xXdeJpKnyLwKWncjP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T13:39:34.131Z", "modifiedAt": null, "url": null, "title": "June 2012 Media Thread", "slug": "june-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/myFE8Nrw37kWQHdYX/june-2012-media-thread", "pageUrlRelative": "/posts/myFE8Nrw37kWQHdYX/june-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/myFE8Nrw37kWQHdYX/june-2012-media-thread", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20June%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJune%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmyFE8Nrw37kWQHdYX%2Fjune-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=June%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmyFE8Nrw37kWQHdYX%2Fjune-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmyFE8Nrw37kWQHdYX%2Fjune-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the monthly thread for posting media of various types that you've found that you enjoy. I find that reading the sequences makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/media_thread/\">older threads</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres, which&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/94z/january_2012_media_thread/5kos\">I was apparently too dumb to do</a>.</li>\n<li>If you have a thread to add, such as a video game thread or an Anime thread, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "myFE8Nrw37kWQHdYX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 9.140571247137388e-07, "legacy": true, "legacyId": "16589", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T13:47:47.451Z", "modifiedAt": null, "url": null, "title": "Son of Shit Rationalists Say", "slug": "son-of-shit-rationalists-say", "viewCount": null, "lastCommentedAt": "2021-02-28T20:01:11.995Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p5jwZE6hTz92sSCcY/son-of-shit-rationalists-say", "pageUrlRelative": "/posts/p5jwZE6hTz92sSCcY/son-of-shit-rationalists-say", "linkUrl": "https://www.lesswrong.com/posts/p5jwZE6hTz92sSCcY/son-of-shit-rationalists-say", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Son%20of%20Shit%20Rationalists%20Say&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASon%20of%20Shit%20Rationalists%20Say%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp5jwZE6hTz92sSCcY%2Fson-of-shit-rationalists-say%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Son%20of%20Shit%20Rationalists%20Say%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp5jwZE6hTz92sSCcY%2Fson-of-shit-rationalists-say", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp5jwZE6hTz92sSCcY%2Fson-of-shit-rationalists-say", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>A long time ago, in the colder seasons, I <a href=\"/lw/9ki/shit_rationalists_say/\">asked for suggestions</a> for a Shit Rationalists Say video. Due to other concerns it took me this long to put it together, and the meme has long since passed. However, here it is.</p>\n<p><a href=\"http://www.youtube.com/watch?v=jlT3MeCzVao\">Shit Rationalists Say</a></p>\n<p>It is my first time in front of a camera, so I'm shakey. But I learned, and there it is.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"p8nXWqwPH7mPSZf6p": 1, "izp6eeJJEg9v5zcur": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p5jwZE6hTz92sSCcY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 79, "extendedScore": null, "score": 0.000176, "legacy": true, "legacyId": "16592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 79, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8xQ8hTxo6Rk2qqZfj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T14:24:59.902Z", "modifiedAt": null, "url": null, "title": "How can I argue without people online and not come out feeling bad?", "slug": "how-can-i-argue-without-people-online-and-not-come-out", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wantstojoinin", "createdAt": "2012-03-04T08:25:54.624Z", "isAdmin": false, "displayName": "wantstojoinin"}, "userId": "bpCnfLPB7xxfgWmtH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zciNxEAAkZy4u6F33/how-can-i-argue-without-people-online-and-not-come-out", "pageUrlRelative": "/posts/zciNxEAAkZy4u6F33/how-can-i-argue-without-people-online-and-not-come-out", "linkUrl": "https://www.lesswrong.com/posts/zciNxEAAkZy4u6F33/how-can-i-argue-without-people-online-and-not-come-out", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20can%20I%20argue%20without%20people%20online%20and%20not%20come%20out%20feeling%20bad%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20can%20I%20argue%20without%20people%20online%20and%20not%20come%20out%20feeling%20bad%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzciNxEAAkZy4u6F33%2Fhow-can-i-argue-without-people-online-and-not-come-out%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20can%20I%20argue%20without%20people%20online%20and%20not%20come%20out%20feeling%20bad%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzciNxEAAkZy4u6F33%2Fhow-can-i-argue-without-people-online-and-not-come-out", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzciNxEAAkZy4u6F33%2Fhow-can-i-argue-without-people-online-and-not-come-out", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>I wont be the only one here who \"wastes time\" arguing about things they care about online (note: I am referring to web forums and things like subreddits, I am not including Less Wrong whose dynamic is completely different). It seems like something that is worth optimising in some direction.</p>\n<p>The theory behind it is that one should expose themselves to counter-arguments allowing their claims to be attacked so they that have a chance to substantiate them or reject them upon realising they are mistaken.</p>\n<p>In practice they generally follow a pattern that starts with people pointing out what they believe are mistakes then ignoring or intentionally misunderstanding the other party when he refutes or backs up claims.. and ends up with insults, patronising sarcastic remarks and nobody changing their mind about anything.</p>\n<p>I don't particularly care about changing other peoples minds to make them agree with me (well, it would be great but I think it's practically impossible) so one thing I would like is for both people to at least end up feeling good.</p>\n<p>So I'm interested in three things: Do other LWers recognize this pattern <em>now that I have mentioned it</em>?<em> </em>What decision did those that were already aware of it make, in order to optimise this activity?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zciNxEAAkZy4u6F33", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 9.140774813880557e-07, "legacy": true, "legacyId": "16593", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T14:44:27.007Z", "modifiedAt": null, "url": null, "title": "Low Hanging Fruit in Computer Hardware", "slug": "low-hanging-fruit-in-computer-hardware", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:08.235Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "dbXvnf6ofsdsGCrdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NWPhXTgQpKAR9DeZb/low-hanging-fruit-in-computer-hardware", "pageUrlRelative": "/posts/NWPhXTgQpKAR9DeZb/low-hanging-fruit-in-computer-hardware", "linkUrl": "https://www.lesswrong.com/posts/NWPhXTgQpKAR9DeZb/low-hanging-fruit-in-computer-hardware", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Low%20Hanging%20Fruit%20in%20Computer%20Hardware&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALow%20Hanging%20Fruit%20in%20Computer%20Hardware%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNWPhXTgQpKAR9DeZb%2Flow-hanging-fruit-in-computer-hardware%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Low%20Hanging%20Fruit%20in%20Computer%20Hardware%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNWPhXTgQpKAR9DeZb%2Flow-hanging-fruit-in-computer-hardware", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNWPhXTgQpKAR9DeZb%2Flow-hanging-fruit-in-computer-hardware", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 592, "htmlBody": "<h1><span style=\"font-weight: normal;\">Introduction:</span></h1>\n<p><span style=\"font-weight: normal;\">Since my fellow LWers have been making posts about <a href=\"/r/discussion/lw/cjf/shaving_less_long/\">shaving</a>, <a href=\"/lw/cqz/rational_toothpaste_a_case_study/\">dental hygiene</a> and <a href=\"/r/discussion/lw/cru/low_hanging_fruit_basic_bedroom_decorating/\">bedroom decorating</a>, I thought I might as well make a post pointing out low hanging fruit in computer hardware. In this post however, I will not make value calculations. Instead, I am going to provide low-cost sampling methods and let you experiment and decide for yourself whether the gains are worthwhile. It bears pointing out we nowadays use our computers more often than we like to admit, so any small improvements in our computing experience can have tremendous value.</span></p>\n<h1><span style=\"font-weight: normal;\"> Notebooks</span></h1>\n<p><span style=\"font-weight: normal;\"> When comparison-shopping for notebooks, most consumers overlook two important aspects, namely the screen quality and the hard drive performance. In return, most manufacturers try to save costs by fitting their notebooks with the cheapest screens and hard drives they can find. <br /></span></p>\n<h1><span style=\"font-weight: normal;\"> Notebook Monitors:</span></h1>\n<p><em style=\"font-weight: normal;\">Low-cost sampling method:</em><span style=\"font-weight: normal;\"> Compare the screen quality of your notebook with that of your smartphone.</span></p>\n<p><span style=\"font-weight: normal;\"> Most&nbsp;notebook monitors are abysmal. You cannot tilt the screen a little without it shifting in colours, and the contrast ratios are so low the colours become very washed out and the blacks start looking like greys, and don't get me started on the colour accuracy. Watching a beautiful painting or high definition video on such horrible screens is like watching natural scenery through a dirty window.</span></p>\n<p><span style=\"font-weight: normal;\"> There aren't many notebooks in the market with good screens. This is not necessarily bad because it is <a href=\"http://www.ted.com/talks/barry_schwartz_on_the_paradox_of_choice.html\">a good idea to artificially limit your choices while shopping</a>.&nbsp;</span>Some well-known notebooks with good screens at the time of writing: all Apple&nbsp;Macbooks, some ASUS notebooks, upgraded Dell XPS 15 and upgraded&nbsp;Lenovo X220. For further reading, check out Digital Versus'&nbsp;<a href=\"http://www.digitalversus.com/laptop/notebooks-best-10-14-screens-a1528.html\">Notebooks: The Best 10'' to 14'' Screens</a>&nbsp;and&nbsp;<a href=\"http://www.digitalversus.com/laptop/notebooks-best-15-17-screens-a1530.html\">Notebooks: The Best 15&rsquo;&rsquo; to 17&rsquo;&rsquo; Screens</a>. Also read the latest reviews from <a href=\"http://www.anandtech.com/tag/mobile\">Anandtech</a> and&nbsp;<a href=\"/notebookreview.com\">notebookreview.com</a></p>\n<h1><span style=\"font-weight: normal;\"> Notebook hard Drives:</span></h1>\n<h2><em style=\"font-weight: normal;\">Low-cost sampling method:</em><span style=\"font-weight: normal;\"> Watch youtube <a href=\"http://www.youtube.com/watch?v=FQpiZ44GyYU\">videos</a> of <a href=\"http://www.youtube.com/watch?v=S5ZrBM_EPC8\">SSDs</a> in <a href=\"http://www.youtube.com/watch?v=KT75O20uXf8\">action</a>.</span></h2>\n<p><span style=\"font-weight: normal;\">A slow hard drive will not necessarily slow down CPU intensive tasks like gaming or encoding, but few consumers do these tasks on their notebooks. What most consumers actually do is log in their chat client, browse the Internet and maybe listen to music or watch a movie. Their only gauge while performing these tasks is how quickly the OS and applications load, and how quickly they load is almost entirely dependent on the speed of the hard drive.</span></p>\n<p><span style=\"font-weight: normal;\"> The unit you measure the snappiness of a hard drive with is called the access time. The access time of an average desktop hard drive is around 12ms. For notebook hard drives, it may reach 20ms or even more. An SSD has access times of around 0.1ms; very fast indeed and definitely worth upgrading to. SSDs are expensive for the storage they offer, but you are not buying them for their space. You are buying them for their performance.</span></p>\n<h1><span style=\"font-weight: normal;\">Headphones:</span><span style=\"font-weight: normal;\">&nbsp;</span></h1>\n<p><span style=\"font-weight: normal;\">Low-cost sampling method: Buy the Koss KSC-75 for $14. You should find them vastly superior to your average cheap headphones. If not, then you are unlikely to notice a difference when you buy higher end headphones.</span></p>\n<p><span style=\"font-weight: normal;\"> A good starting point when researching headphones would be the comprehensive <a href=\"http://www.head-fi.org/t/433318\">Shootout: 102 Portable Headphones Reviewed</a>&nbsp;written by ljokerl in the forums head-fi.org.<br /> The headphones he gave a value rating of 9.5 or more at the time of writing are:<br />$14 - Koss KSC-75<br />$30 - Panasonic RP-HTF600<br />$50 - Superlux HD668B<br />$159 - Audio-Technica ATH-M50*</span></p>\n<p><span style=\"font-weight: normal;\">* lukeprog has endorsed the Audio-Technica ATH-M50 in his post&nbsp;<a href=\"/r/discussion/lw/cnr/share_your_checklists/\">Share Your Checklists</a>.</span></p>\n<h1><strong>Keyboards:</strong></h1>\n<p><span style=\"font-size: 16px; white-space: pre;\">Mechanical keyboards are the way to go, or so I hear. If you type a lot, they may be worth looking at.</span></p>\n<p><em>Low-cost sampling method:</em>&nbsp;None! I have not used mechanical keyboards before.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NWPhXTgQpKAR9DeZb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 9.140861978256656e-07, "legacy": true, "legacyId": "16588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h1 id=\"Introduction_\"><span style=\"font-weight: normal;\">Introduction:</span></h1>\n<p><span style=\"font-weight: normal;\">Since my fellow LWers have been making posts about <a href=\"/r/discussion/lw/cjf/shaving_less_long/\">shaving</a>, <a href=\"/lw/cqz/rational_toothpaste_a_case_study/\">dental hygiene</a> and <a href=\"/r/discussion/lw/cru/low_hanging_fruit_basic_bedroom_decorating/\">bedroom decorating</a>, I thought I might as well make a post pointing out low hanging fruit in computer hardware. In this post however, I will not make value calculations. Instead, I am going to provide low-cost sampling methods and let you experiment and decide for yourself whether the gains are worthwhile. It bears pointing out we nowadays use our computers more often than we like to admit, so any small improvements in our computing experience can have tremendous value.</span></p>\n<h1 id=\"_Notebooks\"><span style=\"font-weight: normal;\"> Notebooks</span></h1>\n<p><span style=\"font-weight: normal;\"> When comparison-shopping for notebooks, most consumers overlook two important aspects, namely the screen quality and the hard drive performance. In return, most manufacturers try to save costs by fitting their notebooks with the cheapest screens and hard drives they can find. <br></span></p>\n<h1 id=\"_Notebook_Monitors_\"><span style=\"font-weight: normal;\"> Notebook Monitors:</span></h1>\n<p><em style=\"font-weight: normal;\">Low-cost sampling method:</em><span style=\"font-weight: normal;\"> Compare the screen quality of your notebook with that of your smartphone.</span></p>\n<p><span style=\"font-weight: normal;\"> Most&nbsp;notebook monitors are abysmal. You cannot tilt the screen a little without it shifting in colours, and the contrast ratios are so low the colours become very washed out and the blacks start looking like greys, and don't get me started on the colour accuracy. Watching a beautiful painting or high definition video on such horrible screens is like watching natural scenery through a dirty window.</span></p>\n<p><span style=\"font-weight: normal;\"> There aren't many notebooks in the market with good screens. This is not necessarily bad because it is <a href=\"http://www.ted.com/talks/barry_schwartz_on_the_paradox_of_choice.html\">a good idea to artificially limit your choices while shopping</a>.&nbsp;</span>Some well-known notebooks with good screens at the time of writing: all Apple&nbsp;Macbooks, some ASUS notebooks, upgraded Dell XPS 15 and upgraded&nbsp;Lenovo X220. For further reading, check out Digital Versus'&nbsp;<a href=\"http://www.digitalversus.com/laptop/notebooks-best-10-14-screens-a1528.html\">Notebooks: The Best 10'' to 14'' Screens</a>&nbsp;and&nbsp;<a href=\"http://www.digitalversus.com/laptop/notebooks-best-15-17-screens-a1530.html\">Notebooks: The Best 15\u2019\u2019 to 17\u2019\u2019 Screens</a>. Also read the latest reviews from <a href=\"http://www.anandtech.com/tag/mobile\">Anandtech</a> and&nbsp;<a href=\"/notebookreview.com\">notebookreview.com</a></p>\n<h1 id=\"_Notebook_hard_Drives_\"><span style=\"font-weight: normal;\"> Notebook hard Drives:</span></h1>\n<h2 id=\"Low_cost_sampling_method__Watch_youtube_videos_of_SSDs_in_action_\"><em style=\"font-weight: normal;\">Low-cost sampling method:</em><span style=\"font-weight: normal;\"> Watch youtube <a href=\"http://www.youtube.com/watch?v=FQpiZ44GyYU\">videos</a> of <a href=\"http://www.youtube.com/watch?v=S5ZrBM_EPC8\">SSDs</a> in <a href=\"http://www.youtube.com/watch?v=KT75O20uXf8\">action</a>.</span></h2>\n<p><span style=\"font-weight: normal;\">A slow hard drive will not necessarily slow down CPU intensive tasks like gaming or encoding, but few consumers do these tasks on their notebooks. What most consumers actually do is log in their chat client, browse the Internet and maybe listen to music or watch a movie. Their only gauge while performing these tasks is how quickly the OS and applications load, and how quickly they load is almost entirely dependent on the speed of the hard drive.</span></p>\n<p><span style=\"font-weight: normal;\"> The unit you measure the snappiness of a hard drive with is called the access time. The access time of an average desktop hard drive is around 12ms. For notebook hard drives, it may reach 20ms or even more. An SSD has access times of around 0.1ms; very fast indeed and definitely worth upgrading to. SSDs are expensive for the storage they offer, but you are not buying them for their space. You are buying them for their performance.</span></p>\n<h1 id=\"Headphones__\"><span style=\"font-weight: normal;\">Headphones:</span><span style=\"font-weight: normal;\">&nbsp;</span></h1>\n<p><span style=\"font-weight: normal;\">Low-cost sampling method: Buy the Koss KSC-75 for $14. You should find them vastly superior to your average cheap headphones. If not, then you are unlikely to notice a difference when you buy higher end headphones.</span></p>\n<p><span style=\"font-weight: normal;\"> A good starting point when researching headphones would be the comprehensive <a href=\"http://www.head-fi.org/t/433318\">Shootout: 102 Portable Headphones Reviewed</a>&nbsp;written by ljokerl in the forums head-fi.org.<br> The headphones he gave a value rating of 9.5 or more at the time of writing are:<br>$14 - Koss KSC-75<br>$30 - Panasonic RP-HTF600<br>$50 - Superlux HD668B<br>$159 - Audio-Technica ATH-M50*</span></p>\n<p><span style=\"font-weight: normal;\">* lukeprog has endorsed the Audio-Technica ATH-M50 in his post&nbsp;<a href=\"/r/discussion/lw/cnr/share_your_checklists/\">Share Your Checklists</a>.</span></p>\n<h1 id=\"Keyboards_\"><strong>Keyboards:</strong></h1>\n<p><span style=\"font-size: 16px; white-space: pre;\">Mechanical keyboards are the way to go, or so I hear. If you type a lot, they may be worth looking at.</span></p>\n<p><em>Low-cost sampling method:</em>&nbsp;None! I have not used mechanical keyboards before.</p>", "sections": [{"title": "Introduction:", "anchor": "Introduction_", "level": 1}, {"title": " Notebooks", "anchor": "_Notebooks", "level": 1}, {"title": " Notebook Monitors:", "anchor": "_Notebook_Monitors_", "level": 1}, {"title": " Notebook hard Drives:", "anchor": "_Notebook_hard_Drives_", "level": 1}, {"title": "Low-cost sampling method: Watch youtube videos of SSDs in action.", "anchor": "Low_cost_sampling_method__Watch_youtube_videos_of_SSDs_in_action_", "level": 2}, {"title": "Headphones:\u00a0", "anchor": "Headphones__", "level": 1}, {"title": "Keyboards:", "anchor": "Keyboards_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3xdpKgFakvYka7wag", "NHuLAS3oKZWr2X9hP", "vr8yoKg7R52mTKDMm", "XKXsJAFnnBLeqfPiY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T15:00:29.005Z", "modifiedAt": null, "url": null, "title": "Questions on SI Research", "slug": "questions-on-si-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.102Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Y258DNcCKQvNGj8o/questions-on-si-research", "pageUrlRelative": "/posts/8Y258DNcCKQvNGj8o/questions-on-si-research", "linkUrl": "https://www.lesswrong.com/posts/8Y258DNcCKQvNGj8o/questions-on-si-research", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20on%20SI%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20on%20SI%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Y258DNcCKQvNGj8o%2Fquestions-on-si-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20on%20SI%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Y258DNcCKQvNGj8o%2Fquestions-on-si-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Y258DNcCKQvNGj8o%2Fquestions-on-si-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>Hello LessWrong,</p>\n<p>As one of my assignments at the Singularity Institute (SI), I am writing a research FAQ answering the most frequently asked questions regarding the Singularity Institute's research program.&nbsp;</p>\n<p>For a short summary of what SI is about, see our <a href=\"http://intelligence.org/summary\">concise summary</a>.&nbsp;</p>\n<p>Here are some examples of questions I'm currently planning to include:</p>\n<p>1) who conducts research at SI?</p>\n<p>2) what are the specific research topics being investigated?</p>\n<p>3) what is the history of SI's research program?</p>\n<p>4) where does SI see its research program in 5, 10, and 20 years?</p>\n<p>5) what other organizations conduct research similar to SI?</p>\n<p>Please submit other questions that come to mind below. Unfortunately, due to limited time, we cannot answer every question posed to us. However, I hope to answer some of the questions that receive the most upvotes. Thank you for your participation!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Y258DNcCKQvNGj8o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 9.140933825447105e-07, "legacy": true, "legacyId": "16595", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T15:18:05.845Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Chicago, Copenhagen, Melbourne, Pittsburgh, Sydney", "slug": "weekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:58.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GdEzch7BvzutYqAqe/weekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh", "pageUrlRelative": "/posts/GdEzch7BvzutYqAqe/weekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh", "linkUrl": "https://www.lesswrong.com/posts/GdEzch7BvzutYqAqe/weekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Chicago%2C%20Copenhagen%2C%20Melbourne%2C%20Pittsburgh%2C%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Chicago%2C%20Copenhagen%2C%20Melbourne%2C%20Pittsburgh%2C%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGdEzch7BvzutYqAqe%2Fweekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Chicago%2C%20Copenhagen%2C%20Melbourne%2C%20Pittsburgh%2C%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGdEzch7BvzutYqAqe%2Fweekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGdEzch7BvzutYqAqe%2Fweekly-lw-meetups-chicago-copenhagen-melbourne-pittsburgh", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 469, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/ae\">Second Copenhagen meetup:&nbsp;<span class=\"date\">26 May 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/ag\">Less Wrong Sydney -Social:&nbsp;<span class=\"date\">31 May 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/ai\">Pittsburgh: Making Beliefs Pay Rent:&nbsp;<span class=\"date\">01 June 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/9s\">First Berlin meetup:&nbsp;<span class=\"date\">05 June 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/9p\">Phoenix, Arizona:&nbsp;<span class=\"date\">15 June 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/a6\">Weekly Chicago Meetups Resume 5/26:&nbsp;<span class=\"date\">26 May 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/af\">Melbourne, practical rationality:&nbsp;<span class=\"date\">01 June 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/ac\">Less Wrong Cambridge (MA) first-Sundays meetup:&nbsp;<span class=\"date\">03 June 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/ad\">Less Wrong Cambridge (MA) third-Sundays meetup:&nbsp;<span class=\"date\">17 June 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GdEzch7BvzutYqAqe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.141012757189933e-07, "legacy": true, "legacyId": "16393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T15:43:29.295Z", "modifiedAt": null, "url": null, "title": "Robot ethics [link]", "slug": "robot-ethics-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:57.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fwpqfdn8onMH32Lme/robot-ethics-link", "pageUrlRelative": "/posts/Fwpqfdn8onMH32Lme/robot-ethics-link", "linkUrl": "https://www.lesswrong.com/posts/Fwpqfdn8onMH32Lme/robot-ethics-link", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Robot%20ethics%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARobot%20ethics%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwpqfdn8onMH32Lme%2Frobot-ethics-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Robot%20ethics%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwpqfdn8onMH32Lme%2Frobot-ethics-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwpqfdn8onMH32Lme%2Frobot-ethics-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p><em>The Economist</em> has <a href=\"http://www.economist.com/node/21556234?fsrc=nlw|hig|5-31-2012|1977420|37123855|\">a new article</a> on ethical dilemmas faced by machine designers.</p>\r\n<p>Evidently:</p>\r\n<p>1. In the event of an immoral decision by a machine, neural networks make it too hard to know who is at fault--the programmer, the operator, the manufacturer, or the designer. Thus, neural networks might be a bad idea.</p>\r\n<p>2. Robots' ethical systems ought to resonate with \"most people.\"</p>\r\n<p>3. Proper robot consciences are more likely to arise given greater collaboration among engineers, ethicists, policymakers, and lawyers. Key quotation:</p>\r\n<blockquote>\r\n<p>Both ethicists and engineers stand to benefit from working together: ethicists may gain a greater understanding of their field by trying to teach ethics to machines, and engineers need to reassure society that they are not taking any ethical short-cuts.</p>\r\n</blockquote>\r\n<p>The second clause of the above sentence is quite similar to something Yudkowsky wrote, perhaps more than once, about the value of approaching ethics from an AI standpoint. I do not recall where he wrote it, nor did my search turn up the appropriate post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fwpqfdn8onMH32Lme", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 9.14112654050828e-07, "legacy": true, "legacyId": "16596", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T15:53:54.529Z", "modifiedAt": null, "url": null, "title": "The Truth Points to Itself, Part I", "slug": "the-truth-points-to-itself-part-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:57.873Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arbitrarity", "createdAt": "2010-09-03T20:04:37.975Z", "isAdmin": false, "displayName": "Arbitrarity"}, "userId": "ME2AY34aknt5zpHzZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SMnQQzgMQK7HWWuzp/the-truth-points-to-itself-part-i", "pageUrlRelative": "/posts/SMnQQzgMQK7HWWuzp/the-truth-points-to-itself-part-i", "linkUrl": "https://www.lesswrong.com/posts/SMnQQzgMQK7HWWuzp/the-truth-points-to-itself-part-i", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Truth%20Points%20to%20Itself%2C%20Part%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Truth%20Points%20to%20Itself%2C%20Part%20I%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSMnQQzgMQK7HWWuzp%2Fthe-truth-points-to-itself-part-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Truth%20Points%20to%20Itself%2C%20Part%20I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSMnQQzgMQK7HWWuzp%2Fthe-truth-points-to-itself-part-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSMnQQzgMQK7HWWuzp%2Fthe-truth-points-to-itself-part-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1316, "htmlBody": "<p><em>(Cross-posted from another blog but written with LessWrong in mind. Don't worry, if this post isn't well-received then for LessWrong the series will end with this post.)</em></p>\n<hr />\n<p><em>Summary: </em>This post is the beginning of a systematic attempt to answer the question \"what is the most important thing?\". (Updateless) decision theory is used to provisionally define \"importance\" and Juergen Schmidhuber's theory of beauty is introduced as a possible answer to the question. The motivations for bringing in Schmidhuber's theory are discussed. This post is also intended to serve as an example of how to understand and solve hard problems in general, and emphasizes the heuristic \"go meta\".</p>\n<hr />\n<p>&nbsp;</p>\n<p>This post is the first in a series about what might be the most important question we know to ask:&nbsp;What is the most important thing?</p>\n<p>Don't try to answer the question yet. When faced with a tough question our first instinct should always be to&nbsp;go meta. What is it that causes me to ask the question \"what is the most important thing\"? What makes me think the question is itself important? Is&nbsp;that thing important? Does it point to itself as the most important thing? If not, then where does it point? Does the thing it points to, point to itself? If we follow this chain, where do we end up? How path-dependent is the answer? How much good faith do we have to assume on the part of the various things, to trust that they'll give their honest opinions? If we can't simply assume good faith, can we&nbsp;<a href=\"http://en.wikipedia.org/wiki/Mechanism_design\">design a mechanism</a>&nbsp;to promote honesty? What mechanisms are already in place, and are there cheap, local improvements we can make for those mechanisms?</p>\n<p>And to ask all those questions we have to assume various commonsense notions that we might in fact need to pin down more precisely beforehand. Like, what is&nbsp;importance? Luckily we have some tools we can use to try to figure that part out.</p>\n<p>Decision theory is one such tool. In Bayesian&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a>&nbsp;\"importance\"&nbsp;might be a fair name for what is measured by your decision policy, which you get by multiplying your beliefs by your value function. Informally, your decision policy tells you what options or actions to pay most attention to, or what possibilities are most important. But arguably it's your values themselves that should be considered \"important\", and your beliefs just tell you how the important stuff relates to what is actually going on in the world. Of the decision policy and the utility function, which should we provisionally consider a better referent for \"importance\"?<br /><br />Luckily, decision theories like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_the\">updateless decision theory</a>&nbsp;(UDT) un-ask the question for us. As the name suggests, unlike Bayesian decision theories like Eliezer's timeless decision theory, UDT doesn't update its beliefs. It just has a utility function which specifies what actions it should take in all of the possible worlds it finds itself in. It doesn't care about the state of the world on top of its utility function&mdash;i.e., it doesn't have beliefs&mdash;because what worlds it cares about is a fact already specified by its utility function, and not something added in. So \"importance\" can only be one thing, and it's a surprisingly simple notion that's powerful enough to solve simple decision problems. UDT has problems with mathematical uncertainty and reflection&mdash;it has a magical \"mathematical intuition module\", and weird things happen when it proves things about its own output after taking into account that it will always give the \"optimal\" solution to a problem&mdash;but those issues don't change the fact that decision theory's notion of importance is a decent provisional notion for us to work with.<br /><br />Of course, many meta-ethicists would have reservations about defining importance this way. They would say that (moral) importance isn't something agent-specific: it's an objective fact of the universe what's (morally) important. But even given that, as bounded agents we have to find out what's actually important somehow, so when we're making decisions we can talk about our best guess at what's important without committing ourselves to any meta-ethical position. The kind of importance that has bearing on all our decisions is a prescriptive notion of importance, not a descriptive one nor a normative one. It's our agent-specific, best approximation of normative importance.<br /><br />So given our decision theoretic notion of importance we can get back to the question given above:&nbsp;what is the most important thing? If counterfactually we had all of our values represented as a utility function, what would be the term that had the most utility associated with it? We don't know how to talk about them computationally, but for now we'll let ourselves use vague human concepts. Would the most important thing be eudaimonia, maybe?<sup>&nbsp;</sup>How about those other Aristotelian emphases of arete (virtue) and phronesis (practical and moral wisdom)? Maybe the sum of all three? Taken together they surely cover a lot of ground.<br /><br />Various answers are plausible, but again, this is a perfect time to go meta. What causes the question \"what is the most important thing?\" to rise to our attention, and what causes us to try to find the answer?<br /><br />One reason we ask is that it's an&nbsp;interesting&nbsp;question of its own accord. We want to understand the world, and we're curious about the answers to some questions even when they don't seem to have any practical significance, like with chess problems or with jigsaw puzzles. We're curious by nature.<br /><br />We can always go meta again, we can always&nbsp;<a href=\"http://www.econ.tuwien.ac.at/hanappi/Lehre/EvoEco/hofstadter.pdf\">seek whence cometh a sequence</a>&nbsp;[pdf]. What causes us to be interested in things, and what causes things to be interesting? It might be a subtle point that these can be distinct questions. Maybe aliens are way more interested in sorting pebbles into prime-numbered heaps than we are. In that case we might want to acknowledge that sorting pebbles into prime-numbered heaps can be&nbsp;interesting&nbsp;in a certain general sense&mdash;it just doesn't really interest us. But we might be interested that the aliens find it interesting: I'd certainly want to know why the aliens are so into prime numbers, pebbles, and the conjunction of the two. Given my knowledge of psychology and sociology their hypothetical fixation strikes me as highly unlikely. And that brings us to the question of what in general, in a fairly mind-universal sense, causes things to be interesting.<br /><br />Luckily we can take a computational perspective to get a preliminary answer.&nbsp;<a href=\"http://www.idsia.ch/~juergen/creativity.html\">Juergen Schmidhuber's theory of beauty and other stuff</a>&nbsp;is an attempt to answer the question of what makes things interesting.&nbsp;The best introduction to his theory is his&nbsp;<a href=\"http://www.idsia.ch/~juergen/driven2009.pdf\">descriptively-titled paper</a>&nbsp;\"Driven by Compression Progress:&nbsp;A Simple Principle Explains Essential Aspects of&nbsp;Subjective Beauty, Novelty, Surprise,&nbsp;Interestingness, Attention, Curiosity, Creativity,&nbsp;Art, Science, Music, Jokes\". Here's the abstract:</p>\n<blockquote class=\"tr_bq\">I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or&nbsp;compress the data in a better way, thus making it subjectively simpler and more&nbsp;beautiful. Curiosity is the desire to create or discover more non-random, nonarbitrary, regular data that is novel and surprising not in the traditional sense of&nbsp;Boltzmann and Shannon but in the sense that it allows for compression progress&nbsp;because its regularity was not yet known. This drive maximizes interestingness, the&nbsp;\ufb01rst derivative of subjective beauty or compressibility, that is, the steepness of the&nbsp;learning curve. It motivates exploring infants, pure mathematicians, composers,&nbsp;artists, dancers, comedians, yourself, and (since 1990) arti\ufb01cial systems.</blockquote>\n<p>This compression-centric formulation of beauty and interestingness reminds me of a&nbsp;<a href=\"http://www.qwantz.com/index.php?comic=354\">Dinosaur Comic</a>:</p>\n<table class=\"tr-caption-container\" style=\"padding: 4px; margin-bottom: 0.5em; margin-left: auto; margin-right: auto; text-align: center;\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td><a href=\"http://www.qwantz.com/comics/comic2-391.png\"><img style=\"border-width: 9px; border-style: none; position: relative; box-sizing: border-box; display: inline-block; height: auto; margin: 10px auto; max-width: 100%; padding: 8px; background-color: white;\" src=\"http://www.qwantz.com/comics/comic2-391.png\" border=\"0\" alt=\"\" width=\"640\" height=\"435\" /></a></td>\n</tr>\n</tbody>\n</table>\n<p>In Schmidhuber's beautiful and interesting theory,&nbsp;compression&nbsp;plays a key role, and explains many things that we find important. So is compression the most important thing? Should we structure our decision theory around a compression progress drive, as Schmidhuber has done with some of his artificial intelligences?<br /><br />I doubt it&mdash;I don't think we've gone meta enough. But we'll further consider that question, and continue our exploration of the more important question \"what's the most important thing?\" in future posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SMnQQzgMQK7HWWuzp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 1, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "16597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T17:40:50.687Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-15", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xtXjxAKpstKg6eE7c/meetup-austin-tx-15", "pageUrlRelative": "/posts/xtXjxAKpstKg6eE7c/meetup-austin-tx-15", "linkUrl": "https://www.lesswrong.com/posts/xtXjxAKpstKg6eE7c/meetup-austin-tx-15", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtXjxAKpstKg6eE7c%2Fmeetup-austin-tx-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtXjxAKpstKg6eE7c%2Fmeetup-austin-tx-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtXjxAKpstKg6eE7c%2Fmeetup-austin-tx-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/an'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 June 2012 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78712</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Austin meetup continues, returning again to Caffe Medici. (We're meeting every week, which is far more frequently than I post reminders to come.) We sit on the second floor to the left, near (or often on) the stage.</p>\n\n<p>This week, we're going to look at some of the rationality exercises from the <a href=\"http://lesswrong.com/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">May Minicamp</a>, as well as our regular discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/an'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xtXjxAKpstKg6eE7c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.141652480261734e-07, "legacy": true, "legacyId": "16600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/an\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 June 2012 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78712</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Austin meetup continues, returning again to Caffe Medici. (We're meeting every week, which is far more frequently than I post reminders to come.) We sit on the second floor to the left, near (or often on) the stage.</p>\n\n<p>This week, we're going to look at some of the rationality exercises from the <a href=\"http://lesswrong.com/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">May Minicamp</a>, as well as our regular discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/an\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fkhbBE2ZTSytvsy9x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T18:43:52.963Z", "modifiedAt": null, "url": null, "title": "Far negatives of cryonics?", "slug": "far-negatives-of-cryonics", "viewCount": null, "lastCommentedAt": "2019-06-04T06:09:16.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bart119", "createdAt": "2012-04-19T14:46:10.499Z", "isAdmin": false, "displayName": "Bart119"}, "userId": "JQyd5QFQyrRxdo9G4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Tvn4jvXJWFbeCGwXt/far-negatives-of-cryonics", "pageUrlRelative": "/posts/Tvn4jvXJWFbeCGwXt/far-negatives-of-cryonics", "linkUrl": "https://www.lesswrong.com/posts/Tvn4jvXJWFbeCGwXt/far-negatives-of-cryonics", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Far%20negatives%20of%20cryonics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFar%20negatives%20of%20cryonics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvn4jvXJWFbeCGwXt%2Ffar-negatives-of-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Far%20negatives%20of%20cryonics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvn4jvXJWFbeCGwXt%2Ffar-negatives-of-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTvn4jvXJWFbeCGwXt%2Ffar-negatives-of-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>\n<p class=\"MsoNormal\">In considering the pros and cons of cryonics, has anyone addressed the possibility of being revived in an unpleasant future, for instance as a \"torture the infidel\" exhibit in a theme park of a theocratic state? I had some thoughts on the issue but figured I would see what else has been written previously.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Tvn4jvXJWFbeCGwXt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 9.141935010493041e-07, "legacy": true, "legacyId": "16601", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-01T20:33:49.518Z", "modifiedAt": null, "url": null, "title": "Less Wrong: The podcast", "slug": "less-wrong-the-podcast", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mapnoterritory", "createdAt": "2012-03-14T20:44:02.961Z", "isAdmin": false, "displayName": "mapnoterritory"}, "userId": "EvXiCAYEyRqnXMWAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dviNz3KrAST6hTEjJ/less-wrong-the-podcast", "pageUrlRelative": "/posts/dviNz3KrAST6hTEjJ/less-wrong-the-podcast", "linkUrl": "https://www.lesswrong.com/posts/dviNz3KrAST6hTEjJ/less-wrong-the-podcast", "postedAtFormatted": "Friday, June 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%3A%20The%20podcast&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%3A%20The%20podcast%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdviNz3KrAST6hTEjJ%2Fless-wrong-the-podcast%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%3A%20The%20podcast%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdviNz3KrAST6hTEjJ%2Fless-wrong-the-podcast", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdviNz3KrAST6hTEjJ%2Fless-wrong-the-podcast", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<p>Would it be possible to have a monthly podcast on Less Wrong&nbsp;topics? A possible format could be roughly four panelist (maybe half&nbsp;core and half rotating members) discussing theoretical and practical&nbsp;aspects or rationality, AI/singularity, cognitive science etc.</p>\n<p>Episodes can be also easily framed by assigning some reading from&nbsp;sequences or recent LW articles and then discussing them in podcast&nbsp;form. This format seems to work great for&nbsp;<a title=\"partiallyexaminedlife\" href=\"http://www.partiallyexaminedlife.com\" target=\"_blank\">www.partiallyexaminedlife.com</a>&nbsp;(quite entertaining and informative&nbsp;podcast albeit on the diseased discipline of philosophy).</p>\n<p>To keep things interesting occasional episodes could be done in the&nbsp;form of discussions with guests (via skype), e.g. the usual suspects&nbsp;from the SAIA and fellow AI scientists, people like Robin Hanson,&nbsp;Aubrey de Grey, other rationalist/skeptical bloggers/podcasters but&nbsp;also AI skeptics and so on.</p>\n<p>The level of the podcast should be still accesible to newcomers but&nbsp;the discussion could thread into bit deeper waters. I would love to&nbsp;hear discussions also on more technical topics (like CEV, Solomonoff&nbsp;induction, AIXI etc.). Just imagine how exciting could be discussions&nbsp;on the more controversial decision-theoretic paradoxes!</p>\n<p>Further possibility is from time to time plan ahead and cover in more&nbsp;depth the topic from that week's Harry Potter and the Methods of&nbsp;Rationality podcast, which would potentially help to increase the&nbsp;audience (it would be like a post-grad HPMoR).</p>\n<p>What do you think? Could this be done? With the depth and breadth of&nbsp;material we have here and all the interesting people to talk to I&nbsp;don't think there would be a shortage of topics.</p>\n<p>&nbsp;</p>\n<p>Edit: Changed the&nbsp;(bi-)weekly timescale to monthly.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dviNz3KrAST6hTEjJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 9, "extendedScore": null, "score": 9.142427800085116e-07, "legacy": true, "legacyId": "16602", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T00:08:25.408Z", "modifiedAt": null, "url": null, "title": "This post is for sacrificing my credibility!", "slug": "this-post-is-for-sacrificing-my-credibility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:04.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hz9B27XvbEGkeWQyQ/this-post-is-for-sacrificing-my-credibility", "pageUrlRelative": "/posts/Hz9B27XvbEGkeWQyQ/this-post-is-for-sacrificing-my-credibility", "linkUrl": "https://www.lesswrong.com/posts/Hz9B27XvbEGkeWQyQ/this-post-is-for-sacrificing-my-credibility", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20This%20post%20is%20for%20sacrificing%20my%20credibility!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThis%20post%20is%20for%20sacrificing%20my%20credibility!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHz9B27XvbEGkeWQyQ%2Fthis-post-is-for-sacrificing-my-credibility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=This%20post%20is%20for%20sacrificing%20my%20credibility!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHz9B27XvbEGkeWQyQ%2Fthis-post-is-for-sacrificing-my-credibility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHz9B27XvbEGkeWQyQ%2Fthis-post-is-for-sacrificing-my-credibility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<p>Thank you for your cooperation and understanding.&nbsp;Don't worry, there won't be future posts like this, so you don't have to delete my LessWrong account, and anyway I could make another, and another.</p>\n<p>But since you've dared to read this far:</p>\n<p>Credibility. Should you maximize it, or minimize it? Have I made an error?</p>\n<p>Discuss.</p>\n<p>Don't be shallow, don't just consider the obvious points. Consider that I've thought about this for many, many hours, and that you don't have any privileged information. Whence our disagreement, if one exists?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hz9B27XvbEGkeWQyQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": -50, "extendedScore": null, "score": -9.1e-05, "legacy": true, "legacyId": "16604", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 347, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T04:31:32.387Z", "modifiedAt": null, "url": null, "title": "Marketplace Transactions Open Thread", "slug": "marketplace-transactions-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:36.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qY3XmmHHG7s9wnwNq/marketplace-transactions-open-thread", "pageUrlRelative": "/posts/qY3XmmHHG7s9wnwNq/marketplace-transactions-open-thread", "linkUrl": "https://www.lesswrong.com/posts/qY3XmmHHG7s9wnwNq/marketplace-transactions-open-thread", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Marketplace%20Transactions%20Open%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMarketplace%20Transactions%20Open%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY3XmmHHG7s9wnwNq%2Fmarketplace-transactions-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Marketplace%20Transactions%20Open%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY3XmmHHG7s9wnwNq%2Fmarketplace-transactions-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY3XmmHHG7s9wnwNq%2Fmarketplace-transactions-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 355, "htmlBody": "<p>Social scientists think humans operate under one of two different sets of norms, depending on the circumstances:&nbsp;\"market norms\" or \"social norms\".&nbsp;The basic idea is that when exchanging money for goods and services formally, it's considered okay to be much more calculating and self-interested than when exchanging favors with friends informally.&nbsp;You can read <a href=\"http://danariely.com/the-books/excerpted-from-chapter-4-&ndash;-the-cost-of-social-norms/\">this blog post</a> by Dan Ariely for more.</p>\n<p>It's often&nbsp;considered rude to introduce market norms in an area where they don't traditionally apply. For example, by <a href=\"http://www.reddit.com/r/todayilearned/comments/ufl15/til_carl_sagan_sued_apple_for_using_his_name_as/c4v03on\">charging money for your presence at a barbecue</a>.</p>\n<p>This is a thread where it's okay to talk about trading money for goods and services with other Less Wrong users, which might otherwise be considered rude because you'd be inappropriately introducing market norms.&nbsp;Things you're encouraged to do include:</p>\n<ul>\n<li>Post your resume</li>\n<li>Advertise a product sold by you or your company</li>\n<li>Advertise a service provided by you or your company</li>\n<li>Advertise an open position working for you or your company</li>\n</ul>\n<p>The argument for having a thread like this is as follows. Less Wrong users have a variety of goals they wish to accomplish. Some of these goals involve engaging in marketplace transactions. It's plausible that a thread facilitating marketplace transactions between LW users&nbsp;will buy just as much or more collective goal accomplishment per unit attention consumed than a traditional Less Wrong thread.</p>\n<p>Anecdotally it seems that introducing market norms takes a certain amount of chutzpah. For example, apparently it takes a certain kind of person to actually be able to name a dollar figure in a sales conversation, and that's why you need a professional salesperson to come along with a sales engineer when selling a technical product. One LWer friend of mine struggled for a while before she was able to get herself to charge money for talk therapy she had been providing to friends for free.</p>\n<p>To combat this, please feel inclined to vote up folks who post in this thread. They likely overcame some akrasia in the act of promoting their offer.</p>\n<p>To discuss the concept of this thread, as opposed to advertising a transaction you wish to engage in, please reply to&nbsp;<a href=\"/r/discussion/lw/ctg/marketplace_transactions_open_thread/6q87\">this comment</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qY3XmmHHG7s9wnwNq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 46, "extendedScore": null, "score": 9.144569578421817e-07, "legacy": true, "legacyId": "16612", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T05:28:39.935Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Possibility and Could-ness", "slug": "seq-rerun-possibility-and-could-ness", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GAiJjGiSZ5evRTLNe/seq-rerun-possibility-and-could-ness", "pageUrlRelative": "/posts/GAiJjGiSZ5evRTLNe/seq-rerun-possibility-and-could-ness", "linkUrl": "https://www.lesswrong.com/posts/GAiJjGiSZ5evRTLNe/seq-rerun-possibility-and-could-ness", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Possibility%20and%20Could-ness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Possibility%20and%20Could-ness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGAiJjGiSZ5evRTLNe%2Fseq-rerun-possibility-and-could-ness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Possibility%20and%20Could-ness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGAiJjGiSZ5evRTLNe%2Fseq-rerun-possibility-and-could-ness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGAiJjGiSZ5evRTLNe%2Fseq-rerun-possibility-and-could-ness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>Today's post, <a href=\"/lw/rb/possibility_and_couldness/\">Possibility and Could-ness</a> was originally published on 14 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Possibility_and_Could-ness\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Our sense of \"could-ness\", as in \"I could have not rescued the child from the burning orphanage\", comes from our own decision making algorithms labeling some end states as \"reachable\". If we wanted to achieve the world-state of the child being burned, there is a series of actions that would lead to that state.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/csa/seq_rerun_causality_and_moral_responsibility/\">Causality and Moral Responsibility</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GAiJjGiSZ5evRTLNe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.144825754766101e-07, "legacy": true, "legacyId": "16613", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3buXtNiSK8gcRLMSG", "fmoB2JZFoXDYfpxsr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T12:53:57.751Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Sydney", "slug": "meetup-less-wrong-sydney", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NRDCGP9tiQAMys3SZ/meetup-less-wrong-sydney", "pageUrlRelative": "/posts/NRDCGP9tiQAMys3SZ/meetup-less-wrong-sydney", "linkUrl": "https://www.lesswrong.com/posts/NRDCGP9tiQAMys3SZ/meetup-less-wrong-sydney", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNRDCGP9tiQAMys3SZ%2Fmeetup-less-wrong-sydney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNRDCGP9tiQAMys3SZ%2Fmeetup-less-wrong-sydney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNRDCGP9tiQAMys3SZ%2Fmeetup-less-wrong-sydney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ao'>Less Wrong Sydney</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 June 2012 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all! As usual, we'll be conducting our meet up at the Norita Cafe and Board Games. And furthermore, as usual, we'll be soliciting topics from the floor.</p>\n\n<p>FB group availible at <a href=\"http://www.facebook.com/#!/groups/219526434802422/\" rel=\"nofollow\">http://www.facebook.com/#!/groups/219526434802422/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ao'>Less Wrong Sydney</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NRDCGP9tiQAMys3SZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.146823090268999e-07, "legacy": true, "legacyId": "16630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney\">Discussion article for the meetup : <a href=\"/meetups/ao\">Less Wrong Sydney</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 June 2012 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all! As usual, we'll be conducting our meet up at the Norita Cafe and Board Games. And furthermore, as usual, we'll be soliciting topics from the floor.</p>\n\n<p>FB group availible at <a href=\"http://www.facebook.com/#!/groups/219526434802422/\" rel=\"nofollow\">http://www.facebook.com/#!/groups/219526434802422/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney1\">Discussion article for the meetup : <a href=\"/meetups/ao\">Less Wrong Sydney</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Sydney", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Sydney", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T16:12:42.358Z", "modifiedAt": null, "url": null, "title": "Suggestion: Less Wrong Writing Circle?", "slug": "suggestion-less-wrong-writing-circle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:23.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gQBDz5CQRfWjQ5hMn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jYRRsPQmQN5uAEpFY/suggestion-less-wrong-writing-circle", "pageUrlRelative": "/posts/jYRRsPQmQN5uAEpFY/suggestion-less-wrong-writing-circle", "linkUrl": "https://www.lesswrong.com/posts/jYRRsPQmQN5uAEpFY/suggestion-less-wrong-writing-circle", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestion%3A%20Less%20Wrong%20Writing%20Circle%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestion%3A%20Less%20Wrong%20Writing%20Circle%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjYRRsPQmQN5uAEpFY%2Fsuggestion-less-wrong-writing-circle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestion%3A%20Less%20Wrong%20Writing%20Circle%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjYRRsPQmQN5uAEpFY%2Fsuggestion-less-wrong-writing-circle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjYRRsPQmQN5uAEpFY%2Fsuggestion-less-wrong-writing-circle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>This community has a recurring interest in \"rationalist fiction,\" and several members who are writers. I wonder if it would be useful to create a space where Less Wrong members could provide each other constructive criticism and encouragement on in-progress original writing projects?</p>\n<p>Disclosure: I'm working on a sci-fi novel right now, and my regular circle of \"beta readers\" are fantasy fans and aren't providing much feedback on the new project. I am much, much more productive as a writer when I get steady feedback, so I have a personal interest in looking for something like this. Less Wrong came to mind as a community of intelligent, creative, forward-looking types who are likely to enjoy sci-fi.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jYRRsPQmQN5uAEpFY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 9.147714783885054e-07, "legacy": true, "legacyId": "16631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T16:33:26.772Z", "modifiedAt": null, "url": null, "title": "Open thread, June 2-16, 2012", "slug": "open-thread-june-2-16-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:57.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nPAdBkmkyMjK56uKi/open-thread-june-2-16-2012", "pageUrlRelative": "/posts/nPAdBkmkyMjK56uKi/open-thread-june-2-16-2012", "linkUrl": "https://www.lesswrong.com/posts/nPAdBkmkyMjK56uKi/open-thread-june-2-16-2012", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20June%202-16%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20June%202-16%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnPAdBkmkyMjK56uKi%2Fopen-thread-june-2-16-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20June%202-16%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnPAdBkmkyMjK56uKi%2Fopen-thread-june-2-16-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnPAdBkmkyMjK56uKi%2Fopen-thread-june-2-16-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nPAdBkmkyMjK56uKi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "16632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T17:14:39.130Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes June 2012", "slug": "rationality-quotes-june-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:19.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F96W9oT4qdikaNkh8/rationality-quotes-june-2012", "pageUrlRelative": "/posts/F96W9oT4qdikaNkh8/rationality-quotes-june-2012", "linkUrl": "https://www.lesswrong.com/posts/F96W9oT4qdikaNkh8/rationality-quotes-june-2012", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20June%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20June%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF96W9oT4qdikaNkh8%2Frationality-quotes-june-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20June%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF96W9oT4qdikaNkh8%2Frationality-quotes-june-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF96W9oT4qdikaNkh8%2Frationality-quotes-june-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>\n<div id=\"entry_t3_bdo\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Here's the new thread for posting quotes, with the usual rules:</span></p>\n<ul>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Please   post all quotes separately, so that they can be voted up/down   separately. &nbsp;(If they are strongly related, reply to your own comments.   &nbsp;If strongly ordered, then go ahead and post them together.)</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote yourself</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote comments/posts on LW/OB</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">No more than 5 quotes per person per monthly thread, please.</span></li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F96W9oT4qdikaNkh8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 9.147990698617433e-07, "legacy": true, "legacyId": "16591", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 413, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T19:25:37.362Z", "modifiedAt": null, "url": null, "title": "Focus on rationality", "slug": "focus-on-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bBFLpfKCgndBw9iGp/focus-on-rationality", "pageUrlRelative": "/posts/bBFLpfKCgndBw9iGp/focus-on-rationality", "linkUrl": "https://www.lesswrong.com/posts/bBFLpfKCgndBw9iGp/focus-on-rationality", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Focus%20on%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFocus%20on%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBFLpfKCgndBw9iGp%2Ffocus-on-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Focus%20on%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBFLpfKCgndBw9iGp%2Ffocus-on-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbBFLpfKCgndBw9iGp%2Ffocus-on-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 561, "htmlBody": "<p>(This is my view in the recent debate about posts giving a \"rational\" discussion of some random topic. It was originally at <a href=\"/lw/crd/only_say_rational_when_you_cant_eliminate_the_word/6pvq\">comment</a> level but I've extended it and posted it in discussion because I want to know if and where people disagree with me, and for what reasons.)</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I come to Less Wrong to learn about how to think and how to act effectively. I care about general algorithms that are useful for many problems, like \"Hold off on proposing solutions\" or \"Habits are ingrained faster when you pay concious attention to your thoughts when you perform the action\". These posts have very high value to me because they improve my effectiveness across a wide range of areas.</p>\n<p>Another such technique is \"Dissolving the question\". Yvain's \"<a href=\"/lw/2as/diseased_thinking_dissolving_questions_about/\">Diseased thinking: dissolving questions about disease</a>\" is valuable as an exemplary performance of this technique. It adds to Eliezer's description of question-dissolving by giving a demonstration of its use on a real question. It's main value comes from this, anything I learnt about disease whilst reading it is just a bonus.</p>\n<p>To quote badger in the recent thread \"<a href=\"/lw/cqz/rational_toothpaste_a_case_study/\">Rational Toothpaste: A Case Study</a>\"</p>\n<blockquote>\n<p>I claim a post on \"rational toothpaste buying\" could be on-topic and useful, if correctly written to illustrate determining goals, assessing tradeoffs, and implementing the final conclusions. A post detailing the pros and cons of various toothpaste brands is for a dentistry or personal hygiene forum; a post about algorithms for how to determine the best brands or whether to do so at all is for a rationality forum.</p>\n</blockquote>\n<p>But we don't need more than one or two such examples! Yvain's post about question-dissolving was the only such post I ever need to read.</p>\n<p>Posts about toothpaste, house-buying, room-decoration, fashion, shaving or computer hardware only tell me about that particular thing. As good as many of them are they'll never be as useful as a post that teaches me a general method of thought applicable on many problems. And if I want to know about some particular topic I'll just look it up on Google, or go to a library.</p>\n<p>It's not possible for LessWrong to give a rational treatment of every subject. There are just too many of them. Even if we did I wouldn't be able to carry all that info around in my head. That's why I need to learn general algorithms for producing rational decisions.</p>\n<p>Even though badger makes it clear in the quote I gave that the post is supposed to about the algorithms used, the in the rest of the post almost all the discussion is on the object level (although the conclusion is good). That is, even though badger talks about which methods he's using and why, the focus is still on \"What can these methods teach us about toothpaste?\" and not \"What can optimising toothpaste teach us about our methods?\". I'd prefer it if posts tried to answer questions more like the latter. The comments exhibit the same phenomenon. Only one of the comments (kilobug's) is talking about the methods used. Most of the rest are actually <em>talking about toothpaste</em>.</p>\n<p>So what I'm suggesting is that LessWrong posts (don't forget there's a whole internet to post things on) should <em>focus on rationality</em>. They can talk about other things too, but the question should always be \"What can X teach us about rationality?\" and not \"What can rationality teach us about X?\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bBFLpfKCgndBw9iGp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 28, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "16633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["895quRDaK6gR2rM82", "NHuLAS3oKZWr2X9hP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T20:17:15.012Z", "modifiedAt": null, "url": null, "title": "Confronting the Mindkiller - a series of posts exploring Political Landscape (Part 1)", "slug": "confronting-the-mindkiller-a-series-of-posts-exploring", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuxedage", "createdAt": "2012-03-22T17:13:05.551Z", "isAdmin": false, "displayName": "Tuxedage"}, "userId": "Ezvcs6nqmgXbpD5bN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/si24Citdas6tZJRcd/confronting-the-mindkiller-a-series-of-posts-exploring", "pageUrlRelative": "/posts/si24Citdas6tZJRcd/confronting-the-mindkiller-a-series-of-posts-exploring", "linkUrl": "https://www.lesswrong.com/posts/si24Citdas6tZJRcd/confronting-the-mindkiller-a-series-of-posts-exploring", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confronting%20the%20Mindkiller%20-%20a%20series%20of%20posts%20exploring%20Political%20Landscape%20(Part%201)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfronting%20the%20Mindkiller%20-%20a%20series%20of%20posts%20exploring%20Political%20Landscape%20(Part%201)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsi24Citdas6tZJRcd%2Fconfronting-the-mindkiller-a-series-of-posts-exploring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confronting%20the%20Mindkiller%20-%20a%20series%20of%20posts%20exploring%20Political%20Landscape%20(Part%201)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsi24Citdas6tZJRcd%2Fconfronting-the-mindkiller-a-series-of-posts-exploring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsi24Citdas6tZJRcd%2Fconfronting-the-mindkiller-a-series-of-posts-exploring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1307, "htmlBody": "<h1>Confronting the Mindkiller<br /></h1>\n<p>Hi LessWrong. <br />&nbsp;<br />I will be attempting a series of 11 posts on how to defeat the mindkiller -- Politics.</p>\n<p>This is a bit of an exaggeration -- \"defeating\" it is a seriously difficult task, and it's something I cannot possibly hope to do alone. However, I hope to be able to <a href=\"/lw/1e/raising_the_sanity_waterline\">raise the Sanity Waterline</a>, as well as actually contribute something to this Community that I've begun to regard myself as part of.<br /><br />I also beg patience from you, the reader. Lesswrong is far more critical than most other Communities, and frankly, even writing this post has begun to give me second thoughts, in fear of being downvoted into oblivion.<br /><br />However, one who attempts nothing in the fear of consequences will live a life of regret - only by attempting to <a href=\"/lw/up/shut_up_and_do_the_impossible/\">do the \"impossible\"</a>, and <a href=\"/lw/8ns/hack_away_at_the_edges/\">hacking away at the edges</a>, can one succeed. This post was written with <a href=\"/lw/nb/something_to_protect/\">something to protect in mind</a>. I hope you be patient with me, at least until the last posts have been written. This might take a week or two to complete.</p>\n<p>I prefer to post one post at a time, Eliezer Yudkowsky style, in order to better adjust my writing style to suit your tastes, to receive continuous feedback, and to break up this task into smaller chunks to better avoid Akrasia, but if you'd rather I post this in one long essay, I can begrudgingly do that too. <br /><br />Since my idea of these series of posts in <a href=\"/lw/cs8/open_thread_june_115_2012/6pv2\">the Open Thread</a> seems to have been relatively well-received, I'll proceed to courageously embarrass myself in front of everyone. <br /><br />This is my first serious post on this site, and I really welcome constructive criticism and any attempts to help me improve my writing skills. Likewise, if you don't think I should continue writing this series of posts, let me know, although please be gentle. I assure you that I will not <a href=\"/lw/c6f/seeking_links_for_the_best_arguments_for_economic/\">fall trapping to partisan arguments</a> or <a href=\"/lw/c48/groupthink_of_opinions/\">argue one side over another in an irrational fashion.</a> <br /><br />The purpose of these posts is to improve the way we think about politics, and help you find the \"right side\" in politics, rather than directly tell you which side is right or wrong. This post, I hope, does not break LessWrong's rules regarding no politics. <br /><br />I also won't mess up too badly. <em>I promise.</em></p>\n<h2>Confronting the Mindkiller - Why the Mindkiller?<br /></h2>\n<p>&nbsp;</p>\n<p>Why am I trying to actively confront the mind-killer? <a href=\"/lw/7i/rationality_is_systematized_winning/\">Because Rationalists should win.</a> Because as Rationalists, we ought to be able to <a href=\"/lw/6lx/rationalist_judo_or_using_the_availability/\">go out into the real world</a> and <a href=\"/lw/aa7/get_curious/\">use our skills for awesomeness.</a><br /><br /><br />A Martial Arts Grandmaster cannot become stronger by <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">only fighting amateurs and beginners</a>. He can only do so by fighting the strongest, by pushing himself beyond his limits. Even doing so, however, can be unsatisfactory. How can the <em>Sensei </em>know that his skills are truly useful, not something that fails when applied in real life? By simply testing himself in difficult real-world situations. Nothing else can suffice, no tournament or medal may be adequate in proving that he truly is the best fighter.<br /><br />Likewise, the true test of Rationality is not one that we can easily overcome, but the one that is the most difficult to overcome. In order to learn to fight better, our <a href=\"/lw/q9/the_failures_of_eld_science/\"><em>\"Sensei\"</em></a> has taught us to kick and punch dummies. But dummies are not a measure of your ability to fight. Dummies cannot fight back, they cannot actively test you, and they certainly cannot plan confrontations and strike when you least expect it. Dummies are useful, but only as a stepping stone towards fighting more difficult opponents.&nbsp; <br /><br />The more advanced and experienced of us have<a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\"> gone out into the real world and fought with sparring partners.</a> This is a step higher from punching dummies, but this is not the best we can do. <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>. In the words of Eliezer, <a href=\"/lw/h9/tsuyoku_vs_the_egalitarian_instinct/\">the epitome of Intelligence is not those of the past, nor those in the present, but that of the future.</a><br /><br />Rationality is similar. <a href=\"/lw/uu/why_does_power_corrupt/\">We run on corrupted hardware,&nbsp; </a>and even the most rational of us is still comparatively irrational to the mindspace of possible Rationalists. That is why me must aspire to do better, even if we already are proficient in our art.<br /><br />The best test of Rationality is to confront that which makes us the most irrational. We cannot become a master in martial arts without sparring and getting hurt, and we cannot become masters in Rationality by simply refusing to engage in politics. It makes sense to avoid certain spheres of politics until we have learned the basics of Rationality. However, such avoidance cannot be permanent.&nbsp;</p>\n<p>&nbsp;</p>\n<h3>We should not agree to Disagree.</h3>\n<p>Furthermore, I want to confront Politics simply because it is so mind-boggling. <a href=\"http://wiki.lesswrong.com/wiki/Aumann%27s_agreement_theorem\">Aumann's agreement theorem </a>states than any two Perfect Bayesians with similar priors cannot agree to disagree. And yet the fact that we have people from all political spectrums means that politics has become so infectious, so divisive, that even X-Rationalists are split by it.<br /><br /><em><a href=\"/lw/8p4/2011_survey_results/\"><br /></a></em></p>\n<address><em><a href=\"/lw/8p4/2011_survey_results/\">The most popular political view, at least according to the much-maligned categories on the survey, was liberalism, with 376 adherents and 34.5% of the vote. Libertarianism followed at 352 (32.3%), then socialism at 290 (26.6%), conservativism at 30 (2.8%) and communism at 5 (.5%).</a></em></address>\n<p><br />Funnily enough, although many outsiders have accused us of agreeing far too much, political ideologies are one aspect where it is not clear cut at all. About one-third of this site are Liberals, one-third of this site are Libertarians, followed by a quarter of us being Socialism, and the rest being divided into Conservatives and Communists.<br /><br />Compare this with issues commonly touted as Controversial, such as religion, where 92% of us are either Atheistic or Agnostic - a clearly cut issue. <br /><br />Politics is the mind-killer on a scale where nothing else can compare, especially to x-rationalists. And yet since we cannot avoid its confrontation entirely I would argue we should not avoid confronting it at all. The arena of political debates are packed with the most subtle and devious dark arts and irrationality, leaving it to rot would be a bad decision; after all, politics is already playing a hand in every aspect of our lives. It would benefit from a healthy dose of Rationality, even if it cannot be made completely sane.<br /><br />If the step to begin raising the sanity waterline does not begin here, where does it? If we, aspiring Rationalists who have studied probability theory, psychology, Bayesian inference, neuroscience, philosophy, epistemology, and as much information as possible on the science of decision making under uncertainty<a href=\"/lw/3h/why_our_kind_cant_cooperate/ \"> cannot even engage in politics without bickering and divisiveness</a>, who can? <br /><br /><br />The answer is nobody. Which is the reason why our political arena is so screwed up today. <br /><br />And finally...</p>\n<p><br />We all dream of a better world to live in. I hope it will not drive you away to suggest that many a life has been lost over disagreements between which better world is the best. Many more lives have been lost because of people who refused to engage in political endeavors.</p>\n<p>Rationality is not an abstract Art, to be appreciated in itself -- it's a tool that you use to <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">mold the world to your desires, to win</a>. I have this mad hope that someday political spheres will be sane, rather than hopelessly filled with the dark arts. Perhaps by attempting this sequence of posts, the world will be one small step closer towards a saner future.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "si24Citdas6tZJRcd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -3, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "16635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h1 id=\"Confronting_the_Mindkiller\">Confronting the Mindkiller<br></h1>\n<p>Hi LessWrong. <br>&nbsp;<br>I will be attempting a series of 11 posts on how to defeat the mindkiller -- Politics.</p>\n<p>This is a bit of an exaggeration -- \"defeating\" it is a seriously difficult task, and it's something I cannot possibly hope to do alone. However, I hope to be able to <a href=\"/lw/1e/raising_the_sanity_waterline\">raise the Sanity Waterline</a>, as well as actually contribute something to this Community that I've begun to regard myself as part of.<br><br>I also beg patience from you, the reader. Lesswrong is far more critical than most other Communities, and frankly, even writing this post has begun to give me second thoughts, in fear of being downvoted into oblivion.<br><br>However, one who attempts nothing in the fear of consequences will live a life of regret - only by attempting to <a href=\"/lw/up/shut_up_and_do_the_impossible/\">do the \"impossible\"</a>, and <a href=\"/lw/8ns/hack_away_at_the_edges/\">hacking away at the edges</a>, can one succeed. This post was written with <a href=\"/lw/nb/something_to_protect/\">something to protect in mind</a>. I hope you be patient with me, at least until the last posts have been written. This might take a week or two to complete.</p>\n<p>I prefer to post one post at a time, Eliezer Yudkowsky style, in order to better adjust my writing style to suit your tastes, to receive continuous feedback, and to break up this task into smaller chunks to better avoid Akrasia, but if you'd rather I post this in one long essay, I can begrudgingly do that too. <br><br>Since my idea of these series of posts in <a href=\"/lw/cs8/open_thread_june_115_2012/6pv2\">the Open Thread</a> seems to have been relatively well-received, I'll proceed to courageously embarrass myself in front of everyone. <br><br>This is my first serious post on this site, and I really welcome constructive criticism and any attempts to help me improve my writing skills. Likewise, if you don't think I should continue writing this series of posts, let me know, although please be gentle. I assure you that I will not <a href=\"/lw/c6f/seeking_links_for_the_best_arguments_for_economic/\">fall trapping to partisan arguments</a> or <a href=\"/lw/c48/groupthink_of_opinions/\">argue one side over another in an irrational fashion.</a> <br><br>The purpose of these posts is to improve the way we think about politics, and help you find the \"right side\" in politics, rather than directly tell you which side is right or wrong. This post, I hope, does not break LessWrong's rules regarding no politics. <br><br>I also won't mess up too badly. <em>I promise.</em></p>\n<h2 id=\"Confronting_the_Mindkiller___Why_the_Mindkiller_\">Confronting the Mindkiller - Why the Mindkiller?<br></h2>\n<p>&nbsp;</p>\n<p>Why am I trying to actively confront the mind-killer? <a href=\"/lw/7i/rationality_is_systematized_winning/\">Because Rationalists should win.</a> Because as Rationalists, we ought to be able to <a href=\"/lw/6lx/rationalist_judo_or_using_the_availability/\">go out into the real world</a> and <a href=\"/lw/aa7/get_curious/\">use our skills for awesomeness.</a><br><br><br>A Martial Arts Grandmaster cannot become stronger by <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">only fighting amateurs and beginners</a>. He can only do so by fighting the strongest, by pushing himself beyond his limits. Even doing so, however, can be unsatisfactory. How can the <em>Sensei </em>know that his skills are truly useful, not something that fails when applied in real life? By simply testing himself in difficult real-world situations. Nothing else can suffice, no tournament or medal may be adequate in proving that he truly is the best fighter.<br><br>Likewise, the true test of Rationality is not one that we can easily overcome, but the one that is the most difficult to overcome. In order to learn to fight better, our <a href=\"/lw/q9/the_failures_of_eld_science/\"><em>\"Sensei\"</em></a> has taught us to kick and punch dummies. But dummies are not a measure of your ability to fight. Dummies cannot fight back, they cannot actively test you, and they certainly cannot plan confrontations and strike when you least expect it. Dummies are useful, but only as a stepping stone towards fighting more difficult opponents.&nbsp; <br><br>The more advanced and experienced of us have<a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\"> gone out into the real world and fought with sparring partners.</a> This is a step higher from punching dummies, but this is not the best we can do. <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>. In the words of Eliezer, <a href=\"/lw/h9/tsuyoku_vs_the_egalitarian_instinct/\">the epitome of Intelligence is not those of the past, nor those in the present, but that of the future.</a><br><br>Rationality is similar. <a href=\"/lw/uu/why_does_power_corrupt/\">We run on corrupted hardware,&nbsp; </a>and even the most rational of us is still comparatively irrational to the mindspace of possible Rationalists. That is why me must aspire to do better, even if we already are proficient in our art.<br><br>The best test of Rationality is to confront that which makes us the most irrational. We cannot become a master in martial arts without sparring and getting hurt, and we cannot become masters in Rationality by simply refusing to engage in politics. It makes sense to avoid certain spheres of politics until we have learned the basics of Rationality. However, such avoidance cannot be permanent.&nbsp;</p>\n<p>&nbsp;</p>\n<h3 id=\"We_should_not_agree_to_Disagree_\">We should not agree to Disagree.</h3>\n<p>Furthermore, I want to confront Politics simply because it is so mind-boggling. <a href=\"http://wiki.lesswrong.com/wiki/Aumann%27s_agreement_theorem\">Aumann's agreement theorem </a>states than any two Perfect Bayesians with similar priors cannot agree to disagree. And yet the fact that we have people from all political spectrums means that politics has become so infectious, so divisive, that even X-Rationalists are split by it.<br><br><em><a href=\"/lw/8p4/2011_survey_results/\"><br></a></em></p>\n<address><em><a href=\"/lw/8p4/2011_survey_results/\">The most popular political view, at least according to the much-maligned categories on the survey, was liberalism, with 376 adherents and 34.5% of the vote. Libertarianism followed at 352 (32.3%), then socialism at 290 (26.6%), conservativism at 30 (2.8%) and communism at 5 (.5%).</a></em></address>\n<p><br>Funnily enough, although many outsiders have accused us of agreeing far too much, political ideologies are one aspect where it is not clear cut at all. About one-third of this site are Liberals, one-third of this site are Libertarians, followed by a quarter of us being Socialism, and the rest being divided into Conservatives and Communists.<br><br>Compare this with issues commonly touted as Controversial, such as religion, where 92% of us are either Atheistic or Agnostic - a clearly cut issue. <br><br>Politics is the mind-killer on a scale where nothing else can compare, especially to x-rationalists. And yet since we cannot avoid its confrontation entirely I would argue we should not avoid confronting it at all. The arena of political debates are packed with the most subtle and devious dark arts and irrationality, leaving it to rot would be a bad decision; after all, politics is already playing a hand in every aspect of our lives. It would benefit from a healthy dose of Rationality, even if it cannot be made completely sane.<br><br>If the step to begin raising the sanity waterline does not begin here, where does it? If we, aspiring Rationalists who have studied probability theory, psychology, Bayesian inference, neuroscience, philosophy, epistemology, and as much information as possible on the science of decision making under uncertainty<a href=\"/lw/3h/why_our_kind_cant_cooperate/ \"> cannot even engage in politics without bickering and divisiveness</a>, who can? <br><br><br>The answer is nobody. Which is the reason why our political arena is so screwed up today. <br><br>And finally...</p>\n<p><br>We all dream of a better world to live in. I hope it will not drive you away to suggest that many a life has been lost over disagreements between which better world is the best. Many more lives have been lost because of people who refused to engage in political endeavors.</p>\n<p>Rationality is not an abstract Art, to be appreciated in itself -- it's a tool that you use to <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">mold the world to your desires, to win</a>. I have this mad hope that someday political spheres will be sane, rather than hopelessly filled with the dark arts. Perhaps by attempting this sequence of posts, the world will be one small step closer towards a saner future.</p>", "sections": [{"title": "Confronting the Mindkiller", "anchor": "Confronting_the_Mindkiller", "level": 1}, {"title": "Confronting the Mindkiller - Why the Mindkiller?", "anchor": "Confronting_the_Mindkiller___Why_the_Mindkiller_", "level": 2}, {"title": "We should not agree to Disagree.", "anchor": "We_should_not_agree_to_Disagree_", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf", "nCvvhFBaayaXyuBiD", "6bSHiD9TxsJwe2WqT", "SGR4GxFK7KmW7ckCB", "Q7picjbgde8cf5NB4", "zwrRFKb22FqD8pC4i", "4ARtkT3EYox3THYjF", "ztEcQtZsjXnqLc3xB", "bGtdeqbgTzuLvZ5zn", "DoLQN5ryZ9XkZjq5h", "ZxR8P8hBFQ9kC8wMy", "fkhbBE2ZTSytvsy9x", "gWGA8Da539EQmAR9F", "v8rghtzWCziYuMdJ5", "HAEPbGaMygJq8L59k", "7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T20:50:07.098Z", "modifiedAt": null, "url": null, "title": "Paper: Iterated Prisoner\u2019s Dilemma contains strategies that dominate any evolutionary opponent", "slug": "paper-iterated-prisoner-s-dilemma-contains-strategies-that", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.711Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mapnoterritory", "createdAt": "2012-03-14T20:44:02.961Z", "isAdmin": false, "displayName": "mapnoterritory"}, "userId": "EvXiCAYEyRqnXMWAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WcD4puSxkbqinRs6R/paper-iterated-prisoner-s-dilemma-contains-strategies-that", "pageUrlRelative": "/posts/WcD4puSxkbqinRs6R/paper-iterated-prisoner-s-dilemma-contains-strategies-that", "linkUrl": "https://www.lesswrong.com/posts/WcD4puSxkbqinRs6R/paper-iterated-prisoner-s-dilemma-contains-strategies-that", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paper%3A%20Iterated%20Prisoner%E2%80%99s%20Dilemma%20contains%20strategies%20that%20dominate%20any%20evolutionary%20opponent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APaper%3A%20Iterated%20Prisoner%E2%80%99s%20Dilemma%20contains%20strategies%20that%20dominate%20any%20evolutionary%20opponent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcD4puSxkbqinRs6R%2Fpaper-iterated-prisoner-s-dilemma-contains-strategies-that%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paper%3A%20Iterated%20Prisoner%E2%80%99s%20Dilemma%20contains%20strategies%20that%20dominate%20any%20evolutionary%20opponent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcD4puSxkbqinRs6R%2Fpaper-iterated-prisoner-s-dilemma-contains-strategies-that", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWcD4puSxkbqinRs6R%2Fpaper-iterated-prisoner-s-dilemma-contains-strategies-that", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 507, "htmlBody": "<p>Bill \"Numerical Recipes\" Press and Freeman \"Dyson sphere\" Dyson <a href=\"http://www.pnas.org/content/early/2012/05/16/1206569109.full.pdf\">have a new paper on iterated prisoner dilemas</a> (IPD). Interestingly they found new surprising results:</p>\n<blockquote>\n<p>It is generally assumed that there exists no simple ultimatum strategy whereby one player can enforce a unilateral claim to an unfair share of rewards. Here, we show that such strategies unexpectedly do exist. In particular, a player X who is witting of these strategies can (i) deterministically set her opponent Y&rsquo;s score, independently of his strategy or response, or (ii) enforce an extortionate linear relation between her and his scores.</p>\n</blockquote>\n<p>They discuss a special class of strategies - zero determinant (ZD) strategies of which tit-for-tat (TFT) is a special case:</p>\n<blockquote>\n<p>The extortionate ZD strategies have the peculiar property of sharply distinguishing between &ldquo;sentient&rdquo; players, who have a theory of mind about their opponents, and &ldquo;evolutionary&rdquo; players, who may be arbitrarily good at exploring a \ufb01tness landscape (either locally or globally), but who have no theory of mind.</p>\n</blockquote>\n<p>The evolutionary player adjusts his strategy to maximize score, but doesn't take his opponent explicitly into account in another way (hence has \"no theory of mind\" of the opponent). Possible outcomes are:</p>\n<p>A)</p>\n<blockquote>\n<p>If X alone is witting of ZD strategies, then IPD reduces to one of two cases, depending on whether Y has a theory of mind. If Y has a theory of mind, then IPD is simply an ultimatum game (15, 16), where X proposes an unfair division and Y can either accept or reject the proposal. If he does not (or if, equivalently, X has \ufb01xed her strategy and then gone to lunch), then the game is dilemma-free for Y. He can maximize his own score only by giving X even more; there is no bene\ufb01t to him in defecting.</p>\n</blockquote>\n<p>B)</p>\n<blockquote>\n<p>If X and Y are both witting of ZD, then they may choose to negotiate to each set the other&rsquo;s score to the maximum cooperative value. Unlike naive PD, there is no advantage in defection, because neither can affect his or her own score and each can punish any irrational defection by the other. Nor is this equivalent to the classical TFT strategy (7), which produces indeterminate scores if played by both players.</p>\n</blockquote>\n<p>This latter case sounds like a formalization of Hosfstadter's superrational agents. The cooperation enforcement via cross-setting the scores is very interesting.</p>\n<p>Is this connection true or am I misinterpreting it? (This is not my field and I've only skimmed the paper up to now.) What are the implications for FAI? If we'd get into an IPD situation with an agent for which we simply can not put together a theory of mind, do we have to live with extortion? What would effectively mean to have a useful theory of mind in this case?</p>\n<p>The paper ends in a grand style (spoiler alert):</p>\n<blockquote>\n<p>It is worth contemplating that, though an evolutionary player Y is so easily beaten within the con\ufb01nes of the IPD game, it is exactly evolution, on the hugely larger canvas of DNA-based life, that ultimately has produced X, the player with the mind.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WcD4puSxkbqinRs6R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 39, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "16636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T21:07:00.394Z", "modifiedAt": null, "url": null, "title": "Reaching young math/compsci talent", "slug": "reaching-young-math-compsci-talent", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.410Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4dFzBkpjx6aHZpjqL/reaching-young-math-compsci-talent", "pageUrlRelative": "/posts/4dFzBkpjx6aHZpjqL/reaching-young-math-compsci-talent", "linkUrl": "https://www.lesswrong.com/posts/4dFzBkpjx6aHZpjqL/reaching-young-math-compsci-talent", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reaching%20young%20math%2Fcompsci%20talent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReaching%20young%20math%2Fcompsci%20talent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4dFzBkpjx6aHZpjqL%2Freaching-young-math-compsci-talent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reaching%20young%20math%2Fcompsci%20talent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4dFzBkpjx6aHZpjqL%2Freaching-young-math-compsci-talent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4dFzBkpjx6aHZpjqL%2Freaching-young-math-compsci-talent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 355, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>Here is yet another way to purchase AI risk reduction...</p>\n<p>Much of the work needed for <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a> and <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\">improved algorithmic decision theories</a> requires researchers to <em>invent new math</em>. That's why the Singularity Institute's recruiting efforts have been aimed a talent in math and computer science. Specifically, we're looking for <em>young</em>&nbsp;talent in math and compsci, because <em>young</em> talent&nbsp;is (1) more open to considering radical ideas like AI risk, (2) not yet entrenched in careers and status games, and (3) better at inventing new math (due to cognitive decline with age).</p>\n<p>So how can the Singularity Institute reach out to young math/compsci talent? Perhaps surprisingly, <em><a href=\"http://hpmor.com/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;is one of the best tools we have for this. It is read by a surprisingly large proportion of people in math and CS departments. Here are some other projects we have in the works:</p>\n<ul>\n<li>Run <a href=\"http://www.appliedrationality.com/\">SPARC</a>, a summer program on rationality for high school students with exceptional math ability. <em>Cost:</em>&nbsp;roughly $30,000. (There won't be classes on x-risk at SPARC, but it will attract young talent toward efficient altruism in general.)</li>\n<li>Print copies of the first few chapters of <em>HPMoR</em>&nbsp;cheaply in Taiwan, ship them here, distribute them to leading math and compsci departments. <em>Cost estimate in progress</em>.</li>\n<li>Send copies of <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em>&nbsp;to lists of bright young students. <em>Cost estimate in progress</em>.</li>\n</ul>\n<p>Here are some things we <em>could</em>&nbsp;be doing if we had sufficient funding:</p>\n<ul>\n<li>Sponsor and be present at events where young math/compsci talent gathers, e.g. <a href=\"http://community.topcoder.com/tc?module=Static&amp;d1=hs&amp;d2=home\">TopCoder High School</a>&nbsp;and the <a href=\"http://www.imo-official.org/\">International Math Olympiad</a>. <em>Cost estimate in progress</em>.</li>\n<li>Cultivate a network of x-risk reducers with high mathematical ability, build a database of conversations for them to have with strategically important young math/compsci talent, schedule those conversations and develop a pipeline so that interested prospects have a \"next person\" to talk to. <em>Cost estimate in progress</em>.</li>\n<li>Write <a href=\"/lw/cr7/proposal_for_open_problems_in_friendly_ai/\">Open Problems in Friendly AI</a>, send it to interested parties so that even those who don't think AI risk is <em>important</em>&nbsp;will at least see \"Ooh, look at these sexy, interesting problems I could work on!\"</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4dFzBkpjx6aHZpjqL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 9.149033444022686e-07, "legacy": true, "legacyId": "16567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "af9MjBqF2hgu3EN6r", "YiyAdz2DcmkR7Nz2H"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T21:11:51.377Z", "modifiedAt": null, "url": null, "title": "Complexity of value has implications for Torture vs Specks", "slug": "complexity-of-value-has-implications-for-torture-vs-specks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.227Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Smq78wqcuocFHK5pd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MKpM9Ppu3eBTGuNzr/complexity-of-value-has-implications-for-torture-vs-specks", "pageUrlRelative": "/posts/MKpM9Ppu3eBTGuNzr/complexity-of-value-has-implications-for-torture-vs-specks", "linkUrl": "https://www.lesswrong.com/posts/MKpM9Ppu3eBTGuNzr/complexity-of-value-has-implications-for-torture-vs-specks", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complexity%20of%20value%20has%20implications%20for%20Torture%20vs%20Specks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplexity%20of%20value%20has%20implications%20for%20Torture%20vs%20Specks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMKpM9Ppu3eBTGuNzr%2Fcomplexity-of-value-has-implications-for-torture-vs-specks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complexity%20of%20value%20has%20implications%20for%20Torture%20vs%20Specks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMKpM9Ppu3eBTGuNzr%2Fcomplexity-of-value-has-implications-for-torture-vs-specks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMKpM9Ppu3eBTGuNzr%2Fcomplexity-of-value-has-implications-for-torture-vs-specks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 346, "htmlBody": "<blockquote>\n<p><a title=\"Value\" href=\"/lw/y3/value_is_fragile/\" target=\"_self\">Value</a> isn't just complicated, it's <em>fragile.</em>&nbsp; There is <em>more than one dimension</em> of human value, where <em>if just that one thing is lost</em>, the Future becomes null.&nbsp; A <em>single</em> blow and <em>all </em>value shatters.&nbsp; Not every <em>single </em>blow will shatter <em>all </em>value - but more than one possible \"single blow\" will do so.</p>\n</blockquote>\n<p>Reasoning using a representation of human utility that's a simple continuum from pain to pleasure, as <a title=\"Torture vs Dust Specks\" href=\"/lw/kn/torture_vs_dust_specks/\" target=\"_blank\">torture vs dust specks</a> does, is a shattering blow to the complexity of value.&nbsp;</p>\n<p>Making moral decisions of such vast scope without understanding the full multidimensionality of human experience and utility is completely irresponsible. An AI using the kind of reasoning found in Torture vs Specks would probably just wirehead everyone for huge-integer-pleasure for eternity.</p>\n<p>I don't pretend to know the correct answer to Torture vs Specks because I don't have a full understanding of human value, and because I don't understand how to do calculations with <a title=\"hypercomplex numbers\" href=\"http://en.wikipedia.org/wiki/Hypercomplex_number\" target=\"_self\">hypercomplex numbers</a>.&nbsp; A friendly AI *has* to take into account the full complexity of our value and not just a one-dimensional continuum whenever it makes any moral decision.&nbsp; So only a friendly AI which has correctly extrapolated our values can know to high confidence the best answer to torture vs specks.</p>\n<p>&nbsp;</p>\n<p>(edit 1) re:Oscar Cunningham</p>\n<p>Why does complexity of value apply here specifically and not a curiosity stopper? Well consequentialist problems come in different difficulty levels - Torture for 5 years vs Torture for 50 years is easy - torture is bad, so less torture is less bad. You are comparing amounts of the same thing. You don't have to understand complexity of value to do that. To compare the value of two very different things, like Torture and Specks, requires you to understand the complexity of value. You can't simplify experiences to integers, because complex value isn't simply an integer.</p>\n<p>The intuition that torture must be outweighed by a large enough number of specks, is just that: an intuition. You don't know the dynamics involved in a formal comparison based on a technical understanding of complex value. <br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MKpM9Ppu3eBTGuNzr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -13, "extendedScore": null, "score": 9.149057259316509e-07, "legacy": true, "legacyId": "16637", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GNnHHmm8EzePmKzPk", "3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-02T21:39:04.860Z", "modifiedAt": null, "url": null, "title": "Raising safety-consciousness among AGI researchers", "slug": "raising-safety-consciousness-among-agi-researchers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:09.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6Lg8RWL9pEvoAeEvr/raising-safety-consciousness-among-agi-researchers", "pageUrlRelative": "/posts/6Lg8RWL9pEvoAeEvr/raising-safety-consciousness-among-agi-researchers", "linkUrl": "https://www.lesswrong.com/posts/6Lg8RWL9pEvoAeEvr/raising-safety-consciousness-among-agi-researchers", "postedAtFormatted": "Saturday, June 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20safety-consciousness%20among%20AGI%20researchers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20safety-consciousness%20among%20AGI%20researchers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Lg8RWL9pEvoAeEvr%2Fraising-safety-consciousness-among-agi-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20safety-consciousness%20among%20AGI%20researchers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Lg8RWL9pEvoAeEvr%2Fraising-safety-consciousness-among-agi-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Lg8RWL9pEvoAeEvr%2Fraising-safety-consciousness-among-agi-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 282, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>Another method for purchasing AI risk reduction is to raise the safety-consciousness of researchers doing work related to AGI.</p>\n<p>The Singularity Institute is conducting a study of&nbsp;scientists who decided to either (1) stop researching some topic after realizing it might be dangerous, or who (2) forked their career into advocacy, activism, ethics, etc. because they became concerned about the potential negative consequences of their work. From this historical inquiry we hope to learn some things about what causes scientists to become so concerned about the consequences of their work that they&nbsp;<em>take action</em>. Some of the examples we've found so far: Michael Michaud (resigned from SETI in part due to worries about the safety of trying to contact ET),&nbsp;Joseph Rotblat (resigned from the Manhattan Project before the end of the war due to concerns about the destructive impact of nuclear weapons), and Paul Berg (became part of a self-imposed moratorium on recombinant DNA back when it was still unknown how dangerous this new technology could be).</p>\n<p>What else can be done?</p>\n<ul>\n<li>Academic outreach, in the form of <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">conversations with AGI researchers</a> and \"basics\" papers like <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a> or <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a>.</li>\n<li><a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">A scholarly AI risk wiki</a>.</li>\n<li><a href=\"/lw/cr0/short_primers_on_crucial_topics/\">Short primers on crucial topics</a>.</li>\n<li>Whatever is suggested by our analysis of past researchers who took action in response to their concerns about the ethics of their research, and by other analyses of human behavior.</li>\n</ul>\n<p>Naturally, these efforts should be directed toward researchers who are both highly competent and whose work is very relevant to development toward AGI: researchers like <a href=\"http://web.mit.edu/cocosci/josh.html\">Josh Tenenbaum</a>, <a href=\"http://www.vetta.org/\">Shane Legg</a>, and <a href=\"http://en.wikipedia.org/wiki/Henry_Markram\">Henry Markram</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6Lg8RWL9pEvoAeEvr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 21, "extendedScore": null, "score": 9.149179450975879e-07, "legacy": true, "legacyId": "16638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "mYuMdmMmGM7fFj382", "LFfizpc5YupK7pWNv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-03T02:50:07.532Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Ultimate Source", "slug": "seq-rerun-the-ultimate-source", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:33.451Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/heN3BvykQnYn9bRyy/seq-rerun-the-ultimate-source", "pageUrlRelative": "/posts/heN3BvykQnYn9bRyy/seq-rerun-the-ultimate-source", "linkUrl": "https://www.lesswrong.com/posts/heN3BvykQnYn9bRyy/seq-rerun-the-ultimate-source", "postedAtFormatted": "Sunday, June 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Ultimate%20Source&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Ultimate%20Source%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FheN3BvykQnYn9bRyy%2Fseq-rerun-the-ultimate-source%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Ultimate%20Source%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FheN3BvykQnYn9bRyy%2Fseq-rerun-the-ultimate-source", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FheN3BvykQnYn9bRyy%2Fseq-rerun-the-ultimate-source", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>Today's post, <a href=\"/lw/rc/the_ultimate_source/\">The Ultimate Source</a> was originally published on 15 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Ultimate_Source\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is a school of thought in philosophy that says that even if you make a decision, that still isn't enough to conclude that you have free will. You have to have been the ultimate source of your decision. Nothing else can have influenced it previously. This doesn't work. There is no such thing as \"the ultimate source\" of your decisions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cth/seq_rerun_possibility_and_couldness/\">Possibility and Could-ness</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "heN3BvykQnYn9bRyy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.150575705457727e-07, "legacy": true, "legacyId": "16639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EsMhFZuycZorZNRF5", "GAiJjGiSZ5evRTLNe", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-03T04:36:25.012Z", "modifiedAt": null, "url": null, "title": "Seeking ethical rules-of-thumb for comparison", "slug": "seeking-ethical-rules-of-thumb-for-comparison", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HM2P77CuSW5fXSycC/seeking-ethical-rules-of-thumb-for-comparison", "pageUrlRelative": "/posts/HM2P77CuSW5fXSycC/seeking-ethical-rules-of-thumb-for-comparison", "linkUrl": "https://www.lesswrong.com/posts/HM2P77CuSW5fXSycC/seeking-ethical-rules-of-thumb-for-comparison", "postedAtFormatted": "Sunday, June 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20ethical%20rules-of-thumb%20for%20comparison&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20ethical%20rules-of-thumb%20for%20comparison%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHM2P77CuSW5fXSycC%2Fseeking-ethical-rules-of-thumb-for-comparison%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20ethical%20rules-of-thumb%20for%20comparison%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHM2P77CuSW5fXSycC%2Fseeking-ethical-rules-of-thumb-for-comparison", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHM2P77CuSW5fXSycC%2Fseeking-ethical-rules-of-thumb-for-comparison", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 336, "htmlBody": "<p>Rules-of-thumb are handy, in that they let you use a solution you've figured out beforehand without having to take the time and effort to re-derive it in the heat of the moment. They may not apply in all situations, they may not provide the absolutely maximally best answer, but in situations where you have limited time to come up with an answer, they can certainly provide the best answer that it's possible for you to come up with in the time you have to think about it.</p>\n<p>I'm currently seeking fairly fundamental rules-of-thumb, which can serve as overall ethical guidelines, or even as the axioms for a full ethical system; and preferably ones that can pass at least the basic sniff-test of actually being usable in everyday life; so that I can compare them with each other, and try to figure out ahead of time whether any of them would work better than the others, either in specific sorts of situations or in general.</p>\n<p>Here are a few examples of what I'm thinking of:</p>\n<p>* Pacifism. Violence is bad, so never use violence. In game theory, this would be the 'always cooperate' strategy of the Iterated Prisoner's Dilemma, and is the simplest strategy that satisfies the criteria of being 'nice'.</p>\n<p>* Zero-Aggression Principle. Do not /initiate/ violence, but if violence is used against you, act violently in self-defense. The foundation of many variations of libertarianism. In the IPD, this satisfies both the criteria of being 'nice' and being 'retaliating'.</p>\n<p>* Proportional Force. Aim for the least amount of violence to be done: \"Avoid rather than check, check rather than harm...\". This meets being 'nice', 'retaliating', and in a certain sense, 'forgiving', for the IPD.</p>\n<p>&nbsp;</p>\n<p>I'm hoping to learn of rules-of-thumb which are at least as useful as the ZAP; I know and respect certain people who base their own ethics on the ZAP, but reject the idea of proportional force, and am hoping to learn of additional alternatives so I can have a better idea of the range of available options.</p>\n<p>Any suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HM2P77CuSW5fXSycC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "16640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-03T11:02:24.212Z", "modifiedAt": null, "url": null, "title": "What are you working on? June 2012", "slug": "what-are-you-working-on-june-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.580Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sCDabKF2B3vqwsk8Z/what-are-you-working-on-june-2012", "pageUrlRelative": "/posts/sCDabKF2B3vqwsk8Z/what-are-you-working-on-june-2012", "linkUrl": "https://www.lesswrong.com/posts/sCDabKF2B3vqwsk8Z/what-are-you-working-on-june-2012", "postedAtFormatted": "Sunday, June 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20June%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20June%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCDabKF2B3vqwsk8Z%2Fwhat-are-you-working-on-june-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20June%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCDabKF2B3vqwsk8Z%2Fwhat-are-you-working-on-june-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCDabKF2B3vqwsk8Z%2Fwhat-are-you-working-on-june-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>This is the bimonthly 'What are you working On?' thread. Previous threads are <a href=\"http://lesswrong.com/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why  this project and not others? Mention reasons why you're doing the  project and/or why others should contribute to your project (if  applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sCDabKF2B3vqwsk8Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "16641", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T01:37:52.788Z", "modifiedAt": null, "url": null, "title": "Meetup : Summer Festival Megameetup at NYC", "slug": "meetup-summer-festival-megameetup-at-nyc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CGAeib9xGEbmjtqBP/meetup-summer-festival-megameetup-at-nyc", "pageUrlRelative": "/posts/CGAeib9xGEbmjtqBP/meetup-summer-festival-megameetup-at-nyc", "linkUrl": "https://www.lesswrong.com/posts/CGAeib9xGEbmjtqBP/meetup-summer-festival-megameetup-at-nyc", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Summer%20Festival%20Megameetup%20at%20NYC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Summer%20Festival%20Megameetup%20at%20NYC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCGAeib9xGEbmjtqBP%2Fmeetup-summer-festival-megameetup-at-nyc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Summer%20Festival%20Megameetup%20at%20NYC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCGAeib9xGEbmjtqBP%2Fmeetup-summer-festival-megameetup-at-nyc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCGAeib9xGEbmjtqBP%2Fmeetup-summer-festival-megameetup-at-nyc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/ap\">Summer Festival Megameetup at NYC</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">23 June 2012 02:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">North Meadow New York, 10029</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The weekend of June 23rd, the New York Less Wrong community will be hosting a summer festival. Less Wrongers from around the world are invited to intend.</p>\n<p>Features include: <br /> &bull; A Saturday of sun and fun in Central Park. (Unless rain is forecast, in which case this will be on Sunday). <br /> &bull; Picnic Lunch. Share food and swap stories with Less Wrong folks. Engage in intellectual discussion while stimulating your monkey brain's desire for tribal bonding. <br /> &bull; Compete in feats of strength and sport. <br /> &bull; Singing and dancing.</p>\n<p>There will be other get-togethers on the surrounding days, most notably a dinner party on Friday the 22nd.</p>\n<p>If you are coming, please RSVP so we can plan appropriately (replying to this thread is fine)</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/ap\">Summer Festival Megameetup at NYC</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CGAeib9xGEbmjtqBP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.156719925008663e-07, "legacy": true, "legacyId": "16646", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Summer_Festival_Megameetup_at_NYC\">Discussion article for the meetup : <a href=\"/meetups/ap\">Summer Festival Megameetup at NYC</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">23 June 2012 02:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">North Meadow New York, 10029</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The weekend of June 23rd, the New York Less Wrong community will be hosting a summer festival. Less Wrongers from around the world are invited to intend.</p>\n<p>Features include: <br> \u2022 A Saturday of sun and fun in Central Park. (Unless rain is forecast, in which case this will be on Sunday). <br> \u2022 Picnic Lunch. Share food and swap stories with Less Wrong folks. Engage in intellectual discussion while stimulating your monkey brain's desire for tribal bonding. <br> \u2022 Compete in feats of strength and sport. <br> \u2022 Singing and dancing.</p>\n<p>There will be other get-togethers on the surrounding days, most notably a dinner party on Friday the 22nd.</p>\n<p>If you are coming, please RSVP so we can plan appropriately (replying to this thread is fine)</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Summer_Festival_Megameetup_at_NYC1\">Discussion article for the meetup : <a href=\"/meetups/ap\">Summer Festival Megameetup at NYC</a></h2>", "sections": [{"title": "Discussion article for the meetup : Summer Festival Megameetup at NYC", "anchor": "Discussion_article_for_the_meetup___Summer_Festival_Megameetup_at_NYC", "level": 1}, {"title": "Discussion article for the meetup : Summer Festival Megameetup at NYC", "anchor": "Discussion_article_for_the_meetup___Summer_Festival_Megameetup_at_NYC1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T03:51:58.920Z", "modifiedAt": null, "url": null, "title": "\"Progress\"", "slug": "progress", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jMc4oP48AtJvAY6gD/progress", "pageUrlRelative": "/posts/jMc4oP48AtJvAY6gD/progress", "linkUrl": "https://www.lesswrong.com/posts/jMc4oP48AtJvAY6gD/progress", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Progress%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Progress%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjMc4oP48AtJvAY6gD%2Fprogress%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Progress%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjMc4oP48AtJvAY6gD%2Fprogress", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjMc4oP48AtJvAY6gD%2Fprogress", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 373, "htmlBody": "<p>I often hear people speak of democracy as the next, or the final, inevitable stage of human social development.&nbsp; Its inevitability is usually justified not by describing power relations that result in democracy being a stable attractor, but in terms of morality - democracy is more \"enlightened\".&nbsp; I don't see any inevitability to it - China and the Soviet Union manage(d) to maintain large, technologically-advanced nations for a long time without it - but suppose, for the sake of argument, that democracy is the inevitable next stage of human progress.</p>\n<p>The May 18 2012 issue of Science has an article on p. 844, \"Ancestral hierarchy and conflict\", by Christopher Boehm, which, among other things, describes the changes over time of equality among male hominids.&nbsp; If we add its timeline to recent human history, then here is the history of democracy over time in the evolutionary line leading to humans:</p>\n<ol>\n<li>Pre-human male hominids, we infer from observing bonobos and chimpanzees, were dominated by one alpha male per group, who got the best food and most of the females.</li>\n<li>Then, in the human lineage, hunter-gatherers developed larger social groups, and the ability to form stronger coalitions against the alpha; and they became more egalitarian.</li>\n<li>Then, human social groups even became larger, and it became possible for a central alpha-male chieftain to control a large area; and the groups became less egalitarian.</li>\n<li>Then, they became even larger, so that they were too large for a central authority to administer efficiently; and decentralized market-based methods of production led to democracy.&nbsp; (Or so goes one story.)</li>\n</ol>\n<p>There are two points to observe in this data:</p>\n<ul>\n<li>There is no linear relationship between social complexity, and equality.&nbsp; Steadily-increasing social complexity lead to more equality, then less, then more.</li>\n<li>Enlightenment has nothing to do with it - if any theory makes sense, it is that social equality tunes itself to the level that provides maximal social competitive fitness.&nbsp; Even if we agree that democracy is the most-enlightened political system, this realization says nothing about what the future holds.</li>\n</ul>\n<p>I do believe \"progress\" is a meaningful term.&nbsp; But there isn't some cosmic niceness built into the universe that makes everything improve monotonically along every dimension at once.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jMc4oP48AtJvAY6gD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": -4, "extendedScore": null, "score": 9.15732273056731e-07, "legacy": true, "legacyId": "16652", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T04:04:17.048Z", "modifiedAt": null, "url": null, "title": "Boltzmann Brains and Anthropic Reference Classes (Updated)", "slug": "boltzmann-brains-and-anthropic-reference-classes-updated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:29.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pragmatist", "createdAt": "2011-08-26T17:36:14.792Z", "isAdmin": false, "displayName": "pragmatist"}, "userId": "gs25cnPDLYqK8H68Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ipXsAPZ7kEmDyrjtS/boltzmann-brains-and-anthropic-reference-classes-updated", "pageUrlRelative": "/posts/ipXsAPZ7kEmDyrjtS/boltzmann-brains-and-anthropic-reference-classes-updated", "linkUrl": "https://www.lesswrong.com/posts/ipXsAPZ7kEmDyrjtS/boltzmann-brains-and-anthropic-reference-classes-updated", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boltzmann%20Brains%20and%20Anthropic%20Reference%20Classes%20(Updated)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoltzmann%20Brains%20and%20Anthropic%20Reference%20Classes%20(Updated)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipXsAPZ7kEmDyrjtS%2Fboltzmann-brains-and-anthropic-reference-classes-updated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boltzmann%20Brains%20and%20Anthropic%20Reference%20Classes%20(Updated)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipXsAPZ7kEmDyrjtS%2Fboltzmann-brains-and-anthropic-reference-classes-updated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipXsAPZ7kEmDyrjtS%2Fboltzmann-brains-and-anthropic-reference-classes-updated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1124, "htmlBody": "<p><em>Summary: There are claims that Boltzmann brains pose a significant problem for contemporary cosmology. But this problem relies on assuming that Boltzmann brains would be part of the appropriate reference class for anthropic reasoning. Is there a good reason to accept this assumption?</em></p>\n<p>Nick Bostrom's Self Sampling Assumption (SSA) says that when accounting for indexical information, one should reason as if one were a random sample from the set of all observer's in one's reference class. As an example of the scientific usefulness of anthropic reasoning, Bostrom shows how the SSA rules out a particular cosmological model suggested by Boltzmann. Boltzmann was trying to construct a model that is symmetric under time reversal, but still accounts for the pervasive temporal asymmetry we observe. The idea is that the universe is eternal and, at most times and places, at thermodynamic equilibrium. Occasionally, there will be chance fluctuations away from equilibrium, creating pockets of low entropy. Life can only develop in these low entropy pockets, so it is no surprise that we find ourselves in such a region, even though it is atypical.</p>\n<p>The objection to this model is that smaller fluctuations from equilibrium will be more common. In particular, fluctuations that produce disembodied brains floating in a high entropy soup with the exact brain state I am in right now (called Boltzmann brains) would be vastly more common than fluctuations that actually produce me and the world around me. If we reason according to SSA, the Boltzmann model predicts I am one of those brains and all my experiences are spurious. Conditionalizing on the model, the probability that my experiences are not spurious is minute. But my experiences are in fact not spurious (or at least, I must operate under the assumption that they are not if I am to meaningfully engage in scientific inquiry). So the Boltzmann model is heavily disconfirmed. [EDIT: As <a href=\"/r/discussion/lw/cuj/boltzmann_brains_and_anthropic_reference_classes/6rab\">AlexSchell points out</a>, this is not actually Bostrom's argument. The argument has been made by others. <a href=\"http://arxiv.org/abs/1008.0808\">Here</a>, for example.]</p>\n<p>Now, no one (not even Boltzmann) actually believed the Boltzmann model, so this might seem like an unproblematic result. Unfortunately, it turns out that our current best cosmological models also predict a preponderance of Boltzmann brains. They predict that the universe is evolving towards an eternally expanding cold <a href=\"http://okc.albanova.se/blog/the-return-of-de-sitter/\">de Sitter phase</a>. Once the universe is in this phase, thermal fluctuations of quantum fields will lead to an infinity of Boltzmann brains. So if the argument against the original Boltzmann model is correct, these cosmological models should also be rejected. Some people have drawn this conclusion. For instance, <a href=\"http://www.sciencedirect.com/science/article/pii/S0370269308010459 \">Don Page</a> considers the anthropic argument strong evidence against the claim that the universe will last forever. This seems like the SSA's version of Bostrom's <a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">Presumptuous Philosopher objection to the Self Indication Assumption</a>, except here we have a presumptuous physicist. If your intuitions in the Presumptuous Philosopher case lead you to reject SIA, then perhaps the right move in this case is to reject SSA.</p>\n<p>But maybe SSA can be salvaged. The rule specifies that one need only consider observers <em>in one's reference class</em>. If Boltzmann brains can be legitimately excluded from the reference class, then the SSA does not threaten cosmology. But Bostrom claims that the reference class must <em>at least</em> contain all observers whose phenomenal state is subjectively indistinguishable from mine. If that's the case, then all Boltzmann brains in brain states sufficiently similar to mine such that there is no phenomenal distinction must be in my reference class, and there's going to be a lot of them.</p>\n<p>Why accept this subjective indistinguishability criterion though? I think the intuition behind it is that if two observers are subjectively indistinguishable (it feels the same to be either one), then they are evidentially indistinguishable, i.e. the evidence available to them is the same. If A and B are in the exact same brain state, then, according to this claim, A has no evidence that she is in fact A and not B. And in this case, it is illegitimate for her to exclude B from her anthropic reference class. For all she knows, she might be B!</p>\n<p>But the move from subjective indistinguishability to evidential indistinguishability seems to ignore an important point: <a href=\"http://plato.stanford.edu/entries/content-externalism/\">meanings ain't just in the head</a>. Even if two brains are in the exact same physical state, the contents of their representational states (beliefs, for example) can differ. The contents of these states depend not just on the brain state but also on the brain's environment and causal history. For instance, I have beliefs about Barack Obama. A spontaneously congealed Boltzmann brain in an identical brain state could not have those beliefs. There is no appropriate causal connection between Obama and that brain, so how could its beliefs be about him? And if we have different beliefs, then I can know things the brain doesn't know. Which means I can have evidence the brain doesn't have. Subjective indistinguishability does not entail evidential indistinguishability.</p>\n<p>So at least this argument for including all subjectively indistinguishable observers in one's reference class fails. Is there another good reason for this constraint I haven't considered?</p>\n<p><strong>Update</strong>: There seems to be a common misconception arising in the comments, so I thought I'd address it up here. A number of commenters are equating the Boltzmann brain problem with radical skepticism. The claim is that the problem shows that we can't really know we are not Boltzmann brains. Now this might be a problem some people are interested in. It is not one that I am interested in, nor is it the problem that exercises cosmologists. The Boltzmann brain hypothesis is not just a physically plausible variant of the Matrix hypothesis.</p>\n<p>The purported problem for cosmology is that certain cosmological models, in conjunction with the SSA, predict that I am a Boltzmann brain. This is not a problem because it shows that I am in fact a Boltzmann brain. It is a problem because it is an apparent disconfirmation of the cosmological model. I am <em>not</em> actually a Boltzmann brain, I assure you. So if a model says that it is highly probable I am one, then the observation that I am not stands as strong evidence against the model. This argument explicitly relies on the rejection of radical skepticism.</p>\n<p>Are we justified in rejecting radical skepticism? I think the answer is obviously yes, but if you are in fact a skeptic then I guess this won't sway you. Still, if you are a skeptic, your response to the Boltzmann brain problem shouldn't be, \"Aha, here's support for my skepticism!\" It should be \"Well, all of the physics on which this problem is based comes from experimental evidence that <em>doesn't actually exist</em>! So I have no reason to take the problem seriously. Let me move on to another imaginary post.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ipXsAPZ7kEmDyrjtS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -7, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "16651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcvyJjPQwimAeapNg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T05:10:00.611Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Passing the Recursive Buck", "slug": "seq-rerun-passing-the-recursive-buck", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M3PxXoYJrSwPk7aKy/seq-rerun-passing-the-recursive-buck", "pageUrlRelative": "/posts/M3PxXoYJrSwPk7aKy/seq-rerun-passing-the-recursive-buck", "linkUrl": "https://www.lesswrong.com/posts/M3PxXoYJrSwPk7aKy/seq-rerun-passing-the-recursive-buck", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Passing%20the%20Recursive%20Buck&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Passing%20the%20Recursive%20Buck%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM3PxXoYJrSwPk7aKy%2Fseq-rerun-passing-the-recursive-buck%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Passing%20the%20Recursive%20Buck%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM3PxXoYJrSwPk7aKy%2Fseq-rerun-passing-the-recursive-buck", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM3PxXoYJrSwPk7aKy%2Fseq-rerun-passing-the-recursive-buck", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/rd/passing_the_recursive_buck/\">Passing the Recursive Buck</a> was originally published on 16 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Passing_the_Recursive_Buck\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When confronted with a difficult question, don't try to point backwards to a misunderstood black box. Ask yourself, what's inside the black box? If the answer is another black box, you likely have a problem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cu7/seq_rerun_the_ultimate_source/\">The Ultimate Source</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M3PxXoYJrSwPk7aKy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.157673509108036e-07, "legacy": true, "legacyId": "16653", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rw3oKLjG85BdKNXS2", "heN3BvykQnYn9bRyy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T09:04:19.816Z", "modifiedAt": null, "url": null, "title": "A plan for Pascal's mugging?", "slug": "a-plan-for-pascal-s-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yttrium", "createdAt": "2011-11-26T23:30:29.876Z", "isAdmin": false, "displayName": "yttrium"}, "userId": "BBzE4abSkhSfkQ89D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RaCKodEM7FEJ2FwFw/a-plan-for-pascal-s-mugging", "pageUrlRelative": "/posts/RaCKodEM7FEJ2FwFw/a-plan-for-pascal-s-mugging", "linkUrl": "https://www.lesswrong.com/posts/RaCKodEM7FEJ2FwFw/a-plan-for-pascal-s-mugging", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20plan%20for%20Pascal's%20mugging%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20plan%20for%20Pascal's%20mugging%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRaCKodEM7FEJ2FwFw%2Fa-plan-for-pascal-s-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20plan%20for%20Pascal's%20mugging%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRaCKodEM7FEJ2FwFw%2Fa-plan-for-pascal-s-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRaCKodEM7FEJ2FwFw%2Fa-plan-for-pascal-s-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 313, "htmlBody": "<p>The idea is to compare not the results of actions, but the results of decision algorithms. The question that the agent should ask itself is thus:</p>\n<p>\"Suppose everyone<sup>1</sup> who runs the same thinking procedure like me uses decision algorithm X. What utility would I get at the 50th percentile (not: what expected utility should I get), after my life is finished?\"<br />Then, he should of course look for the X that maximizes this value.</p>\n<p>Now, if you formulate a turing-complete \"decision algorithm\", this heads into an infinite loop. But suppose that \"decision algorithm\" is defined as a huge table for lots of different possible situations, and the appropriate outputs.</p>\n<p>Let's see what results such a thing should give:</p>\n<ul>\n<li>If the agent has the possibility to play a gamble, and the probabilities involved are not small, and he expects to be allowed to play many gambles like this in the future, he should decide exactly as if he was maximizing expected utility: If he has made many decisions like this, he will get a positive utility difference in the 50th percentile if and only if his expected utility from playing the gamble is positive.</li>\n<li>However, if Pascal's mugger comes along, he will decline: The complete probability of living in a universe where people like this mugger ought to be taken seriously is small. In the probability distribution over expected utility at the end of the agent's lifetime, the possibility of getting tortured will manifest itself only very slightly at the 50th percentile - much less than the possibility of losing 5 Dollars.</li>\n</ul>\n<p>The reason why humans will intuitively decline to give money to the mugger might be similar: They imagine not the expected utility with both decisions, but the typical outcome of giving the mugger some money, versus declining to.</p>\n<p><sup>1</sup>I say this to make agents of the same type cooperate in prisoner-like dilemmas.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RaCKodEM7FEJ2FwFw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -2, "extendedScore": null, "score": 9.158727044608318e-07, "legacy": true, "legacyId": "16668", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T09:33:57.353Z", "modifiedAt": null, "url": null, "title": "\"Where Am I?\" by Daniel Dennett", "slug": "where-am-i-by-daniel-dennett-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "qxLM4trnENBcywbCi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ST8diCnRYStmXLxw/where-am-i-by-daniel-dennett-0", "pageUrlRelative": "/posts/3ST8diCnRYStmXLxw/where-am-i-by-daniel-dennett-0", "linkUrl": "https://www.lesswrong.com/posts/3ST8diCnRYStmXLxw/where-am-i-by-daniel-dennett-0", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Where%20Am%20I%3F%22%20by%20Daniel%20Dennett&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Where%20Am%20I%3F%22%20by%20Daniel%20Dennett%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ST8diCnRYStmXLxw%2Fwhere-am-i-by-daniel-dennett-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Where%20Am%20I%3F%22%20by%20Daniel%20Dennett%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ST8diCnRYStmXLxw%2Fwhere-am-i-by-daniel-dennett-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ST8diCnRYStmXLxw%2Fwhere-am-i-by-daniel-dennett-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \" lang=\"EN-US\">[&rdquo;Where Am I?&rdquo;](</span><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \"><a href=\"http://www.newbanner.com/SecHumSCM/WhereAmI.html\"><span style=\"color:blue;mso-ansi-language:EN-US\" lang=\"EN-US\">http://www.newbanner.com/SecHumSCM/WhereAmI.html</span></a></span><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \" lang=\"EN-US\">)&nbsp;short story by&nbsp;[Daniel Dennett&rsquo;s](</span><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \"><a href=\"http://en.wikipedia.org/wiki/Daniel_Dennett\"><span style=\"color:blue;mso-ansi-language:EN-US\" lang=\"EN-US\">http://en.wikipedia.org/wiki/Daniel_Dennett</span></a></span><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \" lang=\"EN-US\">) f</span><span style=\"font-family: Verdana, sans-serif; font-size: 10pt; background-color: white;\">rom his book</span><span style=\"font-family: Verdana, sans-serif; font-size: 10pt;\">&nbsp;</span><em style=\"font-family: Verdana, sans-serif; font-size: 10pt;\">Brainstorms: Philosophical Essays on Mind and Psychology (1978). </em><span style=\"font-family: Verdana, sans-serif; font-size: x-small;\">A few people might already be&nbsp;</span><span style=\"background-color: white; font-family: Verdana, sans-serif; font-size: 10pt; line-height: 115%;\">familiar with it.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \" lang=\"EN-US\"> The story is a humorous semi-science fiction one, where Dennett gets a job offer form Pentagon that entails moving his brain into a vat, without actually moving his point of view. Later on it brings up questions about uploading and what it would mean in terms of diverging perspectives and so on. Aside from being a joy to read, it offers&nbsp;solutions to&nbsp;a few hurdles about the&nbsp;<em>nature of consciousnesses&nbsp;</em>and&nbsp;<em>personal&nbsp;identity.&nbsp;</em><br /> </span>\n<p><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \" lang=\"EN-US\"><br /></span><span style=\"font-size: 10pt; font-family: Verdana, sans-serif; \" lang=\"EN-US\"> &gt;<span style=\"background:white\">Suppose, I argued to myself, I were now to fly to California, rob a bank, and be apprehended.&nbsp; In which state would I be tried:&nbsp; in California, where the robbery took place, or in Texas, where the brains of the outfit were located?&nbsp; Would I be a California felon with an out-of-state brain, or a Texas felon remotely controlling an accomplice of sorts in California? It seemed possible that I might beat such a rap just on the undecidability of that jurisdictional question, though perhaps it would be deemed an interstate, and hence Federal, offense.</span></span></p>\n</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ST8diCnRYStmXLxw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "16669", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T09:45:02.041Z", "modifiedAt": null, "url": null, "title": "\"Where Am I?\", by Daniel Dennett ", "slug": "where-am-i-by-daniel-dennett", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "qxLM4trnENBcywbCi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p9i4fQM5tcfNv6FjM/where-am-i-by-daniel-dennett", "pageUrlRelative": "/posts/p9i4fQM5tcfNv6FjM/where-am-i-by-daniel-dennett", "linkUrl": "https://www.lesswrong.com/posts/p9i4fQM5tcfNv6FjM/where-am-i-by-daniel-dennett", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Where%20Am%20I%3F%22%2C%20by%20Daniel%20Dennett%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Where%20Am%20I%3F%22%2C%20by%20Daniel%20Dennett%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9i4fQM5tcfNv6FjM%2Fwhere-am-i-by-daniel-dennett%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Where%20Am%20I%3F%22%2C%20by%20Daniel%20Dennett%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9i4fQM5tcfNv6FjM%2Fwhere-am-i-by-daniel-dennett", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9i4fQM5tcfNv6FjM%2Fwhere-am-i-by-daniel-dennett", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p><span style=\"font-size: 10pt; line-height: 115%; font-family: Verdana, sans-serif;\" lang=\"EN-US\"><a href=\"http://www.newbanner.com/SecHumSCM/WhereAmI.html\">&rdquo;Where Am I?&rdquo;</a>&nbsp;is a</span><span style=\"font-size: 10pt; line-height: 115%; font-family: Verdana, sans-serif;\" lang=\"EN-US\">&nbsp;short story by&nbsp;<a href=\"http://en.wikipedia.org/wiki/Daniel_Dennett\">Daniel Dennett</a>&nbsp;from his book&nbsp;</span><em><span style=\"font-family: Verdana, sans-serif; font-size: 10pt; line-height: 115%;\">Brainstorms: Philosophical Essays on Mind and Psychology.</span></em><span style=\"font-size: 10pt; line-height: 115%; font-family: Verdana, sans-serif;\" lang=\"EN-US\">&nbsp;</span><span style=\"font-size: 10pt; line-height: 115%; font-family: Verdana, sans-serif;\" lang=\"EN-US\">Some of you might already be familiar with it.</span></p>\n<p><span lang=\"EN-US\"><span style=\"font-size: 10pt; line-height: 115%; font-family: Verdana, sans-serif;\" lang=\"EN-US\">The story is a humorous semi-science fiction one, where Dennett gets a job offer form Pentagon that entails moving his brain into a vat, without actually moving his point of view. Later on it brings up questions about uploading and what it would mean in terms of diverging perspectives and so on. Aside from being a joy to read, it offers&nbsp;solutions to&nbsp;a few hurdles about the&nbsp;<em>nature of consciousnesses&nbsp;</em>and&nbsp;<em>personal&nbsp;identity.&nbsp;</em><br /> <!--[if !supportLineBreakNewLine]--> <!--[endif]--></span> <span style=\"font-family: Verdana, sans-serif; font-size: 10pt; line-height: 115%; background-color: white; background-position: initial initial; background-repeat: initial initial;\"><br /></span></span></p>\n<blockquote>\n<p><span lang=\"EN-US\"><span style=\"font-family: Verdana, sans-serif; font-size: 10pt; line-height: 115%; background-color: white; background-position: initial initial; background-repeat: initial initial;\">Suppose, I argued to myself, I were now to fly to California, rob a bank, and be apprehended.&nbsp; In which state would I be tried:&nbsp; in California, where the robbery took place, or in Texas, where the brains of the outfit were located?&nbsp; Would I be a California felon with an out-of-state brain, or a Texas felon remotely controlling an accomplice of sorts in California? It seemed possible that I might beat such a rap just on the undecidability of that jurisdictional question, though perhaps it would be deemed an interstate, and hence Federal, offense.</span></span></p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p9i4fQM5tcfNv6FjM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "16670", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T13:49:35.558Z", "modifiedAt": null, "url": null, "title": "Meetup : First Cali, Colombia meetup", "slug": "meetup-first-cali-colombia-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.811Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jpulgarin", "createdAt": "2011-03-23T08:05:56.812Z", "isAdmin": false, "displayName": "jpulgarin"}, "userId": "rco4fXwJv2s5XCyvT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o6oFH8hc6KNgHFiYh/meetup-first-cali-colombia-meetup", "pageUrlRelative": "/posts/o6oFH8hc6KNgHFiYh/meetup-first-cali-colombia-meetup", "linkUrl": "https://www.lesswrong.com/posts/o6oFH8hc6KNgHFiYh/meetup-first-cali-colombia-meetup", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Cali%2C%20Colombia%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Cali%2C%20Colombia%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6oFH8hc6KNgHFiYh%2Fmeetup-first-cali-colombia-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Cali%2C%20Colombia%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6oFH8hc6KNgHFiYh%2Fmeetup-first-cali-colombia-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6oFH8hc6KNgHFiYh%2Fmeetup-first-cali-colombia-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/aq'>First Cali, Colombia meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 July 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Chipichape, Cali, Valle del Cauca, Colombia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Specific Location: Juan Valdez Cafe inside Chipichape Mall</p>\n\n<p><a href=\"http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/\" rel=\"nofollow\">http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/</a> motivated me to post this meetup on the off chance there are any LessWrongers in Cali, Colombia.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/aq'>First Cali, Colombia meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o6oFH8hc6KNgHFiYh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.160009916350714e-07, "legacy": true, "legacyId": "16671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Cali__Colombia_meetup\">Discussion article for the meetup : <a href=\"/meetups/aq\">First Cali, Colombia meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 July 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Chipichape, Cali, Valle del Cauca, Colombia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Specific Location: Juan Valdez Cafe inside Chipichape Mall</p>\n\n<p><a href=\"http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/\" rel=\"nofollow\">http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/</a> motivated me to post this meetup on the off chance there are any LessWrongers in Cali, Colombia.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Cali__Colombia_meetup1\">Discussion article for the meetup : <a href=\"/meetups/aq\">First Cali, Colombia meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Cali, Colombia meetup", "anchor": "Discussion_article_for_the_meetup___First_Cali__Colombia_meetup", "level": 1}, {"title": "Discussion article for the meetup : First Cali, Colombia meetup", "anchor": "Discussion_article_for_the_meetup___First_Cali__Colombia_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4YMFSdxWSK9JgQHDv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T18:03:22.087Z", "modifiedAt": null, "url": null, "title": "Meetup : Garden Grove CA: Less wrong and more tasty.", "slug": "meetup-garden-grove-ca-less-wrong-and-more-tasty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cloudlicker", "createdAt": "2012-04-13T17:26:28.083Z", "isAdmin": false, "displayName": "cloudlicker"}, "userId": "bDgBMkfZ48D53TsnH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sRPk52eSk7XxKJiWG/meetup-garden-grove-ca-less-wrong-and-more-tasty", "pageUrlRelative": "/posts/sRPk52eSk7XxKJiWG/meetup-garden-grove-ca-less-wrong-and-more-tasty", "linkUrl": "https://www.lesswrong.com/posts/sRPk52eSk7XxKJiWG/meetup-garden-grove-ca-less-wrong-and-more-tasty", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Garden%20Grove%20CA%3A%20Less%20wrong%20and%20more%20tasty.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Garden%20Grove%20CA%3A%20Less%20wrong%20and%20more%20tasty.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRPk52eSk7XxKJiWG%2Fmeetup-garden-grove-ca-less-wrong-and-more-tasty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Garden%20Grove%20CA%3A%20Less%20wrong%20and%20more%20tasty.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRPk52eSk7XxKJiWG%2Fmeetup-garden-grove-ca-less-wrong-and-more-tasty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRPk52eSk7XxKJiWG%2Fmeetup-garden-grove-ca-less-wrong-and-more-tasty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ar'>Garden Grove CA: Less wrong and more tasty.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 June 2012 06:01:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10130 Garden Grove Blvd., Garden Grove, CA.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After our nice meet and greet last time it is decided to work on goals. To that end we will be once again returning to the ideally located Genki Living: Snackery and Bakery. So come on out and consume a large dose of logic and peanut butter bacon frosting with us. Plus they jenga</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ar'>Garden Grove CA: Less wrong and more tasty.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sRPk52eSk7XxKJiWG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.161151453556096e-07, "legacy": true, "legacyId": "16672", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Garden_Grove_CA__Less_wrong_and_more_tasty_\">Discussion article for the meetup : <a href=\"/meetups/ar\">Garden Grove CA: Less wrong and more tasty.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 June 2012 06:01:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10130 Garden Grove Blvd., Garden Grove, CA.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After our nice meet and greet last time it is decided to work on goals. To that end we will be once again returning to the ideally located Genki Living: Snackery and Bakery. So come on out and consume a large dose of logic and peanut butter bacon frosting with us. Plus they jenga</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Garden_Grove_CA__Less_wrong_and_more_tasty_1\">Discussion article for the meetup : <a href=\"/meetups/ar\">Garden Grove CA: Less wrong and more tasty.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Garden Grove CA: Less wrong and more tasty.", "anchor": "Discussion_article_for_the_meetup___Garden_Grove_CA__Less_wrong_and_more_tasty_", "level": 1}, {"title": "Discussion article for the meetup : Garden Grove CA: Less wrong and more tasty.", "anchor": "Discussion_article_for_the_meetup___Garden_Grove_CA__Less_wrong_and_more_tasty_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T18:04:52.435Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social/games meetup", "slug": "meetup-melbourne-social-games-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.958Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XyzLfRkLBY25pK7j7/meetup-melbourne-social-games-meetup", "pageUrlRelative": "/posts/XyzLfRkLBY25pK7j7/meetup-melbourne-social-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/XyzLfRkLBY25pK7j7/meetup-melbourne-social-games-meetup", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%2Fgames%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%2Fgames%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXyzLfRkLBY25pK7j7%2Fmeetup-melbourne-social-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%2Fgames%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXyzLfRkLBY25pK7j7%2Fmeetup-melbourne-social-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXyzLfRkLBY25pK7j7%2Fmeetup-melbourne-social-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/as'>Melbourne social/games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 June 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday 15th June, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288 or Scott (shokwave) on 0432 862 932.</p>\n\n<p>Some form of take-away will be organised for dinner and there will be snacks available. BYO drinks and games.</p>\n\n<p>We had about 15 people last time. We always look forward to meeting new people!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/as'>Melbourne social/games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XyzLfRkLBY25pK7j7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.161158227648823e-07, "legacy": true, "legacyId": "16673", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/as\">Melbourne social/games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 June 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday 15th June, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288 or Scott (shokwave) on 0432 862 932.</p>\n\n<p>Some form of take-away will be organised for dinner and there will be snacks available. BYO drinks and games.</p>\n\n<p>We had about 15 people last time. We always look forward to meeting new people!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/as\">Melbourne social/games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social/games meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social/games meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T19:54:46.006Z", "modifiedAt": null, "url": null, "title": "Have you changed your mind lately? On what?", "slug": "have-you-changed-your-mind-lately-on-what", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:56.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e49D6yNRyoLuiEy3f/have-you-changed-your-mind-lately-on-what", "pageUrlRelative": "/posts/e49D6yNRyoLuiEy3f/have-you-changed-your-mind-lately-on-what", "linkUrl": "https://www.lesswrong.com/posts/e49D6yNRyoLuiEy3f/have-you-changed-your-mind-lately-on-what", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Have%20you%20changed%20your%20mind%20lately%3F%20On%20what%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHave%20you%20changed%20your%20mind%20lately%3F%20On%20what%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe49D6yNRyoLuiEy3f%2Fhave-you-changed-your-mind-lately-on-what%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Have%20you%20changed%20your%20mind%20lately%3F%20On%20what%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe49D6yNRyoLuiEy3f%2Fhave-you-changed-your-mind-lately-on-what", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe49D6yNRyoLuiEy3f%2Fhave-you-changed-your-mind-lately-on-what", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>Admitting to being wrong isn't easy, but it's something we want to encourage.</p>\n<p>So ... were you convinced by someone's arguments lately? Did you realize a heated disagreement was actually a misunderstanding? Here's the place to talk about it!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3RnEKrsNgNEDxuNnw": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e49D6yNRyoLuiEy3f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 39, "extendedScore": null, "score": 9.161652628305539e-07, "legacy": true, "legacyId": "16674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-04T21:49:41.397Z", "modifiedAt": null, "url": null, "title": "Meetup : Tucson, Arizona", "slug": "meetup-tucson-arizona", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DeevGrape", "createdAt": "2011-11-02T19:26:47.963Z", "isAdmin": false, "displayName": "DeevGrape"}, "userId": "m9JPq6irpKromu9x9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z7tnc7w8BzDXweMiq/meetup-tucson-arizona", "pageUrlRelative": "/posts/z7tnc7w8BzDXweMiq/meetup-tucson-arizona", "linkUrl": "https://www.lesswrong.com/posts/z7tnc7w8BzDXweMiq/meetup-tucson-arizona", "postedAtFormatted": "Monday, June 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tucson%2C%20Arizona&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tucson%2C%20Arizona%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7tnc7w8BzDXweMiq%2Fmeetup-tucson-arizona%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tucson%2C%20Arizona%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7tnc7w8BzDXweMiq%2Fmeetup-tucson-arizona", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz7tnc7w8BzDXweMiq%2Fmeetup-tucson-arizona", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/at'>Tucson, Arizona</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2443 North Campbell Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey Tucson, After the fun and inspiration of the Rationality Mini-Camp in Berkeley last month, I'm pretty jazzed about rationality, and I'd like to meet some more of my fellow LessWrongers :) So I'll be hanging out in Coffee X Change Wednesday the 20th from 7 to 10, and anyone who's free should come out. I'll be the one with a print-out of the HPMOR cover on my table (<a href=\"http://mike-obee-lay.deviantart.com/art/Harry-Potter-and-the-Methods-of-Rationality-Cover-280590525\" rel=\"nofollow\">http://mike-obee-lay.deviantart.com/art/Harry-Potter-and-the-Methods-of-Rationality-Cover-280590525</a>) We'll be talking about some of the information that was covered at the Mini-Camp, especially how to apply rationality to your own life and decisions; but I'm sure conversation will wander immensely. (Oh, and if you like in-group-out-group conflicts, then just look at Phoenix, with their LW meetup on the 15th... you don't want to let them <em>win</em>, do you? Tucson Uber Alles!)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/at'>Tucson, Arizona</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z7tnc7w8BzDXweMiq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.162169710225704e-07, "legacy": true, "legacyId": "16676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tucson__Arizona\">Discussion article for the meetup : <a href=\"/meetups/at\">Tucson, Arizona</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2443 North Campbell Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey Tucson, After the fun and inspiration of the Rationality Mini-Camp in Berkeley last month, I'm pretty jazzed about rationality, and I'd like to meet some more of my fellow LessWrongers :) So I'll be hanging out in Coffee X Change Wednesday the 20th from 7 to 10, and anyone who's free should come out. I'll be the one with a print-out of the HPMOR cover on my table (<a href=\"http://mike-obee-lay.deviantart.com/art/Harry-Potter-and-the-Methods-of-Rationality-Cover-280590525\" rel=\"nofollow\">http://mike-obee-lay.deviantart.com/art/Harry-Potter-and-the-Methods-of-Rationality-Cover-280590525</a>) We'll be talking about some of the information that was covered at the Mini-Camp, especially how to apply rationality to your own life and decisions; but I'm sure conversation will wander immensely. (Oh, and if you like in-group-out-group conflicts, then just look at Phoenix, with their LW meetup on the 15th... you don't want to let them <em>win</em>, do you? Tucson Uber Alles!)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tucson__Arizona1\">Discussion article for the meetup : <a href=\"/meetups/at\">Tucson, Arizona</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tucson, Arizona", "anchor": "Discussion_article_for_the_meetup___Tucson__Arizona", "level": 1}, {"title": "Discussion article for the meetup : Tucson, Arizona", "anchor": "Discussion_article_for_the_meetup___Tucson__Arizona1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T00:08:27.479Z", "modifiedAt": null, "url": null, "title": "Satire of Journal of Personality and Social Psychology's publication bias", "slug": "satire-of-journal-of-personality-and-social-psychology-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5u2XQf2eTTujszLAY/satire-of-journal-of-personality-and-social-psychology-s", "pageUrlRelative": "/posts/5u2XQf2eTTujszLAY/satire-of-journal-of-personality-and-social-psychology-s", "linkUrl": "https://www.lesswrong.com/posts/5u2XQf2eTTujszLAY/satire-of-journal-of-personality-and-social-psychology-s", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Satire%20of%20Journal%20of%20Personality%20and%20Social%20Psychology's%20publication%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASatire%20of%20Journal%20of%20Personality%20and%20Social%20Psychology's%20publication%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5u2XQf2eTTujszLAY%2Fsatire-of-journal-of-personality-and-social-psychology-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Satire%20of%20Journal%20of%20Personality%20and%20Social%20Psychology's%20publication%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5u2XQf2eTTujszLAY%2Fsatire-of-journal-of-personality-and-social-psychology-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5u2XQf2eTTujszLAY%2Fsatire-of-journal-of-personality-and-social-psychology-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>Follow-up to:&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 25px; text-align: justify;\">&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 25px; text-align: justify;\" href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">Follow-up on ESP study: \"We don't publish replications\"</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 25px; text-align: justify;\">, <a href=\"/lw/a9f/using_degrees_of_freedom_to_change_the_past_for/\">Using degrees of freedom to change the past for fun and profit</a></span></p>\n<p>As I discussed in the above posts, the Journal of Personality and Social Psychology, a leading psych journal, published a deeply flawed parapsychology study (see the second post for details) which had apparently been tortured to produce results. Then they rejected an attempt to replicate that found no effect, citing a sadly typical policy of not publishing replications. Some of you may enjoy reading one enterprising researcher's <a href=\"http://www.projectimplicit.net/arina/B2012.pdf\">amusing satire article</a>, purportedly (not actually) \"tallying\" past confirmations and disconfirmations in JPSP and drawing conclusions.</p>\n<p>&nbsp;</p>\n<p>ETA: To clarify the last sentence, they didn't really find 4800+ confirmation and two disconfirmations. As they say in small print, the data were made up. It's right by the chart.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5u2XQf2eTTujszLAY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 43, "extendedScore": null, "score": 9.162794148159333e-07, "legacy": true, "legacyId": "16678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["b9vvmMn2kF76aThHn", "kXgyLuyRSvsxozFRs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T03:15:32.232Z", "modifiedAt": null, "url": null, "title": "Where Fermi Fails: What is hard to estimate?", "slug": "where-fermi-fails-what-is-hard-to-estimate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.031Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tgb", "createdAt": "2011-11-22T01:42:56.795Z", "isAdmin": false, "displayName": "tgb"}, "userId": "ZSMRTvQtfA4eieBPk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oYqHM9sYiKKze8sR4/where-fermi-fails-what-is-hard-to-estimate", "pageUrlRelative": "/posts/oYqHM9sYiKKze8sR4/where-fermi-fails-what-is-hard-to-estimate", "linkUrl": "https://www.lesswrong.com/posts/oYqHM9sYiKKze8sR4/where-fermi-fails-what-is-hard-to-estimate", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20Fermi%20Fails%3A%20What%20is%20hard%20to%20estimate%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20Fermi%20Fails%3A%20What%20is%20hard%20to%20estimate%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoYqHM9sYiKKze8sR4%2Fwhere-fermi-fails-what-is-hard-to-estimate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20Fermi%20Fails%3A%20What%20is%20hard%20to%20estimate%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoYqHM9sYiKKze8sR4%2Fwhere-fermi-fails-what-is-hard-to-estimate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoYqHM9sYiKKze8sR4%2Fwhere-fermi-fails-what-is-hard-to-estimate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 424, "htmlBody": "<p>I have a whimsical challenge for you: come up with problems with numerical solutions that are hard to estimate.</p>\n<p>This, like surprisingly many things, originates from a Richard Feynman story:</p>\n<blockquote>\n<p>One day I was feeling my oats. It was lunch time in the technical area, and I don't<br />know how I got the idea, but I announced, \"<strong>I can work out in sixty seconds the answer to<br />any problem that anybody can state in ten seconds, to 10 percent!</strong>\"<br />People started giving me problems they thought were difficult, such as integrating<br />a function like 1/(1 + x 4 ), which hardly changed over the range they gave me. The hardest<br />one somebody gave me was the binomial coefficient of x 10 in (1 + x) 20 \u037e I got that just in<br />time.<br />They were all giving me problems and I was feeling great, when Paul Olum<br />walked by in the hall. Paul had worked with me for a while at Princeton before coming<br />out to Los Alamos, and he was always cleverer than I was.</p>\n<p>..</p>\n<p>So Paul is walking past the lunch place and these guys are all excited. \"Hey,<br />Paul!\" they call out. \"Feynman's terrific! We give him a problem that can be stated in ten<br />seconds, and in a minute he gets the answer to 10 percent. Why don't you give him one?\"<br />Without hardly stopping, he says, \"The tangent of 10 to the 100th.\"<br />I was sunk: you have to divide by pi to 100 decimal places! It was hopeless.</p>\n<p>(From <em>Surely You're Joking Mr. Feynman</em> section \"Lucky Numbers\")</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>So what would <em>you </em>ask Richard Feynman to solve? Think of this as the reverse of <a href=\"http://en.wikipedia.org/wiki/Fermi_problems\">Fermi Problems</a>.</p>\n<p>&nbsp;</p>\n<p>Number theory may be a rich source of possibilities here; many functions there are wildly fluctuating, require prime factorization and depend upon the <em>exact</em> value of the number rather than it's order of magnitude. For example, I challenge you to compute the largest prime factor of 650238.</p>\n<p>&nbsp;</p>\n<p>(My original example was: \"For example, I challenge you to compute the greatest common denominator of 10643 and 15047 without a computer. This problem has the nice advantage of being trivial to make harder to compute - just throw in some extra primes.\" It has been pointed out that I forgot <a href=\"http://en.wikipedia.org/wiki/Euclidean_algorithm\">Euclid's algorithm</a> and have managed to choose about the only number theoretic question that <em>does</em> have an efficient solution.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AeqCtS3BaY3cwzKAs": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oYqHM9sYiKKze8sR4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 9.163636098714628e-07, "legacy": true, "legacyId": "16683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T03:15:57.201Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Likelihood Ratios", "slug": "meetup-west-la-meetup-likelihood-ratios", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oTwZ4JjGAxgykdwKL/meetup-west-la-meetup-likelihood-ratios", "pageUrlRelative": "/posts/oTwZ4JjGAxgykdwKL/meetup-west-la-meetup-likelihood-ratios", "linkUrl": "https://www.lesswrong.com/posts/oTwZ4JjGAxgykdwKL/meetup-west-la-meetup-likelihood-ratios", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Likelihood%20Ratios&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Likelihood%20Ratios%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoTwZ4JjGAxgykdwKL%2Fmeetup-west-la-meetup-likelihood-ratios%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Likelihood%20Ratios%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoTwZ4JjGAxgykdwKL%2Fmeetup-west-la-meetup-likelihood-ratios", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoTwZ4JjGAxgykdwKL%2Fmeetup-west-la-meetup-likelihood-ratios", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 223, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/au'>West LA Meetup - Likelihood Ratios</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, June 6th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week will be all about gaining a useful intuition for <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a>. We're going to focus on the formalization with odds and likelihood ratios. If you are already familiar with the concept of likelihood ratios, I recommend you read <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">this article</a>. If you have no idea what I'm talking about, read that Bayes' Theorem post I linked and, if you have time, check out one of the blog posts or external links from the article.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p>\n\n<p><strong>P.S.</strong> One of our routines is to ask if anyone has any predictions they would like to commit. Feel free to organize your thoughts ahead of time!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/au'>West LA Meetup - Likelihood Ratios</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oTwZ4JjGAxgykdwKL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.163637971603121e-07, "legacy": true, "legacyId": "16684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Likelihood_Ratios\">Discussion article for the meetup : <a href=\"/meetups/au\">West LA Meetup - Likelihood Ratios</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, June 6th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week will be all about gaining a useful intuition for <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a>. We're going to focus on the formalization with odds and likelihood ratios. If you are already familiar with the concept of likelihood ratios, I recommend you read <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">this article</a>. If you have no idea what I'm talking about, read that Bayes' Theorem post I linked and, if you have time, check out one of the blog posts or external links from the article.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p>\n\n<p><strong>P.S.</strong> One of our routines is to ask if anyone has any predictions they would like to commit. Feel free to organize your thoughts ahead of time!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Likelihood_Ratios1\">Discussion article for the meetup : <a href=\"/meetups/au\">West LA Meetup - Likelihood Ratios</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Likelihood Ratios", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Likelihood_Ratios", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Likelihood Ratios", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Likelihood_Ratios1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T04:12:18.453Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 6/4/12", "slug": "group-rationality-diary-6-4-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GBxYEuDpyRPvZaN22/group-rationality-diary-6-4-12", "pageUrlRelative": "/posts/GBxYEuDpyRPvZaN22/group-rationality-diary-6-4-12", "linkUrl": "https://www.lesswrong.com/posts/GBxYEuDpyRPvZaN22/group-rationality-diary-6-4-12", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%206%2F4%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%206%2F4%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGBxYEuDpyRPvZaN22%2Fgroup-rationality-diary-6-4-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%206%2F4%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGBxYEuDpyRPvZaN22%2Fgroup-rationality-diary-6-4-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGBxYEuDpyRPvZaN22%2Fgroup-rationality-diary-6-4-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for the week of June 4th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">(Previously:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/cew/group_rationality_diary_51412/\">5/14/12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/ckm/group_rationality_diary_52112/\">5/21/12</a>,&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"/r/discussion/lw/cpg/group_rationality_diary_52812\">5/28/12</a></span>)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GBxYEuDpyRPvZaN22", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 9.163891620462054e-07, "legacy": true, "legacyId": "16688", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JA2MnwunfZnFXb3by", "aFRC3J3TNmKMf2p4T", "RpeHoK89Ra3hBR26r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T04:33:17.992Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Grasping Slippery Things", "slug": "seq-rerun-grasping-slippery-things", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5cQjafoPJneoswLT3/seq-rerun-grasping-slippery-things", "pageUrlRelative": "/posts/5cQjafoPJneoswLT3/seq-rerun-grasping-slippery-things", "linkUrl": "https://www.lesswrong.com/posts/5cQjafoPJneoswLT3/seq-rerun-grasping-slippery-things", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Grasping%20Slippery%20Things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Grasping%20Slippery%20Things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cQjafoPJneoswLT3%2Fseq-rerun-grasping-slippery-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Grasping%20Slippery%20Things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cQjafoPJneoswLT3%2Fseq-rerun-grasping-slippery-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cQjafoPJneoswLT3%2Fseq-rerun-grasping-slippery-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Today's post, <a href=\"/lw/re/grasping_slippery_things/\">Grasping Slippery Things</a> was originally published on 17 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Grasping_Slippery_Things\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An illustration of a few ways that trying to perform reductionism can go wrong.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/cul/seq_rerun_passing_the_recursive_buck/\">Passing the Recursive Buck</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5cQjafoPJneoswLT3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.163986109505624e-07, "legacy": true, "legacyId": "16689", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HnS6c5Xm9p9sbm4a8", "M3PxXoYJrSwPk7aKy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T08:08:45.546Z", "modifiedAt": "2022-03-01T06:39:25.650Z", "url": null, "title": "Consider a robot vacuum.", "slug": "consider-a-robot-vacuum", "viewCount": null, "lastCommentedAt": "2012-06-06T01:38:23.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrandonReinhart", "createdAt": "2009-03-06T04:00:54.689Z", "isAdmin": false, "displayName": "BrandonReinhart"}, "userId": "ugRLNpaFuDrXntoeK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/89PRgSHbcyPXwfkom/consider-a-robot-vacuum", "pageUrlRelative": "/posts/89PRgSHbcyPXwfkom/consider-a-robot-vacuum", "linkUrl": "https://www.lesswrong.com/posts/89PRgSHbcyPXwfkom/consider-a-robot-vacuum", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consider%20a%20robot%20vacuum.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsider%20a%20robot%20vacuum.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89PRgSHbcyPXwfkom%2Fconsider-a-robot-vacuum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consider%20a%20robot%20vacuum.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89PRgSHbcyPXwfkom%2Fconsider-a-robot-vacuum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89PRgSHbcyPXwfkom%2Fconsider-a-robot-vacuum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 746, "htmlBody": "<p>My wife and I recently acquired a robot vacuum. It has turned out to be a really great time-saving and life-improving investment. Some simple math suggests it may be worth you also considering buying one.</p>\n<p>Let's say you spend 20 minutes a week vacuuming. That's about 17 hours of vacuuming per year. The Neato XV-11 costs about $350 bucks with basic shipping. For the purposes of our Fermi calculation we will say that your time spent vacuuming with the robot is zero. This is close to true. See below for exceptions.</p>\n<p>At $350, if you value an hour of your time at more than about $20 you would be better off buying the robot than doing the vacuuming yourself in the first year. (17*$20=$340 - close enough for our fermi estimate)</p>\n<p>Consider also that if you spend at least 20 minutes or less a week vacuuming, you can also instruct the robot to vacuum 20 minutes a week or more and raise the quality of your life by living in a better cared-for environment by some amount. For example, you could increase the pay-out of the robot by having it vacuum every other day.</p>\n<p>If you have the robot do 60 minutes of work a week, then you'd only have to value your time at about $7 for the robot to be worthwhile in the first year. (52*$7=$364)</p>\n<p>Do the calculation to see if it makes sense for you:</p>\n<p>b = value of your time in dollars/hour</p>\n<p>y = hours/year you spend vacuuming</p>\n<p>350 = estimated price of a robot</p>\n<p>x = b*y - 350</p>\n<p>If x &gt; 0, then the robot would save you money in time, according to how you value your time. If x &lt; 0, then you either don't clean often enough or value your time so low that doing the work yourself makes sense. (So this is a simple model, feel free to make it more complex but the purpose of this post is to illustrate a fermi calc that seems to yield an easy choice.)</p>\n<p>Given the cost of many upright vacuums, if you can avoid buying an upright and only buy the robot the calculation shifts drastically in favor of only getting the robot (and perhaps borrowing an upright if you really need one).</p>\n<p>If vacuuming causes you particular disutility, you could put a dollar premium on that disutility and add it to b. On the flip side, if you really like to vacuum you'd want to discount b to reflect the extra utility you get from spending your time doing something you enjoy.</p>\n<p>Considerations:</p>\n<p>- The robots are claimed to be pretty good at navigating complex room layouts. Our robot rarely (but sometimes) gets stuck behind places where it has little clearance to enter. You can adjust furniture layout to compensate or lay down (ugly) magnetic strips that stop the robot. You might want to try out the robot to make sure it can navigate your layout before you commit.</p>\n<p>- Once our robot failed to back itself fully into its charging dock and it ran out of juice and missed a scheduled vacuum session.</p>\n<p>- The robot won't drive itself off cliffs (down stairs to its doom). On the down side, it won't vacuum stairs. You may still need an upright to handle stairs.</p>\n<p>- You can make the robot do a lot more vacuuming than you would normally do yourself.</p>\n<p>- They are really quiet on carpet. Somewhat noisier on hard wood. Depending on your sensitivity, you may be able to run it while you sleep.</p>\n<p>- If you shed hair, you'll need to regularly clip the hair from the brush (like a normal upright). This takes almost no time. Do it as a part of the bin-emptying ritual.</p>\n<p>- It isn't clear to me how long the robot will last, so I don't know what the replacement period or cost is.</p>\n<p>- This is the robot we use, but there are many types. It isn't clear to me if the upgraded types are worth the extra money:&nbsp;<a href=\"http://www.amazon.com/Neato-XV-11-Robotic-Vacuum-System/dp/B003UBPB6E/ref=sr_1_1?ie=UTF8&amp;qid=1338882167&amp;sr=8-1\">http://www.amazon.com/Neato-XV-11-Robotic-Vacuum-System/dp/B003UBPB6E/ref=sr_1_1?ie=UTF8&amp;qid=1338882167&amp;sr=8-1</a></p>\n<p>- I haven't investigated central vac, so I don't know what the trade-offs are. It seems like central vac still requires time to use and our goal was reducing time spent doing an automatable home maintenance task.</p>\n<p>Maybe this is a trivial post, but I hadn't realized how much cleaner our environment could be or how much happier we could be for such a small relative investment. Much of the benefit comes from the robot being able to vacuum far more often than we'd ever have a desire to do ourselves.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Tg9aFPFCPBHxGABRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "89PRgSHbcyPXwfkom", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 25, "extendedScore": null, "score": 9.164953918059174e-07, "legacy": true, "legacyId": "16699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-06-05T08:08:45.546Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T10:43:21.278Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-14", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:07.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zLzeYygLAEc8CKPod/meetup-brussels-meetup-14", "pageUrlRelative": "/posts/zLzeYygLAEc8CKPod/meetup-brussels-meetup-14", "linkUrl": "https://www.lesswrong.com/posts/zLzeYygLAEc8CKPod/meetup-brussels-meetup-14", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLzeYygLAEc8CKPod%2Fmeetup-brussels-meetup-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLzeYygLAEc8CKPod%2Fmeetup-brussels-meetup-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLzeYygLAEc8CKPod%2Fmeetup-brussels-meetup-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/av'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 June 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll discuss personal catastrophic events and how to recover from them and trying to answer <a href=\"http://www.quora.com/Does-embodiment-prevent-unbiased-moral-decisions\" rel=\"nofollow\">http://www.quora.com/Does-embodiment-prevent-unbiased-moral-decisions</a></p>\n\n<p>Warning: attendance will be lower because of the exam period. We'll meet in the lobby as usual, should you arrive late we will have moved to the cafeteria (just go straight once you're past the entrance, you can't miss it) If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/av'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zLzeYygLAEc8CKPod", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.165652057908853e-07, "legacy": true, "legacyId": "16700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/av\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 June 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll discuss personal catastrophic events and how to recover from them and trying to answer <a href=\"http://www.quora.com/Does-embodiment-prevent-unbiased-moral-decisions\" rel=\"nofollow\">http://www.quora.com/Does-embodiment-prevent-unbiased-moral-decisions</a></p>\n\n<p>Warning: attendance will be lower because of the exam period. We'll meet in the lobby as usual, should you arrive late we will have moved to the cafeteria (just go straight once you're past the entrance, you can't miss it) If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/av\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T12:27:21.310Z", "modifiedAt": null, "url": null, "title": "Meetup : Small Berkeley meetup: Nisan's house", "slug": "meetup-small-berkeley-meetup-nisan-s-house", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pPpKXtkqjNgDTjz2E/meetup-small-berkeley-meetup-nisan-s-house", "pageUrlRelative": "/posts/pPpKXtkqjNgDTjz2E/meetup-small-berkeley-meetup-nisan-s-house", "linkUrl": "https://www.lesswrong.com/posts/pPpKXtkqjNgDTjz2E/meetup-small-berkeley-meetup-nisan-s-house", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20Berkeley%20meetup%3A%20Nisan's%20house&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20Berkeley%20meetup%3A%20Nisan's%20house%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPpKXtkqjNgDTjz2E%2Fmeetup-small-berkeley-meetup-nisan-s-house%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20Berkeley%20meetup%3A%20Nisan's%20house%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPpKXtkqjNgDTjz2E%2Fmeetup-small-berkeley-meetup-nisan-s-house", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPpKXtkqjNgDTjz2E%2Fmeetup-small-berkeley-meetup-nisan-s-house", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/aw'>Small Berkeley meetup: Nisan's house</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Wednesday's meetup will be at my house. See the  for the address.</p>\n\n<p>Feel free to arrive between 7pm and 7:30pm. At 7:30pm we'll do/play some rationality exercises/games that Eliezer presented at last month's minicamp, including Rationalist Taboo. (Maybe we can come up with fun variants.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/aw'>Small Berkeley meetup: Nisan's house</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pPpKXtkqjNgDTjz2E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.166120354220078e-07, "legacy": true, "legacyId": "16701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_meetup__Nisan_s_house\">Discussion article for the meetup : <a href=\"/meetups/aw\">Small Berkeley meetup: Nisan's house</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 June 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Wednesday's meetup will be at my house. See the  for the address.</p>\n\n<p>Feel free to arrive between 7pm and 7:30pm. At 7:30pm we'll do/play some rationality exercises/games that Eliezer presented at last month's minicamp, including Rationalist Taboo. (Maybe we can come up with fun variants.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_meetup__Nisan_s_house1\">Discussion article for the meetup : <a href=\"/meetups/aw\">Small Berkeley meetup: Nisan's house</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small Berkeley meetup: Nisan's house", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_meetup__Nisan_s_house", "level": 1}, {"title": "Discussion article for the meetup : Small Berkeley meetup: Nisan's house", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_meetup__Nisan_s_house1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T17:16:23.448Z", "modifiedAt": null, "url": null, "title": "Global Workspace Theory", "slug": "global-workspace-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZecFM3uCpxctoePHp/global-workspace-theory", "pageUrlRelative": "/posts/ZecFM3uCpxctoePHp/global-workspace-theory", "linkUrl": "https://www.lesswrong.com/posts/ZecFM3uCpxctoePHp/global-workspace-theory", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Global%20Workspace%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGlobal%20Workspace%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZecFM3uCpxctoePHp%2Fglobal-workspace-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Global%20Workspace%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZecFM3uCpxctoePHp%2Fglobal-workspace-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZecFM3uCpxctoePHp%2Fglobal-workspace-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Much research has been done upon visual perception.&nbsp;&nbsp; Humans have the illusion that they are directly aware of everything in their 'field of view', but it turns out that they actually navigate not through reality, but through a model of reality that their brain stitches together mainly from the bits the eye is directly looking at as it darts about, with the rest supplied by interpolation based upon expectations.</p>\n<p>For more info, read <a href=\"http://sensation.vizkult.hu/wp-content/uploads/2012/05/Berliner.Cohen_.continuity.pdf\">The Illusion of Continuity: Active Perception and the Classical Editing System</a>, by Berliner and Cohen.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Global_Workspace_Theory\">Global Workspace Theory</a> is the idea that our awareness of our own thought process works the same way.&nbsp; We have the illusion of an unbroken stream of consciousness, but what we're actually referencing is a model of what the brain thinks it has been consciously thinking about, that is stitched together from brief fragments, the way a spotlight in a theatre might move about shining on different parts of a stage, revealing actors making speeches and interacting with each other.&nbsp;&nbsp; Even when the spotlight moves on, the actors, stage hands and directors remain and keep working.&nbsp;&nbsp; When the spotlight returns, to catch a later part of the drama in that area, we interpolate what the actors would have been doing while we were paying attention elsewhere.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZecFM3uCpxctoePHp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 9.167422054205279e-07, "legacy": true, "legacyId": "16702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-05T20:15:59.055Z", "modifiedAt": null, "url": null, "title": "Ask an X", "slug": "ask-an-x", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "falenas108", "createdAt": "2010-10-28T17:32:39.696Z", "isAdmin": false, "displayName": "falenas108"}, "userId": "BCX7q7NMQphQiXc8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/erc4JZzqyv2XoWpv5/ask-an-x", "pageUrlRelative": "/posts/erc4JZzqyv2XoWpv5/ask-an-x", "linkUrl": "https://www.lesswrong.com/posts/erc4JZzqyv2XoWpv5/ask-an-x", "postedAtFormatted": "Tuesday, June 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20an%20X&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20an%20X%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ferc4JZzqyv2XoWpv5%2Fask-an-x%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20an%20X%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ferc4JZzqyv2XoWpv5%2Fask-an-x", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ferc4JZzqyv2XoWpv5%2Fask-an-x", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p>In many previous comments, people call on professionals to answer questions about specific fields, like physicists, neuroscientists, economists, or computer scientists. There are many people in all these professions on this site willing to respond to questions, but most of the time none of them happen to read that comment.</p>\n<p>As a way to fix this, I propose that people well-educated in certain fields volunteer to make an \"Ask an X\" post where they list their credentials and specialties, and anyone can ask questions about that field. &nbsp;Obviously, this would also be a good place to have a discussion between professionals in that field.</p>\n<p>Another possibility is to ask people&nbsp;who don't mind being asked random questions&nbsp;to volunteer to be part of a list that can be posted to the wiki. Then, people could just PM that person directly.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "erc4JZzqyv2XoWpv5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "16703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T00:26:00.625Z", "modifiedAt": null, "url": null, "title": "List of Problems That Motivated UDT", "slug": "list-of-problems-that-motivated-udt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:15.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4kvaocbkDDS2AMoPG/list-of-problems-that-motivated-udt", "pageUrlRelative": "/posts/4kvaocbkDDS2AMoPG/list-of-problems-that-motivated-udt", "linkUrl": "https://www.lesswrong.com/posts/4kvaocbkDDS2AMoPG/list-of-problems-that-motivated-udt", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20of%20Problems%20That%20Motivated%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20of%20Problems%20That%20Motivated%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kvaocbkDDS2AMoPG%2Flist-of-problems-that-motivated-udt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20of%20Problems%20That%20Motivated%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kvaocbkDDS2AMoPG%2Flist-of-problems-that-motivated-udt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kvaocbkDDS2AMoPG%2Flist-of-problems-that-motivated-udt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>I noticed that recently I wrote several comments of the form \"UDT can be seen as a step towards solving X\" and thought it might be a good idea to list in one place all of the problems that helped motivate <a href=\"/lw/15m/towards_a_new_decision_theory/\">UDT1</a>&nbsp;(not including problems that came up subsequent to that post).&nbsp;</p>\n<ul>\n<li><a href=\"http://extropians.weidai.com/extropians.3Q97/4116.html\">decision making for minds that can copy themselves</a></li>\n<li><a href=\"http://www.weidai.com/everything.html\">Doomsday Argument</a></li>\n<li>Sleeping Beauty</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Absent-Minded_driver\">Absent-Minded Driver</a></li>\n<li><a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">Presumptuous Philosopher</a></li>\n<li><a href=\"http://groups.google.com/group/everything-list/browse_thread/thread/8c25168e232a7efd/\">anthropic reasoning for non-sentient AIs</a></li>\n<li><a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/1f0b\">Simulation Argument</a></li>\n<li><a href=\"/lw/102/indexical_uncertainty_and_the_axiom_of/\">indexical uncertainty</a> in general</li>\n<li><a href=\"http://www.mail-archive.com/everything-list@googlegroups.com/msg03620.html\">wireheading/<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Cartesianism</span></a>&nbsp;(<a href=\"/lw/cej/general_purpose_intelligence_arguing_the/6mm0\">how</a> to formulate something like AIXI that cares about an external world instead of just its sensory inputs)</li>\n<li><a href=\"http://extropians.weidai.com/extropians/0302/2444.html\">How</a> to make decisions if all possible worlds exist? (a la Tegmark or Schmidhuber, or just in the MWI)</li>\n<li>Quantum Immortality/Suicide</li>\n<li>Logical Uncertainty (<a href=\"http://www.sl4.org/archive/0509/12317.html\">how</a> to formulate something like Godel machine that can make reasonable decisions involving P=NP)</li>\n<li><a href=\"https://groups.google.com/group/everything-list/browse_thread/thread/c7442c13ff1396ec\">uncertainty about hypercomputation</a> (how to avoid assuming we must be living in a computable universe)</li>\n<li><a href=\"/lw/1iy/what_are_probabilities_anyway/\">What are probabilities?</a></li>\n<li><a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/606k\">What are decisions</a> and what kind of consequences should be considered when making decisions?</li>\n<li>Newcomb's Problem</li>\n<li><a href=\"/lw/az7/video_paul_christianos_impromptu_tutorial_on_aixi/6cv7\">Smoking Lesion</a></li>\n<li>Prisoner's&nbsp;Dilemma</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a></li>\n<li><a href=\"http://extropians.weidai.com/extropians/0302/2567.html\">FAI</a></li>\n</ul>\n<div><br /></div>\n<div><ol> </ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "GQyPQcdEQF4zXhJBq": 1, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4kvaocbkDDS2AMoPG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 42, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "16569", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv", "RcvyJjPQwimAeapNg", "qij9v3YqPfyur2PbX", "J7Gkz8aDxxSEQKXTN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T00:26:10.035Z", "modifiedAt": null, "url": null, "title": "Open Problems Related to Solomonoff Induction", "slug": "open-problems-related-to-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:35.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fC248GwrWLT4Dkjf6/open-problems-related-to-solomonoff-induction", "pageUrlRelative": "/posts/fC248GwrWLT4Dkjf6/open-problems-related-to-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/fC248GwrWLT4Dkjf6/open-problems-related-to-solomonoff-induction", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Problems%20Related%20to%20Solomonoff%20Induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Problems%20Related%20to%20Solomonoff%20Induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfC248GwrWLT4Dkjf6%2Fopen-problems-related-to-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Problems%20Related%20to%20Solomonoff%20Induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfC248GwrWLT4Dkjf6%2Fopen-problems-related-to-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfC248GwrWLT4Dkjf6%2Fopen-problems-related-to-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p>Solomonoff Induction seems clearly \"on the right track\", but there are a number of problems with it that I've been&nbsp;puzzling&nbsp;over for several years and have not made much progress on. I think I've talked about all of them in various comments in the past, but never collected them in one place.</p>\n<h4>Apparent Unformalizability of &ldquo;Actual&rdquo; Induction</h4>\n<h5><a href=\"https://groups.google.com/group/everything-list/browse_thread/thread/c7442c13ff1396ec\">Argument</a> via Tarski&rsquo;s <a href=\"http://en.wikipedia.org/wiki/Tarski's_undefinability_theorem\">Indefinability of Truth</a></h5>\n<blockquote>\n<p>Informally, the theorem states that arithmetical truth cannot be defined in arithmetic. The theorem applies more generally to any sufficiently strong formal system, showing that truth in the standard model of the system cannot be defined within the system.</p>\n</blockquote>\n<p>Suppose we define a generalized version of Solomonoff Induction based on some second-order logic. The truth predicate for this logic can&rsquo;t be defined within the logic and therefore a device that can decide the truth value of arbitrary statements in this logical has no finite description within this logic. If an alien claimed to have such a device, this generalized Solomonoff induction would assign the hypothesis that they're telling the truth zero probability, whereas we would assign it some small but positive probability.</p>\n<h5><a href=\"https://groups.google.com/group/one-logic/browse_thread/thread/b499a90ef9e5fd84\">Argument</a> via Berry&rsquo;s Paradox</h5>\n<p>Consider an arbitrary probability distribution P, and the smallest integer (or the lexicographically least object) x such that P(x) &lt; 1/3^^^3 (in Knuth's up-arrow notation). Since x has a short description, a universal distribution shouldn't assign it such a low probability, but P does, so P can't be a universal distribution.</p>\n<h4>Is Solomonoff Induction &ldquo;good enough&rdquo;?</h4>\n<p>Given the above, is Solomonoff Induction nevertheless &ldquo;<a href=\"/lw/4iy/does_solomonoff_always_win/\">good enough</a>&rdquo; for practical purposes? In other words, would an AI programmed to approximate Solomonoff Induction do as well as any other possible agent we might build, even though it wouldn&rsquo;t have what we&rsquo;d consider correct beliefs?</p>\n<h4>Is complexity objective?</h4>\n<p>Solomonoff Induction is supposed to be a formalization of Occam&rsquo;s Razor, and it&rsquo;s confusing that the formalization has a free parameter in the form of a universal Turing machine that is used to define the notion of complexity. What&rsquo;s the significance of the fact that we can&rsquo;t seem to define a parameterless concept of complexity? That complexity is subjective?</p>\n<h4>Is Solomonoff an ideal or an approximation?</h4>\n<p>Is it the case that the universal prior&nbsp;(or some suitable generalization of it that somehow overcomes the above \"unformalizability problems\")&nbsp;is the &ldquo;true&rdquo; prior and that Solomonoff Induction represents idealized reasoning, or does Solomonoff just &ldquo;work well enough&rdquo; (in some sense) at approximating any rational agent?</p>\n<h4>How can we apply Solomonoff when our inputs are not symbol strings?</h4>\n<p>Solomonoff Induction is defined over symbol strings (for example bit strings) but our perceptions are made of &ldquo;qualia&rdquo; instead of symbols. How is Solomonoff Induction supposed to work for us?</p>\n<h4>What does Solomonoff Induction actually say?</h4>\n<p>What does Solomonoff Induction actually say about, for example, whether we live in a creatorless universe that runs on physics? Or the Simulation Argument?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bTeiZr6YAEaSPQTC8": 9, "Pa2SdZsLFmqhs42Do": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fC248GwrWLT4Dkjf6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 40, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "16705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Solomonoff Induction seems clearly \"on the right track\", but there are a number of problems with it that I've been&nbsp;puzzling&nbsp;over for several years and have not made much progress on. I think I've talked about all of them in various comments in the past, but never collected them in one place.</p>\n<h4 id=\"Apparent_Unformalizability_of__Actual__Induction\">Apparent Unformalizability of \u201cActual\u201d Induction</h4>\n<h5><a href=\"https://groups.google.com/group/everything-list/browse_thread/thread/c7442c13ff1396ec\">Argument</a> via Tarski\u2019s <a href=\"http://en.wikipedia.org/wiki/Tarski's_undefinability_theorem\">Indefinability of Truth</a></h5>\n<blockquote>\n<p>Informally, the theorem states that arithmetical truth cannot be defined in arithmetic. The theorem applies more generally to any sufficiently strong formal system, showing that truth in the standard model of the system cannot be defined within the system.</p>\n</blockquote>\n<p>Suppose we define a generalized version of Solomonoff Induction based on some second-order logic. The truth predicate for this logic can\u2019t be defined within the logic and therefore a device that can decide the truth value of arbitrary statements in this logical has no finite description within this logic. If an alien claimed to have such a device, this generalized Solomonoff induction would assign the hypothesis that they're telling the truth zero probability, whereas we would assign it some small but positive probability.</p>\n<h5><a href=\"https://groups.google.com/group/one-logic/browse_thread/thread/b499a90ef9e5fd84\">Argument</a> via Berry\u2019s Paradox</h5>\n<p>Consider an arbitrary probability distribution P, and the smallest integer (or the lexicographically least object) x such that P(x) &lt; 1/3^^^3 (in Knuth's up-arrow notation). Since x has a short description, a universal distribution shouldn't assign it such a low probability, but P does, so P can't be a universal distribution.</p>\n<h4 id=\"Is_Solomonoff_Induction__good_enough__\">Is Solomonoff Induction \u201cgood enough\u201d?</h4>\n<p>Given the above, is Solomonoff Induction nevertheless \u201c<a href=\"/lw/4iy/does_solomonoff_always_win/\">good enough</a>\u201d for practical purposes? In other words, would an AI programmed to approximate Solomonoff Induction do as well as any other possible agent we might build, even though it wouldn\u2019t have what we\u2019d consider correct beliefs?</p>\n<h4 id=\"Is_complexity_objective_\">Is complexity objective?</h4>\n<p>Solomonoff Induction is supposed to be a formalization of Occam\u2019s Razor, and it\u2019s confusing that the formalization has a free parameter in the form of a universal Turing machine that is used to define the notion of complexity. What\u2019s the significance of the fact that we can\u2019t seem to define a parameterless concept of complexity? That complexity is subjective?</p>\n<h4 id=\"Is_Solomonoff_an_ideal_or_an_approximation_\">Is Solomonoff an ideal or an approximation?</h4>\n<p>Is it the case that the universal prior&nbsp;(or some suitable generalization of it that somehow overcomes the above \"unformalizability problems\")&nbsp;is the \u201ctrue\u201d prior and that Solomonoff Induction represents idealized reasoning, or does Solomonoff just \u201cwork well enough\u201d (in some sense) at approximating any rational agent?</p>\n<h4 id=\"How_can_we_apply_Solomonoff_when_our_inputs_are_not_symbol_strings_\">How can we apply Solomonoff when our inputs are not symbol strings?</h4>\n<p>Solomonoff Induction is defined over symbol strings (for example bit strings) but our perceptions are made of \u201cqualia\u201d instead of symbols. How is Solomonoff Induction supposed to work for us?</p>\n<h4 id=\"What_does_Solomonoff_Induction_actually_say_\">What does Solomonoff Induction actually say?</h4>\n<p>What does Solomonoff Induction actually say about, for example, whether we live in a creatorless universe that runs on physics? Or the Simulation Argument?</p>", "sections": [{"title": "Apparent Unformalizability of \u201cActual\u201d Induction", "anchor": "Apparent_Unformalizability_of__Actual__Induction", "level": 1}, {"title": "Is Solomonoff Induction \u201cgood enough\u201d?", "anchor": "Is_Solomonoff_Induction__good_enough__", "level": 1}, {"title": "Is complexity objective?", "anchor": "Is_complexity_objective_", "level": 1}, {"title": "Is Solomonoff an ideal or an approximation?", "anchor": "Is_Solomonoff_an_ideal_or_an_approximation_", "level": 1}, {"title": "How can we apply Solomonoff when our inputs are not symbol strings?", "anchor": "How_can_we_apply_Solomonoff_when_our_inputs_are_not_symbol_strings_", "level": 1}, {"title": "What does Solomonoff Induction actually say?", "anchor": "What_does_Solomonoff_Induction_actually_say_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "103 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oHwt2JmDBefiN8rvg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T12:08:53.635Z", "modifiedAt": null, "url": null, "title": "[Link] Thick and thin ", "slug": "link-thick-and-thin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.963Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5qMxWi9ryt9Ym4xmW/link-thick-and-thin", "pageUrlRelative": "/posts/5qMxWi9ryt9Ym4xmW/link-thick-and-thin", "linkUrl": "https://www.lesswrong.com/posts/5qMxWi9ryt9Ym4xmW/link-thick-and-thin", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Thick%20and%20thin%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Thick%20and%20thin%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5qMxWi9ryt9Ym4xmW%2Flink-thick-and-thin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Thick%20and%20thin%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5qMxWi9ryt9Ym4xmW%2Flink-thick-and-thin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5qMxWi9ryt9Ym4xmW%2Flink-thick-and-thin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 692, "htmlBody": "<p>A new interesting entry on <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s well known blog (<a href=\"http://westhunt.wordpress.com/\">West Hunter</a>). For me the information I gained from the LessWrong articles on <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distances</a> complemented it nicely. <a href=\"https://westhunt.wordpress.com/2012/06/06/thick-and-thin/\">Link to source. </a></p>\n<blockquote>\n<p><strong>There is a spectrum of problem-solving, ranging from, at one extreme, simplicity&nbsp; and clear chains of logical reasoning (sometimes long chains) and, at the other,&nbsp; building a picture by sifting through a vast mass of evidence of&nbsp; varying quality.</strong>&nbsp; I will give some examples. Just the other day, when I was conferring, conversing and otherwise hobnobbing with my fellow physicists, I mentioned high-altitude lighting, sprites and elves and blue jets.&nbsp;&nbsp; I said that you could think of a thundercloud as a vertical dipole,&nbsp; with an electric field that decreased as the cube of altitude, while the breakdown voltage varied with air pressure, which declines exponentially with altitude. At which point the prof I was talking to said &rdquo; and so the curves must cross!&rdquo;.&nbsp; That&rsquo;s how physicists think, and it can be very effective. The amount of information required to solve the problem is not very large. I call this a &lsquo;thin&rsquo; problem&rsquo;.</p>\n<p>At the other extreme,&nbsp; consider Darwin gathering and pondering on a vast amount of natural-history information, eventually coming up with natural selection as the explanation.&nbsp;&nbsp; Some of the information in the literature&nbsp; wasn&rsquo;t correct, and much&nbsp; key information that would have greatly aided his&nbsp; quest, such as basic genetics, was still unknown.&nbsp;&nbsp; That didn&rsquo;t stop him, anymore than not knowing the <em>cause</em> of continental drift stopped Wegener.</p>\n<p>In another example at the messy end of the spectrum, Joe Rochefort, running Hypo in the spring of 1942,&nbsp; needed to figure out Japanese plans. He had an an ever-growing mass of Japanese radio intercepts, some of which were partially decrypted &ndash; say, one word of five, with luck.&nbsp;&nbsp; He had data from radio direction-finding; his people were beginning to be able to recognize particular Japanese radio operators by their &lsquo;fist&rsquo;.&nbsp; He&rsquo;d studied in Japan, knew the Japanese well.&nbsp; He had plenty of Navy experience &ndash; knew what was possible. I would call this a classic &lsquo;thick&rsquo; problem, one in which an analyst needs to deal with an enormous amount of data of varying quality.&nbsp; Being smart is necessary but not sufficient: you also need to know lots of&nbsp; stuff.</p>\n<p>At this point he was utterly saturated with information about the Japanese Navy.&nbsp; He&rsquo;d been&nbsp; living and breathing JN-25 for months. The Japanese were aimed somewhere,&nbsp; that somewhere designated by an untranslated codegroup &ndash; &lsquo;AF&rsquo;.&nbsp; Rochefort thought it meant Midway, based on many clues, plausibility, etc.&nbsp; OP-20-G, back in Washington,&nbsp; thought otherwise. They thought the main attack might be against Alaska, or Port Moresby, or even the West Coast.</p>\n<p>Nimitz believed Rochefort &ndash; who was correct.&nbsp; Because of that, we managed to prevail at Midway, losing one carrier and one destroyer while the the Japanese lost four carriers and a heavy cruiser*.&nbsp; As so often happens, OP-20-G won the bureaucratic war:&nbsp; Rochefort embarrassed them by proving them wrong, and they kicked him out of Hawaii, assigning him to a floating drydock.</p>\n<p>The usual explanation of Joe Rochefort&rsquo;s fall argues that John Redman&rsquo;s ( head of OP-20-G, the Navy&rsquo;s main signals intelligence and cryptanalysis group) geographical proximity to Navy headquarters&nbsp; was a key factor in winning the bureaucratic struggle, along with his brother&rsquo;s influence (Rear Admiral Joseph Redman).&nbsp; That and being a shameless liar.</p>\n<p>Personally, <strong>I wonder if part of the problem is the great difficulty of explaining the analysis of a thick problem to someone without a similar depth of knowledge.&nbsp; At best, they believe you because you&rsquo;ve&nbsp; been right in the past.&nbsp; Or, sometimes, once you have developed the answer, there is a &lsquo;thin&rsquo; way of confirming your answer</strong> &ndash; as when Rochefort took Jasper Holmes&rsquo;s suggestion and had Midway broadcast an uncoded complaint about the failure of their distillation system &ndash; soon followed by a Japanese report that &lsquo;AF&rsquo; was short of water.</p>\n<p><strong>Most problems in the social sciences are &lsquo;thick&rsquo;, and unfortunately, almost all of the researchers are as well. There are a lot more Redmans than Rocheforts.</strong></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 1, "x3zyEPFaJANB2BHmP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5qMxWi9ryt9Ym4xmW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 39, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "16724", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T15:21:31.259Z", "modifiedAt": null, "url": null, "title": "[Link] FreakoStats and CEV", "slug": "link-freakostats-and-cev", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.688Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Filipe", "createdAt": "2011-11-30T22:27:14.259Z", "isAdmin": false, "displayName": "Filipe"}, "userId": "eXxJNrGGCYkRKHThQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NM9tjAAmYQeGyPdoY/link-freakostats-and-cev", "pageUrlRelative": "/posts/NM9tjAAmYQeGyPdoY/link-freakostats-and-cev", "linkUrl": "https://www.lesswrong.com/posts/NM9tjAAmYQeGyPdoY/link-freakostats-and-cev", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20FreakoStats%20and%20CEV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20FreakoStats%20and%20CEV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNM9tjAAmYQeGyPdoY%2Flink-freakostats-and-cev%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20FreakoStats%20and%20CEV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNM9tjAAmYQeGyPdoY%2Flink-freakostats-and-cev", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNM9tjAAmYQeGyPdoY%2Flink-freakostats-and-cev", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 481, "htmlBody": "<p class=\"MsoNormal\"><span lang=\"EN-US\">I would like to point you guys to the blog <a title=\"FreakoStats\" href=\" http://garthzietsman.blogspot.com/\">FreakoStats</a>, &nbsp;by Garth Zietsman, who according to his profile, \"Scored an IQ of 185 on the Mega27 and has a degree in psychology and statistics and 25 years experience in psychometrics and statistics.\"&nbsp;</span>The main concept discussed there is&nbsp;<a title=\"Smart Vote\" href=\"http://garthzietsman.blogspot.com/2011/10/smart-vote-concept.html\">\"The Smart Vote\"</a>, whose essence,&nbsp; in the author&rsquo;s words, is as follows:</p>\n<blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">\"If there is no difference of opinion by intelligence then reason is not relevant in deciding between them and none of the opinions being considered is more correct than any of the others. However if opinions do differ systematically with intelligence then relatively more correct or better alternatives probably do exist, and that they are those relatively more favoured by the more intelligent. Statistical differences in the independent opinions of people of different intellectual ability point to the most reasonable responses to controversies&rdquo;</span></p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Many of his posts are based on the choice of relevant, if controversial, topics , and his analysis of the direction and the proportionality of which an opinion on it is related to intelligence, as measured by IQ scores.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">An obvious objection would be that smart people would have in many cases common interests, and this could bias the results simply to their interests, in detriment to the interests of the less smart.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">Zietsman answers it with the statistical fact that people many times don&rsquo;t vote selfishly,&nbsp; and that he can (and will) control for some of their interests anyway:</span></p>\n<blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">\"My first response is that this isn&rsquo;t always a factor. Political research shows that people frequently don&rsquo;t vote their narrow selfish interests e.g. the elderly are less likely to vote for social security than the young and women are less likely to support abortion on demand than men. However there are enough cases where narrow interests obviously do play a role for it to be taken seriously. Fortunately this possibility can be dealt with by controlling for interest differences. For example we could control for class when looking at musical taste, income when looking at welfare, age when looking at social security policy, race when looking at affirmative action, etc.\"</span></p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">This seems somewhat <em>related </em>to the notorious concept of Coherent Extrapolation Volition (<a title=\"cev\" href=\"http://intelligence.org/upload/CEV.html\">CEV</a>) of humankind. To have a clue of the direction it might take, I believe it is a nice idea to look at the opinion of the smarter <em>(&ldquo;if we knew more, thought faster, [&hellip;]&rdquo;)</em>, corrected for selfish interests <em>(&ldquo;[&hellip;]were more the people we wished we were, had grown up farther together[&hellip;]&rdquo;)</em>, specially bearing in mind that most results tend to converge <em>(&ldquo;[&hellip;]where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated[&hellip;]&rdquo;)</em>.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-US\">There are analyses on issues such as <a title=\"Abortion\" href=\"http://garthzietsman.blogspot.com/2011/12/intelligent-thinking-about-abortion.html\">Abortion</a>, <a href=\" http://garthzietsman.blogspot.com/2011/11/freedom-of-speech-and-keeping-secrets.html\">Free Speech</a>&nbsp;,<a title=\"Punishment\" href=\"http://garthzietsman.blogspot.com/2011/11/discipline-and-punishment.html\">Capital Punishment and&nbsp;Corporal Punishments on Children</a>&nbsp;,<a title=\"Immigration\" href=\"http://garthzietsman.blogspot.com/2012/03/smart-immigration-policy.html\">Immigration</a>,&nbsp;<a title=\"Gay Rights\" href=\"http://garthzietsman.blogspot.com.br/2012/01/stupidity-of-opposing-gay-rights.html\">Gay Rights</a>&nbsp;and many more. The results look good to me personally, and I wouldn't be surprised if they pleased many here too.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NM9tjAAmYQeGyPdoY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "16726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T15:51:15.873Z", "modifiedAt": null, "url": null, "title": "Help please!", "slug": "help-please", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:42.573Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Michelle_Z", "createdAt": "2011-07-14T22:50:16.205Z", "isAdmin": false, "displayName": "Michelle_Z"}, "userId": "ExbXRgKKWx59L4m4W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HFdpDFGWXqiaiN2ua/help-please", "pageUrlRelative": "/posts/HFdpDFGWXqiaiN2ua/help-please", "linkUrl": "https://www.lesswrong.com/posts/HFdpDFGWXqiaiN2ua/help-please", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20please!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20please!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFdpDFGWXqiaiN2ua%2Fhelp-please%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20please!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFdpDFGWXqiaiN2ua%2Fhelp-please", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFdpDFGWXqiaiN2ua%2Fhelp-please", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Yesterday my mom noticed (at a funeral) that I wasn't praying or participating in the mass. She confronted me about it, and I told her that no, I am not Catholic. Apparently it's sinking in and she's a bit hysterical... crying and screaming that she doesn't know me anymore.</p>\n<p>What do I do? I don't know how to react/behave when she's doing this. It's like she wants me to feel like I'm doing something wrong, but it isn't working, so she's getting hysterical.</p>\n<p>&nbsp;</p>\n<p>*edit*</p>\n<p>I gave her a hug when she calmed down and told her I love her. That seemed to help, a little. Based on her previous behavior in situations where I've done something \"wrong,\" she will (in the future) make barbs and slight passes at my beliefs. (Already she made one: insisting my love of science is causing my social anxiety disorder.) The advice given in the comments is really helpful. I plan on making the most of it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HFdpDFGWXqiaiN2ua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 18, "extendedScore": null, "score": 9.173528218952226e-07, "legacy": true, "legacyId": "16727", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T17:02:54.980Z", "modifiedAt": null, "url": null, "title": "Strategic research on AI risk", "slug": "strategic-research-on-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:03.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D9xvHv3hZj56cbTEe/strategic-research-on-ai-risk", "pageUrlRelative": "/posts/D9xvHv3hZj56cbTEe/strategic-research-on-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/D9xvHv3hZj56cbTEe/strategic-research-on-ai-risk", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strategic%20research%20on%20AI%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrategic%20research%20on%20AI%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD9xvHv3hZj56cbTEe%2Fstrategic-research-on-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strategic%20research%20on%20AI%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD9xvHv3hZj56cbTEe%2Fstrategic-research-on-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD9xvHv3hZj56cbTEe%2Fstrategic-research-on-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<p><small>Series: <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a></small></p>\n<p>Norman Rasmussen's <a href=\"http://en.wikipedia.org/wiki/WASH-1400\">analysis</a> of the safety of nuclear power plants, written before any nuclear accidents had occurred, correctly predicted several details of the <a href=\"http://en.wikipedia.org/wiki/Three_Mile_Island_accident\">Three Mile Island incident</a>&nbsp;in ways that that previous experts had not (see <a href=\"/lw/774/a_history_of_bayes_theorem/\">McGrayne 2011</a>, p. 180). Had Rasmussen's analysis been heeded, the Three Mile Island incident might not have occurred.</p>\n<p>This is the kind of <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">strategic analysis</a>, risk analysis, and technological forecasting that could help us to pivot the world in important ways.</p>\n<p>Our AI risk situation is very complicated. There are many uncertainties about the future, and many interacting strategic variables. Though it is often hard to see whether a strategic analysis will pay off, the alternative is to act blindly.</p>\n<p>Here are some examples of strategic research that may help (or have <em>already</em> helped) to inform our attempts to shape the future:</p>\n<ul>\n<li>FHI's <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">Whole Brain Emulation roadmap</a> and SI's <a href=\"http://intelligence.org/upload/Singularity%20Summit%202011%20Workshop%20Report.pdf\">WBE discussion</a> at the Summit 2011 workshop.</li>\n<li><a href=\"http://nickbostrom.com/\">Nick Bostrom</a>'s forthcoming book on machine superintelligence.</li>\n<li><em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em>, which locates AI risk in the context of other catastrophic risks.</li>\n<li>A model of AI risk currently being developed in MATLAB by Anna Salamon and others.</li>\n<li>A study of past researchers who abandoned certain kinds of research when they came to believe it might be dangerous, and what might have caused such action. (This project is underway at SI.)</li>\n</ul>\n<p>Here are some additional projects of strategic research that could help inform x-risk decisions, if funding were available to perform them:</p>\n<ul>\n<li>A study of opportunities for <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, and how to actually achieve them.</li>\n<li>A study of&nbsp;microeconomic models of WBEs and self-improving systems.</li>\n<li>A study of which research topics should and should not be discussed in public for the purposes of x-risk prevention. (E.g. we may wish to keep AGI discoveries secret for the same reason we'd want to keep the DNA of a synthetically developed supervirus secret, but we may wish to publish research on safe AGI goals because they are safe for a broader community to work on. But it's often difficult to see whether a subject fits into one category or the other.)</li>\n</ul>\n<p>I'll note that for as long as FHI is working on AI risk, FHI probably has an advantage over SI in producing actionable strategic research, given past successes like the WBE roadmap and the&nbsp;<em>GCR</em>&nbsp;volume. But SI is also performing actionable strategic research, as described above.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D9xvHv3hZj56cbTEe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 9.173851340535204e-07, "legacy": true, "legacyId": "16642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "RTt59BtFLqQbsSiqd", "i2XoqtYEykc4XWp9B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T17:20:59.020Z", "modifiedAt": null, "url": null, "title": "Phil Zimbardo answering questions on Reddit", "slug": "phil-zimbardo-answering-questions-on-reddit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Document", "createdAt": "2010-02-08T04:14:47.949Z", "isAdmin": false, "displayName": "Document"}, "userId": "vaMNHjzaCGqF8yTMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ckqSmqyyBxcMqimSK/phil-zimbardo-answering-questions-on-reddit", "pageUrlRelative": "/posts/ckqSmqyyBxcMqimSK/phil-zimbardo-answering-questions-on-reddit", "linkUrl": "https://www.lesswrong.com/posts/ckqSmqyyBxcMqimSK/phil-zimbardo-answering-questions-on-reddit", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Phil%20Zimbardo%20answering%20questions%20on%20Reddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhil%20Zimbardo%20answering%20questions%20on%20Reddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FckqSmqyyBxcMqimSK%2Fphil-zimbardo-answering-questions-on-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Phil%20Zimbardo%20answering%20questions%20on%20Reddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FckqSmqyyBxcMqimSK%2Fphil-zimbardo-answering-questions-on-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FckqSmqyyBxcMqimSK%2Fphil-zimbardo-answering-questions-on-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>http://www.reddit.com/r/IAmA/comments/untpp/i_am_a_published_psychologist_author_of_the/</p>\n<p>Starts tomorrow (June 7) at 12PM ET (1600 GMT) (thanks to khafra for the correction).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ckqSmqyyBxcMqimSK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 9.173932820243761e-07, "legacy": true, "legacyId": "16728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-06-06T17:41:58.133Z", "modifiedAt": null, "url": null, "title": "[Link] SMBC on choosing your simulations carefully", "slug": "link-smbc-on-choosing-your-simulations-carefully", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.124Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N7r4CqXpyuqmxKSFn/link-smbc-on-choosing-your-simulations-carefully", "pageUrlRelative": "/posts/N7r4CqXpyuqmxKSFn/link-smbc-on-choosing-your-simulations-carefully", "linkUrl": "https://www.lesswrong.com/posts/N7r4CqXpyuqmxKSFn/link-smbc-on-choosing-your-simulations-carefully", "postedAtFormatted": "Wednesday, June 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20SMBC%20on%20choosing%20your%20simulations%20carefully&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20SMBC%20on%20choosing%20your%20simulations%20carefully%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN7r4CqXpyuqmxKSFn%2Flink-smbc-on-choosing-your-simulations-carefully%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20SMBC%20on%20choosing%20your%20simulations%20carefully%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN7r4CqXpyuqmxKSFn%2Flink-smbc-on-choosing-your-simulations-carefully", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN7r4CqXpyuqmxKSFn%2Flink-smbc-on-choosing-your-simulations-carefully", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p><a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2633\">Link</a></p>\n<p>I'm increasingly impressed by the power of Zach Wiener's comic to demonstrate in a few images why hard problems are hard. It would be a vast task, but perhaps it would be useful to create an index of such problem-demonstrating comics to add to the Wiki, giving us something to point newbies at which would be less intimidating than formal Sequence postings. I get the impression that a common hurdle is just to get people to accept that problems of AI (and simulation, ethics, what have you) are actually difficult.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N7r4CqXpyuqmxKSFn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 12, "extendedScore": null, "score": 9.174025295617131e-07, "legacy": true, "legacyId": "16729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}