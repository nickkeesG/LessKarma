{"results": [{"createdAt": null, "postedAt": "2012-04-04T11:57:43.352Z", "modifiedAt": null, "url": null, "title": "Paraconsistency and relevance: avoid logical explosions", "slug": "paraconsistency-and-relevance-avoid-logical-explosions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:52.709Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oeMYMKnALbniFaoQL/paraconsistency-and-relevance-avoid-logical-explosions", "pageUrlRelative": "/posts/oeMYMKnALbniFaoQL/paraconsistency-and-relevance-avoid-logical-explosions", "linkUrl": "https://www.lesswrong.com/posts/oeMYMKnALbniFaoQL/paraconsistency-and-relevance-avoid-logical-explosions", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paraconsistency%20and%20relevance%3A%20avoid%20logical%20explosions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParaconsistency%20and%20relevance%3A%20avoid%20logical%20explosions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoeMYMKnALbniFaoQL%2Fparaconsistency-and-relevance-avoid-logical-explosions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paraconsistency%20and%20relevance%3A%20avoid%20logical%20explosions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoeMYMKnALbniFaoQL%2Fparaconsistency-and-relevance-avoid-logical-explosions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoeMYMKnALbniFaoQL%2Fparaconsistency-and-relevance-avoid-logical-explosions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 841, "htmlBody": "<p>EDIT: corrected from previous version.</p>\n<p style=\"padding-left: 30px;\">If the moon is made of cheese, then Rafael Delago was elected president of Ecuador in 2005.</p>\n<p style=\"padding-left: 30px;\">If you believe that Kennedy was shot in 1962, then you must believe that Santa Claus is the Egyptian god of the dead.</p>\n<p>Both of these are perfectly sound arguments of classical logic. The premise is false, hence the argument is logically correct, no matter what the conclusion is: if A is false, then A&rarr;B is true.</p>\n<p>It does feel&nbsp;counterintuitive, though, especially because human beliefs do not work in this way. Consider instead the much more intuitive statement:</p>\n<p style=\"padding-left: 30px; \">If you believe that Kennedy was shot in 1962, then you must believe that Lee Harry Oswald was also shot in 1962.</p>\n<p>Here there seems to be a connection between the two clauses; we feel&nbsp;A&rarr;B is more justified when \"&rarr;\" actually does some work in establishing a relationship between A and B. But can this intuition be formalised?</p>\n<p>One way to do so is to use relevance logics, which are a subset of&nbsp;\"<a href=\"http://plato.stanford.edu/entries/logic-paraconsistent/\">paraconsistent</a>\" logics.&nbsp;Paraconsistent&nbsp;logics are those that avoid the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">principle of explosion</a>. This is the rule in classical logic that if you accept one single contradiction - one single (A and not-A) - then you can prove everything. This is akin to accepting one false belief that contradict your other beliefs - after that, anything goes. The contradiction explodes and takes everything down with it. But why would we be interested in avoiding either the principle of explosion or unjustified uses of&nbsp;\"&rarr;\"?</p>\n<p><a id=\"more\"></a></p>\n<p>There seems to be three groups that could benefit from avoiding this. Firstly, those who are worried about the potential for the occasional error in their data or their premises, or a missed step in their reasoning, and don't want to collapse into incoherence because of a single mistake (paraconsistency has had application in database management, for instance). These generally need only 'weakly' paraconsistent theories. Secondly, the <a href=\"http://en.wikipedia.org/wiki/Dialetheism\">dialethics</a>, who believe in the existence of true contradictions. The liar's paradox is an example of this: if L=\"L is false\", then a dialethic would simply say that L is true, and not-L is also true, accepting the contradiction (L and not-L). This has the advantage of allowing a language to talk about its own <a href=\"http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem\">truth</a>: arithmetic truths can be defined in arithmetic, if we accept a few contradictions along the way.</p>\n<p>For Less Wrong, the best use of&nbsp; relevance&nbsp;logic would be to articulate counterfactuals without falling into the <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">L<span style=\"color: #000000; \">&ouml;</span>bian</a>/self-confirming&nbsp;<a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">trap</a> and blowing up. Consider the toy problem:</p>\n<pre style=\"text-align: justify; \">def U():<br />&nbsp;&nbsp;if A()==1:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 5<br />&nbsp;&nbsp;else:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 10</pre>\n<p>Then the problem is that in UDT, sentences such as L=\"(A()==1&nbsp;&rarr; U() == 5) and (A()!=1&nbsp;&rarr; U() == -200)\" are self-confirming: if they are accepted by the utility maximising agent A(), then they will become valid. This is because A() will then output 1, making the first clause valid by calculation, and the second clause valid because the antecedent is false. This leads to all sorts of L&ouml;bian problems. However, if we reject the gratuitous use of&nbsp;&nbsp;\"&rarr;\", then even if we kept L&ouml;bian reasoning, the argument would fail, as L would no longer be self-confirming.</p>\n<p>Ok, as the actor said, that's my motivation; now, how do we do it? Where does the principle of explosion come from, and what do we have to do to get rid of it?&nbsp;Allegedly, one mathematician once defended the argument \"(0=1) implies (I am God!)\" by saying \"(0=1) implies (1=2); (I and God are two) hence (I and God are one)!\". The more rigorous proof, starting from the false premise (A and not-A), and proving any B, goes as follows (terminology will be explained):</p>\n<ol>\n<li><strong>A and not-A</strong> <span style=\"white-space: pre;\"> </span>(premise)</li>\n<li><strong>A</strong> <span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>(by conjunction elimination from (1))</li>\n<li><strong>not-A</strong> &nbsp;(by conjunction elimination from (1))</li>\n<li><strong>A or B</strong> &nbsp;(by disjunction introduction from (2))</li>\n<li><strong>B</strong><span style=\"white-space: pre;\"><strong> </strong><span style=\"white-space: pre;\"> </span></span>(by disjunction syllogism from (3) and (4))</li>\n</ol>\n<p>To reject this proof, we have four options: reject conjunction elimination, reject disjunction introduction, reject the&nbsp;disjunction syllogism, or reject transitive proofs: say that, for instance, \"(2) and (3)\" implies \"(4)\", \"(3) and (4)\" implies \"(5)\", but reject the implication that \"(2) and (3)\" implies \"(5)\".</p>\n<p>Rejecting transitive proofs is almost never done: what is the point of a proof system if you can't build on previous results? Conjunction elimination says that \"(A and B) is true\" means that both A and B are true; this seems far too fundamental to our understanding of 'and' to be tossed aside.</p>\n<p>Disjunction introduction says that \"A is true\" implies that \"(A or B) is true\" for any B. This is also intuitive, though possibly a little less so; we are randomly inserting a B of which we know nothing. There are paraconsistent logics that <a href=\"http://en.wikipedia.org/wiki/Paraconsistent_logic#Tradeoff\">reject</a> disjunction introduction, but we won't be looking at them here (and why, I hear you ask? For the deep philosophical reason that the book I'm reading doesn't go into them).</p>\n<p>That leaves the disjunction syllogism. This claims that from (A or B) and (not-A) we can deduce B. It is certainly formally intuitive, so in my next post I'll present a simple and&nbsp;reasonably&nbsp;intuitive paraconsistent system of logic that rejects it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oeMYMKnALbniFaoQL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 13, "extendedScore": null, "score": 8.779006371672936e-07, "legacy": true, "legacyId": "14824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ALCnqX6Xx8bpFMZq3", "2GebvAXXfRMTjY2g7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T12:24:08.595Z", "modifiedAt": null, "url": null, "title": "Rationality and Decorating", "slug": "rationality-and-decorating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:51.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HZZAPrQSo3TWqtqLW/rationality-and-decorating", "pageUrlRelative": "/posts/HZZAPrQSo3TWqtqLW/rationality-and-decorating", "linkUrl": "https://www.lesswrong.com/posts/HZZAPrQSo3TWqtqLW/rationality-and-decorating", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20and%20Decorating&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20and%20Decorating%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZZAPrQSo3TWqtqLW%2Frationality-and-decorating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20and%20Decorating%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZZAPrQSo3TWqtqLW%2Frationality-and-decorating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZZAPrQSo3TWqtqLW%2Frationality-and-decorating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>From <a href=\"http://www.wired.com/wiredscience/2012/03/the-psychology-of-casinos/\">Wired</a>:</p>\n<blockquote>Thomas&rsquo;s sumptuous designs led people to spend as they&rsquo;d never spent before, and, in the years since the Bellagio was completed, research has supported the psychological assumptions that went into its creation. Karen Finlay is a professor at the University of Guelph, in Ontario, who focusses on the behavior of gamblers. Her latest experiments have immersed subjects in the interiors of various Vegas hotels by means of a Panoscope, which projects three hundred and sixty degrees of high-de\ufb01nition video footage. There are slot machines and card tables in every direction. Using the Panoscope method, Finlay compared the mental e\ufb00ects of classic casinos, with low ceilings and a mazelike layout, to those of casinos designed by Thomas. Subjects surrounded by footage of Thomas&rsquo;s interiors exhibited far higher levels of what Finlay terms mental &ldquo;restoration&rdquo;&mdash;that is, they were much more likely to say that the space felt like a &ldquo;refuge&rdquo; and reduced their stress level. They also manifested a much stronger desire to gamble. In every Panoscopic matchup, gamblers in Thomas&rsquo;s rooms were more likely to spend money than those in Friedmanesque designs. Although subjects weren&rsquo;t forced to focus on the slot machines, the pleasant atmosphere encouraged them to give the machines a try.</blockquote>\n<p>One point is just that a huge number of casino owners (who are presumably highly incentivized to make money) jumped to a conclusion about what would work best, and they were wrong.</p>\n<p>A possible angle is that people don't trust pleasure enough-- the assumption was that people couldn't be led to gamble, they had to be forced.</p>\n<p>On the other hand, if the story in the article is correct, getting a better solution wasn't a process of thought and testing, it was a result of the right weird person trusting himself, and convincing a casino owner to trust him.</p>\n<p>Does the article imply that it's extremely important to make your work space as pleasant as possible in all respects? Would that make it too hard to get out for socializing?</p>\n<p>Mild aggravation: The test was set in simulated casinos, not real ones.</p>\n<p>Gender nitpick: The article describes the female-friendly casino as having every surface covered with something expensive. It seems to me that the male-friendly decorating scheme (British club-- it didn't work because men weren't showing up at all) just has everything either covered with or made of something expensive. Not a huge difference.</p>\n<p>I'm left wondering whether the male-friendly scheme was actually the wrong male-friendly scheme and something else would work. Is there such a thing as human-friendly decorating which is gender non-specific?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HZZAPrQSo3TWqtqLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 8.779116601322921e-07, "legacy": true, "legacyId": "14825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T16:02:59.870Z", "modifiedAt": null, "url": null, "title": "[LINK] Making Big Decisions about Money", "slug": "link-making-big-decisions-about-money", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:52.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4TPQcQwTLhQ2MreGD/link-making-big-decisions-about-money", "pageUrlRelative": "/posts/4TPQcQwTLhQ2MreGD/link-making-big-decisions-about-money", "linkUrl": "https://www.lesswrong.com/posts/4TPQcQwTLhQ2MreGD/link-making-big-decisions-about-money", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Making%20Big%20Decisions%20about%20Money&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Making%20Big%20Decisions%20about%20Money%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4TPQcQwTLhQ2MreGD%2Flink-making-big-decisions-about-money%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Making%20Big%20Decisions%20about%20Money%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4TPQcQwTLhQ2MreGD%2Flink-making-big-decisions-about-money", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4TPQcQwTLhQ2MreGD%2Flink-making-big-decisions-about-money", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>Seth Godin has an article about how to translate monetary values into something intuitively commensurable with the thing you're buying:</p>\r\n<p><a href=\"http://sethgodin.typepad.com/seths_blog/2012/03/making-big-decisions-about-money.html\">Making Big Decisions about Money</a></p>\r\n<p>The conclusion:</p>\r\n<p>If you go to the free school, you can drive there in a brand new Mini convertible, and every summer you can spend $25,000 on a top-of-the-line internship/experience, and you can create a jazz series and pay your favorite musicians to come to campus to play for you and your fifty coolest friends, and you can have Herbie Hancock give you piano lessons and you can still have enough money left over to live without debt for a year after you graduate while you look for the perfect gig...</p>\r\n<p>Suddenly, you're not comparing \"this is my dream,\" with a number that means very little. You're comparing one version of your dream with another version.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4TPQcQwTLhQ2MreGD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 8.78002977485323e-07, "legacy": true, "legacyId": "14828", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T19:44:59.063Z", "modifiedAt": null, "url": null, "title": "You only live once: a reframing of working towards posthumanity", "slug": "you-only-live-once-a-reframing-of-working-towards", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:05.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HsEhzqhJH3uahSyS3/you-only-live-once-a-reframing-of-working-towards", "pageUrlRelative": "/posts/HsEhzqhJH3uahSyS3/you-only-live-once-a-reframing-of-working-towards", "linkUrl": "https://www.lesswrong.com/posts/HsEhzqhJH3uahSyS3/you-only-live-once-a-reframing-of-working-towards", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20only%20live%20once%3A%20a%20reframing%20of%20working%20towards%20posthumanity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20only%20live%20once%3A%20a%20reframing%20of%20working%20towards%20posthumanity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsEhzqhJH3uahSyS3%2Fyou-only-live-once-a-reframing-of-working-towards%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20only%20live%20once%3A%20a%20reframing%20of%20working%20towards%20posthumanity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsEhzqhJH3uahSyS3%2Fyou-only-live-once-a-reframing-of-working-towards", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsEhzqhJH3uahSyS3%2Fyou-only-live-once-a-reframing-of-working-towards", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<p>Let's say you recently purchased a new computer game. Reviewers like the game, but they say that the game itself pales in comparison to a massive, difficult-to-access secret area near the end. Bizarrely, this secret area contains over 90% of the content in the game. It's also rumored to be a lot better designed.</p>\n<p>If you're like most gamers, you'll probably play through the game normally a few times, and go for the secret area on your second or third try. But what if the game was real life, and you died as soon as your first try ended?</p>\n<p>Edit for clarifying points:</p>\n<div>\n<ul>\n<li>Posthumanity is definitely an area within the game. It's not prohibited by the laws of physics. The relevant questions are how difficult it would be to achieve and whether it would be enough fun to be worth the effort.</li>\n<li>The reframing is meant to deal with the issue that humans tend to be bad at thinking about unprecedented events (black swans).</li>\n<li>But the reframing is not perfect. For example, it ignores the fact that posthumanity may be achieved without your help. It also ignores the fact that by achieving posthumanity, you are helping more people than just yourself.</li>\n<li>If no effective plan for achieving posthumanity exists, you'll have to think of one one in order to access the proverbial secret area. This isn't about any specific plan.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HsEhzqhJH3uahSyS3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 7, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "14830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T21:03:10.977Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City Meetup (Location Changed!)", "slug": "meetup-salt-lake-city-meetup-location-changed", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cYbmSirsyf49eTNdF/meetup-salt-lake-city-meetup-location-changed", "pageUrlRelative": "/posts/cYbmSirsyf49eTNdF/meetup-salt-lake-city-meetup-location-changed", "linkUrl": "https://www.lesswrong.com/posts/cYbmSirsyf49eTNdF/meetup-salt-lake-city-meetup-location-changed", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%20Meetup%20(Location%20Changed!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%20Meetup%20(Location%20Changed!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcYbmSirsyf49eTNdF%2Fmeetup-salt-lake-city-meetup-location-changed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%20Meetup%20(Location%20Changed!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcYbmSirsyf49eTNdF%2Fmeetup-salt-lake-city-meetup-location-changed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcYbmSirsyf49eTNdF%2Fmeetup-salt-lake-city-meetup-location-changed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8l'>Salt Lake City Meetup (Location Changed!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 April 2012 12:00:42PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holladay Library: 2150 Murray Holladay Rd, Holladay, UT 84117</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Presentation on social influence, based off Influencer, and discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8l'>Salt Lake City Meetup (Location Changed!)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cYbmSirsyf49eTNdF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "14831", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_Meetup__Location_Changed__\">Discussion article for the meetup : <a href=\"/meetups/8l\">Salt Lake City Meetup (Location Changed!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 April 2012 12:00:42PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holladay Library: 2150 Murray Holladay Rd, Holladay, UT 84117</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Presentation on social influence, based off Influencer, and discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_Meetup__Location_Changed__1\">Discussion article for the meetup : <a href=\"/meetups/8l\">Salt Lake City Meetup (Location Changed!)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City Meetup (Location Changed!)", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_Meetup__Location_Changed__", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City Meetup (Location Changed!)", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_Meetup__Location_Changed__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T22:48:05.585Z", "modifiedAt": null, "url": null, "title": "Cryonics without freezers: resurrection possibilities in a Big World", "slug": "cryonics-without-freezers-resurrection-possibilities-in-a", "viewCount": null, "lastCommentedAt": "2020-12-05T00:55:58.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MkKcnPdTZ3pQ9F5yC/cryonics-without-freezers-resurrection-possibilities-in-a", "pageUrlRelative": "/posts/MkKcnPdTZ3pQ9F5yC/cryonics-without-freezers-resurrection-possibilities-in-a", "linkUrl": "https://www.lesswrong.com/posts/MkKcnPdTZ3pQ9F5yC/cryonics-without-freezers-resurrection-possibilities-in-a", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20without%20freezers%3A%20resurrection%20possibilities%20in%20a%20Big%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20without%20freezers%3A%20resurrection%20possibilities%20in%20a%20Big%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkKcnPdTZ3pQ9F5yC%2Fcryonics-without-freezers-resurrection-possibilities-in-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20without%20freezers%3A%20resurrection%20possibilities%20in%20a%20Big%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkKcnPdTZ3pQ9F5yC%2Fcryonics-without-freezers-resurrection-possibilities-in-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkKcnPdTZ3pQ9F5yC%2Fcryonics-without-freezers-resurrection-possibilities-in-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1887, "htmlBody": "<blockquote>\n<p><em>And fear not lest Existence closing your<br />Account, should lose, or know the type no more;<br />The Eternal Saki from the Bowl has pour'd<br />Millions of Bubbles like us, and will pour.<br /><br />When You and I behind the Veil are past,<br />Oh, but the long long while the World shall last,<br />Which of our Coming and Departure heeds<br />As much as Ocean of a pebble-cast.<br /></em></p>\n</blockquote>\n<p>&nbsp;&nbsp;&nbsp; -- Omar Khayyam, <a href=\"http://www.sacred-texts.com/isl/khayyam.txt\">Rubaiyat</a></p>\n<p>&nbsp;</p>\n<p><strong>A CONSEQUENTIALIST VIEW OF IDENTITY</strong><br /><br />The typical argument for cryonics says that if we can preserve brain data, one day we may be able to recreate a functioning brain and bring the dead back to life.<br /><br />The typical argument against cryonics says that even if we could do that, the recreation wouldn't be \"you\". It would be someone who thinks and acts exactly like you.<br /><br />The typical response to the typical argument against cryonics says that <a href=\"/lw/pm/identity_isnt_in_specific_atoms\">identity isn't in specific atoms</a>, so it's probably in algorithms, and the recreation would have the same mental algorithms as you and so be you. The gap in consciousness of however many centuries is no more significant than the gap in consciousness between going to bed at night and waking up in the morning, or the gap between going into a coma and coming out of one.<br /><br />We can call this a \"consequentialist\" view of identity, because it's a lot like the consequentialist views of morality. Whether a person is \"me\" isn't a function of how we got to that person, but only of where that person is right now: that is, how similar that person's thoughts and actions are to my own. It doesn't matter if we got to him by having me go to sleep and wake up as him, or got to him by having aliens disassemble my brain and then simulate it on a cellular automaton. If he thinks like me, he's me.<br /><br />A corollary of the consequentialist view of identity says that if someone wants to create fifty perfect copies of me, all fifty will \"be me\" in whatever sense that means something.<br /><br /><strong>GRADATIONS OF IDENTITY</strong><br /><br />An argument against cryonics I have never heard, but which must exist somewhere, says that even the best human technology is imperfect, and likely a few atoms here and there - or even a few entire neurons - will end up out of place. Therefore, the recreation will not be you, but someone very very similar to you.<br /><br />And the response to this argument is \"Who cares?\" If by \"me\" you mean Yvain as of 10:20 PM 4th April 2012, then even Yvain as of 10:30 is going to have some serious differences at the atomic scale. Since I don't consider myself a different person every ten minutes, I shouldn't consider myself a different person if the resurrection-machine misplaces a few cells here or there.<br /><br />But this is a slippery slope. If my recreation is exactly like me except for one neuron, is he the same person? Signs point to yes. What about five neurons? Five million? Or on a functional level, what if he blinked at exactly one point where I would not have done so? What if he prefers a different flavor of ice cream? What if he has exactly the same memories as I do, except for the outcome of one first-grade spelling bee I haven't thought about in years anyway? What if he is a Hindu fundamentalist?<br /><br />If we're going to take a consequentialist view of identity, then my continued ability to identify with myself even if I naturally switch ice cream preferences suggests I should identify with a botched resurrection who also switches ice cream preferences. The only solution here that really makes sense is to view identity in shades of gray instead of black-and-white. An exact clone is <em>more</em> me than a clone with different ice cream preferences, who is <em>more</em> me than a clone who is a Hindu fundamentalist, who is <em>more</em> me than LeBron James is.<br /><br /><strong>BIG WORLDS</strong><br /><br />There are <a href=\"/lw/1t0/shock_level_5/\">various theories lumped together under the title \"big world\"</a>.<br /><br />The simplest is the theory that the universe (or multiverse) is Very Very Big. Although the universe is probably only 15 billion years old, which means the <em>visible</em> universe is only 30 billion light years in size, inflation allows the <em>entire</em> universe to get around the speed of light restriction; it could be very large or possibly infinite. I don't have the numbers available, but I remember a back of the envelope calculation being posted on Less Wrong once about exactly how big the universe would have to be to contain repeating patches of about the size of the Earth. That is, just as the first ten digits of pi, 3141592653, must repeat somewhere else in pi because pi is infinite and patternless, and just as I would believe this with high probability even if pi were not infinite but just very very large, so the arrangement of atoms that make up Earth would recur in an infinite or very very large universe. This arrangement would obviously include you, exactly as you are now. A much larger class of Earth-sized patches would include slightly different versions of you like the one with different ice cream preferences. This would also work, as Omar Khayyam mentioned in the quote at the top, if the universe were to last forever or a very very long time.<br /><br />The second type of \"big world\" is the one posited by the Many Worlds theory of quantum mechanics, in which each quantum event causes the Universe to split into several branches. Because quantum events determine larger-level events, and because each branch continues branching, some these branches could be similar to our universe but with observable macro-scale differences. For example, there might be a branch in which you are the President of the United States, or the Pope, or died as an infant. Although this sounds like a silly popular science version of the principle, I don't <em>think </em>it's unfair or incorrect.<br /><br />The third type of \"big world\" is modal realism: the belief that all possible worlds exist, maybe in proportion to their simplicity (whatever that means). We notice the existence of our own world only for indexical reasons: that is, just as there are many countries, but when I look around me I only see my own; so there are many possibilities, but when I look around me I only see my own. If this is true, it is not only possible but certain that there is a world where I am Pope and so on.<br /><br />There are other types of \"big worlds\" that I won't get into here, but if any type at all is correct, then there should be very many copies of me or people very much like me running around.<br /><br /><strong>CRYONICS WITHOUT FREEZERS</strong><br /><br />Cryonicists say that if you freeze your brain, you may experience \"waking up\" a few centuries later when someone uses the brain to create a perfect copy of you.<br /><br />But whether or not you freeze your brain, a Big World is creating perfect copies of you all the time. The consequentialist view of identity says that your causal connection with these copies is unnecessary for them to be you. So why should a copy of you created by a far-future cryonicist with access to your brain be better able to \"resurrect\" you than a copy of you that comes to exist for some other reason?<br /><br />For example, suppose I choose not to sign up for cryonics, have a sudden heart attack, and die in my sleep. Somewhere in a Big World, there is someone exactly like me except that they didn't have the heart attack and they wake up healthy the next morning.<br /><br />The cryonicists believe that having a healthy copy of you come into existence after you die is sufficient for you to \"wake up\" as that copy. So why wouldn't I \"wake up\" as the healthy, heart-attack-free version of me in the universe next door?<br /><br />Or: suppose that a Friendly AI fills a human-sized three-dimensional grid with atoms, using a quantum dice to determine which atom occupies each \"pixel\" in the grid. This splits the universe into as many branches as there are possible permutations of the grid (presumably <em>a lot</em>) and in one of those branches, the AI's experiment creates a perfect copy of me at the moment of my death, except healthy. If creating a perfect copy of me causes my \"resurrection\", then that AI has just resurrected me as surely as cryonics would have.<br /><br />The only downside I can see here is that I have less measure (meaning I exist in a lower proportion of worlds) than if I had signed up for cryonics directly. This might be a problem if I think that my existence benefits others - but I don't think I should be concerned for my own sake. Right now I don't go to bed at night weeping that my father only met my mother through a series of unlikely events and so most universes probably don't contain me; I'm not sure why I should do so after having been resurrected in the far future.<br /><br /><strong>RESURRECTION AS SOMEONE ELSE</strong><br /><br />What if the speculative theories involved in Big Worlds all turn out to be false? All hope is still not lost.<br /><br />Above I wrote:</p>\n<blockquote>\n<p>An exact clone is <em>more</em> me than a clone with different ice cream preferences, who is <em>more</em> me than a clone who is a Hindu fundamentalist, who is <em>more</em> me than LeBron James is.</p>\n</blockquote>\n<p>I used LeBron James because from what I know about him, he's quite different from me. But what if I had used someone else? One thing I learned upon discovering Less Wrong is that I had previously underestimated just how many people out there are *really similar to me*, even down to weird interests, personality quirks, and sense of humor. So let's take the person living in 2050 who is most similar to me now. I can think of several people on this site alone who would make a pretty impressive lower bound on how similar the most similar person to me would have to be.<br /><br />In what way is this person waking up on the morning of January 1 2050 equivalent to me being sort of resurrected? What if this person is more similar to Yvain(2012) than Yvain(1995) is? What if I signed up for cryonics, died tomorrow, and was resurrected in 2050 by a process about as lossy as the difference between me and this person?<br /><br /><strong>SUMMARY</strong><br /><br />Personal identity remains confusing. But some of the assumptions cryonicists make are, in certain situations, sufficient to guarantee personal survival after death without cryonics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MkKcnPdTZ3pQ9F5yC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 72, "extendedScore": null, "score": 0.000148, "legacy": true, "legacyId": "14832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RLScTpwc5W2gGGrL9", "SkXLrDXyHeekqgbFg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T23:14:20.362Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Board Games Meetup", "slug": "meetup-vancouver-board-games-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3jKiFAZokseLpi8dA/meetup-vancouver-board-games-meetup", "pageUrlRelative": "/posts/3jKiFAZokseLpi8dA/meetup-vancouver-board-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/3jKiFAZokseLpi8dA/meetup-vancouver-board-games-meetup", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Board%20Games%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Board%20Games%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3jKiFAZokseLpi8dA%2Fmeetup-vancouver-board-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Board%20Games%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3jKiFAZokseLpi8dA%2Fmeetup-vancouver-board-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3jKiFAZokseLpi8dA%2Fmeetup-vancouver-board-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8m'>Vancouver Board Games Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 April 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Drexoll Games, 2880 W 4th Ave, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello everybody. We are having our first monthly friendly social games meetup. The idea is to have a bigger meetup where newcomers can feel comfortable because it's not held in my sketchy dungeon of doom.</p>\n\n<p>So we are going to go to Drexoll Games and play board games. Adding players to games halfway thru is no fun, so try to show up at or before 13:00. If you can't make it on time, come anyways. We estimate that the meetup will go until about 16:00, so don't worry about missing dinner.</p>\n\n<p>As usual, see the mailing list for more details: <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">vancouver-rationalists</a></p>\n\n<p>If you haven't been coming for whatever reason, this is the time to start, or at least check us out. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8m'>Vancouver Board Games Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3jKiFAZokseLpi8dA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.781830049759021e-07, "legacy": true, "legacyId": "14833", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Board_Games_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8m\">Vancouver Board Games Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 April 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Drexoll Games, 2880 W 4th Ave, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello everybody. We are having our first monthly friendly social games meetup. The idea is to have a bigger meetup where newcomers can feel comfortable because it's not held in my sketchy dungeon of doom.</p>\n\n<p>So we are going to go to Drexoll Games and play board games. Adding players to games halfway thru is no fun, so try to show up at or before 13:00. If you can't make it on time, come anyways. We estimate that the meetup will go until about 16:00, so don't worry about missing dinner.</p>\n\n<p>As usual, see the mailing list for more details: <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">vancouver-rationalists</a></p>\n\n<p>If you haven't been coming for whatever reason, this is the time to start, or at least check us out. See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Board_Games_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8m\">Vancouver Board Games Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Board Games Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Board_Games_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Board Games Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Board_Games_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T00:55:08.530Z", "modifiedAt": null, "url": null, "title": "Bayesianism and use of Evidence in Social Deduction Games", "slug": "bayesianism-and-use-of-evidence-in-social-deduction-games", "viewCount": null, "lastCommentedAt": "2019-07-24T08:02:58.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DeevGrape", "createdAt": "2011-11-02T19:26:47.963Z", "isAdmin": false, "displayName": "DeevGrape"}, "userId": "m9JPq6irpKromu9x9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xgM4FjQeBQgmNEgkb/bayesianism-and-use-of-evidence-in-social-deduction-games", "pageUrlRelative": "/posts/xgM4FjQeBQgmNEgkb/bayesianism-and-use-of-evidence-in-social-deduction-games", "linkUrl": "https://www.lesswrong.com/posts/xgM4FjQeBQgmNEgkb/bayesianism-and-use-of-evidence-in-social-deduction-games", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesianism%20and%20use%20of%20Evidence%20in%20Social%20Deduction%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesianism%20and%20use%20of%20Evidence%20in%20Social%20Deduction%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxgM4FjQeBQgmNEgkb%2Fbayesianism-and-use-of-evidence-in-social-deduction-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesianism%20and%20use%20of%20Evidence%20in%20Social%20Deduction%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxgM4FjQeBQgmNEgkb%2Fbayesianism-and-use-of-evidence-in-social-deduction-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxgM4FjQeBQgmNEgkb%2Fbayesianism-and-use-of-evidence-in-social-deduction-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 615, "htmlBody": "<p>You look around the table at four friends -- people who share your hatred for the evil empire, or so you thought. At this table, where the resistance meet to plan their missions, fully two of five the operatives are spies, infiltrating the rebels to sabotage their missions. You've seen your loyalty card, so you know you're resistance... but how do you figure out which of your so-called allies are the spies?</p>\n<p>The Resistance, like Werewolf, Mafia, Battlestar Galactica, and other social deduction games, tasks the majority of players with rooting out the spies in their midst -- while the spies win by staying hidden.&nbsp;Among my friends, accusations of spyhood tend to be absolute: \"Did you see how long he hesitated? He<em> must </em>be a spy!\" Whether the suspicion is based on social cues or in-game actions, players rapidly become very sure of those beliefs they discuss at the table. They seem to divide their observations into two neat boxes, based on whether the data can decisively show someone's identity. If evidence seems convincing, it becomes concrete proof, immune to discussion; and if it doesn't, then it's disregarded.&nbsp;</p>\n<p>This treatment of evidence can lead to overconfidence: once when I was well-framed by the spies, my fellow resistance member refused to even <em>imagine</em>&nbsp;how I could be innocent. And why should he listen to me? He had <em>evidence</em>&nbsp;that I was a spy.&nbsp;On the other hand, it can just as easily lead to under-confidence: when new players see that there is no conclusive proof one way or the other, they often disregard the hints and suggestive evidence (in someone's tone of voice, or their eagerness to go on a mission), and throw their hands up at the supposed randomness of the game.&nbsp;</p>\n<p>Using Bayesianism as an alternative to this dichotomy allows me to treat evidence with the appropriate scrutiny, rather than using narrative ideas to guide my play. A two-person mission succeeds; the next mission adds a player to that team, and it fails. According to story logic, the first two players are trustworthy, so the third must have sabotaged the new mission. For more experienced players, the first mission is treated as having no informational value: spies may lay low, so any of the three players could be the saboteur, and it's a 1/3 shot. According to Bayesianism, P(player 3 is a spy) is influenced by all available evidence, given proper weighting. How likely is it for a spy to lay low on the first mission? Who chose for player 3 to join the mission? What is player 3's strategy as a spy? I find that this approach, of investigating all available evidence and updating my suspicions accordingly, allows me to have better precision in my accusations, and hopefully leads my teammates to start valuing evidence in the gradient way that these games, and investigation in life in general, requires for success.</p>\n<p>&nbsp;</p>\n<p>I post this not only because I love playing Resistance (obviously!), but also because I think this game could be a fun and useful exercise in Bayesian reasoning, for the same reasons that <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">Paranoid Debating</a> may be: the group's appraisal of the evidence needs to be accurate for the resistance to win, while it must be inaccurate for the spies to win. This encourages proper Bayesian technique among the resistance, and clever, bias-abusing rhetoric from the spies to twist the game in their favor.&nbsp;</p>\n<p>If anyone would like to use this game at a LessWrong meetup, or as an activity run by the Center for Modern Rationality, all you need are the rules&nbsp;(<a href=\"http://www.scribd.com/doc/88041256/The-Resistance-Rules-Summary\">here</a>&nbsp;and <a href=\"http://www.scribd.com/doc/88041585/ResistancePNP-cheat-rev1-2-5x3-5\">here</a>), a deck of playing cards, and the power of Bayes!</p>\n<p>&nbsp;</p>\n<p>(Special thanks to Julia Galef, for thinking the game sounded like a fun idea for teaching Bayesianism)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xgM4FjQeBQgmNEgkb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "14838", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T03:00:29.170Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Quantum Arena", "slug": "seq-rerun-the-quantum-arena", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:05.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q8zFxc73rLvoWgen3/seq-rerun-the-quantum-arena", "pageUrlRelative": "/posts/Q8zFxc73rLvoWgen3/seq-rerun-the-quantum-arena", "linkUrl": "https://www.lesswrong.com/posts/Q8zFxc73rLvoWgen3/seq-rerun-the-quantum-arena", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Quantum%20Arena&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Quantum%20Arena%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8zFxc73rLvoWgen3%2Fseq-rerun-the-quantum-arena%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Quantum%20Arena%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8zFxc73rLvoWgen3%2Fseq-rerun-the-quantum-arena", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ8zFxc73rLvoWgen3%2Fseq-rerun-the-quantum-arena", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>Today's post, <a href=\"/lw/pj/the_quantum_arena/\">The Quantum Arena</a> was originally published on 15 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Instead of a system state being associated with a single point in a classical configuration space, the instantaneous real state of a quantum system is a complex amplitude distribution over a quantum configuration space. What creates the illusion of \"individual particles\", like an electron caught in a trap, is a plaid distribution - one that happens to factor into the product of two parts. It is the whole distribution that evolves when a quantum system evolves. Individual configurations don't have physics; amplitude distributions have physics. Quantum entanglement is the general case; quantum <em>independence </em>is the special case.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bfq/seq_rerun_classical_configuration_spaces/\">Classical Configuration Spaces</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q8zFxc73rLvoWgen3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.782774169559482e-07, "legacy": true, "legacyId": "14848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eHeJxJZii6tqQupZL", "8494ns9GH3urHJnDY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T07:31:45.149Z", "modifiedAt": null, "url": null, "title": "Finding big donors", "slug": "finding-big-donors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.666Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vojCboFAwc6F9kK7L/finding-big-donors", "pageUrlRelative": "/posts/vojCboFAwc6F9kK7L/finding-big-donors", "linkUrl": "https://www.lesswrong.com/posts/vojCboFAwc6F9kK7L/finding-big-donors", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Finding%20big%20donors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFinding%20big%20donors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvojCboFAwc6F9kK7L%2Ffinding-big-donors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Finding%20big%20donors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvojCboFAwc6F9kK7L%2Ffinding-big-donors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvojCboFAwc6F9kK7L%2Ffinding-big-donors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p><a href=\"http://copperbadge.livejournal.com/3481169.html#cutid1\">How large foundations look for donors</a></p>\n<p>I don't know how much of this SIAI is already doing, but I thought it would be of interest to anyone who's building up an organization which needs donors. It's also interesting (at least to me-- I've spent my life in a very small business) for how much serious thought can be brought to a project.</p>\n<p>One surprising fact: \"A large not for profit did a study a few years ago which told them that  most of the people who left a million dollars or more to them in their  will, unasked-for, gave an average of $17 a year to them during their  life.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vojCboFAwc6F9kK7L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 8.783906892734735e-07, "legacy": true, "legacyId": "14856", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T09:44:06.382Z", "modifiedAt": null, "url": null, "title": "Skoll World Forum: Catastrophic Risk and Threats to the Global Commons", "slug": "skoll-world-forum-catastrophic-risk-and-threats-to-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9z58F7TedgqbMGADL/skoll-world-forum-catastrophic-risk-and-threats-to-the", "pageUrlRelative": "/posts/9z58F7TedgqbMGADL/skoll-world-forum-catastrophic-risk-and-threats-to-the", "linkUrl": "https://www.lesswrong.com/posts/9z58F7TedgqbMGADL/skoll-world-forum-catastrophic-risk-and-threats-to-the", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Skoll%20World%20Forum%3A%20Catastrophic%20Risk%20and%20Threats%20to%20the%20Global%20Commons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASkoll%20World%20Forum%3A%20Catastrophic%20Risk%20and%20Threats%20to%20the%20Global%20Commons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9z58F7TedgqbMGADL%2Fskoll-world-forum-catastrophic-risk-and-threats-to-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Skoll%20World%20Forum%3A%20Catastrophic%20Risk%20and%20Threats%20to%20the%20Global%20Commons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9z58F7TedgqbMGADL%2Fskoll-world-forum-catastrophic-risk-and-threats-to-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9z58F7TedgqbMGADL%2Fskoll-world-forum-catastrophic-risk-and-threats-to-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 385, "htmlBody": "<p>\n<object width=\"560\" height=\"315\" data=\"http://www.youtube.com/v/E1vqMcfQ2UA?version=3&amp;hl=en_US&amp;rel=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/E1vqMcfQ2UA?version=3&amp;hl=en_US&amp;rel=0\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p><strong>More:</strong> <a href=\"http://www.skollglobalthreats.org/\">Skoll Global Threats Fund | To Safeguard Humanity from Global Threats</a></p>\n<blockquote>\n<p>The panel surfaced a number of issues that contribute to our  inability to date to make serious strides on global challenges,  including income inequality, failure of governance and lack of  leadership.&nbsp; It also explored some deeper issues around pysche and  society&nbsp; &ndash; people&rsquo;s inability to convert information to wisdom, the loss  of sense of self, the challenges of hyperconnectivity, and questions  about economic models and motivations that have long underpinned  concepts of growth and wellbeing.&nbsp; The session was filmed, and we&rsquo;ll  make public that link once the file is available.&nbsp; In the meantime, here  are some of the more memorable quotes (which may not be verbatim, but  this is how I wrote them down):</p>\n<p>&ldquo;When people say something is impossible, that just means it&rsquo;s hard.&rdquo;</p>\n<p>&ldquo;Inequality is becoming an existential threat.&rdquo;</p>\n<p>&ldquo;We&rsquo;re at a crossroads.&nbsp; We can make progress against these big issues or we can kill ourselves.&rdquo;</p>\n<p>&ldquo;We need inclusive globalization, to give <strong>everyone</strong> a stake in the future.&rdquo;</p>\n<p>&lsquo;Fatalism is our most deadly adversary.&rdquo;</p>\n<p>&ldquo;What we&rsquo;re lacking is not IQ, but wisdom.&rdquo;</p>\n<p>&ldquo;We need to tap into the timeless to solve the urgent.&rdquo;</p>\n</blockquote>\n<blockquote>\n<p><strong>What we mean by global threats</strong></p>\n<p>Global threats have the potential to kill or debilitate very large numbers of people or cause significant economic or social dislocation or paralysis throughout the world. Global threats cannot be solved by any one country; they require some sort of a collective response. Global threats are often non-linear, and are likely to become exponentially more difficult to manage if we don&rsquo;t begin making serious strides in the right direction in the next 5-10 years.</p>\n</blockquote>\n<p><strong>More on existential risks:</strong> <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">wiki.lesswrong.com/wiki/Existential_risk</a></p>\n<h3><span id=\"Organisations\" class=\"mw-headline\">Organisations</span></h3>\n<p>A list of organisations and charities concerned with existential risk research.</p>\n<ul>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://intelligence.org/\">Singularity Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.fhi.ox.ac.uk/\">The Future of Humanity Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.futuretech.ox.ac.uk/\">The Oxford Martin Programme on the Impacts of Future Technology</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.gcrinstitute.org/\">Global Catastrophic Risk Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://shfhs.org/\">Saving Humanity from Homo Sapiens</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.skollglobalthreats.org/\">Skoll Global Threats Fund (To Safeguard Humanity from Global Threats)</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.foresight.org/\">Foresight Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://nuclearrisk.org/\">Defusing the Nuclear Threat</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.leverageresearch.org/\">Leverage Research</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://lifeboat.com/\">The Lifeboat Foundation</a> </li>\n</ul>\n<h3><span id=\"Resources\" class=\"mw-headline\">Resources</span></h3>\n<ul>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.existential-risk.org/\">existential-risk.org</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://sethbaum.com/research/gcr/\">Global Catastrophic Risk Research Page</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.global-catastrophic-risks.com/\">Global Catastrophic Risks</a> </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9z58F7TedgqbMGADL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.784457720596933e-07, "legacy": true, "legacyId": "14868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>\n<object width=\"560\" height=\"315\" data=\"http://www.youtube.com/v/E1vqMcfQ2UA?version=3&amp;hl=en_US&amp;rel=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\">\n<param name=\"allowscriptaccess\" value=\"always\">\n<param name=\"src\" value=\"http://www.youtube.com/v/E1vqMcfQ2UA?version=3&amp;hl=en_US&amp;rel=0\">\n<param name=\"allowfullscreen\" value=\"true\">\n</object>\n</p>\n<p><strong>More:</strong> <a href=\"http://www.skollglobalthreats.org/\">Skoll Global Threats Fund | To Safeguard Humanity from Global Threats</a></p>\n<blockquote>\n<p>The panel surfaced a number of issues that contribute to our  inability to date to make serious strides on global challenges,  including income inequality, failure of governance and lack of  leadership.&nbsp; It also explored some deeper issues around pysche and  society&nbsp; \u2013 people\u2019s inability to convert information to wisdom, the loss  of sense of self, the challenges of hyperconnectivity, and questions  about economic models and motivations that have long underpinned  concepts of growth and wellbeing.&nbsp; The session was filmed, and we\u2019ll  make public that link once the file is available.&nbsp; In the meantime, here  are some of the more memorable quotes (which may not be verbatim, but  this is how I wrote them down):</p>\n<p>\u201cWhen people say something is impossible, that just means it\u2019s hard.\u201d</p>\n<p>\u201cInequality is becoming an existential threat.\u201d</p>\n<p>\u201cWe\u2019re at a crossroads.&nbsp; We can make progress against these big issues or we can kill ourselves.\u201d</p>\n<p>\u201cWe need inclusive globalization, to give <strong>everyone</strong> a stake in the future.\u201d</p>\n<p>\u2018Fatalism is our most deadly adversary.\u201d</p>\n<p>\u201cWhat we\u2019re lacking is not IQ, but wisdom.\u201d</p>\n<p>\u201cWe need to tap into the timeless to solve the urgent.\u201d</p>\n</blockquote>\n<blockquote>\n<p><strong id=\"What_we_mean_by_global_threats\">What we mean by global threats</strong></p>\n<p>Global threats have the potential to kill or debilitate very large numbers of people or cause significant economic or social dislocation or paralysis throughout the world. Global threats cannot be solved by any one country; they require some sort of a collective response. Global threats are often non-linear, and are likely to become exponentially more difficult to manage if we don\u2019t begin making serious strides in the right direction in the next 5-10 years.</p>\n</blockquote>\n<p><strong>More on existential risks:</strong> <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">wiki.lesswrong.com/wiki/Existential_risk</a></p>\n<h3 id=\"Organisations\"><span id=\"Organisations\" class=\"mw-headline\">Organisations</span></h3>\n<p>A list of organisations and charities concerned with existential risk research.</p>\n<ul>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://intelligence.org/\">Singularity Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.fhi.ox.ac.uk/\">The Future of Humanity Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.futuretech.ox.ac.uk/\">The Oxford Martin Programme on the Impacts of Future Technology</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.gcrinstitute.org/\">Global Catastrophic Risk Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://shfhs.org/\">Saving Humanity from Homo Sapiens</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.skollglobalthreats.org/\">Skoll Global Threats Fund (To Safeguard Humanity from Global Threats)</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.foresight.org/\">Foresight Institute</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://nuclearrisk.org/\">Defusing the Nuclear Threat</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.leverageresearch.org/\">Leverage Research</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://lifeboat.com/\">The Lifeboat Foundation</a> </li>\n</ul>\n<h3 id=\"Resources\"><span id=\"Resources\" class=\"mw-headline\">Resources</span></h3>\n<ul>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.existential-risk.org/\">existential-risk.org</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://sethbaum.com/research/gcr/\">Global Catastrophic Risk Research Page</a> </li>\n<li> <a class=\"external text\" rel=\"nofollow\" href=\"http://www.global-catastrophic-risks.com/\">Global Catastrophic Risks</a> </li>\n</ul>", "sections": [{"title": "What we mean by global threats", "anchor": "What_we_mean_by_global_threats", "level": 2}, {"title": "Organisations", "anchor": "Organisations", "level": 1}, {"title": "Resources", "anchor": "Resources", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T10:10:27.343Z", "modifiedAt": null, "url": null, "title": "Logic of Paradox: a (too) simple paraconsistent logic", "slug": "logic-of-paradox-a-too-simple-paraconsistent-logic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rwysnoyCxMLqQZnCY/logic-of-paradox-a-too-simple-paraconsistent-logic", "pageUrlRelative": "/posts/rwysnoyCxMLqQZnCY/logic-of-paradox-a-too-simple-paraconsistent-logic", "linkUrl": "https://www.lesswrong.com/posts/rwysnoyCxMLqQZnCY/logic-of-paradox-a-too-simple-paraconsistent-logic", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logic%20of%20Paradox%3A%20a%20(too)%20simple%20paraconsistent%20logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogic%20of%20Paradox%3A%20a%20(too)%20simple%20paraconsistent%20logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwysnoyCxMLqQZnCY%2Flogic-of-paradox-a-too-simple-paraconsistent-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logic%20of%20Paradox%3A%20a%20(too)%20simple%20paraconsistent%20logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwysnoyCxMLqQZnCY%2Flogic-of-paradox-a-too-simple-paraconsistent-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwysnoyCxMLqQZnCY%2Flogic-of-paradox-a-too-simple-paraconsistent-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 505, "htmlBody": "<p>The logic of paradox (LP) is the simplest, and one of the oldest, of the <a href=\"/r/discussion/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">paraconsistent logics</a>. Instead of&nbsp;assigning&nbsp;truths to statements A, it instead uses relationships binary relationships v(A,1) and v(A,0).&nbsp;\"A is true\" is encoded by v(A,1); \"A is false\" is similarly encoded by v(A,0). Each statement A is required to be true or false, but it can be both (in which case we could say that \"A is undetermined\").&nbsp;\"A is strictly true\" means v(A,1) but not v(A,0); strict falsity is the converse.</p>\n<p>The usual symbols &rarr;, &or;, &and; and &not;, retain their standard meanings, and a compound statement takes all possible values it could take, seeing all the possible values its components could take. So, for instance if A is (strictly) true, then &not;A is (strictly) false. If A is undetermined, then &not;A is undetermined. If A is undetermined and B is strictly false, then A &or; B is undetermined and A &and; B is strictly false - though if B were strictly true, then A &or; B would be strictly true and A &and; B undetermined.</p>\n<p>These properties make LP quite easy to work with, and one can determine the truths of many statements using truth tables. In fact, it can be seen that every tautology of classical logic is a tautology of LP. This derives from the fact that tautologies are true regardless of the truth values of their components; hence they remain true in LP whether we take undertermined statements to be true or false. Consequently, all of the following are true in LP:</p>\n<ol>\n<li>A&nbsp; &rarr; (B&nbsp;&rarr; A)</li>\n<li>(A&nbsp;&and; &not;A)&nbsp; &rarr; B</li>\n<li>((A&nbsp;&or; B)&nbsp; &and; &not;A) &rarr; B</li>\n<li>A&nbsp;&and; (A &rarr; B)&nbsp; &rarr; B</li>\n</ol>\n<p>But wait a second. Isn't the second line a statement of the <a href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">principle of explosion</a> - the fact that we can derive anything from a contradiction? Indeed it is. LP can state the principle of explosion as a (true) theorem - but it can't actually use it as a rule of deduction. Similarly, the third line is a statement of the disjunctive syllogism - a true theorem, but not a valid rule of deduction. That is easy to see: let A be undetermined, and B strictly false. Then A &or; B is true, and so is &not;A - and yet we cannot deduce that B is true from this information.</p>\n<p>So LP can accept contradictions without blowing up, has all the tautologies of classical logic, but lacks some of the rules of inference.&nbsp;\"Some\" of the rules of inference? LP even lacks modus ponens! As before, let A be undetermined and B strictly false; then A and (A &rarr; B) are both true, but B is not.</p>\n<p>So while LP is a pleasant logic to play with, it isn't particularly useful. Another weakness is that is still defines the material conditional (A&nbsp;&rarr; B)&nbsp;as (&not;A&nbsp;&or;&nbsp;B): false statements still imply anything, and we haven't solved the L&ouml;bian problem for UDT. In the next post, I'll look at relevance logics, which have a more restricted use of&nbsp;&rarr;, and do allow modus ponens.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rwysnoyCxMLqQZnCY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 8.784569706911963e-07, "legacy": true, "legacyId": "14865", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oeMYMKnALbniFaoQL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T15:21:32.715Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-6", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.385Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MBEbKbdX9pSt3Pjgb/meetup-austin-tx-6", "pageUrlRelative": "/posts/MBEbKbdX9pSt3Pjgb/meetup-austin-tx-6", "linkUrl": "https://www.lesswrong.com/posts/MBEbKbdX9pSt3Pjgb/meetup-austin-tx-6", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMBEbKbdX9pSt3Pjgb%2Fmeetup-austin-tx-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMBEbKbdX9pSt3Pjgb%2Fmeetup-austin-tx-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMBEbKbdX9pSt3Pjgb%2Fmeetup-austin-tx-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8n'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 April 2012 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78712</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome back! The Austin meetup continues, and I hope to try out some of the proposed <a href=\"http://lesswrong.com/lw/bc3/sotw_be_specific/\">Be Specific</a> exercises. We sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8n'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MBEbKbdX9pSt3Pjgb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.785869212674877e-07, "legacy": true, "legacyId": "14870", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/8n\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 April 2012 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78712</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome back! The Austin meetup continues, and I hope to try out some of the proposed <a href=\"http://lesswrong.com/lw/bc3/sotw_be_specific/\">Be Specific</a> exercises. We sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/8n\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T18:36:05.380Z", "modifiedAt": null, "url": null, "title": "File Under \"Keep Your Identity Small\"", "slug": "file-under-keep-your-identity-small", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9STuttH45AehMx3hm/file-under-keep-your-identity-small", "pageUrlRelative": "/posts/9STuttH45AehMx3hm/file-under-keep-your-identity-small", "linkUrl": "https://www.lesswrong.com/posts/9STuttH45AehMx3hm/file-under-keep-your-identity-small", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20File%20Under%20%22Keep%20Your%20Identity%20Small%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFile%20Under%20%22Keep%20Your%20Identity%20Small%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9STuttH45AehMx3hm%2Ffile-under-keep-your-identity-small%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=File%20Under%20%22Keep%20Your%20Identity%20Small%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9STuttH45AehMx3hm%2Ffile-under-keep-your-identity-small", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9STuttH45AehMx3hm%2Ffile-under-keep-your-identity-small", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p>We know politics makes us stupid, but now there's <a href=\"http://www.sitemaker.umich.edu/eob/files/obrienellsworth2012b.pdf%E2%80%9D.pdf\">evidence</a>&nbsp;(pdf) that politics makes us less likely to consider things from another's point of view. From the abstract:</p>\n<blockquote>\n<p>Replicating prior research, we found that participants who were outside during winter overestimated the extent to which other people were bothered by cold (Study 1), and participants who ate salty snacks without water thought other people were overly bothered by thirst (Study 2). However, in both studies, this effect evaporated when participants believed that the other people under consideration held opposing political views from their own.&nbsp;Participants who judged these dissimilar others were unaffected by their own strong visceral-drive states, a finding that highlights the power of dissimilarity in social judgment. Dissimilarity may thus represent a boundary condition for embodied cognition and inhibit an empathic understanding of shared out-group pain.</p>\n</blockquote>\n<p>As Will Wilkinson <a href=\"http://bigthink.com/ideas/politics-vs-empathy?page=all\">notes</a>:</p>\n<blockquote>\n<p>Got that? We overestimate the extent to which others feel what we're feeling, unless they're on another team.</p>\n</blockquote>\n<p>Now this isn't necessarily a negative effect - you might argue that it's bias <em>correcting</em>. But implicitly viewing <em>them</em>&nbsp;as so different that it's not even worth thinking about things from their perspective is scary in itself.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9STuttH45AehMx3hm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 8.786682050578097e-07, "legacy": true, "legacyId": "14871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-05T22:28:00.382Z", "modifiedAt": null, "url": null, "title": "Meetup : First Dorset UK Meetup", "slug": "meetup-first-dorset-uk-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.184Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MayDaniel", "createdAt": "2010-12-30T01:36:27.927Z", "isAdmin": false, "displayName": "danielmamay"}, "userId": "z5ZgxLzThZQ9t4bD9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zajFjWT8TAachXs8w/meetup-first-dorset-uk-meetup", "pageUrlRelative": "/posts/zajFjWT8TAachXs8w/meetup-first-dorset-uk-meetup", "linkUrl": "https://www.lesswrong.com/posts/zajFjWT8TAachXs8w/meetup-first-dorset-uk-meetup", "postedAtFormatted": "Thursday, April 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Dorset%20UK%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Dorset%20UK%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzajFjWT8TAachXs8w%2Fmeetup-first-dorset-uk-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Dorset%20UK%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzajFjWT8TAachXs8w%2Fmeetup-first-dorset-uk-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzajFjWT8TAachXs8w%2Fmeetup-first-dorset-uk-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8o'>First Dorset UK Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 April 2012 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">8A Cornhill, Town Centre, Dorchester DT1 1BA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If you're in or around the area, come along and bring friends.</p>\n\n<p>Let me know if you're interested and can't make the date, as we can probably reschedule to a time all of us can make.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8o'>First Dorset UK Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zajFjWT8TAachXs8w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.787651209851845e-07, "legacy": true, "legacyId": "14872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Dorset_UK_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8o\">First Dorset UK Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 April 2012 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">8A Cornhill, Town Centre, Dorchester DT1 1BA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If you're in or around the area, come along and bring friends.</p>\n\n<p>Let me know if you're interested and can't make the date, as we can probably reschedule to a time all of us can make.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Dorset_UK_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8o\">First Dorset UK Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Dorset UK Meetup", "anchor": "Discussion_article_for_the_meetup___First_Dorset_UK_Meetup", "level": 1}, {"title": "Discussion article for the meetup : First Dorset UK Meetup", "anchor": "Discussion_article_for_the_meetup___First_Dorset_UK_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T03:57:08.683Z", "modifiedAt": null, "url": null, "title": "Forked Russian Roulette and Anticipation of Survival", "slug": "forked-russian-roulette-and-anticipation-of-survival", "viewCount": null, "lastCommentedAt": "2018-06-18T18:06:35.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FeepingCreature", "createdAt": "2010-07-26T04:00:23.863Z", "isAdmin": false, "displayName": "FeepingCreature"}, "userId": "XZeHMPK6hNC7uTKxM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TDFNESpg69J7oLZHa/forked-russian-roulette-and-anticipation-of-survival", "pageUrlRelative": "/posts/TDFNESpg69J7oLZHa/forked-russian-roulette-and-anticipation-of-survival", "linkUrl": "https://www.lesswrong.com/posts/TDFNESpg69J7oLZHa/forked-russian-roulette-and-anticipation-of-survival", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Forked%20Russian%20Roulette%20and%20Anticipation%20of%20Survival&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AForked%20Russian%20Roulette%20and%20Anticipation%20of%20Survival%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDFNESpg69J7oLZHa%2Fforked-russian-roulette-and-anticipation-of-survival%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Forked%20Russian%20Roulette%20and%20Anticipation%20of%20Survival%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDFNESpg69J7oLZHa%2Fforked-russian-roulette-and-anticipation-of-survival", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDFNESpg69J7oLZHa%2Fforked-russian-roulette-and-anticipation-of-survival", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>This is a thought exercise I came up with on IRC to help with the iffiness of \"freezing yourself for a thousand years\" with regards to continuity of self.</p>\n<p>Let's say we live in a post-singularity world as uploads and are pretty bored and always up for terrible entertainment (our god is FAI but has a scary sense of humor ..). So some crazy person creates a very peculiar black cube in our shared reality. You walk into it, a fork of you is created and you duke it out via russian roulette. The winner walks out the other side.</p>\n<p>Before entering, should you accurately anticipate dying with 50% probability?</p>\n<p>I argued that you should anticipate surviving with 100% probability, since the single you that walked out of the box would turn out to be correct in his prediction. Surprisingly, someone disagreed.</p>\n<p>So I extended the scenario by another black box with two doors, but this one is just a tunnel. In this case, everybody can agree that you should anticipate a 100% probability of surviving it unscathed. But if we delete our memory of what just happened when exiting the black boxes, and the boxes themselves, then the resulting universes would be indistinguishable!</p>\n<p>One easy way to demonstrate this is to chain ten boxes and put a thousand dollars at the end. The person that anticipates dying with 50% probability (so over all the boxes, 1/1024 chance of surviving) would stay well outside. The person that anticipates surviving just walks through and comes away $1000 richer. \"But at least my anticipation was correct\", in this scenario, reminds me somewhat of the cries of \"but at least my reasoning was correct\" on the part of two-boxers.</p>\n<p>What I'm wondering is: is there a general rule underlying this about the follies of allowing causally-indistinguishable-in-retrospect effects to differently affect our anticipation? Can somebody formalize this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TDFNESpg69J7oLZHa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 8.789026971430728e-07, "legacy": true, "legacyId": "14890", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T04:40:48.798Z", "modifiedAt": null, "url": null, "title": "Meetup : Monday Madison Meetup", "slug": "meetup-monday-madison-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YSJDexm93Gd2LZqPb/meetup-monday-madison-meetup-1", "pageUrlRelative": "/posts/YSJDexm93Gd2LZqPb/meetup-monday-madison-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/YSJDexm93Gd2LZqPb/meetup-monday-madison-meetup-1", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monday%20Madison%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monday%20Madison%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSJDexm93Gd2LZqPb%2Fmeetup-monday-madison-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monday%20Madison%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSJDexm93Gd2LZqPb%2Fmeetup-monday-madison-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSJDexm93Gd2LZqPb%2Fmeetup-monday-madison-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8p'>Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 April 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 monroe st., madison wi</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I plan to run a couple of nascent exercises on explicit specificness, a la <a href=\"http://lesswrong.com/lw/bc3/sotw_be_specific/.\" rel=\"nofollow\">http://lesswrong.com/lw/bc3/sotw_be_specific/.</a> This seems like a deeply useful skill to turn into a habit.</p>\n\n<p>Further suggestions, of course, are warmly welcomed. :)</p>\n\n<p>See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8p'>Monday Madison Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YSJDexm93Gd2LZqPb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.789209530006943e-07, "legacy": true, "legacyId": "14892", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8p\">Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 April 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 monroe st., madison wi</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I plan to run a couple of nascent exercises on explicit specificness, a la <a href=\"http://lesswrong.com/lw/bc3/sotw_be_specific/.\" rel=\"nofollow\">http://lesswrong.com/lw/bc3/sotw_be_specific/.</a> This seems like a deeply useful skill to turn into a habit.</p>\n\n<p>Further suggestions, of course, are warmly welcomed. :)</p>\n\n<p>See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8p\">Monday Madison Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T04:58:25.173Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Feynman Paths", "slug": "seq-rerun-feynman-paths", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:59.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xRycLWJBcRSkSnXxa/seq-rerun-feynman-paths", "pageUrlRelative": "/posts/xRycLWJBcRSkSnXxa/seq-rerun-feynman-paths", "linkUrl": "https://www.lesswrong.com/posts/xRycLWJBcRSkSnXxa/seq-rerun-feynman-paths", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Feynman%20Paths&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Feynman%20Paths%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxRycLWJBcRSkSnXxa%2Fseq-rerun-feynman-paths%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Feynman%20Paths%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxRycLWJBcRSkSnXxa%2Fseq-rerun-feynman-paths", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxRycLWJBcRSkSnXxa%2Fseq-rerun-feynman-paths", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>Today's post, <a href=\"/lw/pk/feynman_paths/\">Feynman Paths</a> was originally published on 17 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Instead of thinking that a photon takes a single straight path through space, we can regard it as taking all possible paths through space, and adding the amplitudes for every possible path. Nearly all the paths cancel out - unless we do clever quantum things, so that some paths add instead of canceling out. Then we can make light do funny tricks for us, like reflecting off a mirror in such a way that the angle of incidence doesn't equal the angle of reflection. But ordinarily, nearly all the paths except an extremely narrow band, cancel out - this is one of the keys to recovering the hallucination of classical physics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bgg/seq_rerun_the_quantum_arena/\">The Quantum Arena</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xRycLWJBcRSkSnXxa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.789283135707474e-07, "legacy": true, "legacyId": "14893", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oiu7YhzrDTvCxMhdS", "Q8zFxc73rLvoWgen3", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T05:20:01.130Z", "modifiedAt": null, "url": null, "title": "A Poisson process paradox", "slug": "a-poisson-process-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.986Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nerfhammer", "createdAt": "2009-07-21T19:45:50.831Z", "isAdmin": false, "displayName": "nerfhammer"}, "userId": "TnRcc3ezfxzQs7Phn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/is9c3r67FbfqaFCoh/a-poisson-process-paradox", "pageUrlRelative": "/posts/is9c3r67FbfqaFCoh/a-poisson-process-paradox", "linkUrl": "https://www.lesswrong.com/posts/is9c3r67FbfqaFCoh/a-poisson-process-paradox", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Poisson%20process%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Poisson%20process%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fis9c3r67FbfqaFCoh%2Fa-poisson-process-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Poisson%20process%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fis9c3r67FbfqaFCoh%2Fa-poisson-process-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fis9c3r67FbfqaFCoh%2Fa-poisson-process-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<p>From Steven Pinker's latest book:</p>\n<blockquote>\n<p>Suppose you live in a place that has a constant chance of being struck by lightning at any time throughout the year. Suppose that the strikes are random: every day the chance of a strike is the same, and the rate works out to one strike a month. Your house is hit by lightning today, Monday. What is the most likely day for the <em>next</em> bolt to strike your house?</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<blockquote>\n<p>The answer is &ldquo;tomorrow,&rdquo; Tuesday. That probability, to be sure, is not very high; let&rsquo;s approximate it at 0.03 (about once a month). Now think about the chance that the next strike will be the day after tomorrow, Wednesday. For that to happen, two things have to take place. First lightning has to strike on Wednesday, a probability of 0.03. Second, lightning <em>can&rsquo;t have struck on Tuesday</em>, or else Tuesday would have been the day of the next strike, not Wednesday. To calculate that probability, you have to multiply the chance that lightning will not strike on Tuesday (0.97, or 1 minus 0.03) by the chance that lightning will strike on Wednesday (0.03), which is 0.0291, a bit lower than Tuesday&rsquo;s chances. What about Thursday? For that to be the day, lightning can&rsquo;t have struck on Tuesday (0.97) or on Wednesday either (0.97 again) but it must strike on Thursday, so the chances are 0.97&times;0.97&times;0.03, which is 0.0282. What about Friday? It&rsquo;s 0.97&times;0.97&times;0.97&times;0.03, or 0.274. With each day, the odds go down (0.0300 &hellip; 0.0291 &hellip; 0.0282 &hellip; 0.0274), because for a given day to be the next day that lightning strikes, all the previous days have to have been strike-free, and the more of these days there are, the lower the chances are that the streak will continue. To be exact, the probability goes down exponentially, accelerating at an accelerating rate. The chance that the next strike will be thirty days from today is 0.97^29x0.03, barely more than 1 percent.</p>\n<p>Almost no one gets this right. I gave the question to a hundred Internet users, with the word <em>next</em> italicized so they couldn&rsquo;t miss it. Sixty-seven picked the option &ldquo;every day has the same chance.&rdquo; But that answer, though intuitively compelling, is wrong. If every day were equally likely to be the next one, then a day a thousand years from now would be just as likely as a day a month from now. That would mean that the house would be just as likely to go a thousand years without a strike as to suffer one next month. Of the remaining respondents, nineteen thought that the most likely day was a month from today. Only five of the hundred correctly guessed &ldquo;tomorrow.&rdquo;</p>\n</blockquote>\n<div>I wonder if there's a name for this problem. He doesn't cite any prior art.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 1, "bh7uxTTqmsQ8jZJdB": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "is9c3r67FbfqaFCoh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 29, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "14894", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T06:34:18.932Z", "modifiedAt": null, "url": null, "title": "Looking for reductionism help", "slug": "looking-for-reductionism-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F8fyxDPrcKgRosg5S/looking-for-reductionism-help", "pageUrlRelative": "/posts/F8fyxDPrcKgRosg5S/looking-for-reductionism-help", "linkUrl": "https://www.lesswrong.com/posts/F8fyxDPrcKgRosg5S/looking-for-reductionism-help", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20reductionism%20help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20reductionism%20help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF8fyxDPrcKgRosg5S%2Flooking-for-reductionism-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20reductionism%20help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF8fyxDPrcKgRosg5S%2Flooking-for-reductionism-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF8fyxDPrcKgRosg5S%2Flooking-for-reductionism-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p>I have read the sequences on reductionism and quantum physics some time ago now and I was hoping for some help finding the right places to go back and re-read there to address a question. If the way I describe my question reveals other ignorance on my part, please feel free to add comments above and beyond sequence references.</p>\n<p>When trying to talk a little about reductionism, most (non-LW) people I speak to seem to want to play the following game: <br /><br />What's an airplane made out of? <em>Molecules and atoms that comprise materials like metal, plastic, glass, rubber, etc.</em>&nbsp;What are molecules and atoms made out of? <em>Well, molecules are collections of atoms bonded together, and atoms are made up of three basic particles: protons, neutrons, and electrons.</em>&nbsp;What are basic particles made out of? <em>Well, here things start to get a little more dicey. Some of the basic particles are known to be made up of even smaller sub-atomic elementary particles, such as quarks, leptons, and bosons. Some of the basic particles are examples of these elementary particles. </em>Well, what's an elementary particle made of? <em>Well, that's a pretty tough one, but basically there's this sort of fabric of stuff underlying everything called quantum amplitude, and a certain configuration of quantum amplitude corresponds to an elementary particle. </em>So what's quantum amplitude made up of? <em>Well, I'm not sure that is a coherent question. It just sort of is.</em>&nbsp;A ha! I've caught you. So ultimately way down at the bottom of it all, you're telling me that some something \"just exists\" (i.e. is ontologically basic). But then why do you call it reductionism if it ultimately boils down to a Platonistic ideal of quantum amplitude (no one actually says this, but it's my translation of the objections I tend to face).<br /><br />Is it more or less right to say that, as far as we can tell, the only reasonable thing to which we can attribute ontologically basic status is quantum amplitude? Given that amplitude is a mathematical device that allows calculation of probability, and probability describes my ignorance about the world (i.e. my best guess as to what the territory is, as opposed to the actual territory), do we view quantum amplitude as some sort of pre-states-of-knowledge concept? How does that mesh up with \"what is an elementary particle made of?\" It makes me want to call it \"the residue of intrinsic uncertainty\" or something, but why would that \"really exist?\" I don't think that my uncertainty about tomorrow's weather \"really exists\" in any Platonistic way.<br /><br />How do you explain to people that reductionism = (reduce until you have good reason not to); am I even right to say that or is this a harmful oversimplification?<br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F8fyxDPrcKgRosg5S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 8.789684061611981e-07, "legacy": true, "legacyId": "14906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T07:18:10.770Z", "modifiedAt": null, "url": null, "title": "SMBC comic: poorly programmed average-utility-maximizing AI", "slug": "smbc-comic-poorly-programmed-average-utility-maximizing-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:02.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/etAJHCL7GwPFTFRAv/smbc-comic-poorly-programmed-average-utility-maximizing-ai", "pageUrlRelative": "/posts/etAJHCL7GwPFTFRAv/smbc-comic-poorly-programmed-average-utility-maximizing-ai", "linkUrl": "https://www.lesswrong.com/posts/etAJHCL7GwPFTFRAv/smbc-comic-poorly-programmed-average-utility-maximizing-ai", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SMBC%20comic%3A%20poorly%20programmed%20average-utility-maximizing%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASMBC%20comic%3A%20poorly%20programmed%20average-utility-maximizing%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FetAJHCL7GwPFTFRAv%2Fsmbc-comic-poorly-programmed-average-utility-maximizing-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SMBC%20comic%3A%20poorly%20programmed%20average-utility-maximizing%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FetAJHCL7GwPFTFRAv%2Fsmbc-comic-poorly-programmed-average-utility-maximizing-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FetAJHCL7GwPFTFRAv%2Fsmbc-comic-poorly-programmed-average-utility-maximizing-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p>I laughed: <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2569\">SMBC comic.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "etAJHCL7GwPFTFRAv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 12, "extendedScore": null, "score": 8.789867461061082e-07, "legacy": true, "legacyId": "14911", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T12:25:27.082Z", "modifiedAt": null, "url": null, "title": "Peter Singer and Tyler Cowen transcript", "slug": "peter-singer-and-tyler-cowen-transcript", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WHJkPQ8jeCW3FaQGx/peter-singer-and-tyler-cowen-transcript", "pageUrlRelative": "/posts/WHJkPQ8jeCW3FaQGx/peter-singer-and-tyler-cowen-transcript", "linkUrl": "https://www.lesswrong.com/posts/WHJkPQ8jeCW3FaQGx/peter-singer-and-tyler-cowen-transcript", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Peter%20Singer%20and%20Tyler%20Cowen%20transcript&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeter%20Singer%20and%20Tyler%20Cowen%20transcript%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHJkPQ8jeCW3FaQGx%2Fpeter-singer-and-tyler-cowen-transcript%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Peter%20Singer%20and%20Tyler%20Cowen%20transcript%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHJkPQ8jeCW3FaQGx%2Fpeter-singer-and-tyler-cowen-transcript", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHJkPQ8jeCW3FaQGx%2Fpeter-singer-and-tyler-cowen-transcript", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9374, "htmlBody": "<p>In March 2009, <a href=\"http://en.wikipedia.org/wiki/Tyler_Cowen\">Tyler Cowen</a> (<a href=\"http://marginalrevolution.com/\">blog</a>) interviewed <a href=\"http://en.wikipedia.org/wiki/Peter_Singer\">Peter Singer</a> about morality, giving, and how we can most improve the world. They are both thinkers I respect a lot, and I was excited to read their debate. Unfortunately the interview was available only as a <a href=\"http://bloggingheads.tv/videos/2022\">video</a>. I wanted a transcript, so I made one:</p>\n<p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>This is Tyler Cowen of George Mason University. I'm doing a BloggingHeads with Peter Singer, the world-famous philosopher from Princeton. This is a forum on Peter's latest book, which he'll start off by telling you a bit about.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Hi. The book's called \"<a href=\"http://en.wikipedia.org/wiki/The_Life_You_Can_Save:_Acting_Now_to_End_World_Poverty\">The Life You Can Save: Acting Now To End World Poverty</a>\". It begins with an argument that I've used many times in articles about a child drowning in a pond, and suggests that if you saw a child drowning in a pond that you would jump in and save that child, and you think that is what you ought to do, even if it meant that you ruined an expensive pair of shoes that you were wearing.\n<p>From there I pull back to saying \"what does this mean about the problem of world poverty, given that there are, according to Unicef, ten million children dying of avoidable poverty-related causes every year?\" We could save some of them, and probably it wouldn't cost us much more than the cost of an expensive pair of shoes if we find an effective aid agency that is doing something to combat the causes of world poverty, or perhaps to combat the deaths of children from simple conditions like diarrhea or measles, conditions that are not that hard to prevent or to cure. We could probably save a life for the cost of a pair of shoes. So why don't we? What's the problem here? Why do we think it's ok to live a comfortable, even luxurious, life while children are dying? In the book I explore various objections to that view, I don't find any of them really convincing. I look at some of the psychological barriers to giving, and I acknowledge that they are problems. And I consider also some of the objections to aid and questions raised by economists as to whether aid really works. In the end I come to a proposal by which I want to change the culture of giving.</p>\n<p>The aim of the book in a sense is to get us to internalize the view that not to do anything for those living in poverty, when we are living in luxury and abundance, is ethically wrong, that it's not just not a nice thing to do but that a part of living an ethically decent life is at least to do something significant for the poor. The book ends with a chapter in which I propose a realistic standard, which I think most people in the affluent world could meet without great hardship. It involves giving 1% of your income if you're in the bottom 90% of US taxpayers, scaling up through 5% and 10% and even more as you get into the top 10%, the top 5%, the top 1% of US taxpayers. But at no point is the scale I'm proposing what I believe is an excessively burdensome one. I've set up a website, <a href=\"http://thelifeyoucansave.com\">thelifeyoucansave.com</a> that people can go to in order to publicly pledge that they will meet this scale, because I think if people will do it publicly, that in itself will encourage other people to do it and, hopefully, the idea will spread.</p>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Thank you, Peter. Let me first stress: I agree with most of what's in your book; I think we all could give more and should give more. It would be good for other people and it would be good for ourselves. But let me start off the dialogue by mentioning a few points where I don't completely agree with you. One thing that struck me about the book was some of the omissions.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Immigration as an Anti-Poverty Program</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>For instance, in my view, what is by far the best anti-poverty program, the only one that's really been shown to work, and that's what's called \"immigration\". I don't even see the word \"immigration\" in your book's index. So why don't we spend a lot more resources allowing immigration, supporting immigration, lobbying for immigration? This raises people's incomes very dramatically, it's sustainable, for the most part it's also good for us. Why not make that the centerpiece of an anti-poverty platform?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's an interesting point, Tyler. I suppose, one question I'd like to ask is: is it sustainable? Isn't it the case that if we take, as immigrants, the people who are the most enterprising, perhaps, of the poor countries that we're still going to leave those countries in poverty, and their populations may continue to rise, and eventually, even if we keep taking immigrants, we will reach a capacity where we're starting to strain our own country?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>There's two separate issues: one is \"brain drain\" from the third world. I think here's a lot of research by [Michael Clemens], showing that it's not a problem, that third world countries that have even somewhat functional institutions tend to benefit by sending people to other countries. India's a good example: a lot of Indians return to India and start businesses, or they send money back home. Mexico is another example. Maybe North Korea is somewhat different, but for the most part immigration seems to benefit both countries.\n<p>I don't think we could have open borders; I don't think we could have unlimited immigration, but we're both sitting here in the United States and it hardly seems to me that we're at the breaking point. Immigrants would benefit much more: their wages would rise by a factor of twenty or more, and there would be perhaps some costs to us, but in a cost-benefit sense it seems far, far more effective than sending them money. Do you agree?</p>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I must admit that I haven't thought a lot about immigration as a way of dealing with world poverty. Obviously, from what you're saying, I should be thinking more about it, but I can't really say whether I agree until I have thought more about it.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Changing Institutions: Greater Tax Break for True Charity</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try another question along related lines. I think one general way in which I think about your book differently than you do, is that you think more about giving. I'm a big advocate of giving, I've written a whole <a href=\"http://www.amazon.com/Good-Plenty-Creative-Successes-American/dp/0691146268\">book</a> myself on philanthropy, but I think somewhat more in terms of changing institutions. So another thing we might consider doing, along the lines of what you advocate, is to increase the tax benefits of giving. Right now, if you're itemizing deductions and you give $1, you deduct $1 from your taxes. But it wouldn't be very difficult to make it the case that for certain kinds of giving you could deduct $1.10 from your taxes or $1.20. Would you favor this kind of reform?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I might favor that, if giving were defined more narrowly than we do, in the US anyway, because I know I can deduct $1 from my taxes whether I give to <a href=\"http://www.oxfamamerica.org/\">Oxfam America</a>, which I think is an effective organization fighting world poverty, or if I give to the <a href=\"http://www.metmuseum.org/\">Met</a> so they can buy yet another painting to add to the already super-abundant collection of paintings they have. I don't see why the taxpayer should subsidize me if I decide I want to give to the Met but sure, if I'm giving to Oxfam I think that would be good.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>So, in other words, you favor a kind of tax cut as a way to help the world's poor. That, in this country, if targeted properly, tax policy, in essence cutting the taxes of rich people, is one of the very best ways to help the world's poor. Would you sign on to that?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I'm not quite sure why it is ... you seem to have leapt a little from what I was saying and I haven't followed the leap as to why cutting taxes for the rich would be one of the most effective ways of helping the poor. Can you explain that a little more?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If we give a greater tax break to charitable donations, and here I mean only true charity, not say a fancy art museum, disproportionately this will benefit wealthy people. Wealthy people have a lot of money. In essence you're cutting their taxes. They're giving more, they may not have a higher level of consumption, but would you be willing to raise your hand and say \"I, Peter Singer, think that cutting taxes on the US wealthy is in fact one of the very best things we could do for the world's poor, if we do it the right way\"? Yes or no?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Yes, if the tax break only goes to those of the wealthy who are giving to organizations that are effectively helping the poor, I'll raise my hand to that.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>OK; I'm glad to hear that.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Millennium Villages Skepticism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me focus on another point of difference between us in the book. I think when it comes to the effectiveness of aid, I'm not a total skeptic on the effectiveness of aid but I think I'm more skeptical than you are. In a number of places you site the work of <a href=\"http://en.wikipedia.org/wiki/Jeffrey_Sachs\">Jeffrey Sachs</a>. Now my view of Sachs is that his projects are actually doing individual human beings a lot of good but the return on investment I don't think is that high. I think he's improving the health of a lot of people but I don't think he's going to raise any villages, much less countries or continents, out of poverty. Given that my view is that the rate of return on this investment is much lower, and I think that the economics profession as a whole agrees with me, not with Sachs, this to me suggests that to really make a dent in world poverty we would have to give much more than 5% of our incomes, even more than 10%, that we're simply at a point where we can do some good, but that to abolish poverty we would have to engage in a very dramatic redistribution. What's your view on this?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Firstly, I think, as for whether Sachs is really going to succeed in raising villages out of poverty, I think the data isn't in yet. The <a href=\"http://en.wikipedia.org/wiki/Millennium_Villages_Project\">Millennium Villages project</a> which he's working on has only been going a few years, I think we need to give it maybe another five years to see whether it's working. That's more or less what he's said. He hopes that the aid will be short term, that the villages will become self-sustaining, the improvements will last, they'll be out of the poverty trap. If that hasn't happened at the end of another five years I'm going to agree with you that we're going to need more, but I think it's really too early to call the result on that one.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Take the overall opinion of economists, which is again that Sachs's projects can do good, people in those villages might be better off, but if you're in the middle of, say, a totally corrupt African country which is not democratic, which maybe has been fighting wars, which has an absolutely horrible infrastructure, which has a bureaucracy, a kleptocracy, massive problems, lack of literacy, that maybe you could eliminate infectious diseases or malaria within that village. People will be better off, it's worth doing, but at the end of the day is there really any reason to think, given the last 300 years of thinking and writing on development and economic history that this will at all cure poverty? Doesn't it just mean you'll have poor people without malaria, which is better than poor people with malaria, but they're still essentially poor people?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>If the governments and the situation is as bad as you describe, you're probably right, but of course not all countries ... you describe pretty much a worst case scenario. I think there are a lot of countries where there are poor people which do not have governments which are as bad as you painted. I think in those countries we can hope that people actually will lift themselves out of poverty and I think that's what we need to try to do. Now, you may be right that that's still going to leave poor people in countries that are as bad as you describe, and there is a real question then as to how much we can do to help them, whether giving more will really be enough to help them or because of those governments in those situations there's really nothing much we can do. That will be the dilemma. But I don't think we've got to that point yet because we've not really worked out what we can do for people in the countries where the governments aren't so bad.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Chinese Reforms</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I think you and I are both looking for what are the most highly leveraged ways we can reduce poverty in this world.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Uh huh</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If I ask myself, historically, what has been the most successful anti-poverty program in the last century, I look at Communist China, and I would say that the <a href=\"http://en.wikipedia.org/wiki/Chinese_economic_reform\">reforms</a>, starting in the late 1970s, have taken at least 300M-400M people, and probably more, and taken them from extreme poverty, perhaps starvation, to a situation where a lot of them live quite well or at least have some kind of tolerable lower middle class existence. I think that property rights and institutional reforms are they key to fighting poverty. China during that period, the aid it received didn't matter much. It doesn't mean we shouldn't give aid, I'm all for aid, but isn't the big leveraged investment here changing and improving institutions and not giving money?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I do that that's a really important thing when we can do it. The question is, can we do it? Obviously the Chinese reforms that you refer to really were internally driven, I don't think they were a result of things the West did, unless you talk about the entire global economic system, which China clearly wanted to participate in. So the question is how can we be effective in producing those sorts of changes? In some countries we can come in and help, say countries recovering from civil war, and give some help in establishing good institutions, but I'm not sure what ideas you have about what's a good way to bring about that kind of reform in these countries that will lead everywhere to the sorts of benefits that you refer to in China.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>In countries like China in a way it's internally driven. It's not that anyone successfully pressured them, but in another way I think it's highly externally driven, that the Chinese, Taiwanese, Koreans, other countries followed the example of Japan They saw that Japan worked. They saw that an Asian country could rise to moderate wealth or even riches and at some point they decided to copy this in their own way. If we look at Japan, Japan copied the west, so maybe one of the very best most important things we can do is just ourselves be a beacon of progress: be humane, be tolerant, respect others, be wealthy and just show that it's possible. We shouldn't think of that as a substitute for aid, but maybe that's actually our number one priority. Does that make sense to you?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That makes sense. I don't know that we have to strive to be more wealthy than we are--well, maybe just right at this moment we need to strive to get back to being as wealthy as we were a year ago perhaps. But I think we are setting that example, undoubtedly. We are showing countries what can be done with reasonably good government, open economies, and I do hope that other countries will follow that. But maybe not all countries can do it. I think that <a href=\"http://en.wikipedia.org/wiki/Paul_Collier\">Paul Collier</a> argues in <a href=\"http://en.wikipedia.org/wiki/The_Bottom_Billion\">his book</a> that it's going to be difficult for some African countries to get into this game now. There are reasons why it's going to be hard for them to compete with countries that have established positions, have developed markets, have low labor costs. It's not clear to me that this is going to be a path that every poor country can follow.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Military Intervention</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>You mentioned Paul Collier. I found his book very interesting. One argument he makes--I would say I'm not, myself, convinced but I'm curious to hear what you think--is that we could do the world a great deal of good by selective military interventions. So take <a href=\"http://en.wikipedia.org/wiki/War_in_Darfur\">the case of Darfur</a>. A large number of people are suffering, dying. Collier says, or implies, or at least opens the possibility, that we, the United States, the UN, whoever, should just move in and in military terms do something about this. It is again a topic that is not prominent in your book, but it seems that if it can work it's highly leveraged, more leveraged than giving away money. I'm curious as to your views on that.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I did discuss humanitarian intervention in my earlier book <a href=\"http://books.google.com/books/about/One_world.html?id=tk9Oo_SleYwC\">One World</a> and I do support it under the right circumstances. I think, though, we do have to be pretty clear about defining it properly and trying to get support for it. Maybe it would work in Darfur. I think Darfur is quite a large area, relatively thinly populated, and it might take a lot of resources to really protect the people in Darfur. There are underlying issues, too, perhaps about climate change, even, that are causing scarcity in Darfur. But isn't possible, I mean I think that Zimbabwe would be another possibility, though maybe just now with changes in the political system you wouldn't want to do it just now, you'd want to see how that played out for a while. But certainly a year ago you might well have thought that if the South Africans could be persuaded to move in and remove Mugabe that would be a good thing to do. That would have been better, I think, than having a white former colonial power come in, that obviously would have evoked a lot of echos of returning to a past that Zimbabweans don't want. But I'm not, in principle opposed to military intervention, I just think we have to be very very careful about the circumstances in which we do it, because obviously it can trigger a lot of violence and bloodshed and produce results that are the opposite of what you and I would both want.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Colonialism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Do you think the end of colonialism was a good thing or a bad thing for Africa?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's a really difficult question. I think, clearly, there were lots of bad things about colonialism, but you would have to say that some countries were definitely better administered and that some people's lives, although they may have had some sort of humiliation, perhaps through not being independent, being ruled by people of a different race, in some ways they were better. It's hard, really, to draw that balance sheet. Independence has certainly not been the unmitigated blessing that people thought it would be at the time.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's say we have the premise, that with colonialism there would not have been wars between African nations. It's not the case that a British ruled colony would have attacked a French colony, for instance. It's highly unlikely. So given just that millions have perished from wars alone, wouldn't the Utilitarian view, if you're going to take one, suggest that colonialism was essentially a good idea for Africa, it was a shame that we got rid of it, and that the continent would have been better off under foreign rule, European foreign rule.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I don't think we can be so sure that it would have continued to be peaceful. After all we did have militant resistance movements, we had the <a href=\"http://en.wikipedia.org/wiki/Mau_Mau_Uprising\">Mau Mau</a> in Kenya, for example. We had other militant resistance movements. It may simply have been that the fact of white rule would have provoked not one colony going to war against another but civil war within some of those countries. If what you're asking is would colonialism, had it been accepted by the people there, without military conflict, would that have been better than some of the consequences we've had in some of these countries, you would have to say undoubtedly yes. But we can't go back and wind back the clock and say \"how would it have been if\" because we don't really know whether that relative stability and peace would have lasted.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If we compare the Mau Mau, say, to the wars in Kenya and Rwanda, it seems unlikely that rebellions against colonial governments would have reached that scope, especially if England, France, other countries, would have been willing to spend more money to create some tolerable form of order. My guess is you would have had a fair number of rebellions but it's highly highly unlikely it would compare to the kind of virtual holocausts we've had in Africa as it stands.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Aid without stable government</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I certainly agree that if you look at what's been happening in the Congo, just as one example, or countries like Sierra Leone or Liberia, yes, you could certainly think that it might have been better for those countries.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Would you say that Zimbabwe is one example of a country where just giving it money through aid is unlikely to work?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>At present, unless the government changes quite dramatically. Again, as you were saying before, there might be specific things we can do: we may be able to help particular people who have disease or are hungry, but I agree, in the present conditions it's unlikely to lift people out of poverty on any kind of large scale.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's take a country like Madagascar, which as recently as two or three years ago was touted by the Bush administration, and I don't just mean Republicans, it was touted by many people, as being a kind of model for Africa. Here's a country were we could give a lot of aid, the aid would go to some good purpose, we're making progress, and now Madagascar seems to be in the midst of a <a href=\"http://news.bbc.co.uk/2/hi/7937264.stm\">civil war</a> and the polity is collapsing, the economy is doing very poorly. How many countries in Africa do you think are there where aid works? Where do you draw the line? What in your opinion is the marginal country that is hopeless?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Look, I haven't got a list of African countries like that, I must admit. I think there are some countries where things seem to work, and that's not to say I could name a country and say, well, Mozambique, that aid programs have made a positive difference, or Sierra Leone. Maybe in a month there'll be a coup and you'll be able to tell me that I was wrong. I can't see the future. But there are countries where I think aid has worked, ones where it hasn't worked. I haven't got a rank ordering and I don't have a cutoff line where that is, I'm sorry I'm just not sufficiently expert on African politics and conditions to do that.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Genetically modifying ourselves to be more moral</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's try some philosophical questions. You're a philosopher, and I've been very influenced by your writings on personal obligation. Apart from the practical issue that we can give some money and have it do good, there's a deeper philosophical question of how far those obligations extend, to give money to other people. Is it a nice thing we could do, or are we actually morally required to do so? What I see in your book is a tendency to say something like \"people, whether we like it or not, will be more committed to their own life projects than to giving money to others and we need to work within that constraint\". I think we would both agree with that, but when we get to the deeper human nature, or do you feel it represents a human imperfection? If we could somehow question of \"do we in fact like that fact?\", is that a fact you're comfortable with about human nature? If we could imagine an alternative world, where people were, say, only 30% as committed to their personal projects as are the people we know, say the world is more like, in some ways, an ant colony, people are committed to the greater good of the species. Would that be a positive change in human nature or a negative change?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Of course, if you have the image of an ant colony everyone's going to say \"that's horrible, that's negative\", but I think that's a pejorative image for what you're really asking ...</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>No, no, I don't mean a colony in a negative sense. People would cooperate more, ants aren't very bright, we would do an ant colony much better than the ants do. ...</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>But we'd also be thinking differently, right? What people don't like about ant colonies is ants don't think for themselves. What I would like is a society in which people thought for themselves and voluntarily decided that one of the most satisfying and fulfilling things they could do would be to put more of their effort and more of their energy into helping people elsewhere in need. If that's the question you're asking, then yes, I think it would be a better world if people were readier to make those concerns their own projects.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's say genetic engineering is possible, which is now not so far off on the major scale, and your daughter were having a daughter, and she asked you \"daddy, should I program my daughter so that she's willing to sell her baby and take the money and send it to Haitians to save ten babies in Haiti\". Would you recommend to her \"yes, you should program the genes of your baby so she's that way\"?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>So she's going to sell her baby? What's going to happen to the baby?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>She's going to sell it to some wealth white couple that's infertile, they live in the Pacific Northwest, they'll take fine care of it, she'll receive $1M and save, say, 30 lives in Haiti. You've recommended that your granddaughter be programmed to act this way. Would you recommend that?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>And so she's going to be happy with that? She's not going to suffer as current people would the pangs of separation from their daughter or the agonies of not knowing what's happened to my daughter? She's going to feel perfectly comfortable with that, and she's going to feel good about the fact that she's helped 30 babies in Haiti to have a decent life? Is that the assumption?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>We can do it that way, but keep in mind that even if she's unhappy that's outweighed by the 30 Haitian lives which are saved. Either way you want.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Right, but you're asking me and I'm like normal human beings, I haven't been reprogrammed, so I care about my daughter or my granddaughter, or whoever this is.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Ok, she'll be happy.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Ok, good. Then I think I'm on board with your program.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>So you would want people to be much more cooperative in this way, if we could manage it in some way that won't wreck their psyches.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's right.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Do you think people would have a moral obligation to genetically reprogram themselves, or it would just be a nice thing they could do if they felt so inclined?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think if we really had a system that was as good as you're saying, would lead to as good consequences, and would leave people happy, that's something they ought to do. Because that would really be a way of making a huge difference to the world. They would be wrong not to take advantage of this, given the benefits it involves and the absence, it seems, as described, of any major drawbacks.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Problem areas in Utilitarianism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>What do you think is the biggest problem area in Utilitarian moral theory?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>The biggest problem area? One problem people are talking about that's relevant to what I'm talking about is that Utilitarian moral theory leads to highly demanding consequences that people reject. So that's one problem. The second problem, of course, is that it requires very complex calculations because we don't have a set of simple moral rules that say \"don't do this, do that\". We have to work out what the consequences of our actions are. As in this area we're talking about, what kind of aid is effective, what will overcome world poverty, it's very difficult to work out what the consequences are, and it's sometimes very difficult to know what's the right thing to do.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But you think we nevertheless should do what we think is best, no matter how imperfect that guess may be?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Yeah, I don't really see what else we're supposed to do. It would seem to me to be wrong to say \"because I can't calculate the consequences I'm just going to follow this simple set of rules\". Because I can't calculate the consequences. But why follow this simple set of rules? Where do they come from? I don't believe that we have any god-given rules. I don't think that our moral intuitions are a good source of rules, because that's the product of our evolutionary history, which may not be appropriate for the moment that we're in. So, despite the difficulty, I don't really see what the alternative is, to trying our best to figure out what the expected utility is.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Is Utilitarianism independent?</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me toss up a classic criticism of Utilitarianism. I'm curious to see what you say. The criticism is this, that neither pain nor pleasure is a homogeneous thing. There are many different kinds of pains and pleasures and they're not strictly commensurable in terms of any natural unit. So when we're comparing pain and pleasure that's a fine thing to do, but in fact we're calling upon other values. So Utilitarianism is in this sense parasitic upon some deeper sense of philosophic pluralism, and we're not pure utilitarians at all. But that being the case, why don't we sometimes just allow an intuitive sense of right or wrong to override what would otherwise be the Utilitarian conclusion, since Utilitarianism itself cannot avoid value judgments?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think the form of Utilitarianism that you're describing is <a href=\"http://www.utilitarianism.com/hedutil.htm\">Hedonistic Utilitarianism</a> because you were talking about pleasure and pain and you were suggesting that pleasure is a whole range of different things. The form that I hold is <a href=\"http://en.wikipedia.org/wiki/Preference_utilitarianism\">Preference Utilitarianism</a> which looks at people's preferences and tries to asses the importance of the preference for them. Now this is still not an easy thing to know, in fact in some ways you might say it's harder than getting measures of pleasure and pain, but I think it already embraces the pluralism that you're talking about in terms of people's preferences, people's understanding of what it is they're choosing and why. And so I don't think it's up to us to go back and try to pull in other kinds of values that we intuitively hold over the top of people's preferences. We can do it for ourselves, each of us can say \"what are my preferences\", \"I value this\", \"I value the autonomous life over the happy life, and so that's what I'm going to choose\". Of course, when I weigh out your preferences I should say \"well here we give weight for the preference for an autonomous life and here we give weight to the preference for the pleasant life\" but in making the final judgment, in which we take everyone's preferences into account, it would be wrong for us to just pull out some intuitive values and somehow give them weight in the overall calculation because then we're giving more weight to our preferences than we're giving to those of others.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But doesn't preference utilitarianism itself require some means of aggregation? The means we use for weighing different clashing preferences, can require some kind of value judgments above and beyond Utilitarianism?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I don't quite see why that should be so. While acknowledging the practical differences of actually weighing up and calculating all the preferences, I fail to see why it involves other values apart from the preferences themselves.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Peter Singer: Jewish Moralist</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try giving you my reading of Peter Singer, which is highly speculative, and I'm not even saying it's true, it's just what I think when I read you, especially the later Peter Singer, and I'm just curious to hear your reaction to it. My reading is this: that Peter Singer stands in a long and great tradition of what I would call \"Jewish moralists\" who draw upon Jewish moral teachings in somehow asking for or demanding a better world. Someone who stands in the Jewish moralist tradition can nonetheless be quite a secular thinker, but your later works tend more and more to me to reflect this initial upbringing. You're a kind of secular Talmudic scholar of Utilitarianism, trying to do <a href=\"http://en.wikipedia.org/wiki/Mishna\">Mishna</a> on the classic notion of human well being and bring to the world this kind of idea that we all have obligations to do things that make other people better off, that you're very much out of the classic European, Austrian, Viennese, ultimately Biblical tradition about our obligations to the world. What do you say?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I'm amused, I have to say. I think it's interesting. You're right that I come from a Jewish family. It was a pretty secular Jewish family, so I never got as a child, actually, a lot of Jewish teaching, never went to Jewish Sunday school, I never learned Hebrew, I never had a Bar Mitzvah, I never read the Torah. So if I had got some of that it must have come kind of at a distance through, sort of, osmosis, as you say this vaguely Jewish Viennese culture that certainly was part of my family background but was very much secularized. The interesting thing to speculate is whether I'm doing something that, say, someone out of the British Utilitarian tradition, the tradition of <a href=\"http://en.wikipedia.org/wiki/Jeremy_Bentham\">Bentham</a> and <a href=\"http://en.wikipedia.org/wiki/John_Stuart_Mill\">Mill</a> and <a href=\"http://en.wikipedia.org/wiki/Henry_Sidgwick\">Sidgwick</a> could not have done. What are the distinctive features of my version of Utilitarianism that they would have rejected? And if there is something, it probably is attributable to that background you mention. But I'd be interested in your answer, what do you think that there is in my view that Bentham or Mill or Sidgwick could not have whole-heartedly endorsed?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I'm not sure if there's anything, but I think the mere fact that it is you who is doing it nonetheless reflects something about this. I think of you as one of the worlds greatest theologians, in a way, having this understanding of the quality of mercy, which is put into a secular framework, but what the intuitions really consist of, I think none of us really ever know where our moral intuitions come from.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Ok. Well, look. It's a possible view, as I think you said introducing it, you don't know whether it's true but it's an interesting view of me and where I come from. You've put it out there. I find it hard to look internally, so I'll leave it to others to judge which of the elements of my background they see having formed me most strongly.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>What charities does Peter Singer give to?</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try a personal question but feel free to pass on this one. Let's say someone has read your book and they say \"I'm on board, Peter, please tell me what charities you give to.\" You mentioned Oxfam, but would you have anything specific you'd like to say? And why?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I do support Oxfam substantially, I've got a long relationship with, different Oxfams. They're actually autonomous national groups that work together, so when I first became interested in this issue as a graduate student, way back in Oxford in the '70s, I was living in Oxford and that's the headquarters of the original Oxfam, <a href=\"http://www.oxfam.org.uk/\">Oxfam UK</a>, so I got in touch with them and remain connected with their office. Then I went to Australia and was involved with <a href=\"https://www.oxfam.org.au/\">Oxfam Australia</a>, now I'm involved with <a href=\"http://www.oxfamamerica.org/\">Oxfam America</a>. I like what they do good grassroots work, I've seen some of that, helping the most underprivileged people, plus they're not afraid to be a real advocate for the poor, to <a href=\"https://www.oxfam.org.au/explore/mining/\">tackle</a> big mining companies that are pushing the poor off their land, <a href=\"http://www.oxfam.org/en/policy/food-aid-or-hidden-dumping\">tackle</a> the US government and its <a href=\"http://en.wikipedia.org/wiki/Agricultural_subsidy#United_States\">agricultural subsidies</a>. That's one reason that I like them. But there are many good organizations around. I've recently started supporting <a href=\"http://givewell.org\">GiveWell</a>, you can find them on <a href=\"http://givewell.net\">givewell.net</a>, because they're doing something that I'm sure you would support: they're trying to get aid organizations to demonstrate their efficacy, to be more transparent about why they support some projects rather than others, and to show how much it costs for them to achieve their goals, whether those goals are saving lives or lifting people out of poverty. And so it's kind of at a meta level, saying I want to improve aid by helping organizations that are trying to do that. I think that's a really highly leveraged way of making an impact on what's going to happen in aid over the next couple of decades.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Zero-Overhead Giving</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I'm a big fan of what I call <a href=\"http://marginalrevolution.com/marginalrevolution/2007/08/discover-your-i.html\">zero overhead giving</a>, that is I send monetary transfers to poor people, maybe I've met them on my travels, by Western Union. I don't follow up, I don't monitor, there's no tax deduction, there's no overhead, it's just money from me to them. What do you think of that as a way of giving?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Interesting. I suppose I would like to have some followup. I would worry that I was getting conned. Now, you may have a good sense of who's genuine and who's not, but we all know there's con artists working here in New York city, in other cities in America, who could tell you wonderful stories about how they just need the bus fare home and then they'll be fine, and you give them the bus fare home, and you believe them and then next month they come up to you in the same spot and tell you the same story. So I would like some kind of auditing, but let me just say for people who do want to give direct I think not with zero overhead but I think with 10% overhead, if you go to Kiva, <a href=\"http://kiva.org\">kiva.org</a>, you can give a microloan to someone who is online, tells you what they want. You'll eventually, mostly, get your money back and you can lend it to someone else. I think that's quite an effective way of helping people too.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>How do you know a good charity when you see one? Is low overhead really a good measure? Those numbers are very easily manipulated.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I agree. No, \"low overhead\" is not the right measure. Firstly, as you say, the numbers are manipulated. Second, look, you could cut your overhead by cutting your evaluation, exactly what we were talking about. You could say \"look, I'm not going to do any followup or evaluation I'm just going to hand out, basically what you said. I'm going to hand out money to poor people.\" That way you can get your overhead down, but are you actually doing the most good? I think you don't know that until you do have some people in the field who are in touch with what's been happening and do follow up. So I'm looking for the kind of demonstrated effectiveness that you can find in the reports from GiveWell at <a href=\"http://givewell.net\">givewell.net</a> rather than just checking how much of it goes to overheads and administration and how much of it doesn't.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Keep in mind, Utilitarian calculations are very difficult, as we discussed a few minutes ago, but you don't have to listen to con stories from con men. Just fly to an Indian village, ask for people's names, get the village phone number, pick names of people who appear to be poor, they're not expecting you to show up, and send them some money. It seems to me if there's anything were you would think the chance of this doing good is really quite high it would be just sending money, and even well run charities have pretty high overhead, and you can give the money directly. Western Union has a bit of overhead, but it's relatively low. Why not have this method replace a lot of charitable giving? Because we know there's massive poverty, we know there's people who need to eat, and if someone needs to eat and you give them money, they're going to spend it on food, no?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>But you can't say there's no overhead if you, say, fly to an Indian village. There's a lot of overhead, unless you're a very wealthy person. The cost of your trip, not to mention your time, is a very substantial overhead on the amount that you're giving.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But say you're traveling anyway. You take trips, as it is, right? You go to poor countries, for other reasons. You could do a side trip to a poorer part of an urban city, in Calcutta, it would take you an hour, maybe, it wouldn't take much time. I would think at the margin there's a way for it to be quite cheap.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>It may be, and the other thing you have to consider is whether putting money directly in the hands of people, say, is better than bringing in a drill to provide water for an entire village where presently they have to walk two hours to carry water from a river and that water's polluted. Maybe some sort of structural changes like that are going to help them more than just putting money in the hands of individuals.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Keep in mind, you're a Preference Utilitarian. That doesn't mean public goods can't be more valuable, but the tendency of a Preference Utilitiarian should be to just give people resources and let them do what they want, no?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think that's an empirical question. As you say it will depend whether they will actually satisfy their preferences more by individual action or whether there's a kind of cooperative dilemma situation here, that actually they could achieve more good by cooperating, but maybe their culture is such that they don't cooperate unless there's some outside stimulus to get them to do so.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Moral Intuitions</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Here's a philosophical question again: do you trust your own moral intuitions?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>No, not really. Over along time period, I guess, I've thought about them and reflected on them, and I've dropped some or they've faded so maybe now I'm somewhat more comfortable with them, but no, I couldn't really say that I trust them as a whole.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>What's the moral intuition that you have which you trust least?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's a good question. I suppose, the intuitions that you have are that you ... I have intuitions about equality and fairness that make me want to go for more egalitarian solutions and, yet, I'm not sure whether they are really the right thing to do, so I'm somewhat critical of them but I'm still drawn by them to some extent. Obviously things about equality can have Utilitarian benefits if we accept laws of diminishing marginal utility and so on, and I would like to say that's the only sense in which I support equality, but I'm not sure that my intuitions are not actually more egalitarian than I should be as a utilitarian.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Improving the world through commerce</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's say I'm an 18 year old and I'm in college, and I've read <a href=\"http://en.wikipedia.org/wiki/The_Life_You_Can_Save\">your book</a> and I'm more or less convinced by it, and I say to you \"well what I've decided to do is I'm going to have a career in the cell phone industry because I see that cell phones are revolutionizing Africa and making many people much better off. I'm not going to give a dime to poverty but I'm going to work my hardest to become a millionaire by making cheaper and better cell phones.\" What do you say to me?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Well, making cheaper and better cell phones may be great for Africa, and while you're building up your business, of course, you want to reinvest your capital and make the business bigger, but are you going to get to a point, at some stage in your life, where you'll have a lot of money, where you've done your work of providing the cheap cell phones, what are you going to do with that money? I think that's still, for a Utilitarian, a relevant question. It's the kind of question that <a href=\"http://en.wikipedia.org/wiki/Warren_Buffett\">Warren Buffet</a> asked himself. He accumulated a lot of money and said \"look, I can make this money earn money faster than anyone else, so I'm going to wait until I'm old before giving it away.\" And that was a good thing, I guess, although now we might wish he'd given it away <a href=\"http://en.wikipedia.org/wiki/Late-2000s_financial_crisis\">last year rather than this year</a>.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>That's right, but let's say I never give a dime, I've accumulated a fortune of $200M, I've done a lot for the cell phone industry. Am I a better person than someone who's earned $40K/year and every year given 15% of it away to the poor in India?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Well I'm not sure that you're a ... \"better person\" asks for a judgment about the character of the agent. I think it's quite possible you've done more good for the world, and you should be congratulated on the good that you've done for the world. We do tend to judge people by their intentions, and your intentions are a little suspect because, although you've done a lot of good for the cellphone industry and maybe for Africans you've still got this $200M. Would you really be a lot happier with $200M than with $100M, $10M say. And if not, then why not, in addition to the benefits you've conferred on people also use that $190M for something that will help people?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If you're a Utilitarian, isn't it a little irrational to judge people by their intentions? You're retreating to this \"we\". \"We\" judge people by their intentions. You're not willing to say \"I do\" because that would make you inconsistent. Why not just say \"Utility is what matters, I'm a Utilitarian, this person did more for the poor, this person is a better person than the one who gave a lot to charity\". It's not my personal view, I'm less of a strict Utilitarian, but why not indeed embrace that conclusion rather than distance yourself from it?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Because, as a Utilitarian, praise and blame have a function, to encourage people to do good and not to do things that are bad---</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>--This isn't social, this is your true view, all things considered, it's not what you say publicly to incentivise people. It's the \"what you really think\" question. Like, all the viewers need to turn off their BloggingHeads TV, and then you can tell me what you really think and then turn it back on again.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>--but we are on BloggingHeads TV, they haven't turned it off--</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>--what would you say?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Look, if I'm talking to the 18 year old, and the 18 year old is saying \"look, I have these two career options. One is I do this, I confer all these benefits by developing cell phones but the end I end up pretty with my $200M and I don't give it away, and the other is I earn the $40K/year and give away whatever the percentage was\", and we assume, as you said, the benefits are much less. So I'm going to tell the 18 year old to do the thing that will produce the greatest benefits. That's true. Even when he gets to 60 and he has the $200M I'm still going to think, privately, that I gave him the right advice, that was the right thing to do, I'm glad he did it. So if that's what you're asking me, that will be my judgment, and in that sense he's a better person than he would have been if he had just earned the $40K and given a lot of it away.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>You think a Utilitarian has to be a kind of <a href=\"http://en.wikipedia.org/wiki/Leo_Strauss\">Straussian</a> and embrace certain kinds of public lies to incentivise people?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think that's a really interesting issue. Yeah, I would say he has to be a Sidgwickian. I prefer being a Sidgwickian to a Straussian, just because Straussians have a rather bad flavor to it after they were <a href=\"http://www.opendemocracy.net/faith-iraqwarphiloshophy/article_1542.jsp\">used in the Bush administration</a>. You could say that the <a href=\"http://en.wikipedia.org/wiki/Rationale_for_the_Iraq_War\">Iraq War conspiracy</a> was kind of Straussian. But, of course, Henry Sidgwick talked about that, he said that for a Utilitarian it is sometimes going to be the case that you should do good, but you need to do it secretly because if you talk publicly about what you're doing this would set an example that would be misleading to others and would lead to bad consequences. I think that's true, and I think for a Utilitarian it's inevitable that there will sometimes be circumstances in which that's the case.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>What makes Peter Singer happy?</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try another personal question, again feel free to pass. If you just ask yourself, \"what are the things in life that just make me, Peter Singer, happy\", what would you say they are? What's your own self account of what makes you happy?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>It's mixed. For example, I've been touring, talking about this book. I think the book has the potential to do good in the world. I'm happy when I see that people are responding to the book. Somebody told me last night at a dinner that they'd read the book and they'd told an aid organization that they support to find a village where they could support the drilling of a well to provide water and they were going to give whatever it took to drill that well. That makes me happy, that I had this impact. Obviously I've had an impact on people changing their diet too, I have people coming up to me all the time saying \"I read <a href=\"http://en.wikipedia.org/wiki/Animal_Liberation_%28book%29\">Animal Liberation</a> and I became a vegetarian or a vegan and I've been working for animal groups\". That makes me happy too, that my work has had that effect, which I think it a beneficial effect. But I don't want to pretend to you or to the BloggingHeads viewers that I'm a saint. I can be happy when I'm on vacation, I can be hiking in the mountains. I love mountain scenery, I had a vacation in <a>Glacier National Park</a> a year or so ago, which was gorgeous. That sort of thing makes me happy, and I admit that it's probably not doing as much good for the world as I could have done if instead of spending the money on that vacation I had given it to Oxfam.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Human and animal pleasures</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I sometimes ask myself, I struggle with this question, I ask \"are my own deepest pleasures actually quite primeval ones\", basically food, sleep, and sex. In your own writings you've emphasized, correctly, the ties between human beings and non-human animals, and it seems that for other animals these are almost always, maybe always the deepest pleasures. So I tend to think that for human beings, including ourselves, they're the deepest pleasures as well, and the higher pleasures are worth something, but actually they're somewhat of an epiphenomenon and what makes us happy are similar to what make non-humans happy. Do you have a view on this?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I'm not sure I think that the things you mention ... food and sex are obviously important, actually sleep doesn't make me particularly happy. It's something I need to do or I feel bad, but It doesn't make me happy. But yeah, food and sex are important pleasures in life. Are they more important in my life than the things that I do in work? I wouldn't really say that. I think that food and sex are the kind of desires that get satisfied: I eat a good meal, I enjoy it but I don't want to eat again for a few hours, and even sex has its limits in how much you can do at any particular time and still want more of it. Whereas the kind of things we're talking about, you can call them \"higher pleasures\" or \"more purposive, fulfillment sort of activities\" you can just go on and find that it's better and better. So I think there's a difference in that.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>So what makes you happy is pretty different from what makes a non-human animal happy, you would say?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Yes, that's true, I think higher cognitive capacities to make a difference there.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br />\n<h3>Pescatarianism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me ask you a question about animal welfare. I have been very influenced by a lot of what you've written, but I'm also not a pure vegetarian by any means, and when it comes to morality, for instance, my view is that it's perfectly fine to eat fish. There may be practical reasons, like depleting the oceans, that are an issue, but the mere act of killing and eating a fish I don't find anything wrong with. Do you have a view on this?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>There's certainly, as you say, the environmental aspect, which is getting pretty serious with a lot of fish stocks, but the other thing is there's no humane killing of fish, right? If we buy commercially killed fish they have died pretty horrible deaths. They've suffocated in nets or on the decks of ships, or if they're deep sea fish pulled up by nets they've died of decompression, basically their internal organs exploding as they're pulled up. I would really ... I don't need to eat fish that badly that I need to do that to fish. If I was hungry and nothing else to eat I would, perhaps, do it but not given the choices I have.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But now you're being much more the Jewish Moralist and less the Utilitarian. Because the Utilitarian would look at the marginal impact and say \"most fish die horrible deaths anyway, of malnutrition or they're eaten or something else terrible happens to them\". The marginal impact of us killing them to me seems to be basically zero. I'm not even sure a fish's life is happy, and why not just say \"it's fine to eat fish\"? Should it matter that <em>we</em> make them suffer? It's a very non-Utilitarian way of thinking about it, a very moralizing approach.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>You would need to convince me that in fact they're going to die just as horrible deaths in nature, and I'm not sure that that's true. Probably many of them would get gobbled up by some other fish, and that's probably a lot quicker than what we are doing to them.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>You have some good arguments against <a href=\"http://en.wikipedia.org/wiki/Malthusianism\">Malthusianism</a> for human beings in your book. My tendency is to think that fish are ruled by a Malthusian model, and being eaten by another fish has to be painful. Maybe it's over quickly, but having your organs burst as you're pulled up out of the water is probably also pretty quick. I would again think that in marginal terms it doesn't matter, but I'm more struck by the fact that it's not your first instinct to view the question in marginal terms. You view us as active agents and ask \"are we behaving in some manner which is moral, and you're imposing a non-Utilitarian theory on our behavior. Is that something you're willing to embrace, or something that was just a mistake?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Look, I think economists tend to think more in terms of marginal impact than I do and you may be right that is something I may need to think about more. Look, Tyler, I have to finish unfortunately, I've got another interview I've got to go to, so it's been great talking to you, but I think we're going to have to leave it at that point.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Ok, thank you very much.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Thanks a lot.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I've enjoyed it a great deal. Bye!</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Ok, bye!</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>(I also posted this <a href=\"http://www.jefftk.com/news/2012-04-06.html\">on my blog</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhfefamXXee6c2CH8": 1, "qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WHJkPQ8jeCW3FaQGx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 56, "extendedScore": null, "score": 0.000128, "legacy": true, "legacyId": "14921", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In March 2009, <a href=\"http://en.wikipedia.org/wiki/Tyler_Cowen\">Tyler Cowen</a> (<a href=\"http://marginalrevolution.com/\">blog</a>) interviewed <a href=\"http://en.wikipedia.org/wiki/Peter_Singer\">Peter Singer</a> about morality, giving, and how we can most improve the world. They are both thinkers I respect a lot, and I was excited to read their debate. Unfortunately the interview was available only as a <a href=\"http://bloggingheads.tv/videos/2022\">video</a>. I wanted a transcript, so I made one:</p>\n<p>\n</p><table border=\"0\">\n<tbody>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>This is Tyler Cowen of George Mason University. I'm doing a BloggingHeads with Peter Singer, the world-famous philosopher from Princeton. This is a forum on Peter's latest book, which he'll start off by telling you a bit about.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Hi. The book's called \"<a href=\"http://en.wikipedia.org/wiki/The_Life_You_Can_Save:_Acting_Now_to_End_World_Poverty\">The Life You Can Save: Acting Now To End World Poverty</a>\". It begins with an argument that I've used many times in articles about a child drowning in a pond, and suggests that if you saw a child drowning in a pond that you would jump in and save that child, and you think that is what you ought to do, even if it meant that you ruined an expensive pair of shoes that you were wearing.\n<p>From there I pull back to saying \"what does this mean about the problem of world poverty, given that there are, according to Unicef, ten million children dying of avoidable poverty-related causes every year?\" We could save some of them, and probably it wouldn't cost us much more than the cost of an expensive pair of shoes if we find an effective aid agency that is doing something to combat the causes of world poverty, or perhaps to combat the deaths of children from simple conditions like diarrhea or measles, conditions that are not that hard to prevent or to cure. We could probably save a life for the cost of a pair of shoes. So why don't we? What's the problem here? Why do we think it's ok to live a comfortable, even luxurious, life while children are dying? In the book I explore various objections to that view, I don't find any of them really convincing. I look at some of the psychological barriers to giving, and I acknowledge that they are problems. And I consider also some of the objections to aid and questions raised by economists as to whether aid really works. In the end I come to a proposal by which I want to change the culture of giving.</p>\n<p>The aim of the book in a sense is to get us to internalize the view that not to do anything for those living in poverty, when we are living in luxury and abundance, is ethically wrong, that it's not just not a nice thing to do but that a part of living an ethically decent life is at least to do something significant for the poor. The book ends with a chapter in which I propose a realistic standard, which I think most people in the affluent world could meet without great hardship. It involves giving 1% of your income if you're in the bottom 90% of US taxpayers, scaling up through 5% and 10% and even more as you get into the top 10%, the top 5%, the top 1% of US taxpayers. But at no point is the scale I'm proposing what I believe is an excessively burdensome one. I've set up a website, <a href=\"http://thelifeyoucansave.com\">thelifeyoucansave.com</a> that people can go to in order to publicly pledge that they will meet this scale, because I think if people will do it publicly, that in itself will encourage other people to do it and, hopefully, the idea will spread.</p>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Thank you, Peter. Let me first stress: I agree with most of what's in your book; I think we all could give more and should give more. It would be good for other people and it would be good for ourselves. But let me start off the dialogue by mentioning a few points where I don't completely agree with you. One thing that struck me about the book was some of the omissions.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Immigration_as_an_Anti_Poverty_Program\">Immigration as an Anti-Poverty Program</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>For instance, in my view, what is by far the best anti-poverty program, the only one that's really been shown to work, and that's what's called \"immigration\". I don't even see the word \"immigration\" in your book's index. So why don't we spend a lot more resources allowing immigration, supporting immigration, lobbying for immigration? This raises people's incomes very dramatically, it's sustainable, for the most part it's also good for us. Why not make that the centerpiece of an anti-poverty platform?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's an interesting point, Tyler. I suppose, one question I'd like to ask is: is it sustainable? Isn't it the case that if we take, as immigrants, the people who are the most enterprising, perhaps, of the poor countries that we're still going to leave those countries in poverty, and their populations may continue to rise, and eventually, even if we keep taking immigrants, we will reach a capacity where we're starting to strain our own country?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>There's two separate issues: one is \"brain drain\" from the third world. I think here's a lot of research by [Michael Clemens], showing that it's not a problem, that third world countries that have even somewhat functional institutions tend to benefit by sending people to other countries. India's a good example: a lot of Indians return to India and start businesses, or they send money back home. Mexico is another example. Maybe North Korea is somewhat different, but for the most part immigration seems to benefit both countries.\n<p>I don't think we could have open borders; I don't think we could have unlimited immigration, but we're both sitting here in the United States and it hardly seems to me that we're at the breaking point. Immigrants would benefit much more: their wages would rise by a factor of twenty or more, and there would be perhaps some costs to us, but in a cost-benefit sense it seems far, far more effective than sending them money. Do you agree?</p>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I must admit that I haven't thought a lot about immigration as a way of dealing with world poverty. Obviously, from what you're saying, I should be thinking more about it, but I can't really say whether I agree until I have thought more about it.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Changing_Institutions__Greater_Tax_Break_for_True_Charity\">Changing Institutions: Greater Tax Break for True Charity</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try another question along related lines. I think one general way in which I think about your book differently than you do, is that you think more about giving. I'm a big advocate of giving, I've written a whole <a href=\"http://www.amazon.com/Good-Plenty-Creative-Successes-American/dp/0691146268\">book</a> myself on philanthropy, but I think somewhat more in terms of changing institutions. So another thing we might consider doing, along the lines of what you advocate, is to increase the tax benefits of giving. Right now, if you're itemizing deductions and you give $1, you deduct $1 from your taxes. But it wouldn't be very difficult to make it the case that for certain kinds of giving you could deduct $1.10 from your taxes or $1.20. Would you favor this kind of reform?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I might favor that, if giving were defined more narrowly than we do, in the US anyway, because I know I can deduct $1 from my taxes whether I give to <a href=\"http://www.oxfamamerica.org/\">Oxfam America</a>, which I think is an effective organization fighting world poverty, or if I give to the <a href=\"http://www.metmuseum.org/\">Met</a> so they can buy yet another painting to add to the already super-abundant collection of paintings they have. I don't see why the taxpayer should subsidize me if I decide I want to give to the Met but sure, if I'm giving to Oxfam I think that would be good.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>So, in other words, you favor a kind of tax cut as a way to help the world's poor. That, in this country, if targeted properly, tax policy, in essence cutting the taxes of rich people, is one of the very best ways to help the world's poor. Would you sign on to that?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I'm not quite sure why it is ... you seem to have leapt a little from what I was saying and I haven't followed the leap as to why cutting taxes for the rich would be one of the most effective ways of helping the poor. Can you explain that a little more?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If we give a greater tax break to charitable donations, and here I mean only true charity, not say a fancy art museum, disproportionately this will benefit wealthy people. Wealthy people have a lot of money. In essence you're cutting their taxes. They're giving more, they may not have a higher level of consumption, but would you be willing to raise your hand and say \"I, Peter Singer, think that cutting taxes on the US wealthy is in fact one of the very best things we could do for the world's poor, if we do it the right way\"? Yes or no?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Yes, if the tax break only goes to those of the wealthy who are giving to organizations that are effectively helping the poor, I'll raise my hand to that.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>OK; I'm glad to hear that.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Millennium_Villages_Skepticism\">Millennium Villages Skepticism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me focus on another point of difference between us in the book. I think when it comes to the effectiveness of aid, I'm not a total skeptic on the effectiveness of aid but I think I'm more skeptical than you are. In a number of places you site the work of <a href=\"http://en.wikipedia.org/wiki/Jeffrey_Sachs\">Jeffrey Sachs</a>. Now my view of Sachs is that his projects are actually doing individual human beings a lot of good but the return on investment I don't think is that high. I think he's improving the health of a lot of people but I don't think he's going to raise any villages, much less countries or continents, out of poverty. Given that my view is that the rate of return on this investment is much lower, and I think that the economics profession as a whole agrees with me, not with Sachs, this to me suggests that to really make a dent in world poverty we would have to give much more than 5% of our incomes, even more than 10%, that we're simply at a point where we can do some good, but that to abolish poverty we would have to engage in a very dramatic redistribution. What's your view on this?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Firstly, I think, as for whether Sachs is really going to succeed in raising villages out of poverty, I think the data isn't in yet. The <a href=\"http://en.wikipedia.org/wiki/Millennium_Villages_Project\">Millennium Villages project</a> which he's working on has only been going a few years, I think we need to give it maybe another five years to see whether it's working. That's more or less what he's said. He hopes that the aid will be short term, that the villages will become self-sustaining, the improvements will last, they'll be out of the poverty trap. If that hasn't happened at the end of another five years I'm going to agree with you that we're going to need more, but I think it's really too early to call the result on that one.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Take the overall opinion of economists, which is again that Sachs's projects can do good, people in those villages might be better off, but if you're in the middle of, say, a totally corrupt African country which is not democratic, which maybe has been fighting wars, which has an absolutely horrible infrastructure, which has a bureaucracy, a kleptocracy, massive problems, lack of literacy, that maybe you could eliminate infectious diseases or malaria within that village. People will be better off, it's worth doing, but at the end of the day is there really any reason to think, given the last 300 years of thinking and writing on development and economic history that this will at all cure poverty? Doesn't it just mean you'll have poor people without malaria, which is better than poor people with malaria, but they're still essentially poor people?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>If the governments and the situation is as bad as you describe, you're probably right, but of course not all countries ... you describe pretty much a worst case scenario. I think there are a lot of countries where there are poor people which do not have governments which are as bad as you painted. I think in those countries we can hope that people actually will lift themselves out of poverty and I think that's what we need to try to do. Now, you may be right that that's still going to leave poor people in countries that are as bad as you describe, and there is a real question then as to how much we can do to help them, whether giving more will really be enough to help them or because of those governments in those situations there's really nothing much we can do. That will be the dilemma. But I don't think we've got to that point yet because we've not really worked out what we can do for people in the countries where the governments aren't so bad.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Chinese_Reforms\">Chinese Reforms</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I think you and I are both looking for what are the most highly leveraged ways we can reduce poverty in this world.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Uh huh</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If I ask myself, historically, what has been the most successful anti-poverty program in the last century, I look at Communist China, and I would say that the <a href=\"http://en.wikipedia.org/wiki/Chinese_economic_reform\">reforms</a>, starting in the late 1970s, have taken at least 300M-400M people, and probably more, and taken them from extreme poverty, perhaps starvation, to a situation where a lot of them live quite well or at least have some kind of tolerable lower middle class existence. I think that property rights and institutional reforms are they key to fighting poverty. China during that period, the aid it received didn't matter much. It doesn't mean we shouldn't give aid, I'm all for aid, but isn't the big leveraged investment here changing and improving institutions and not giving money?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I do that that's a really important thing when we can do it. The question is, can we do it? Obviously the Chinese reforms that you refer to really were internally driven, I don't think they were a result of things the West did, unless you talk about the entire global economic system, which China clearly wanted to participate in. So the question is how can we be effective in producing those sorts of changes? In some countries we can come in and help, say countries recovering from civil war, and give some help in establishing good institutions, but I'm not sure what ideas you have about what's a good way to bring about that kind of reform in these countries that will lead everywhere to the sorts of benefits that you refer to in China.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>In countries like China in a way it's internally driven. It's not that anyone successfully pressured them, but in another way I think it's highly externally driven, that the Chinese, Taiwanese, Koreans, other countries followed the example of Japan They saw that Japan worked. They saw that an Asian country could rise to moderate wealth or even riches and at some point they decided to copy this in their own way. If we look at Japan, Japan copied the west, so maybe one of the very best most important things we can do is just ourselves be a beacon of progress: be humane, be tolerant, respect others, be wealthy and just show that it's possible. We shouldn't think of that as a substitute for aid, but maybe that's actually our number one priority. Does that make sense to you?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That makes sense. I don't know that we have to strive to be more wealthy than we are--well, maybe just right at this moment we need to strive to get back to being as wealthy as we were a year ago perhaps. But I think we are setting that example, undoubtedly. We are showing countries what can be done with reasonably good government, open economies, and I do hope that other countries will follow that. But maybe not all countries can do it. I think that <a href=\"http://en.wikipedia.org/wiki/Paul_Collier\">Paul Collier</a> argues in <a href=\"http://en.wikipedia.org/wiki/The_Bottom_Billion\">his book</a> that it's going to be difficult for some African countries to get into this game now. There are reasons why it's going to be hard for them to compete with countries that have established positions, have developed markets, have low labor costs. It's not clear to me that this is going to be a path that every poor country can follow.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Military_Intervention\">Military Intervention</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>You mentioned Paul Collier. I found his book very interesting. One argument he makes--I would say I'm not, myself, convinced but I'm curious to hear what you think--is that we could do the world a great deal of good by selective military interventions. So take <a href=\"http://en.wikipedia.org/wiki/War_in_Darfur\">the case of Darfur</a>. A large number of people are suffering, dying. Collier says, or implies, or at least opens the possibility, that we, the United States, the UN, whoever, should just move in and in military terms do something about this. It is again a topic that is not prominent in your book, but it seems that if it can work it's highly leveraged, more leveraged than giving away money. I'm curious as to your views on that.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I did discuss humanitarian intervention in my earlier book <a href=\"http://books.google.com/books/about/One_world.html?id=tk9Oo_SleYwC\">One World</a> and I do support it under the right circumstances. I think, though, we do have to be pretty clear about defining it properly and trying to get support for it. Maybe it would work in Darfur. I think Darfur is quite a large area, relatively thinly populated, and it might take a lot of resources to really protect the people in Darfur. There are underlying issues, too, perhaps about climate change, even, that are causing scarcity in Darfur. But isn't possible, I mean I think that Zimbabwe would be another possibility, though maybe just now with changes in the political system you wouldn't want to do it just now, you'd want to see how that played out for a while. But certainly a year ago you might well have thought that if the South Africans could be persuaded to move in and remove Mugabe that would be a good thing to do. That would have been better, I think, than having a white former colonial power come in, that obviously would have evoked a lot of echos of returning to a past that Zimbabweans don't want. But I'm not, in principle opposed to military intervention, I just think we have to be very very careful about the circumstances in which we do it, because obviously it can trigger a lot of violence and bloodshed and produce results that are the opposite of what you and I would both want.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Colonialism\">Colonialism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Do you think the end of colonialism was a good thing or a bad thing for Africa?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's a really difficult question. I think, clearly, there were lots of bad things about colonialism, but you would have to say that some countries were definitely better administered and that some people's lives, although they may have had some sort of humiliation, perhaps through not being independent, being ruled by people of a different race, in some ways they were better. It's hard, really, to draw that balance sheet. Independence has certainly not been the unmitigated blessing that people thought it would be at the time.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's say we have the premise, that with colonialism there would not have been wars between African nations. It's not the case that a British ruled colony would have attacked a French colony, for instance. It's highly unlikely. So given just that millions have perished from wars alone, wouldn't the Utilitarian view, if you're going to take one, suggest that colonialism was essentially a good idea for Africa, it was a shame that we got rid of it, and that the continent would have been better off under foreign rule, European foreign rule.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I don't think we can be so sure that it would have continued to be peaceful. After all we did have militant resistance movements, we had the <a href=\"http://en.wikipedia.org/wiki/Mau_Mau_Uprising\">Mau Mau</a> in Kenya, for example. We had other militant resistance movements. It may simply have been that the fact of white rule would have provoked not one colony going to war against another but civil war within some of those countries. If what you're asking is would colonialism, had it been accepted by the people there, without military conflict, would that have been better than some of the consequences we've had in some of these countries, you would have to say undoubtedly yes. But we can't go back and wind back the clock and say \"how would it have been if\" because we don't really know whether that relative stability and peace would have lasted.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If we compare the Mau Mau, say, to the wars in Kenya and Rwanda, it seems unlikely that rebellions against colonial governments would have reached that scope, especially if England, France, other countries, would have been willing to spend more money to create some tolerable form of order. My guess is you would have had a fair number of rebellions but it's highly highly unlikely it would compare to the kind of virtual holocausts we've had in Africa as it stands.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Aid_without_stable_government\">Aid without stable government</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I certainly agree that if you look at what's been happening in the Congo, just as one example, or countries like Sierra Leone or Liberia, yes, you could certainly think that it might have been better for those countries.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Would you say that Zimbabwe is one example of a country where just giving it money through aid is unlikely to work?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>At present, unless the government changes quite dramatically. Again, as you were saying before, there might be specific things we can do: we may be able to help particular people who have disease or are hungry, but I agree, in the present conditions it's unlikely to lift people out of poverty on any kind of large scale.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's take a country like Madagascar, which as recently as two or three years ago was touted by the Bush administration, and I don't just mean Republicans, it was touted by many people, as being a kind of model for Africa. Here's a country were we could give a lot of aid, the aid would go to some good purpose, we're making progress, and now Madagascar seems to be in the midst of a <a href=\"http://news.bbc.co.uk/2/hi/7937264.stm\">civil war</a> and the polity is collapsing, the economy is doing very poorly. How many countries in Africa do you think are there where aid works? Where do you draw the line? What in your opinion is the marginal country that is hopeless?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Look, I haven't got a list of African countries like that, I must admit. I think there are some countries where things seem to work, and that's not to say I could name a country and say, well, Mozambique, that aid programs have made a positive difference, or Sierra Leone. Maybe in a month there'll be a coup and you'll be able to tell me that I was wrong. I can't see the future. But there are countries where I think aid has worked, ones where it hasn't worked. I haven't got a rank ordering and I don't have a cutoff line where that is, I'm sorry I'm just not sufficiently expert on African politics and conditions to do that.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Genetically_modifying_ourselves_to_be_more_moral\">Genetically modifying ourselves to be more moral</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's try some philosophical questions. You're a philosopher, and I've been very influenced by your writings on personal obligation. Apart from the practical issue that we can give some money and have it do good, there's a deeper philosophical question of how far those obligations extend, to give money to other people. Is it a nice thing we could do, or are we actually morally required to do so? What I see in your book is a tendency to say something like \"people, whether we like it or not, will be more committed to their own life projects than to giving money to others and we need to work within that constraint\". I think we would both agree with that, but when we get to the deeper human nature, or do you feel it represents a human imperfection? If we could somehow question of \"do we in fact like that fact?\", is that a fact you're comfortable with about human nature? If we could imagine an alternative world, where people were, say, only 30% as committed to their personal projects as are the people we know, say the world is more like, in some ways, an ant colony, people are committed to the greater good of the species. Would that be a positive change in human nature or a negative change?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Of course, if you have the image of an ant colony everyone's going to say \"that's horrible, that's negative\", but I think that's a pejorative image for what you're really asking ...</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>No, no, I don't mean a colony in a negative sense. People would cooperate more, ants aren't very bright, we would do an ant colony much better than the ants do. ...</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>But we'd also be thinking differently, right? What people don't like about ant colonies is ants don't think for themselves. What I would like is a society in which people thought for themselves and voluntarily decided that one of the most satisfying and fulfilling things they could do would be to put more of their effort and more of their energy into helping people elsewhere in need. If that's the question you're asking, then yes, I think it would be a better world if people were readier to make those concerns their own projects.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's say genetic engineering is possible, which is now not so far off on the major scale, and your daughter were having a daughter, and she asked you \"daddy, should I program my daughter so that she's willing to sell her baby and take the money and send it to Haitians to save ten babies in Haiti\". Would you recommend to her \"yes, you should program the genes of your baby so she's that way\"?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>So she's going to sell her baby? What's going to happen to the baby?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>She's going to sell it to some wealth white couple that's infertile, they live in the Pacific Northwest, they'll take fine care of it, she'll receive $1M and save, say, 30 lives in Haiti. You've recommended that your granddaughter be programmed to act this way. Would you recommend that?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>And so she's going to be happy with that? She's not going to suffer as current people would the pangs of separation from their daughter or the agonies of not knowing what's happened to my daughter? She's going to feel perfectly comfortable with that, and she's going to feel good about the fact that she's helped 30 babies in Haiti to have a decent life? Is that the assumption?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>We can do it that way, but keep in mind that even if she's unhappy that's outweighed by the 30 Haitian lives which are saved. Either way you want.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Right, but you're asking me and I'm like normal human beings, I haven't been reprogrammed, so I care about my daughter or my granddaughter, or whoever this is.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Ok, she'll be happy.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Ok, good. Then I think I'm on board with your program.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>So you would want people to be much more cooperative in this way, if we could manage it in some way that won't wreck their psyches.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's right.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Do you think people would have a moral obligation to genetically reprogram themselves, or it would just be a nice thing they could do if they felt so inclined?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think if we really had a system that was as good as you're saying, would lead to as good consequences, and would leave people happy, that's something they ought to do. Because that would really be a way of making a huge difference to the world. They would be wrong not to take advantage of this, given the benefits it involves and the absence, it seems, as described, of any major drawbacks.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Problem_areas_in_Utilitarianism\">Problem areas in Utilitarianism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>What do you think is the biggest problem area in Utilitarian moral theory?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>The biggest problem area? One problem people are talking about that's relevant to what I'm talking about is that Utilitarian moral theory leads to highly demanding consequences that people reject. So that's one problem. The second problem, of course, is that it requires very complex calculations because we don't have a set of simple moral rules that say \"don't do this, do that\". We have to work out what the consequences of our actions are. As in this area we're talking about, what kind of aid is effective, what will overcome world poverty, it's very difficult to work out what the consequences are, and it's sometimes very difficult to know what's the right thing to do.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But you think we nevertheless should do what we think is best, no matter how imperfect that guess may be?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Yeah, I don't really see what else we're supposed to do. It would seem to me to be wrong to say \"because I can't calculate the consequences I'm just going to follow this simple set of rules\". Because I can't calculate the consequences. But why follow this simple set of rules? Where do they come from? I don't believe that we have any god-given rules. I don't think that our moral intuitions are a good source of rules, because that's the product of our evolutionary history, which may not be appropriate for the moment that we're in. So, despite the difficulty, I don't really see what the alternative is, to trying our best to figure out what the expected utility is.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Is_Utilitarianism_independent_\">Is Utilitarianism independent?</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me toss up a classic criticism of Utilitarianism. I'm curious to see what you say. The criticism is this, that neither pain nor pleasure is a homogeneous thing. There are many different kinds of pains and pleasures and they're not strictly commensurable in terms of any natural unit. So when we're comparing pain and pleasure that's a fine thing to do, but in fact we're calling upon other values. So Utilitarianism is in this sense parasitic upon some deeper sense of philosophic pluralism, and we're not pure utilitarians at all. But that being the case, why don't we sometimes just allow an intuitive sense of right or wrong to override what would otherwise be the Utilitarian conclusion, since Utilitarianism itself cannot avoid value judgments?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think the form of Utilitarianism that you're describing is <a href=\"http://www.utilitarianism.com/hedutil.htm\">Hedonistic Utilitarianism</a> because you were talking about pleasure and pain and you were suggesting that pleasure is a whole range of different things. The form that I hold is <a href=\"http://en.wikipedia.org/wiki/Preference_utilitarianism\">Preference Utilitarianism</a> which looks at people's preferences and tries to asses the importance of the preference for them. Now this is still not an easy thing to know, in fact in some ways you might say it's harder than getting measures of pleasure and pain, but I think it already embraces the pluralism that you're talking about in terms of people's preferences, people's understanding of what it is they're choosing and why. And so I don't think it's up to us to go back and try to pull in other kinds of values that we intuitively hold over the top of people's preferences. We can do it for ourselves, each of us can say \"what are my preferences\", \"I value this\", \"I value the autonomous life over the happy life, and so that's what I'm going to choose\". Of course, when I weigh out your preferences I should say \"well here we give weight for the preference for an autonomous life and here we give weight to the preference for the pleasant life\" but in making the final judgment, in which we take everyone's preferences into account, it would be wrong for us to just pull out some intuitive values and somehow give them weight in the overall calculation because then we're giving more weight to our preferences than we're giving to those of others.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But doesn't preference utilitarianism itself require some means of aggregation? The means we use for weighing different clashing preferences, can require some kind of value judgments above and beyond Utilitarianism?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I don't quite see why that should be so. While acknowledging the practical differences of actually weighing up and calculating all the preferences, I fail to see why it involves other values apart from the preferences themselves.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Peter_Singer__Jewish_Moralist\">Peter Singer: Jewish Moralist</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try giving you my reading of Peter Singer, which is highly speculative, and I'm not even saying it's true, it's just what I think when I read you, especially the later Peter Singer, and I'm just curious to hear your reaction to it. My reading is this: that Peter Singer stands in a long and great tradition of what I would call \"Jewish moralists\" who draw upon Jewish moral teachings in somehow asking for or demanding a better world. Someone who stands in the Jewish moralist tradition can nonetheless be quite a secular thinker, but your later works tend more and more to me to reflect this initial upbringing. You're a kind of secular Talmudic scholar of Utilitarianism, trying to do <a href=\"http://en.wikipedia.org/wiki/Mishna\">Mishna</a> on the classic notion of human well being and bring to the world this kind of idea that we all have obligations to do things that make other people better off, that you're very much out of the classic European, Austrian, Viennese, ultimately Biblical tradition about our obligations to the world. What do you say?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I'm amused, I have to say. I think it's interesting. You're right that I come from a Jewish family. It was a pretty secular Jewish family, so I never got as a child, actually, a lot of Jewish teaching, never went to Jewish Sunday school, I never learned Hebrew, I never had a Bar Mitzvah, I never read the Torah. So if I had got some of that it must have come kind of at a distance through, sort of, osmosis, as you say this vaguely Jewish Viennese culture that certainly was part of my family background but was very much secularized. The interesting thing to speculate is whether I'm doing something that, say, someone out of the British Utilitarian tradition, the tradition of <a href=\"http://en.wikipedia.org/wiki/Jeremy_Bentham\">Bentham</a> and <a href=\"http://en.wikipedia.org/wiki/John_Stuart_Mill\">Mill</a> and <a href=\"http://en.wikipedia.org/wiki/Henry_Sidgwick\">Sidgwick</a> could not have done. What are the distinctive features of my version of Utilitarianism that they would have rejected? And if there is something, it probably is attributable to that background you mention. But I'd be interested in your answer, what do you think that there is in my view that Bentham or Mill or Sidgwick could not have whole-heartedly endorsed?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I'm not sure if there's anything, but I think the mere fact that it is you who is doing it nonetheless reflects something about this. I think of you as one of the worlds greatest theologians, in a way, having this understanding of the quality of mercy, which is put into a secular framework, but what the intuitions really consist of, I think none of us really ever know where our moral intuitions come from.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Ok. Well, look. It's a possible view, as I think you said introducing it, you don't know whether it's true but it's an interesting view of me and where I come from. You've put it out there. I find it hard to look internally, so I'll leave it to others to judge which of the elements of my background they see having formed me most strongly.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"What_charities_does_Peter_Singer_give_to_\">What charities does Peter Singer give to?</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try a personal question but feel free to pass on this one. Let's say someone has read your book and they say \"I'm on board, Peter, please tell me what charities you give to.\" You mentioned Oxfam, but would you have anything specific you'd like to say? And why?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I do support Oxfam substantially, I've got a long relationship with, different Oxfams. They're actually autonomous national groups that work together, so when I first became interested in this issue as a graduate student, way back in Oxford in the '70s, I was living in Oxford and that's the headquarters of the original Oxfam, <a href=\"http://www.oxfam.org.uk/\">Oxfam UK</a>, so I got in touch with them and remain connected with their office. Then I went to Australia and was involved with <a href=\"https://www.oxfam.org.au/\">Oxfam Australia</a>, now I'm involved with <a href=\"http://www.oxfamamerica.org/\">Oxfam America</a>. I like what they do good grassroots work, I've seen some of that, helping the most underprivileged people, plus they're not afraid to be a real advocate for the poor, to <a href=\"https://www.oxfam.org.au/explore/mining/\">tackle</a> big mining companies that are pushing the poor off their land, <a href=\"http://www.oxfam.org/en/policy/food-aid-or-hidden-dumping\">tackle</a> the US government and its <a href=\"http://en.wikipedia.org/wiki/Agricultural_subsidy#United_States\">agricultural subsidies</a>. That's one reason that I like them. But there are many good organizations around. I've recently started supporting <a href=\"http://givewell.org\">GiveWell</a>, you can find them on <a href=\"http://givewell.net\">givewell.net</a>, because they're doing something that I'm sure you would support: they're trying to get aid organizations to demonstrate their efficacy, to be more transparent about why they support some projects rather than others, and to show how much it costs for them to achieve their goals, whether those goals are saving lives or lifting people out of poverty. And so it's kind of at a meta level, saying I want to improve aid by helping organizations that are trying to do that. I think that's a really highly leveraged way of making an impact on what's going to happen in aid over the next couple of decades.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Zero_Overhead_Giving\">Zero-Overhead Giving</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I'm a big fan of what I call <a href=\"http://marginalrevolution.com/marginalrevolution/2007/08/discover-your-i.html\">zero overhead giving</a>, that is I send monetary transfers to poor people, maybe I've met them on my travels, by Western Union. I don't follow up, I don't monitor, there's no tax deduction, there's no overhead, it's just money from me to them. What do you think of that as a way of giving?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Interesting. I suppose I would like to have some followup. I would worry that I was getting conned. Now, you may have a good sense of who's genuine and who's not, but we all know there's con artists working here in New York city, in other cities in America, who could tell you wonderful stories about how they just need the bus fare home and then they'll be fine, and you give them the bus fare home, and you believe them and then next month they come up to you in the same spot and tell you the same story. So I would like some kind of auditing, but let me just say for people who do want to give direct I think not with zero overhead but I think with 10% overhead, if you go to Kiva, <a href=\"http://kiva.org\">kiva.org</a>, you can give a microloan to someone who is online, tells you what they want. You'll eventually, mostly, get your money back and you can lend it to someone else. I think that's quite an effective way of helping people too.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>How do you know a good charity when you see one? Is low overhead really a good measure? Those numbers are very easily manipulated.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I agree. No, \"low overhead\" is not the right measure. Firstly, as you say, the numbers are manipulated. Second, look, you could cut your overhead by cutting your evaluation, exactly what we were talking about. You could say \"look, I'm not going to do any followup or evaluation I'm just going to hand out, basically what you said. I'm going to hand out money to poor people.\" That way you can get your overhead down, but are you actually doing the most good? I think you don't know that until you do have some people in the field who are in touch with what's been happening and do follow up. So I'm looking for the kind of demonstrated effectiveness that you can find in the reports from GiveWell at <a href=\"http://givewell.net\">givewell.net</a> rather than just checking how much of it goes to overheads and administration and how much of it doesn't.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Keep in mind, Utilitarian calculations are very difficult, as we discussed a few minutes ago, but you don't have to listen to con stories from con men. Just fly to an Indian village, ask for people's names, get the village phone number, pick names of people who appear to be poor, they're not expecting you to show up, and send them some money. It seems to me if there's anything were you would think the chance of this doing good is really quite high it would be just sending money, and even well run charities have pretty high overhead, and you can give the money directly. Western Union has a bit of overhead, but it's relatively low. Why not have this method replace a lot of charitable giving? Because we know there's massive poverty, we know there's people who need to eat, and if someone needs to eat and you give them money, they're going to spend it on food, no?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>But you can't say there's no overhead if you, say, fly to an Indian village. There's a lot of overhead, unless you're a very wealthy person. The cost of your trip, not to mention your time, is a very substantial overhead on the amount that you're giving.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But say you're traveling anyway. You take trips, as it is, right? You go to poor countries, for other reasons. You could do a side trip to a poorer part of an urban city, in Calcutta, it would take you an hour, maybe, it wouldn't take much time. I would think at the margin there's a way for it to be quite cheap.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>It may be, and the other thing you have to consider is whether putting money directly in the hands of people, say, is better than bringing in a drill to provide water for an entire village where presently they have to walk two hours to carry water from a river and that water's polluted. Maybe some sort of structural changes like that are going to help them more than just putting money in the hands of individuals.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Keep in mind, you're a Preference Utilitarian. That doesn't mean public goods can't be more valuable, but the tendency of a Preference Utilitiarian should be to just give people resources and let them do what they want, no?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think that's an empirical question. As you say it will depend whether they will actually satisfy their preferences more by individual action or whether there's a kind of cooperative dilemma situation here, that actually they could achieve more good by cooperating, but maybe their culture is such that they don't cooperate unless there's some outside stimulus to get them to do so.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Moral_Intuitions\">Moral Intuitions</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Here's a philosophical question again: do you trust your own moral intuitions?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>No, not really. Over along time period, I guess, I've thought about them and reflected on them, and I've dropped some or they've faded so maybe now I'm somewhat more comfortable with them, but no, I couldn't really say that I trust them as a whole.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>What's the moral intuition that you have which you trust least?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>That's a good question. I suppose, the intuitions that you have are that you ... I have intuitions about equality and fairness that make me want to go for more egalitarian solutions and, yet, I'm not sure whether they are really the right thing to do, so I'm somewhat critical of them but I'm still drawn by them to some extent. Obviously things about equality can have Utilitarian benefits if we accept laws of diminishing marginal utility and so on, and I would like to say that's the only sense in which I support equality, but I'm not sure that my intuitions are not actually more egalitarian than I should be as a utilitarian.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Improving_the_world_through_commerce\">Improving the world through commerce</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let's say I'm an 18 year old and I'm in college, and I've read <a href=\"http://en.wikipedia.org/wiki/The_Life_You_Can_Save\">your book</a> and I'm more or less convinced by it, and I say to you \"well what I've decided to do is I'm going to have a career in the cell phone industry because I see that cell phones are revolutionizing Africa and making many people much better off. I'm not going to give a dime to poverty but I'm going to work my hardest to become a millionaire by making cheaper and better cell phones.\" What do you say to me?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Well, making cheaper and better cell phones may be great for Africa, and while you're building up your business, of course, you want to reinvest your capital and make the business bigger, but are you going to get to a point, at some stage in your life, where you'll have a lot of money, where you've done your work of providing the cheap cell phones, what are you going to do with that money? I think that's still, for a Utilitarian, a relevant question. It's the kind of question that <a href=\"http://en.wikipedia.org/wiki/Warren_Buffett\">Warren Buffet</a> asked himself. He accumulated a lot of money and said \"look, I can make this money earn money faster than anyone else, so I'm going to wait until I'm old before giving it away.\" And that was a good thing, I guess, although now we might wish he'd given it away <a href=\"http://en.wikipedia.org/wiki/Late-2000s_financial_crisis\">last year rather than this year</a>.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>That's right, but let's say I never give a dime, I've accumulated a fortune of $200M, I've done a lot for the cell phone industry. Am I a better person than someone who's earned $40K/year and every year given 15% of it away to the poor in India?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Well I'm not sure that you're a ... \"better person\" asks for a judgment about the character of the agent. I think it's quite possible you've done more good for the world, and you should be congratulated on the good that you've done for the world. We do tend to judge people by their intentions, and your intentions are a little suspect because, although you've done a lot of good for the cellphone industry and maybe for Africans you've still got this $200M. Would you really be a lot happier with $200M than with $100M, $10M say. And if not, then why not, in addition to the benefits you've conferred on people also use that $190M for something that will help people?</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>If you're a Utilitarian, isn't it a little irrational to judge people by their intentions? You're retreating to this \"we\". \"We\" judge people by their intentions. You're not willing to say \"I do\" because that would make you inconsistent. Why not just say \"Utility is what matters, I'm a Utilitarian, this person did more for the poor, this person is a better person than the one who gave a lot to charity\". It's not my personal view, I'm less of a strict Utilitarian, but why not indeed embrace that conclusion rather than distance yourself from it?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Because, as a Utilitarian, praise and blame have a function, to encourage people to do good and not to do things that are bad---</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>--This isn't social, this is your true view, all things considered, it's not what you say publicly to incentivise people. It's the \"what you really think\" question. Like, all the viewers need to turn off their BloggingHeads TV, and then you can tell me what you really think and then turn it back on again.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>--but we are on BloggingHeads TV, they haven't turned it off--</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>--what would you say?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Look, if I'm talking to the 18 year old, and the 18 year old is saying \"look, I have these two career options. One is I do this, I confer all these benefits by developing cell phones but the end I end up pretty with my $200M and I don't give it away, and the other is I earn the $40K/year and give away whatever the percentage was\", and we assume, as you said, the benefits are much less. So I'm going to tell the 18 year old to do the thing that will produce the greatest benefits. That's true. Even when he gets to 60 and he has the $200M I'm still going to think, privately, that I gave him the right advice, that was the right thing to do, I'm glad he did it. So if that's what you're asking me, that will be my judgment, and in that sense he's a better person than he would have been if he had just earned the $40K and given a lot of it away.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>You think a Utilitarian has to be a kind of <a href=\"http://en.wikipedia.org/wiki/Leo_Strauss\">Straussian</a> and embrace certain kinds of public lies to incentivise people?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I think that's a really interesting issue. Yeah, I would say he has to be a Sidgwickian. I prefer being a Sidgwickian to a Straussian, just because Straussians have a rather bad flavor to it after they were <a href=\"http://www.opendemocracy.net/faith-iraqwarphiloshophy/article_1542.jsp\">used in the Bush administration</a>. You could say that the <a href=\"http://en.wikipedia.org/wiki/Rationale_for_the_Iraq_War\">Iraq War conspiracy</a> was kind of Straussian. But, of course, Henry Sidgwick talked about that, he said that for a Utilitarian it is sometimes going to be the case that you should do good, but you need to do it secretly because if you talk publicly about what you're doing this would set an example that would be misleading to others and would lead to bad consequences. I think that's true, and I think for a Utilitarian it's inevitable that there will sometimes be circumstances in which that's the case.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"What_makes_Peter_Singer_happy_\">What makes Peter Singer happy?</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me try another personal question, again feel free to pass. If you just ask yourself, \"what are the things in life that just make me, Peter Singer, happy\", what would you say they are? What's your own self account of what makes you happy?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>It's mixed. For example, I've been touring, talking about this book. I think the book has the potential to do good in the world. I'm happy when I see that people are responding to the book. Somebody told me last night at a dinner that they'd read the book and they'd told an aid organization that they support to find a village where they could support the drilling of a well to provide water and they were going to give whatever it took to drill that well. That makes me happy, that I had this impact. Obviously I've had an impact on people changing their diet too, I have people coming up to me all the time saying \"I read <a href=\"http://en.wikipedia.org/wiki/Animal_Liberation_%28book%29\">Animal Liberation</a> and I became a vegetarian or a vegan and I've been working for animal groups\". That makes me happy too, that my work has had that effect, which I think it a beneficial effect. But I don't want to pretend to you or to the BloggingHeads viewers that I'm a saint. I can be happy when I'm on vacation, I can be hiking in the mountains. I love mountain scenery, I had a vacation in <a>Glacier National Park</a> a year or so ago, which was gorgeous. That sort of thing makes me happy, and I admit that it's probably not doing as much good for the world as I could have done if instead of spending the money on that vacation I had given it to Oxfam.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Human_and_animal_pleasures\">Human and animal pleasures</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I sometimes ask myself, I struggle with this question, I ask \"are my own deepest pleasures actually quite primeval ones\", basically food, sleep, and sex. In your own writings you've emphasized, correctly, the ties between human beings and non-human animals, and it seems that for other animals these are almost always, maybe always the deepest pleasures. So I tend to think that for human beings, including ourselves, they're the deepest pleasures as well, and the higher pleasures are worth something, but actually they're somewhat of an epiphenomenon and what makes us happy are similar to what make non-humans happy. Do you have a view on this?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>I'm not sure I think that the things you mention ... food and sex are obviously important, actually sleep doesn't make me particularly happy. It's something I need to do or I feel bad, but It doesn't make me happy. But yeah, food and sex are important pleasures in life. Are they more important in my life than the things that I do in work? I wouldn't really say that. I think that food and sex are the kind of desires that get satisfied: I eat a good meal, I enjoy it but I don't want to eat again for a few hours, and even sex has its limits in how much you can do at any particular time and still want more of it. Whereas the kind of things we're talking about, you can call them \"higher pleasures\" or \"more purposive, fulfillment sort of activities\" you can just go on and find that it's better and better. So I think there's a difference in that.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>So what makes you happy is pretty different from what makes a non-human animal happy, you would say?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Yes, that's true, I think higher cognitive capacities to make a difference there.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td><br>\n<h3 id=\"Pescatarianism\">Pescatarianism</h3>\n</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Let me ask you a question about animal welfare. I have been very influenced by a lot of what you've written, but I'm also not a pure vegetarian by any means, and when it comes to morality, for instance, my view is that it's perfectly fine to eat fish. There may be practical reasons, like depleting the oceans, that are an issue, but the mere act of killing and eating a fish I don't find anything wrong with. Do you have a view on this?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>There's certainly, as you say, the environmental aspect, which is getting pretty serious with a lot of fish stocks, but the other thing is there's no humane killing of fish, right? If we buy commercially killed fish they have died pretty horrible deaths. They've suffocated in nets or on the decks of ships, or if they're deep sea fish pulled up by nets they've died of decompression, basically their internal organs exploding as they're pulled up. I would really ... I don't need to eat fish that badly that I need to do that to fish. If I was hungry and nothing else to eat I would, perhaps, do it but not given the choices I have.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>But now you're being much more the Jewish Moralist and less the Utilitarian. Because the Utilitarian would look at the marginal impact and say \"most fish die horrible deaths anyway, of malnutrition or they're eaten or something else terrible happens to them\". The marginal impact of us killing them to me seems to be basically zero. I'm not even sure a fish's life is happy, and why not just say \"it's fine to eat fish\"? Should it matter that <em>we</em> make them suffer? It's a very non-Utilitarian way of thinking about it, a very moralizing approach.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>You would need to convince me that in fact they're going to die just as horrible deaths in nature, and I'm not sure that that's true. Probably many of them would get gobbled up by some other fish, and that's probably a lot quicker than what we are doing to them.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>You have some good arguments against <a href=\"http://en.wikipedia.org/wiki/Malthusianism\">Malthusianism</a> for human beings in your book. My tendency is to think that fish are ruled by a Malthusian model, and being eaten by another fish has to be painful. Maybe it's over quickly, but having your organs burst as you're pulled up out of the water is probably also pretty quick. I would again think that in marginal terms it doesn't matter, but I'm more struck by the fact that it's not your first instinct to view the question in marginal terms. You view us as active agents and ask \"are we behaving in some manner which is moral, and you're imposing a non-Utilitarian theory on our behavior. Is that something you're willing to embrace, or something that was just a mistake?</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Look, I think economists tend to think more in terms of marginal impact than I do and you may be right that is something I may need to think about more. Look, Tyler, I have to finish unfortunately, I've got another interview I've got to go to, so it's been great talking to you, but I think we're going to have to leave it at that point.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>Ok, thank you very much.</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Thanks a lot.</td>\n</tr>\n<tr>\n<td valign=\"top\">Cowen:</td>\n<td>I've enjoyed it a great deal. Bye!</td>\n</tr>\n<tr>\n<td valign=\"top\">Singer:</td>\n<td>Ok, bye!</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>(I also posted this <a href=\"http://www.jefftk.com/news/2012-04-06.html\">on my blog</a>.)</p>", "sections": [{"title": "Immigration as an Anti-Poverty Program", "anchor": "Immigration_as_an_Anti_Poverty_Program", "level": 1}, {"title": "Changing Institutions: Greater Tax Break for True Charity", "anchor": "Changing_Institutions__Greater_Tax_Break_for_True_Charity", "level": 1}, {"title": "Millennium Villages Skepticism", "anchor": "Millennium_Villages_Skepticism", "level": 1}, {"title": "Chinese Reforms", "anchor": "Chinese_Reforms", "level": 1}, {"title": "Military Intervention", "anchor": "Military_Intervention", "level": 1}, {"title": "Colonialism", "anchor": "Colonialism", "level": 1}, {"title": "Aid without stable government", "anchor": "Aid_without_stable_government", "level": 1}, {"title": "Genetically modifying ourselves to be more moral", "anchor": "Genetically_modifying_ourselves_to_be_more_moral", "level": 1}, {"title": "Problem areas in Utilitarianism", "anchor": "Problem_areas_in_Utilitarianism", "level": 1}, {"title": "Is Utilitarianism independent?", "anchor": "Is_Utilitarianism_independent_", "level": 1}, {"title": "Peter Singer: Jewish Moralist", "anchor": "Peter_Singer__Jewish_Moralist", "level": 1}, {"title": "What charities does Peter Singer give to?", "anchor": "What_charities_does_Peter_Singer_give_to_", "level": 1}, {"title": "Zero-Overhead Giving", "anchor": "Zero_Overhead_Giving", "level": 1}, {"title": "Moral Intuitions", "anchor": "Moral_Intuitions", "level": 1}, {"title": "Improving the world through commerce", "anchor": "Improving_the_world_through_commerce", "level": 1}, {"title": "What makes Peter Singer happy?", "anchor": "What_makes_Peter_Singer_happy_", "level": 1}, {"title": "Human and animal pleasures", "anchor": "Human_and_animal_pleasures", "level": 1}, {"title": "Pescatarianism", "anchor": "Pescatarianism", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "34 comments"}], "headingsCount": 20}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T14:50:54.773Z", "modifiedAt": null, "url": null, "title": "Meetup : Twin Cities, MN (for real this time)", "slug": "meetup-twin-cities-mn-for-real-this-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7s4erYXyf3bbGvJE2/meetup-twin-cities-mn-for-real-this-time", "pageUrlRelative": "/posts/7s4erYXyf3bbGvJE2/meetup-twin-cities-mn-for-real-this-time", "linkUrl": "https://www.lesswrong.com/posts/7s4erYXyf3bbGvJE2/meetup-twin-cities-mn-for-real-this-time", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Twin%20Cities%2C%20MN%20(for%20real%20this%20time)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Twin%20Cities%2C%20MN%20(for%20real%20this%20time)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s4erYXyf3bbGvJE2%2Fmeetup-twin-cities-mn-for-real-this-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Twin%20Cities%2C%20MN%20(for%20real%20this%20time)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s4erYXyf3bbGvJE2%2Fmeetup-twin-cities-mn-for-real-this-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s4erYXyf3bbGvJE2%2Fmeetup-twin-cities-mn-for-real-this-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8q'>Twin Cities, MN (for real this time)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 April 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Purple Onion Coffeeshop, 1301 University Avenue Southeast, Minneapolis, MN</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[This meetup is really being put together by Helloses, but while his karma is just getting started I'm putting the text into a post on his behalf.]</p>\n\n<p>Hi. Let's make this work.</p>\n\n<p>Suggested discussion topics would be:</p>\n\n<ul>\n<li><p>What do we want this group to do? Rationality practice? Skill sharing? Mastermind group?</p></li>\n<li><p>Acquiring guinea pigs for the furtherance of mad science (testing Center for Modern Rationality material)</p></li>\n<li><p>Fun - what it is and how to have almost more of it than you can handle</p></li>\n</ul>\n\n<p>If you'd like to suggest a location closer to you or a different time, please comment to that effect. If you know a good coffeeshop with ample seating in Uptown or South Minneapolis, we could meet there instead. Also comment if you'd like to carpool.</p>\n\n<p>If you're even slightly interested in this, please join up or at least comment.</p>\n\n<p>Folks, let's hang out and take it from there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8q'>Twin Cities, MN (for real this time)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7s4erYXyf3bbGvJE2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.791760781863246e-07, "legacy": true, "legacyId": "14922", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Twin_Cities__MN__for_real_this_time_\">Discussion article for the meetup : <a href=\"/meetups/8q\">Twin Cities, MN (for real this time)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 April 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Purple Onion Coffeeshop, 1301 University Avenue Southeast, Minneapolis, MN</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[This meetup is really being put together by Helloses, but while his karma is just getting started I'm putting the text into a post on his behalf.]</p>\n\n<p>Hi. Let's make this work.</p>\n\n<p>Suggested discussion topics would be:</p>\n\n<ul>\n<li><p>What do we want this group to do? Rationality practice? Skill sharing? Mastermind group?</p></li>\n<li><p>Acquiring guinea pigs for the furtherance of mad science (testing Center for Modern Rationality material)</p></li>\n<li><p>Fun - what it is and how to have almost more of it than you can handle</p></li>\n</ul>\n\n<p>If you'd like to suggest a location closer to you or a different time, please comment to that effect. If you know a good coffeeshop with ample seating in Uptown or South Minneapolis, we could meet there instead. Also comment if you'd like to carpool.</p>\n\n<p>If you're even slightly interested in this, please join up or at least comment.</p>\n\n<p>Folks, let's hang out and take it from there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Twin_Cities__MN__for_real_this_time_1\">Discussion article for the meetup : <a href=\"/meetups/8q\">Twin Cities, MN (for real this time)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Twin Cities, MN (for real this time)", "anchor": "Discussion_article_for_the_meetup___Twin_Cities__MN__for_real_this_time_", "level": 1}, {"title": "Discussion article for the meetup : Twin Cities, MN (for real this time)", "anchor": "Discussion_article_for_the_meetup___Twin_Cities__MN__for_real_this_time_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T15:00:07.382Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berkeley, Madison, Melbourne, Norwich", "slug": "weekly-lw-meetups-berkeley-madison-melbourne-norwich", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:50.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oEDmzRBDu5o8dZGJo/weekly-lw-meetups-berkeley-madison-melbourne-norwich", "pageUrlRelative": "/posts/oEDmzRBDu5o8dZGJo/weekly-lw-meetups-berkeley-madison-melbourne-norwich", "linkUrl": "https://www.lesswrong.com/posts/oEDmzRBDu5o8dZGJo/weekly-lw-meetups-berkeley-madison-melbourne-norwich", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Madison%2C%20Melbourne%2C%20Norwich&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berkeley%2C%20Madison%2C%20Melbourne%2C%20Norwich%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoEDmzRBDu5o8dZGJo%2Fweekly-lw-meetups-berkeley-madison-melbourne-norwich%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Madison%2C%20Melbourne%2C%20Norwich%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoEDmzRBDu5o8dZGJo%2Fweekly-lw-meetups-berkeley-madison-melbourne-norwich", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoEDmzRBDu5o8dZGJo%2Fweekly-lw-meetups-berkeley-madison-melbourne-norwich", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 453, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/89\">First Norwich UK Meetup Sunday 1 April 11am:&nbsp;<span class=\"date\">01 April 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/87\">Brussels meetup:&nbsp;<span class=\"date\">14 April 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/8d\">Shanghai Less Wrong Meetup:&nbsp;<span class=\"date\">15 April 2012 10:36PM</span></a></li>\n<li><a href=\"/meetups/8a\">Sydney meetup - Biased pandemic and other games:&nbsp;<span class=\"date\">17 April 2012 07:30PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/8e\">Monday Madison Meetup:&nbsp;<span class=\"date\">02 April 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/8c\">Big Berkeley meetup:&nbsp;<span class=\"date\">04 April 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/86\">Melbourne, practical rationality:&nbsp;<span class=\"date\">06 April 2012 08:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oEDmzRBDu5o8dZGJo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.791799305813301e-07, "legacy": true, "legacyId": "14685", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T15:39:00.316Z", "modifiedAt": null, "url": null, "title": "Meetup : Longmont  Sparkfun Soldering Competition Field Trip", "slug": "meetup-longmont-sparkfun-soldering-competition-field-trip", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LJipNvwo7ppPzgXqi/meetup-longmont-sparkfun-soldering-competition-field-trip", "pageUrlRelative": "/posts/LJipNvwo7ppPzgXqi/meetup-longmont-sparkfun-soldering-competition-field-trip", "linkUrl": "https://www.lesswrong.com/posts/LJipNvwo7ppPzgXqi/meetup-longmont-sparkfun-soldering-competition-field-trip", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Longmont%20%20Sparkfun%20Soldering%20Competition%20Field%20Trip&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Longmont%20%20Sparkfun%20Soldering%20Competition%20Field%20Trip%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLJipNvwo7ppPzgXqi%2Fmeetup-longmont-sparkfun-soldering-competition-field-trip%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Longmont%20%20Sparkfun%20Soldering%20Competition%20Field%20Trip%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLJipNvwo7ppPzgXqi%2Fmeetup-longmont-sparkfun-soldering-competition-field-trip", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLJipNvwo7ppPzgXqi%2Fmeetup-longmont-sparkfun-soldering-competition-field-trip", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8r'>Longmont  Sparkfun Soldering Competition Field Trip</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 April 2012 11:00:00AM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1555 S Hover Rd Longmont, Colorado 80501</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Sparkfun is sponsoring a Soldering Competition from 11am to 5pm April 28 in Longmont CO.\n<a href=\"http://www.sparkfun.com/products/11126\" rel=\"nofollow\">http://www.sparkfun.com/products/11126</a>\nSoldering starts at 12.</p>\n\n<p>Still lots of spots open to enter.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8r'>Longmont  Sparkfun Soldering Competition Field Trip</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LJipNvwo7ppPzgXqi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.791961945333986e-07, "legacy": true, "legacyId": "14924", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Longmont__Sparkfun_Soldering_Competition_Field_Trip\">Discussion article for the meetup : <a href=\"/meetups/8r\">Longmont  Sparkfun Soldering Competition Field Trip</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 April 2012 11:00:00AM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1555 S Hover Rd Longmont, Colorado 80501</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Sparkfun is sponsoring a Soldering Competition from 11am to 5pm April 28 in Longmont CO.\n<a href=\"http://www.sparkfun.com/products/11126\" rel=\"nofollow\">http://www.sparkfun.com/products/11126</a>\nSoldering starts at 12.</p>\n\n<p>Still lots of spots open to enter.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Longmont__Sparkfun_Soldering_Competition_Field_Trip1\">Discussion article for the meetup : <a href=\"/meetups/8r\">Longmont  Sparkfun Soldering Competition Field Trip</a></h2>", "sections": [{"title": "Discussion article for the meetup : Longmont  Sparkfun Soldering Competition Field Trip", "anchor": "Discussion_article_for_the_meetup___Longmont__Sparkfun_Soldering_Competition_Field_Trip", "level": 1}, {"title": "Discussion article for the meetup : Longmont  Sparkfun Soldering Competition Field Trip", "anchor": "Discussion_article_for_the_meetup___Longmont__Sparkfun_Soldering_Competition_Field_Trip1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T15:49:49.621Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:57.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8wWypWTvvA7xGZFwB/meetup-fort-collins-colorado-meetup-wedneday-7pm", "pageUrlRelative": "/posts/8wWypWTvvA7xGZFwB/meetup-fort-collins-colorado-meetup-wedneday-7pm", "linkUrl": "https://www.lesswrong.com/posts/8wWypWTvvA7xGZFwB/meetup-fort-collins-colorado-meetup-wedneday-7pm", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wWypWTvvA7xGZFwB%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wWypWTvvA7xGZFwB%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wWypWTvvA7xGZFwB%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8s'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 April 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>And again with the conversational coolness.</p>\n\n<p>New skill acquisition: slack lining, yoga, soldering, negotiation.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8s'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8wWypWTvvA7xGZFwB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.79200721220752e-07, "legacy": true, "legacyId": "14925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/8s\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 April 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>And again with the conversational coolness.</p>\n\n<p>New skill acquisition: slack lining, yoga, soldering, negotiation.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/8s\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T16:11:37.513Z", "modifiedAt": null, "url": null, "title": "Meetup : Budapest Meetup", "slug": "meetup-budapest-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pGkuD3jTixcf4NWhc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hMBM8WJ3EHpTxZsQv/meetup-budapest-meetup-0", "pageUrlRelative": "/posts/hMBM8WJ3EHpTxZsQv/meetup-budapest-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/hMBM8WJ3EHpTxZsQv/meetup-budapest-meetup-0", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Budapest%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Budapest%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMBM8WJ3EHpTxZsQv%2Fmeetup-budapest-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Budapest%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMBM8WJ3EHpTxZsQv%2Fmeetup-budapest-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMBM8WJ3EHpTxZsQv%2Fmeetup-budapest-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8t'>Budapest Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 April 2012 05:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Szent Istvan ter 4-5, Budapest</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting at California Coffee Company Basilica (coffee shop), Szent Istvan ter 4-5. <a href=\"http://www.californiacoffeeco.net/?page_id=50&amp;lang=en\" rel=\"nofollow\">link</a>.</p>\n\n<p>Please come and bring friends. If you have questions, contact <a href=\"/user/katyusha\" rel=\"nofollow\">katyusha</a>.</p>\n\n<p>BudLW mailing list: <a href=\"http://groups.google.com/group/budlesswrong\">link</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8t'>Budapest Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hMBM8WJ3EHpTxZsQv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.792098394450694e-07, "legacy": true, "legacyId": "14926", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Budapest_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8t\">Budapest Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 April 2012 05:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Szent Istvan ter 4-5, Budapest</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting at California Coffee Company Basilica (coffee shop), Szent Istvan ter 4-5. <a href=\"http://www.californiacoffeeco.net/?page_id=50&amp;lang=en\" rel=\"nofollow\">link</a>.</p>\n\n<p>Please come and bring friends. If you have questions, contact <a href=\"/user/katyusha\" rel=\"nofollow\">katyusha</a>.</p>\n\n<p>BudLW mailing list: <a href=\"http://groups.google.com/group/budlesswrong\">link</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Budapest_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8t\">Budapest Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Budapest Meetup", "anchor": "Discussion_article_for_the_meetup___Budapest_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Budapest Meetup", "anchor": "Discussion_article_for_the_meetup___Budapest_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T16:42:02.277Z", "modifiedAt": null, "url": null, "title": "Meetup : London", "slug": "meetup-london-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.751Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6e7BPbq7FHQbuz6kb/meetup-london-0", "pageUrlRelative": "/posts/6e7BPbq7FHQbuz6kb/meetup-london-0", "linkUrl": "https://www.lesswrong.com/posts/6e7BPbq7FHQbuz6kb/meetup-london-0", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6e7BPbq7FHQbuz6kb%2Fmeetup-london-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6e7BPbq7FHQbuz6kb%2Fmeetup-london-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6e7BPbq7FHQbuz6kb%2Fmeetup-london-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/8u\">London</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 April 2012 02:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">LShift, Hoxton Point, 6 Rufus St, London, N1 6PE</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>First London meetup in a while, will be good to see everyone again! To talk about how we'll use the time, join the London Less Wrong mailing list: <a rel=\"nofollow\" href=\"http://groups.google.com/group/lesswronglondon\">http://groups.google.com/group/lesswronglondon</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/8u\">London</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6e7BPbq7FHQbuz6kb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.7922256142847e-07, "legacy": true, "legacyId": "14927", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London\">Discussion article for the meetup : <a href=\"/meetups/8u\">London</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 April 2012 02:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">LShift, Hoxton Point, 6 Rufus St, London, N1 6PE</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>First London meetup in a while, will be good to see everyone again! To talk about how we'll use the time, join the London Less Wrong mailing list: <a rel=\"nofollow\" href=\"http://groups.google.com/group/lesswronglondon\">http://groups.google.com/group/lesswronglondon</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___London1\">Discussion article for the meetup : <a href=\"/meetups/8u\">London</a></h2>", "sections": [{"title": "Discussion article for the meetup : London", "anchor": "Discussion_article_for_the_meetup___London", "level": 1}, {"title": "Discussion article for the meetup : London", "anchor": "Discussion_article_for_the_meetup___London1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T17:09:29.708Z", "modifiedAt": null, "url": null, "title": "Complexity based moral values.", "slug": "complexity-based-moral-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Su4cdYK2cs7gB9mgb/complexity-based-moral-values", "pageUrlRelative": "/posts/Su4cdYK2cs7gB9mgb/complexity-based-moral-values", "linkUrl": "https://www.lesswrong.com/posts/Su4cdYK2cs7gB9mgb/complexity-based-moral-values", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complexity%20based%20moral%20values.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplexity%20based%20moral%20values.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSu4cdYK2cs7gB9mgb%2Fcomplexity-based-moral-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complexity%20based%20moral%20values.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSu4cdYK2cs7gB9mgb%2Fcomplexity-based-moral-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSu4cdYK2cs7gB9mgb%2Fcomplexity-based-moral-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 730, "htmlBody": "<p><strong>Preface:</strong> <em>I am just noting that we people seem to be basing our morality on some rather ill defined intuitive notion of complexity. If you think it is not workable for AI, or something like that, such thought clearly does not yet constitute a disagreement with what I am writing here.</em></p>\n<p><strong>More preface:</strong> The utilitarian calculus is an idea that what people value is described simply in terms of <em>summation</em>. The complexity is another kind of f(a,b,c,d) that behaves vaguely like a 'sum' , but is not as simple as summation. If the a,b,c,d are strings, and it is a programming language, the above expression would often be written like f(a+b+c+d) , using + to mean concatenation, while it is something very fundamentally different from summation of real valued numbers. But it can appear confusingly close, as for a,b,c,d that don't share a lot of information among themselves, the result will behave a lot like a function on sum of real numbers. It will, however, diverge from the sum like behaviour as the a,b,c,d share more information among themselves, much in similar to how our intuitions for what is right diverge from sum like behaviour when you start considering exact duplicates of people, which only diverged for a few minutes.</p>\n<p>It's a very rough idea, but it seems to me that a lot of common sense moral values are based on some sort of intuitive notion of complexity. Happiness via highly complex stimuli that pass through highly complex neural circuitry inside your head seems like a good thing to pursue; happiness via wire, resistor, and battery seems like a bad thing. What makes the idea of literal wireheading and hard pleasure inducing drugs so revolting for me, is the simplicity, banality of it. I have much fewer objections to e.g. hallucinogens (never took any myself but I am also an artist and I can guess that other people may have lower levels of certain neurotransmitters, making them unable to imagine what I can imagine).</p>\n<p>The complexity based metrics have a property that they easily eat for breakfast huge numbers like \"a dust speck in the 3^^^3&nbsp; eyes\", and even the infinity. The <a href=\"/lw/kn/torture_vs_dust_specks/\">torture of a conscious being for a long period of time</a> can easily be more complex issue than even the infinite number of dust specks.</p>\n<p>Unfortunately, the complexity metrics like Kolmogorov's complexity are noncomputable on arbitrary input, and are big for truly random values. But in so much as the scenario is specific and has been arrived at by computation, there is this computation's complexity which sets an upper bound on complexity of scenario. The mathematics may also be not here yet. We have the intuitive notion of complexity where the totally random noise is not very complex, the very regular signal is not either, but some forms of patterns are highly complex.</p>\n<p>This may be difficult to formalize. We could of course only define the complexities when we are informed of properties of something, but can not compute them for arbitrary input from scratch; if we map something as 'random numbers', the complexity is low; if it is encrypted volumes of works of Shakespeare, even though we wouldn't be able to distinguish that from random in practice (assuming good encryption), as we are told what it is, we can assign it higher complexity.</p>\n<p>This also aligns with what ever it is that the evolution has been maximizing on the path leading up to H. Sapiens (Note that for the most part, evolution's power gone into improving the bacteria; the path leading up H. Sapiens is a very special case). Maybe we for some reason try to extrapolate this [note: for example, a lot of people rank their preference of animals as food by the animal's complexity of behaviours, which makes the human least desirable food; we have anti-whaling treaties], maybe it is a form of goal convergence between brain as intelligent system, and evolution (both employ hill climbing to arrive at solutions), or maybe we evolved the system that aligns with where evolution was heading because that increased fitness [edit: to address possible comment, we have another system based on evolution - the immune system - it works by evolving the antigens using somatic hypermutation; it's not inconceivable that we use some evolution-like mechanism to tweak our own neural circuitry, given that our circuitry does undergo massive pruning in early stages of life].</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Su4cdYK2cs7gB9mgb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -12, "extendedScore": null, "score": 8.792340473579203e-07, "legacy": true, "legacyId": "14928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T18:59:35.787Z", "modifiedAt": null, "url": null, "title": "Decision Theories: A Semi-Formal Analysis, Part II", "slug": "decision-theories-a-semi-formal-analysis-part-ii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:05.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TxDcvtn2teAMobG2Z/decision-theories-a-semi-formal-analysis-part-ii", "pageUrlRelative": "/posts/TxDcvtn2teAMobG2Z/decision-theories-a-semi-formal-analysis-part-ii", "linkUrl": "https://www.lesswrong.com/posts/TxDcvtn2teAMobG2Z/decision-theories-a-semi-formal-analysis-part-ii", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTxDcvtn2teAMobG2Z%2Fdecision-theories-a-semi-formal-analysis-part-ii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTxDcvtn2teAMobG2Z%2Fdecision-theories-a-semi-formal-analysis-part-ii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTxDcvtn2teAMobG2Z%2Fdecision-theories-a-semi-formal-analysis-part-ii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2121, "htmlBody": "<h3>Or: Causal Decision Theory and Substitution<br /></h3>\n<p><strong>Previously:</strong></p>\n<p>0. <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer\" target=\"_self\">Decision Theories: A Less Wrong Primer</a><br />1. <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i\" target=\"_self\">The Problem with Naive Decision Theory</a></p>\n<p><strong>Summary of Post: </strong><em>We explore the role of substitution in avoiding spurious counterfactuals, introduce an implementation of Causal Decision Theory and a CliqueBot, and set off in the direction of Timeless Decision Theory.</em></p>\n<p>In the last post, we showed the problem with what we termed Naive Decision Theory, which attempts to prove counterfactuals directly and pick the best action: there's a possibility of spurious counterfactuals which lead to terrible decisions. We'll want to implement a decision theory that does better; one that is, by any practical definition of the words, <a href=\"http://en.wikipedia.org/wiki/HAL_9000\" target=\"_blank\">foolproof and incapable of error...</a></p>\n<p><img src=\"http://images.lesswrong.com/t3_az6_0.png?v=90de722d76f435cd93360dd8cb499b68\" alt=\"\" width=\"512\" height=\"384\" /></p>\n<p>I know you're eager to get to Timeless Decision Theory and the others. <a href=\"http://www.imdb.com/character/ch0002900/quotes\" target=\"_blank\">I'm sorry, but I'm afraid I can't do that just yet. This background is too important for me to allow you to skip it...</a></p>\n<p>Over the next few posts, we'll create a sequence of decision theories, each of which will outperform the previous ones (the new ones will do better in some games, without doing worse in others<a href=\"#strict\" target=\"_self\"><sup>0</sup></a>) in a wide range of plausible games.</p>\n<h3><a id=\"more\"></a></h3>\n<p>Let's formalize the setup a bit more: we've written a program <strong>X</strong>, which is paired with another program <strong>Y</strong> in a game <strong>G</strong>. Both <strong>X</strong> and <strong>Y</strong> have access to the source codes of <strong>X</strong>, <strong>Y</strong>, and <strong>G</strong>, and <strong>G</strong> calculates the payouts to the programmers of <strong>X</strong> and of <strong>Y</strong> in terms of their outputs alone. That is, <strong>X</strong> and <strong>Y</strong> should be functions of two inputs, and <strong>G</strong> should do the following:</p>\n<ul>\n<li>Calculate the output of <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Y</strong>).<a href=\"#player1\" target=\"_self\"><sup>1</sup></a></li>\n<li>Calculate the output of <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>).</li>\n<li>From these values, calculate the payoffs to the programmers.</li>\n</ul>\n<p>We write the expected payout to <strong>X</strong>'s programmer as <strong>U</strong>, by which we mean <strong>U</strong>(output <strong>X</strong>, output <strong>Y</strong>), by which we really mean <strong>U</strong>(output <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Y</strong>), output <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>)); similarly, we'll write <strong>V</strong> as the expected payout to <strong>Y</strong>'s programmer. We'll need to use <a href=\"http://en.wikipedia.org/wiki/Quine_%28computing%29\" target=\"_blank\">quining</a> in order for <strong>G</strong> to feed its own source code into <strong>X</strong> and <strong>Y</strong>, but this is straightforward. (I said earlier that <strong>X</strong> should have access to its own source code as well, but the programmer can do this via quining, with no help from <strong>G</strong>.)</p>\n<p>We're also assuming that <strong>X</strong> is equipped with a valid inference module that deduces <em>some</em> true statements and <em>no</em> false ones; it can return \"true\", \"false\" or \"unknown\" when given a statement. (One such example is an automated theorem prover which gives up after a certain length of time.)</p>\n<p>Last time, we tried the most straightforward idea&mdash;trying to directly prove counterfactuals about the value of <strong>U</strong> for various outputs of <strong>X</strong>&mdash;but we ran into the problem of <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/#spurious\" target=\"_blank\">spurious counterfactuals</a>, which are logically true but practically misleading. The various decision theories we'll discuss each represent different ways of getting around this obstacle.</p>\n<h3>Idea 3: Substitute to Avoid Self-Fulfilling Prophecies<br /></h3>\n<p>Our spurious counterfactuals were harmful because of the possibility of a self-fulfilling prediction; one way to avoid such a thing is to base your output only on deductions about statements that are in some sense \"independent\" of the final output. We'll illustrate this principle by doing a simple task that Naive Decision Theory <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/#spurious\" target=\"_blank\">failed to do</a>: infer the correct payoff matrix for <strong>G</strong> from its source code.</p>\n<p>For each pair of outputs (<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>), we can simply substitute <em>x<sub>i</sub></em> for (output <strong>X</strong>) and <em>y<sub>j</sub></em> for (output <strong>Y</strong>) in the code for <strong>G</strong>, obtaining <em>unconditional</em> deductions of <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) and <strong>V</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>).<a href=\"#nash\" target=\"_self\"><sup>2</sup></a> Note that this is <em>not</em> the counterfactual \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> and (output <strong>Y</strong>)=<em>y<sub>j</sub></em> then <strong>U</strong>=<em>u<sub>ij</sub></em>\", but instead a simple mathematical object that doesn't depend on the actual values of (output <strong>X</strong>) and (output <strong>Y</strong>)!</p>\n<p>This is a powerful enough principle that, if we use it properly, we arrive at a formalized version of the hallowed classical theory:</p>\n<h3>Causal Decision Theory: No Longer Naive<br /></h3>\n<p><strong>X</strong> can always simply play a default Nash equilibrium strategy, but there's an obvious case where we want <strong>X</strong> to depart from it: if the output of <strong>Y</strong> is predictable, <strong>X</strong> can often gain more by stepping away from its equilibrium to <a href=\"http://www.youtube.com/watch?v=NMxzU6hxrNA\" target=\"_blank\">exploit <strong>Y</strong>'s stupidity</a> (or <a href=\"http://en.wikipedia.org/wiki/Coordination_game\" target=\"_blank\">coordinate with it</a> on a different equilibrium). We might program <strong>X</strong> as follows:</p>\n<ul>\n<li>Try to deduce the output of <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>); note that this may be a mixed strategy.</li>\n<li>If this succeeds, output the <em>x<sub>i</sub></em> which maximizes <strong>U</strong>(<em>x<sub>i</sub></em>,(output <strong>Y</strong>)). (There must be at least one pure strategy which does so.)</li>\n<li>Otherwise, output a default Nash equilibrium.</li>\n</ul>\n<p>Note that even though there is some circularity involved&mdash;<strong>Y</strong> may base its output on the code of <strong>X</strong>&mdash;this isn't like a spurious counterfactual, because the deduction of <strong>U</strong>(<em>x<sub>i</sub></em>,(output <strong>Y</strong>)) is independent of the output of <strong>X</strong>. In particular, if <strong>X</strong> succeeds at deducing the output of <strong>Y</strong>, then that really will be the output of <strong>Y</strong> in the game (presuming, as usual, that the inference module of <strong>X</strong> is valid).</p>\n<p><a name=\"noexploit\"></a>This decision theory acts optimally in \"one-player games\" where the output of <strong>Y</strong> is independent of the source code of <strong>X</strong> (and can be deduced by <strong>X</strong>'s inference module), and in zero-sum two-player games as well. Finally, <strong>X</strong> is un-exploitable in the following sense: if (<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) is the Nash equilibrium with the lowest value for <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>), then <strong>U</strong>(<strong>X</strong>,<strong>Y</strong>) &ge; <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) or <strong>V</strong>(<strong>X</strong>,<strong>Y</strong>) &le; <strong>V</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>). (Namely, if <strong>Y</strong> is trying to maximize its own payoff, then <strong>X</strong> should wind up with at least the value of its worst Nash equilibrium.)</p>\n<p>In a philosophical sense, <strong>X</strong> does \"consider\" <strong>Y</strong>'s inference of (output <strong>X</strong>) as a factor in deducing (output <strong>Y</strong>), but it considers it as if <strong>Y</strong> were instead inferring the output of some unrelated program, and in particular there's no \"feedback\" from this consideration to the second step. <strong>X</strong> acts as if its own eventual output cannot cause <strong>Y</strong>'s output; that is, we've written an instance of Causal Decision Theory.<a href=\"#variant\"><sup>3</sup></a></p>\n<p>We can also see that if a <a href=\"http://en.wikipedia.org/wiki/Dominated_strategy#Terminology\" target=\"_blank\">dominant strategy</a> exists (like two-boxing in Newcomb's Problem<a href=\"#newcomb\"><sup>4</sup></a> or defecting in the Prisoner's Dilemma), this CDT algorithm will always pick it. We should ask at this point: in our situation with perfect knowledge of source codes, can we do better than the classical solution?</p>\n<h3>Cliquish Decision Theory: Better, but Not Good Enough<br /></h3>\n<p>We can, of course. There's at least one case where defecting on the Prisoner's Dilemma is clearly a bad idea: if the source code of <strong>Y</strong> is identical to the source code of <strong>X</strong>. So we can upgrade <strong>X</strong> by adding an additional rule: If the payoff matrix is a prisoner's dilemma (a <a href=\"http://en.wikipedia.org/wiki/Prisoner%27s_dilemma#Generalized_form\" target=\"_blank\">well-defined subset</a> of payoff matrices), and the code of <strong>Y</strong> equals the code of <strong>X</strong>, then cooperate. (I've <a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies\" target=\"_blank\">called algorithms like this CliqueBots</a> before, since they cooperate only with themselves and defect against everyone else.)</p>\n<p>Furthermore, we can extend this to cases where the source codes are different but equivalent; let <strong>X</strong> do the following when given a game <strong>G</strong> and an opponent <strong>Y</strong>:</p>\n<ul>\n<li>Deduce the payoff matrix of <strong>G</strong>. If <strong>G</strong> is not a prisoner's dilemma, then implement Causal Decision Theory.</li>\n<li>If <strong>G</strong> is a prisoner's dilemma, try to deduce \"for all opponents <strong>Z</strong>, output <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Z</strong>)=<em>Cooperate</em> if and only if output <strong>Y</strong>(code <strong>G</strong>, 1, code <strong>Z</strong>)=<em>Cooperate</em> if and only if output <strong>X</strong>(code <strong>G</strong>, 2, code <strong>Z</strong>)=<em>Cooperate</em> if and only if output <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>Z</strong>)=<em>Cooperate</em>\".<a href=\"#simplify\" target=\"_self\"><sup>5</sup></a></li>\n<li>If this succeeds, then cooperate; else defect.</li>\n</ul>\n<p>This now satisfies #1-3 of my <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/#dobetter\" target=\"_blank\">criteria for a better decision theory</a> (and we could add another rule so that it one-boxes on Newcomblike problems and satisfies #4), but it completely fails #5: this is a well-justified ad-hoc exception, but it's still an ad-hoc exception, and it's not that enlightening as examples go. We'll want something that achieves the same result, but for a genuine reason rather than <a href=\"/lw/b7v/common_mistakes_people_make_when_thinking_about/65f8\" target=\"_blank\">because we've added individual common-sense patches</a>. (One problem with patches is that they don't tell you how to solve more complicated problems- extending the analysis to three-player games, for instance.)</p>\n<p>Furthermore, Cliquish Decision Theory misses out on a lot of potential cooperation: there are multiple strict improvements on Causal Decision Theory which cooperate with each other but don't all cooperate with the same third parties, and it's simple to show that Cliquish Decision Theory can't cooperate with any of these. So we'll need new ideas if we're going to do better.</p>\n<p>Our next steps will be toward Timeless Decision Theory. If you've <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\" target=\"_blank\">considered TDT on the philosophical level</a>, you'll recognize the concept of substituting your (hypothetical) output for your opponent's (hypothetical) prediction of your output, and using <em>those</em> outcomes to decide what to do. Our setup gives us a clean general way to implement this messy idea:</p>\n<h3>Idea 4: Substitute for the Source Code of X (as an input of Y)<br /></h3>\n<p>That is, instead of trying to deduce the output of <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>) and then pair it with each action <em>x<sub>i</sub></em>, we can deduce the output of <strong>Y</strong>(code <strong>G</strong>, 2, <em>some other code that corresponds to x<sub>i</sub></em>) and pair it with the deduced action. I'm being vague about this, partly to generate some suspense for the next post, but mostly because the most obvious way to go about it has two major flaws...</p>\n<p><strong>Next:</strong> <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\" target=\"_self\">Part III: Formalizing Timeless Decision Theory</a></p>\n<h3>Notes</h3>\n<p><strong><a name=\"strict\"></a>0.</strong> This is not quite true for Updateless Decision Theory, which explicitly trades off worse performance in some rounds for a better average performance. But we'll get to that in due time.</p>\n<p>Also, it's possible to write an opponent <strong>Y</strong> that \"rewards stupidity\" by caring about things besides <strong>X</strong>'s outputs: it might let <strong>X</strong> attain its best outcome only if <strong>X</strong> is provably a causal decision theorist, for instance. But in such cases, <strong>Y</strong> isn't really optimizing its own payoffs (as listed) in any meaningful sense, and so this is not a fair problem; we're tacitly assuming throughout this analysis that <strong>X</strong> is a real attempt to optimize <strong>U</strong> and <strong>Y</strong> is a real attempt to optimize <strong>V</strong>, and both of these depend only on the outputs.</p>\n<p><strong><a name=\"player1\"></a>1.</strong> <strong>X</strong> and <strong>Y</strong> need to know which is player 1 in game <strong>G</strong> and which is player 2. Also, the steps of <strong>G</strong> don't have to be done in that sequential order- they can be calculated in parallel or any other sensible way. (In Part III we'll introduce an interesting game which first flips a coin to decide which one is Player 1 and which one is Player 2.)</p>\n<p><strong><a name=\"nash\"></a>2.</strong> In fact, the correct payoff matrix is necessary even to implement the Nash equilbrium strategy! I glossed over this before by assuming that, in addition to the source code, <strong>X</strong> was given a valid payoff matrix; but here, we see that there is a sensible way to read off the matrix from the code of <strong>G</strong> (if the inference module of <strong>X</strong> is up to the task). We'll assume from here on out that this is the case.</p>\n<p><strong><a name=\"variant\"></a>3.</strong> There are other variants of this CDT algorithm: for instance, <strong>X</strong> might only output argmax(<strong>U</strong>(<em>x</em>,(output <strong>Y</strong>))) if the value is greater than the value of <strong>X</strong>'s default Nash equilibrium. This algorithm can \"insist on a good result or else no deal\"; for instance, in the game below, if <strong>X</strong> takes as default the equilibrium that nets xer 2, the new algorithm will refuse to settle for 1, when the old algorithm would often have done so. <br /> \n<table border=\"1\">\n<tbody>\n<tr>\n<td>(2,1)</td>\n<td>(0,0)</td>\n</tr>\n<tr>\n<td>(0,0)</td>\n<td>(1,2)</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>On the other hand, when playing an opponent that always picks the second column (i.e. a one-player game), this new algorithm will wind up with nothing! So there's a tradeoff involved, where other algorithms can get (in many cases) the best of both worlds.</p>\n<ul>\n</ul>\n<p><strong><a name=\"newcomb\"></a>4.</strong> Maybe the best way to implement Newcomb's Problem in this setting is to introduce the game <strong>N</strong> as follows (<strong>X</strong> chooses the row and gets the first listed payoff):</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>(100,1)</td>\n<td>(0,0)</td>\n</tr>\n<tr>\n<td>(101,0)</td>\n<td>(1,1)</td>\n</tr>\n</tbody>\n</table>\n<p>Your opponent is the predictor <strong>P</strong>, which is programmed as follows (when the game is <strong>N</strong>):</p>\n<ul>\n<li>Try to deduce \"output <strong>X</strong>(code <strong>N</strong>, code <strong>P</strong>)=row 1\".</li>\n<li>If this succeeds, output column 1; else output column 2.</li>\n</ul>\n<p>If you expect to face <strong>N</strong> against <strong>P</strong>, then you have every incentive to make it an easy deduction that output <strong>X</strong>(code <strong>N</strong>, code <strong>P</strong>)=1.</p>\n<p><strong><a name=\"simplify\"></a>5.</strong> There may be a better way to do this, but most simplifications will fall short of CDT in one way or another. For instance, if we just have <strong>X</strong> try to deduce \"output <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Y</strong>)=output <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>)\", then it's quite possible for <strong>X</strong> to wind up cooperating with a CooperateBot, which is suboptimal performance on a one-player game. (A proof of that statement can be generated by L&ouml;b's Theorem.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "5f5c37ee1b5cdee568cfb1db": 2, "YyGDbZhGtws5hEPda": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TxDcvtn2teAMobG2Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 25, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "14226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Or__Causal_Decision_Theory_and_Substitution\">Or: Causal Decision Theory and Substitution<br></h3>\n<p><strong id=\"Previously_\">Previously:</strong></p>\n<p>0. <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer\" target=\"_self\">Decision Theories: A Less Wrong Primer</a><br>1. <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i\" target=\"_self\">The Problem with Naive Decision Theory</a></p>\n<p><strong>Summary of Post: </strong><em>We explore the role of substitution in avoiding spurious counterfactuals, introduce an implementation of Causal Decision Theory and a CliqueBot, and set off in the direction of Timeless Decision Theory.</em></p>\n<p>In the last post, we showed the problem with what we termed Naive Decision Theory, which attempts to prove counterfactuals directly and pick the best action: there's a possibility of spurious counterfactuals which lead to terrible decisions. We'll want to implement a decision theory that does better; one that is, by any practical definition of the words, <a href=\"http://en.wikipedia.org/wiki/HAL_9000\" target=\"_blank\">foolproof and incapable of error...</a></p>\n<p><img src=\"http://images.lesswrong.com/t3_az6_0.png?v=90de722d76f435cd93360dd8cb499b68\" alt=\"\" width=\"512\" height=\"384\"></p>\n<p>I know you're eager to get to Timeless Decision Theory and the others. <a href=\"http://www.imdb.com/character/ch0002900/quotes\" target=\"_blank\">I'm sorry, but I'm afraid I can't do that just yet. This background is too important for me to allow you to skip it...</a></p>\n<p>Over the next few posts, we'll create a sequence of decision theories, each of which will outperform the previous ones (the new ones will do better in some games, without doing worse in others<a href=\"#strict\" target=\"_self\"><sup>0</sup></a>) in a wide range of plausible games.</p>\n<h3><a id=\"more\"></a></h3>\n<p>Let's formalize the setup a bit more: we've written a program <strong>X</strong>, which is paired with another program <strong>Y</strong> in a game <strong>G</strong>. Both <strong>X</strong> and <strong>Y</strong> have access to the source codes of <strong>X</strong>, <strong>Y</strong>, and <strong>G</strong>, and <strong>G</strong> calculates the payouts to the programmers of <strong>X</strong> and of <strong>Y</strong> in terms of their outputs alone. That is, <strong>X</strong> and <strong>Y</strong> should be functions of two inputs, and <strong>G</strong> should do the following:</p>\n<ul>\n<li>Calculate the output of <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Y</strong>).<a href=\"#player1\" target=\"_self\"><sup>1</sup></a></li>\n<li>Calculate the output of <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>).</li>\n<li>From these values, calculate the payoffs to the programmers.</li>\n</ul>\n<p>We write the expected payout to <strong>X</strong>'s programmer as <strong>U</strong>, by which we mean <strong>U</strong>(output <strong>X</strong>, output <strong>Y</strong>), by which we really mean <strong>U</strong>(output <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Y</strong>), output <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>)); similarly, we'll write <strong>V</strong> as the expected payout to <strong>Y</strong>'s programmer. We'll need to use <a href=\"http://en.wikipedia.org/wiki/Quine_%28computing%29\" target=\"_blank\">quining</a> in order for <strong>G</strong> to feed its own source code into <strong>X</strong> and <strong>Y</strong>, but this is straightforward. (I said earlier that <strong>X</strong> should have access to its own source code as well, but the programmer can do this via quining, with no help from <strong>G</strong>.)</p>\n<p>We're also assuming that <strong>X</strong> is equipped with a valid inference module that deduces <em>some</em> true statements and <em>no</em> false ones; it can return \"true\", \"false\" or \"unknown\" when given a statement. (One such example is an automated theorem prover which gives up after a certain length of time.)</p>\n<p>Last time, we tried the most straightforward idea\u2014trying to directly prove counterfactuals about the value of <strong>U</strong> for various outputs of <strong>X</strong>\u2014but we ran into the problem of <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/#spurious\" target=\"_blank\">spurious counterfactuals</a>, which are logically true but practically misleading. The various decision theories we'll discuss each represent different ways of getting around this obstacle.</p>\n<h3 id=\"Idea_3__Substitute_to_Avoid_Self_Fulfilling_Prophecies\">Idea 3: Substitute to Avoid Self-Fulfilling Prophecies<br></h3>\n<p>Our spurious counterfactuals were harmful because of the possibility of a self-fulfilling prediction; one way to avoid such a thing is to base your output only on deductions about statements that are in some sense \"independent\" of the final output. We'll illustrate this principle by doing a simple task that Naive Decision Theory <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/#spurious\" target=\"_blank\">failed to do</a>: infer the correct payoff matrix for <strong>G</strong> from its source code.</p>\n<p>For each pair of outputs (<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>), we can simply substitute <em>x<sub>i</sub></em> for (output <strong>X</strong>) and <em>y<sub>j</sub></em> for (output <strong>Y</strong>) in the code for <strong>G</strong>, obtaining <em>unconditional</em> deductions of <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) and <strong>V</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>).<a href=\"#nash\" target=\"_self\"><sup>2</sup></a> Note that this is <em>not</em> the counterfactual \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> and (output <strong>Y</strong>)=<em>y<sub>j</sub></em> then <strong>U</strong>=<em>u<sub>ij</sub></em>\", but instead a simple mathematical object that doesn't depend on the actual values of (output <strong>X</strong>) and (output <strong>Y</strong>)!</p>\n<p>This is a powerful enough principle that, if we use it properly, we arrive at a formalized version of the hallowed classical theory:</p>\n<h3 id=\"Causal_Decision_Theory__No_Longer_Naive\">Causal Decision Theory: No Longer Naive<br></h3>\n<p><strong>X</strong> can always simply play a default Nash equilibrium strategy, but there's an obvious case where we want <strong>X</strong> to depart from it: if the output of <strong>Y</strong> is predictable, <strong>X</strong> can often gain more by stepping away from its equilibrium to <a href=\"http://www.youtube.com/watch?v=NMxzU6hxrNA\" target=\"_blank\">exploit <strong>Y</strong>'s stupidity</a> (or <a href=\"http://en.wikipedia.org/wiki/Coordination_game\" target=\"_blank\">coordinate with it</a> on a different equilibrium). We might program <strong>X</strong> as follows:</p>\n<ul>\n<li>Try to deduce the output of <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>); note that this may be a mixed strategy.</li>\n<li>If this succeeds, output the <em>x<sub>i</sub></em> which maximizes <strong>U</strong>(<em>x<sub>i</sub></em>,(output <strong>Y</strong>)). (There must be at least one pure strategy which does so.)</li>\n<li>Otherwise, output a default Nash equilibrium.</li>\n</ul>\n<p>Note that even though there is some circularity involved\u2014<strong>Y</strong> may base its output on the code of <strong>X</strong>\u2014this isn't like a spurious counterfactual, because the deduction of <strong>U</strong>(<em>x<sub>i</sub></em>,(output <strong>Y</strong>)) is independent of the output of <strong>X</strong>. In particular, if <strong>X</strong> succeeds at deducing the output of <strong>Y</strong>, then that really will be the output of <strong>Y</strong> in the game (presuming, as usual, that the inference module of <strong>X</strong> is valid).</p>\n<p><a name=\"noexploit\"></a>This decision theory acts optimally in \"one-player games\" where the output of <strong>Y</strong> is independent of the source code of <strong>X</strong> (and can be deduced by <strong>X</strong>'s inference module), and in zero-sum two-player games as well. Finally, <strong>X</strong> is un-exploitable in the following sense: if (<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) is the Nash equilibrium with the lowest value for <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>), then <strong>U</strong>(<strong>X</strong>,<strong>Y</strong>) \u2265 <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) or <strong>V</strong>(<strong>X</strong>,<strong>Y</strong>) \u2264 <strong>V</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>). (Namely, if <strong>Y</strong> is trying to maximize its own payoff, then <strong>X</strong> should wind up with at least the value of its worst Nash equilibrium.)</p>\n<p>In a philosophical sense, <strong>X</strong> does \"consider\" <strong>Y</strong>'s inference of (output <strong>X</strong>) as a factor in deducing (output <strong>Y</strong>), but it considers it as if <strong>Y</strong> were instead inferring the output of some unrelated program, and in particular there's no \"feedback\" from this consideration to the second step. <strong>X</strong> acts as if its own eventual output cannot cause <strong>Y</strong>'s output; that is, we've written an instance of Causal Decision Theory.<a href=\"#variant\"><sup>3</sup></a></p>\n<p>We can also see that if a <a href=\"http://en.wikipedia.org/wiki/Dominated_strategy#Terminology\" target=\"_blank\">dominant strategy</a> exists (like two-boxing in Newcomb's Problem<a href=\"#newcomb\"><sup>4</sup></a> or defecting in the Prisoner's Dilemma), this CDT algorithm will always pick it. We should ask at this point: in our situation with perfect knowledge of source codes, can we do better than the classical solution?</p>\n<h3 id=\"Cliquish_Decision_Theory__Better__but_Not_Good_Enough\">Cliquish Decision Theory: Better, but Not Good Enough<br></h3>\n<p>We can, of course. There's at least one case where defecting on the Prisoner's Dilemma is clearly a bad idea: if the source code of <strong>Y</strong> is identical to the source code of <strong>X</strong>. So we can upgrade <strong>X</strong> by adding an additional rule: If the payoff matrix is a prisoner's dilemma (a <a href=\"http://en.wikipedia.org/wiki/Prisoner%27s_dilemma#Generalized_form\" target=\"_blank\">well-defined subset</a> of payoff matrices), and the code of <strong>Y</strong> equals the code of <strong>X</strong>, then cooperate. (I've <a href=\"/lw/767/decision_theory_paradox_pd_with_three_implies\" target=\"_blank\">called algorithms like this CliqueBots</a> before, since they cooperate only with themselves and defect against everyone else.)</p>\n<p>Furthermore, we can extend this to cases where the source codes are different but equivalent; let <strong>X</strong> do the following when given a game <strong>G</strong> and an opponent <strong>Y</strong>:</p>\n<ul>\n<li>Deduce the payoff matrix of <strong>G</strong>. If <strong>G</strong> is not a prisoner's dilemma, then implement Causal Decision Theory.</li>\n<li>If <strong>G</strong> is a prisoner's dilemma, try to deduce \"for all opponents <strong>Z</strong>, output <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Z</strong>)=<em>Cooperate</em> if and only if output <strong>Y</strong>(code <strong>G</strong>, 1, code <strong>Z</strong>)=<em>Cooperate</em> if and only if output <strong>X</strong>(code <strong>G</strong>, 2, code <strong>Z</strong>)=<em>Cooperate</em> if and only if output <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>Z</strong>)=<em>Cooperate</em>\".<a href=\"#simplify\" target=\"_self\"><sup>5</sup></a></li>\n<li>If this succeeds, then cooperate; else defect.</li>\n</ul>\n<p>This now satisfies #1-3 of my <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/#dobetter\" target=\"_blank\">criteria for a better decision theory</a> (and we could add another rule so that it one-boxes on Newcomblike problems and satisfies #4), but it completely fails #5: this is a well-justified ad-hoc exception, but it's still an ad-hoc exception, and it's not that enlightening as examples go. We'll want something that achieves the same result, but for a genuine reason rather than <a href=\"/lw/b7v/common_mistakes_people_make_when_thinking_about/65f8\" target=\"_blank\">because we've added individual common-sense patches</a>. (One problem with patches is that they don't tell you how to solve more complicated problems- extending the analysis to three-player games, for instance.)</p>\n<p>Furthermore, Cliquish Decision Theory misses out on a lot of potential cooperation: there are multiple strict improvements on Causal Decision Theory which cooperate with each other but don't all cooperate with the same third parties, and it's simple to show that Cliquish Decision Theory can't cooperate with any of these. So we'll need new ideas if we're going to do better.</p>\n<p>Our next steps will be toward Timeless Decision Theory. If you've <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\" target=\"_blank\">considered TDT on the philosophical level</a>, you'll recognize the concept of substituting your (hypothetical) output for your opponent's (hypothetical) prediction of your output, and using <em>those</em> outcomes to decide what to do. Our setup gives us a clean general way to implement this messy idea:</p>\n<h3 id=\"Idea_4__Substitute_for_the_Source_Code_of_X__as_an_input_of_Y_\">Idea 4: Substitute for the Source Code of X (as an input of Y)<br></h3>\n<p>That is, instead of trying to deduce the output of <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>) and then pair it with each action <em>x<sub>i</sub></em>, we can deduce the output of <strong>Y</strong>(code <strong>G</strong>, 2, <em>some other code that corresponds to x<sub>i</sub></em>) and pair it with the deduced action. I'm being vague about this, partly to generate some suspense for the next post, but mostly because the most obvious way to go about it has two major flaws...</p>\n<p><strong>Next:</strong> <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\" target=\"_self\">Part III: Formalizing Timeless Decision Theory</a></p>\n<h3 id=\"Notes\">Notes</h3>\n<p><strong><a name=\"strict\"></a>0.</strong> This is not quite true for Updateless Decision Theory, which explicitly trades off worse performance in some rounds for a better average performance. But we'll get to that in due time.</p>\n<p>Also, it's possible to write an opponent <strong>Y</strong> that \"rewards stupidity\" by caring about things besides <strong>X</strong>'s outputs: it might let <strong>X</strong> attain its best outcome only if <strong>X</strong> is provably a causal decision theorist, for instance. But in such cases, <strong>Y</strong> isn't really optimizing its own payoffs (as listed) in any meaningful sense, and so this is not a fair problem; we're tacitly assuming throughout this analysis that <strong>X</strong> is a real attempt to optimize <strong>U</strong> and <strong>Y</strong> is a real attempt to optimize <strong>V</strong>, and both of these depend only on the outputs.</p>\n<p><strong><a name=\"player1\"></a>1.</strong> <strong>X</strong> and <strong>Y</strong> need to know which is player 1 in game <strong>G</strong> and which is player 2. Also, the steps of <strong>G</strong> don't have to be done in that sequential order- they can be calculated in parallel or any other sensible way. (In Part III we'll introduce an interesting game which first flips a coin to decide which one is Player 1 and which one is Player 2.)</p>\n<p><strong><a name=\"nash\"></a>2.</strong> In fact, the correct payoff matrix is necessary even to implement the Nash equilbrium strategy! I glossed over this before by assuming that, in addition to the source code, <strong>X</strong> was given a valid payoff matrix; but here, we see that there is a sensible way to read off the matrix from the code of <strong>G</strong> (if the inference module of <strong>X</strong> is up to the task). We'll assume from here on out that this is the case.</p>\n<p><strong><a name=\"variant\"></a>3.</strong> There are other variants of this CDT algorithm: for instance, <strong>X</strong> might only output argmax(<strong>U</strong>(<em>x</em>,(output <strong>Y</strong>))) if the value is greater than the value of <strong>X</strong>'s default Nash equilibrium. This algorithm can \"insist on a good result or else no deal\"; for instance, in the game below, if <strong>X</strong> takes as default the equilibrium that nets xer 2, the new algorithm will refuse to settle for 1, when the old algorithm would often have done so. <br> \n</p><table border=\"1\">\n<tbody>\n<tr>\n<td>(2,1)</td>\n<td>(0,0)</td>\n</tr>\n<tr>\n<td>(0,0)</td>\n<td>(1,2)</td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>On the other hand, when playing an opponent that always picks the second column (i.e. a one-player game), this new algorithm will wind up with nothing! So there's a tradeoff involved, where other algorithms can get (in many cases) the best of both worlds.</p>\n<ul>\n</ul>\n<p><strong><a name=\"newcomb\"></a>4.</strong> Maybe the best way to implement Newcomb's Problem in this setting is to introduce the game <strong>N</strong> as follows (<strong>X</strong> chooses the row and gets the first listed payoff):</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>(100,1)</td>\n<td>(0,0)</td>\n</tr>\n<tr>\n<td>(101,0)</td>\n<td>(1,1)</td>\n</tr>\n</tbody>\n</table>\n<p>Your opponent is the predictor <strong>P</strong>, which is programmed as follows (when the game is <strong>N</strong>):</p>\n<ul>\n<li>Try to deduce \"output <strong>X</strong>(code <strong>N</strong>, code <strong>P</strong>)=row 1\".</li>\n<li>If this succeeds, output column 1; else output column 2.</li>\n</ul>\n<p>If you expect to face <strong>N</strong> against <strong>P</strong>, then you have every incentive to make it an easy deduction that output <strong>X</strong>(code <strong>N</strong>, code <strong>P</strong>)=1.</p>\n<p><strong><a name=\"simplify\"></a>5.</strong> There may be a better way to do this, but most simplifications will fall short of CDT in one way or another. For instance, if we just have <strong>X</strong> try to deduce \"output <strong>X</strong>(code <strong>G</strong>, 1, code <strong>Y</strong>)=output <strong>Y</strong>(code <strong>G</strong>, 2, code <strong>X</strong>)\", then it's quite possible for <strong>X</strong> to wind up cooperating with a CooperateBot, which is suboptimal performance on a one-player game. (A proof of that statement can be generated by L\u00f6b's Theorem.)</p>", "sections": [{"title": "Or: Causal Decision Theory and Substitution", "anchor": "Or__Causal_Decision_Theory_and_Substitution", "level": 1}, {"title": "Previously:", "anchor": "Previously_", "level": 2}, {"title": "Idea 3: Substitute to Avoid Self-Fulfilling Prophecies", "anchor": "Idea_3__Substitute_to_Avoid_Self_Fulfilling_Prophecies", "level": 1}, {"title": "Causal Decision Theory: No Longer Naive", "anchor": "Causal_Decision_Theory__No_Longer_Naive", "level": 1}, {"title": "Cliquish Decision Theory: Better, but Not Good Enough", "anchor": "Cliquish_Decision_Theory__Better__but_Not_Good_Enough", "level": 1}, {"title": "Idea 4: Substitute for the Source Code of X (as an input of Y)", "anchor": "Idea_4__Substitute_for_the_Source_Code_of_X__as_an_input_of_Y_", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["af9MjBqF2hgu3EN6r", "2JdvZw3CXzafxQugN", "HT8jwNJ6vH7p9gaTT", "szfxvS8nsxTgJLBHs", "AMwzjjvFxEgxvL7xe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-06T22:59:55.015Z", "modifiedAt": null, "url": null, "title": "binomial variance problem", "slug": "binomial-variance-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.684Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nerfhammer", "createdAt": "2009-07-21T19:45:50.831Z", "isAdmin": false, "displayName": "nerfhammer"}, "userId": "TnRcc3ezfxzQs7Phn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8c3atcMRCkpB4KSy8/binomial-variance-problem", "pageUrlRelative": "/posts/8c3atcMRCkpB4KSy8/binomial-variance-problem", "linkUrl": "https://www.lesswrong.com/posts/8c3atcMRCkpB4KSy8/binomial-variance-problem", "postedAtFormatted": "Friday, April 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20binomial%20variance%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Abinomial%20variance%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8c3atcMRCkpB4KSy8%2Fbinomial-variance-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=binomial%20variance%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8c3atcMRCkpB4KSy8%2Fbinomial-variance-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8c3atcMRCkpB4KSy8%2Fbinomial-variance-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>Found in an old Kahneman &amp; Tversky paper:</p>\n<p>\n<blockquote>\n<p>There are two programs in a high school. Boys are a majority (65%) in program A, and a minority (45%) in program B. There is an equal number of classes in each of the two programs.</p>\n<p>You enter a class at random, and observe that 55% of the students are boys. What is your best guess -- does the class belong to program A or to program B?</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8c3atcMRCkpB4KSy8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.793806596939769e-07, "legacy": true, "legacyId": "14929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-07T01:34:45.069Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] No Individual Particles", "slug": "seq-rerun-no-individual-particles", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pma32az74pgDN3QFf/seq-rerun-no-individual-particles", "pageUrlRelative": "/posts/Pma32az74pgDN3QFf/seq-rerun-no-individual-particles", "linkUrl": "https://www.lesswrong.com/posts/Pma32az74pgDN3QFf/seq-rerun-no-individual-particles", "postedAtFormatted": "Saturday, April 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20No%20Individual%20Particles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20No%20Individual%20Particles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPma32az74pgDN3QFf%2Fseq-rerun-no-individual-particles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20No%20Individual%20Particles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPma32az74pgDN3QFf%2Fseq-rerun-no-individual-particles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPma32az74pgDN3QFf%2Fseq-rerun-no-individual-particles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p>Today's post, <a href=\"/lw/pl/no_individual_particles/\">No Individual Particles</a> was originally published on 18 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>One of the chief ways to confuse yourself while thinking about quantum mechanics, is to think as if photons were little billiard balls bouncing around. The appearance of little billiard balls is a special case of a deeper level on which there are only multiparticle configurations and amplitude flows. It is easy to set up physical situations in which there exists no fact of the matter as to which electron was originally which.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bhp/seq_rerun_feynman_paths/\">Feynman Paths</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pma32az74pgDN3QFf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.794454541812535e-07, "legacy": true, "legacyId": "14938", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cpf2jsZsNFNH5TSpc", "xRycLWJBcRSkSnXxa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-07T08:25:58.208Z", "modifiedAt": null, "url": null, "title": "AI Risk & Opportunity: Strategic Analysis Via Probability Tree", "slug": "ai-risk-and-opportunity-strategic-analysis-via-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:01.900Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rvN6LrbtQDR2ee382/ai-risk-and-opportunity-strategic-analysis-via-probability", "pageUrlRelative": "/posts/rvN6LrbtQDR2ee382/ai-risk-and-opportunity-strategic-analysis-via-probability", "linkUrl": "https://www.lesswrong.com/posts/rvN6LrbtQDR2ee382/ai-risk-and-opportunity-strategic-analysis-via-probability", "postedAtFormatted": "Saturday, April 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Risk%20%26%20Opportunity%3A%20Strategic%20Analysis%20Via%20Probability%20Tree&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Risk%20%26%20Opportunity%3A%20Strategic%20Analysis%20Via%20Probability%20Tree%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvN6LrbtQDR2ee382%2Fai-risk-and-opportunity-strategic-analysis-via-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Risk%20%26%20Opportunity%3A%20Strategic%20Analysis%20Via%20Probability%20Tree%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvN6LrbtQDR2ee382%2Fai-risk-and-opportunity-strategic-analysis-via-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvN6LrbtQDR2ee382%2Fai-risk-and-opportunity-strategic-analysis-via-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 676, "htmlBody": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>There are many approaches to strategic analysis (<a href=\"http://www-pilot.universitiesuk.ac.uk/PolicyAndResearch/PolicyAreas/Scenarios/Documents/current_state_of_scenario_development_FORESIGHT.pdf\">Bishop et al. 2007</a>). Though a morphological analysis (<a href=\"http://www.swemorph.com/pdf/psm-gma.pdf\">Ritchey 2006</a>) could model our situation in more detail, the present analysis uses a simple probability tree (<a href=\"http://books.google.com/books?id=zVF47OP_PlIC&amp;lpg=PA459&amp;vq=probability%20tree&amp;dq=Mathematical%20Applications%20for%20the%20Management%2C%20Life%2C%20and%20Social&amp;pg=PA459#v=onepage&amp;q&amp;f=false\">Harshbarger &amp; Reynolds 2008, sec. 7.4</a>) to model potential events and interventions.</p>\n<p>&nbsp;</p>\n<h3>A very simple tree</h3>\n<p>In our initial attempt, the first disjunction concerns which of several (mutually exclusive and exhaustive) transformative events comes first:</p>\n<p>&nbsp;</p>\n<ul>\n<li>\"FAI\" = Friendly AI.</li>\n<li>\"uFAI\" = UnFriendly AI, not including uFAI developed with insights from WBE.</li>\n<li>\"WBE\" = Whole brain emulation.</li>\n<li>\"Doom\" = Human extinction, including simulation shutdown and extinction due to uFAI striking us from beyond our solar system.</li>\n<li>\"Other\" = None of the above four events occur in our solar system, perhaps due to stable global totalitarianism or for unforeseen reasons.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Our probability tree begins simply:</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/Initial-disjunction-1-52-7-35-5.gif\" alt=\"\" /></p>\n<p>Each circle is a <em>chance node</em>, which represents a random variable. The leftmost chance node above represents the variable of whether FAI, uFAI, WBE, Doom, or Other will come first. The rightmost chance nodes are open to further disjunctions: the random variables they represent will be revealed as we continue to develop the probability tree.</p>\n<p>Each left-facing triangle is a <em>terminal node</em>, which for us serves the same function as a <em>utility node</em> in a Bayesian decision network. The only utility node in the tree above assigns a utility of 0 (bad!) to the Doom outcome.</p>\n<p>Each branch in the tree is assigned a probability. For the purposes of illustration, the above tree assigns .01 probability to FAI coming first, .52 probability to uFAI coming first, .07 probability to WBE coming first, .35 to Doom coming first, and .05 to Other coming first.</p>\n<p>&nbsp;</p>\n<h3>How the tree could be expanded</h3>\n<p>The simple tree above could be expanded \"downstream\" by adding additional branches:</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/WBE-expanded.gif\" alt=\"\" /></p>\n<p>We could also make the probability tree more actionable by trying to estimate the probability of desirable and undesirable outcomes given certain that certain shorter-term goals are met. In the example below, \"private push\" means that a non-state actor <em>passionate about safety</em> invests $30 billion or more into developing WBE technology within 30 years from today. Perhaps there's a small chance this safety-conscious actor could get to WBE before state actors, upload FAI researchers, and have them figure out FAI before uFAI is created.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/WBE-expanded-with-action.gif\" alt=\"\" /></p>\n<p>We could also expand the tree \"upstream\" by making the first disjunction be not concerned with our five options for what comes first but instead with a series of disjunctions that feed into which option will come first.</p>\n<p>We could add hundreds or thousands of nodes to our probability tree, and then use <a href=\"http://www.treeage.com/products/overviewPro.html\">the software</a> to test for how much the outcomes change when particular inputs are changed, and learn what things we can do now to most increase our chances of a desirable outcome, given our current model.</p>\n<p>We would also need to decide which \"endgame scenarios\" we want to include as possible terminals, and the utility of each. These choices may be complicated by our beliefs about <a href=\"http://www.nickbostrom.com/ethics/infinite.html\">multiverses</a> and <a href=\"http://www.simulation-argument.com/simulation.html\">simulations</a>.</p>\n<p>However, decision trees become enormously large and complex very quickly as you add more variables. If we had the resources for a more complicated model, we'd probably want to use influence diagrams instead (Howard &amp; Matheson 2005), e.g. one built in <a href=\"http://www.lumina.com/why-analytica/\">Analytica</a>, like the <a href=\"http://www.lumina.com/case-studies/integrated-climate-assessment-model/\">ICAM</a> climate change model. Of course, one must always worry that one's model is internally consistent but disconnected from the real world (Kay 2012).</p>\n<p>&nbsp;</p>\n<h3>References</h3>\n<ul>\n<li><small>Bishop et al. (2007). <a href=\"http://www-pilot.universitiesuk.ac.uk/PolicyAndResearch/PolicyAreas/Scenarios/Documents/current_state_of_scenario_development_FORESIGHT.pdf\">The current state of scenario development: an overview of techniques</a>.</small></li>\n<li><small>Harshbarger &amp; Reynolds (2008). <em><a href=\"http://www.amazon.com/Mathematical-Applications-Management-Textbooks-Available/dp/1133106234/\">Mathematical Applications for the Management, Life, and Social Sciences</a></em>.</small></li>\n<li><small>Howard &amp; Matheson (2005). <a href=\"http://www.cs.ru.nl/~marinav/Teaching/BDMinAI/influencediagrams05.pdf\">Influence Diagrams</a>.</small></li>\n<li><small>Kay (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/01/Kay-The-map-is-not-the-territory-models-scientists-and-the-state-of-modern-macroeconomics.pdf\">The map is not the territory: Models, scientists, and the state of modern macroeconomics</a>.</small></li>\n<li><small>Ritchey (2006). <a href=\"http://www.swemorph.com/pdf/psm-gma.pdf\">Problem structuring using computer-aided morphological analysis</a>.</small></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rvN6LrbtQDR2ee382", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 20, "extendedScore": null, "score": 8.796175806948842e-07, "legacy": true, "legacyId": "14961", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>There are many approaches to strategic analysis (<a href=\"http://www-pilot.universitiesuk.ac.uk/PolicyAndResearch/PolicyAreas/Scenarios/Documents/current_state_of_scenario_development_FORESIGHT.pdf\">Bishop et al. 2007</a>). Though a morphological analysis (<a href=\"http://www.swemorph.com/pdf/psm-gma.pdf\">Ritchey 2006</a>) could model our situation in more detail, the present analysis uses a simple probability tree (<a href=\"http://books.google.com/books?id=zVF47OP_PlIC&amp;lpg=PA459&amp;vq=probability%20tree&amp;dq=Mathematical%20Applications%20for%20the%20Management%2C%20Life%2C%20and%20Social&amp;pg=PA459#v=onepage&amp;q&amp;f=false\">Harshbarger &amp; Reynolds 2008, sec. 7.4</a>) to model potential events and interventions.</p>\n<p>&nbsp;</p>\n<h3 id=\"A_very_simple_tree\">A very simple tree</h3>\n<p>In our initial attempt, the first disjunction concerns which of several (mutually exclusive and exhaustive) transformative events comes first:</p>\n<p>&nbsp;</p>\n<ul>\n<li>\"FAI\" = Friendly AI.</li>\n<li>\"uFAI\" = UnFriendly AI, not including uFAI developed with insights from WBE.</li>\n<li>\"WBE\" = Whole brain emulation.</li>\n<li>\"Doom\" = Human extinction, including simulation shutdown and extinction due to uFAI striking us from beyond our solar system.</li>\n<li>\"Other\" = None of the above four events occur in our solar system, perhaps due to stable global totalitarianism or for unforeseen reasons.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Our probability tree begins simply:</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/Initial-disjunction-1-52-7-35-5.gif\" alt=\"\"></p>\n<p>Each circle is a <em>chance node</em>, which represents a random variable. The leftmost chance node above represents the variable of whether FAI, uFAI, WBE, Doom, or Other will come first. The rightmost chance nodes are open to further disjunctions: the random variables they represent will be revealed as we continue to develop the probability tree.</p>\n<p>Each left-facing triangle is a <em>terminal node</em>, which for us serves the same function as a <em>utility node</em> in a Bayesian decision network. The only utility node in the tree above assigns a utility of 0 (bad!) to the Doom outcome.</p>\n<p>Each branch in the tree is assigned a probability. For the purposes of illustration, the above tree assigns .01 probability to FAI coming first, .52 probability to uFAI coming first, .07 probability to WBE coming first, .35 to Doom coming first, and .05 to Other coming first.</p>\n<p>&nbsp;</p>\n<h3 id=\"How_the_tree_could_be_expanded\">How the tree could be expanded</h3>\n<p>The simple tree above could be expanded \"downstream\" by adding additional branches:</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/WBE-expanded.gif\" alt=\"\"></p>\n<p>We could also make the probability tree more actionable by trying to estimate the probability of desirable and undesirable outcomes given certain that certain shorter-term goals are met. In the example below, \"private push\" means that a non-state actor <em>passionate about safety</em> invests $30 billion or more into developing WBE technology within 30 years from today. Perhaps there's a small chance this safety-conscious actor could get to WBE before state actors, upload FAI researchers, and have them figure out FAI before uFAI is created.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/WBE-expanded-with-action.gif\" alt=\"\"></p>\n<p>We could also expand the tree \"upstream\" by making the first disjunction be not concerned with our five options for what comes first but instead with a series of disjunctions that feed into which option will come first.</p>\n<p>We could add hundreds or thousands of nodes to our probability tree, and then use <a href=\"http://www.treeage.com/products/overviewPro.html\">the software</a> to test for how much the outcomes change when particular inputs are changed, and learn what things we can do now to most increase our chances of a desirable outcome, given our current model.</p>\n<p>We would also need to decide which \"endgame scenarios\" we want to include as possible terminals, and the utility of each. These choices may be complicated by our beliefs about <a href=\"http://www.nickbostrom.com/ethics/infinite.html\">multiverses</a> and <a href=\"http://www.simulation-argument.com/simulation.html\">simulations</a>.</p>\n<p>However, decision trees become enormously large and complex very quickly as you add more variables. If we had the resources for a more complicated model, we'd probably want to use influence diagrams instead (Howard &amp; Matheson 2005), e.g. one built in <a href=\"http://www.lumina.com/why-analytica/\">Analytica</a>, like the <a href=\"http://www.lumina.com/case-studies/integrated-climate-assessment-model/\">ICAM</a> climate change model. Of course, one must always worry that one's model is internally consistent but disconnected from the real world (Kay 2012).</p>\n<p>&nbsp;</p>\n<h3 id=\"References\">References</h3>\n<ul>\n<li><small>Bishop et al. (2007). <a href=\"http://www-pilot.universitiesuk.ac.uk/PolicyAndResearch/PolicyAreas/Scenarios/Documents/current_state_of_scenario_development_FORESIGHT.pdf\">The current state of scenario development: an overview of techniques</a>.</small></li>\n<li><small>Harshbarger &amp; Reynolds (2008). <em><a href=\"http://www.amazon.com/Mathematical-Applications-Management-Textbooks-Available/dp/1133106234/\">Mathematical Applications for the Management, Life, and Social Sciences</a></em>.</small></li>\n<li><small>Howard &amp; Matheson (2005). <a href=\"http://www.cs.ru.nl/~marinav/Teaching/BDMinAI/influencediagrams05.pdf\">Influence Diagrams</a>.</small></li>\n<li><small>Kay (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/01/Kay-The-map-is-not-the-territory-models-scientists-and-the-state-of-modern-macroeconomics.pdf\">The map is not the territory: Models, scientists, and the state of modern macroeconomics</a>.</small></li>\n<li><small>Ritchey (2006). <a href=\"http://www.swemorph.com/pdf/psm-gma.pdf\">Problem structuring using computer-aided morphological analysis</a>.</small></li>\n</ul>", "sections": [{"title": "A very simple tree", "anchor": "A_very_simple_tree", "level": 1}, {"title": "How the tree could be expanded", "anchor": "How_the_tree_could_be_expanded", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i2XoqtYEykc4XWp9B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-07T17:20:27.624Z", "modifiedAt": null, "url": null, "title": "[link] TEDxYale - Keith Chen - The Impact of Language on Economic Behavior ", "slug": "link-tedxyale-keith-chen-the-impact-of-language-on-economic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7pBhbwooNLvWBtJeo/link-tedxyale-keith-chen-the-impact-of-language-on-economic", "pageUrlRelative": "/posts/7pBhbwooNLvWBtJeo/link-tedxyale-keith-chen-the-impact-of-language-on-economic", "linkUrl": "https://www.lesswrong.com/posts/7pBhbwooNLvWBtJeo/link-tedxyale-keith-chen-the-impact-of-language-on-economic", "postedAtFormatted": "Saturday, April 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20TEDxYale%20-%20Keith%20Chen%20-%20The%20Impact%20of%20Language%20on%20Economic%20Behavior%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20TEDxYale%20-%20Keith%20Chen%20-%20The%20Impact%20of%20Language%20on%20Economic%20Behavior%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pBhbwooNLvWBtJeo%2Flink-tedxyale-keith-chen-the-impact-of-language-on-economic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20TEDxYale%20-%20Keith%20Chen%20-%20The%20Impact%20of%20Language%20on%20Economic%20Behavior%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pBhbwooNLvWBtJeo%2Flink-tedxyale-keith-chen-the-impact-of-language-on-economic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pBhbwooNLvWBtJeo%2Flink-tedxyale-keith-chen-the-impact-of-language-on-economic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>http://www.youtube.com/watch?v=CiobJhogNnA</p>\n<p>The short version is that if the language you speak requires different verbs for the present and the future, it causes you to think about it differently. Depending on the magnitude of the effect, this has important implications for <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">construal level theory</a>. If your language allows you to think about the future in Near mode, it may allow you to think about it more rationally.</p>\n<p>Previous discussion on one of Keith Chen's papers <a href=\"/lw/ajp/sapirwhorf_savings_and_discount_rates_link/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7pBhbwooNLvWBtJeo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 8.798413951115411e-07, "legacy": true, "legacyId": "14963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kX3jSdH84WT99d3dL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-07T18:46:36.295Z", "modifiedAt": null, "url": null, "title": "AI risk bibliography (draft)", "slug": "ai-risk-bibliography-draft", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.607Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2ZzhpE54zdBkuFfsE/ai-risk-bibliography-draft", "pageUrlRelative": "/posts/2ZzhpE54zdBkuFfsE/ai-risk-bibliography-draft", "linkUrl": "https://www.lesswrong.com/posts/2ZzhpE54zdBkuFfsE/ai-risk-bibliography-draft", "postedAtFormatted": "Saturday, April 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20risk%20bibliography%20(draft)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20risk%20bibliography%20(draft)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZzhpE54zdBkuFfsE%2Fai-risk-bibliography-draft%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20risk%20bibliography%20(draft)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZzhpE54zdBkuFfsE%2Fai-risk-bibliography-draft", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZzhpE54zdBkuFfsE%2Fai-risk-bibliography-draft", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1970, "htmlBody": "<p><span id=\"internal-source-marker_0.4139942051842809\">\n<h1 style=\"font-family: Times; font-size: medium; font-weight: bold;\" dir=\"ltr\"><strong id=\"internal-source-marker_0.4139942051842809\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Generally, only sources including an extended analysis of AI risk are included, though there are some exceptions among the earliest sources. Listed sources discuss either the likelihood of AI risk or they discuss possible solutions. (This does not include most of the \"machine ethics\" literature, unless an article discusses machine ethics in the explicit context of artificial intelligence as an existential risk.)</span></strong></h1>\n<p>Please let me know what i missed!</p>\n<p>&nbsp;</p>\n<p><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Butler, Samuel [Cellarius, pseud.]. 1863. Darwin among the machines. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Christchurch Press</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, June 13. </span><a href=\"http://www.nzetc.org/tm/scholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http: //www.nzetc.org/tm/scholarly/tei-ButFir-t1-g1-t1-g1-t4-body.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Good, Irving John. 1959. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Speculations on perceptrons and other automata</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Research Lecture, RC-115. IBM, Yorktown Heights, New York, June 2. </span><a href=\"http://domino.research.ibm.com/library/cyberdig.nsf/papers/58DC4EA36A143C218525785E00502E30/$File/rc115.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://domino.research.ibm.com/library/cyberdig.nsf/papers/58DC4EA36A143C218525785E00502E30/$File/rc115.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Good, Irving John. 1965. Speculations concerning the first ultraintelligent machine. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Advances in computers</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Franz L. Alt and Morris Rubinoff, 31&ndash;88. Vol. 6. New York: Academic Press. doi:</span><a href=\"http://dx.doi.org/10.1016/S0065-2458(08)60418-0\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1016/S0065-2458(08)60418-0</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Good, Irving John. 1970. Some future social repercussions of computers. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">International Journal of Environmental Studies</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 1 (1&ndash;4): 67&ndash;79. doi:</span><a href=\"http://dx.doi.org/10.1080/00207237008709398\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1080/00207237008709398</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Versenyi, Laszlo. 1974. Can robots be moral? </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Ethics</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 84 (3): 248&ndash;259. </span><a href=\"http://www.jstor.org/stable/2379958\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.jstor.org/stable/2379958</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Good, Irving John. 1982. Ethical machines. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Machine intelligence</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. J. E. Hayes, Donald Michie, and Y.-H. Pao, 555&ndash;560. Vol. 10. Intelligent Systems: Practice and Perspective. Chichester: Ellis Horwood.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Minsky, Marvin. 1984. Afterword to Vernor Vinge&rsquo;s novel, &ldquo;True Names.&rdquo; Oct. 1. </span><a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Mar. 26, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Moravec, Hans P. 1988. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Mind children: The future of robot and human intelligence</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Cambridge, MA: Harvard University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Crevier, Daniel. 1993. The silicon challengers in our future. Chap. 12 in </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">AI: The tumultuous history of the search for artificial intelligence</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. New York: Basic Books.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Vinge, Vernor. 1993. The coming technological singularity: How to survive in the post-human era. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Vision- 21: Interdisciplinary science and engineering in the era of cyberspace</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, 11&ndash;22. NASA Conference Publication 10129. NASA Lewis Research Center. </span><a href=\"http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19940022855_1994022855.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19940022855_1994022855.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hanson, Robin. 1994. If uploads comes first: The crack of a future dawn. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Extropy</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 6 (2). </span><a href=\"http://hanson.gmu.edu/uploads.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://hanson.gmu.edu/uploads.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bostrom, Nick. 1997. Predictions from philosophy? How philosophers could make themselves useful. Last modified September 19, 1998. </span><a href=\"http://www.nickbostrom.com/old/predict.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.nickbostrom.com/old/predict.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Warwick, Kevin. 1998. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">In the mind of the machine: Breakthrough in artificial intelligence</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. London: Arrow.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Moravec, Hans P. 1999. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Robot: Mere machine to transcendent mind</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. New York: Oxford University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Joy, Bill. 2000. Why the future doesn&rsquo;t need us. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Wired</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, Apr. </span><a href=\"http://www.wired.com/wired/archive/8.04/joy.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.wired.com/wired/archive/8.04/joy.html</span></a><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky, Eliezer. 2001. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Creating friendly AI 1.0: The analysis and design of benevolent goal architectures</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Singularity Institute for Artificial Intelligence, San Francisco, CA, June 15. </span><a href=\"http://intelligence.org/upload/CFAI.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/upload/CFAI.html</span></a><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">6, Perri [David Ashworth]. 2001. Ethics, regulation and the new artificial intelligence, part I: Accountability and power. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Information, Communication &amp; Society</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 4 (2): 199&ndash;229. doi:</span><a href=\"http://dx.doi.org/10.1080/713768525\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1080/713768525</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hibbard, Bill. 2001. Super-intelligent machines. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">ACM SIGGRAPH Computer Graphics</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 35 (1): 13&ndash;15. </span><a href=\"http://www.siggraph.org/publications/newsletter/issues/v35/v35n1.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.siggraph.org/publications/newsletter/issues/v35/v35n1.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bostrom, Nick. 2002. Existential risks: Analyzing human extinction scenarios and related hazards. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Evolution and Technology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 9. </span><a href=\"http://www.jetpress.org/volume9/risks.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.jetpress.org/volume9/risks.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel, Ben. 2002. Thoughts on AI morality. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Dynamical Psychology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. </span><a href=\"http://www.goertzel.org/dynapsyc/2002/AIMorality.htm\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.goertzel.org/dynapsyc/2002/AIMorality.htm</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hibbard, Bill. 2002. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Super-intelligent machines</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. New York: Kluwer Academic/Plenum Publishers.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bostrom, Nick. 2003. Ethical issues in advanced artificial intelligence. In </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Cognitive, emotive and ethical aspects of decision making in humans and in artificial intelligence</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Iva Smit and George E. Lasker. Vol. 2. Windsor, ON: International Institute of Advanced Studies in Systems Research / Cybernetics.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Georges, Thomas M. 2003. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Digital soul: Intelligent machines and human values</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Boulder, CO: Westview Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bostrom, Nick. 2004. The future of human evolution. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Two hundred years after Kant, fifty years after Turing</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Charles Tandy, 339&ndash;371. Vol. 2. Death and Anti-Death. Palo Alto, CA: Ria University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel, Ben. 2004. Encouraging a positive transcension: Issues in transhumanist ethical philosophy. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Dynamical Psychology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. </span><a href=\"http://www.goertzel.org/dynapsyc/2004/PositiveTranscension.htm\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.goertzel.org/dynapsyc/2004/PositiveTranscension.htm</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel, Ben. 2004. The all-seeing A(I): Universal mind simulation as a possible path to stably benevolent superhuman AI. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Dynamical Psychology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. </span><a href=\"http://www.goertzel.org/dynapsyc/2004/AllSeeingAI.htm\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.goertzel.org/dynapsyc/2004/AllSeeingAI.htm</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Posner, Richard A. 2004. What are the catastrophic risks, and how catastrophic are they? Chap. 1 in </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Catastrophe: Risk and response</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. New York: Oxford University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky, Eliezer. 2004. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Coherent extrapolated volition</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Singularity Institute for Artificial Intelligence, San Francisco, CA, May. </span><a href=\"http://intelligence.org/upload/CEV.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/upload/CEV.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">de Garis, Hugo. 2005. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">The artilect war: Cosmists vs. terrans: A bitter controversy concerning whether humanity should build godlike massively intelligent machines</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Palm Springs, CA: ETC Publications.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hibbard, Bill. 2005. The ethics and politics of super-intelligent machines. Unpublished manuscript, July. Microsoft Word file, </span><a href=\"http://sites.google.com/site/whibbard/g/SI_ethics_politics.doc\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://sites.google.com/site/whibbard/g/SI_ethics_politics.doc</span></a><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Apr. 3, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Kurzweil, Ray. 2005. The deeply intertwined promise and peril of GNR. Chap. 8 in </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">The singularity is near: When humans transcend biology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. New York: Viking.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Armstrong, Stuart. 2007. Chaining god: A qualitative approach to AI, trust and moral systems. Unpublished manuscript, Oct. 20. </span><a href=\"http://www.neweuropeancentury.org/GodAI.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.neweuropeancentury.org/GodAI.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Apr. 6, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bugaj, Stephan Vladimir, and Ben Goertzel. 2007. Five ethical imperatives and their implications for human-AGI interaction. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Dynamical Psychology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. </span><a href=\"http://goertzel.org/dynapsyc/2007/Five_Ethical_Imperatives_svbedit.htm\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://goertzel.org/dynapsyc/2007/Five_Ethical_Imperatives_svbedit.htm</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Dietrich, Eric. 2007. After the humans are gone. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Philosophy Now</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, May/June. </span><a href=\"http://www.philosophynow.org/issues/61/After_The_Humans_Are_Gone\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.philosophynow.org/issues/61/After_The_Humans_Are_Gone</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hall, John Storrs. 2007. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Beyond AI: Creating the conscience of the machine</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Amherst, NY: Prometheus Books.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hall, John Storrs. 2007. Ethics for artificial intellects. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Nanoethics: The ethical and social implications of nanotechnology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Fritz Allhoff, Patrick Lin, James Moor, John Weckert, and Mihail C. Roco, 339&ndash;352. Hoboken, N.J: John Wiley &amp; Sons.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hall, John Storrs. 2007. Self-improving AI: An analysis. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Minds and Machines</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 17 (3): 249&ndash;259. doi:</span><a href=\"http://dx.doi.org/10.1007/s11023-007-9065-3\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1007/s11023-007-9065-3</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Omohundro, Stephen M. 2007. The nature of self-improving artificial intelligence. Paper presented at the Singularity Summit 2007, San Francisco, CA, Sept. 8&ndash;9. </span><a href=\"http://intelligence.org/summit2007/overview/abstracts/#omohundro\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/summit2007/overview/abstracts/#omohundro</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Blake, Thomas, Bernd Carsten Stahl, and N. B. Fairweather. 2008. Robot ethics: Why &ldquo;Friendly AI&rdquo; won&rsquo;t work. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Proceedings of the tenth international conference ETHICOMP 2008: Living, working and learning beyond technology</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Terrel Ward Bynum, Maria Carla Calzarossa, Ivo De Lotto, and Simon Rogerson. isbn: 9788890286995.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hall, John Storrs. 2008. Engineering utopia. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial general intelligence 2008: Proceedings of the first AGI conference</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Pei Wang, Ben Goertzel, and Stan Franklin, 460&ndash;467. Vol. 171. Frontiers in Artificial Intelligence and Applications. Amsterdam: IOS Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hanson, Robin. 2008. Economics of the singularity. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">IEEE Spectrum</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 45 (6): 45&ndash;50. doi:</span><a href=\"http://dx.doi.org/10.1109/MSPEC.2008.4531461\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1109/MSPEC.2008.4531461</span></a><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Omohundro, Stephen M. 2008. The basic AI drives. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial general intelligence 2008: Proceedings of the first AGI conference</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Pei Wang, Ben Goertzel, and Stan Franklin, 483&ndash;492. Vol. 171. Frontiers in Artificial Intelligence and Applications. Amsterdam: IOS Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky, Eliezer. 2008. Artificial intelligence as a positive and negative factor in global risk. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Global catastrophic risks</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Nick Bostrom and Milan M. \u0106irkovi\u0107, 308&ndash;345. New York: Oxford University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Freeman, Tim. 2009. Using compassion and respect to motivate an artificial intelligence. Unpublished manuscript, Mar. 8. </span><a href=\"http://fungible.com/respect/paper.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://fungible.com/respect/paper.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Apr. 7, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Russell, Stuart J., and Peter Norvig. 2009. Philosophical foundations. Chap. 26 in </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial intelligence: A modern approach</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, 3rd ed. Upper Saddle River, NJ: Prentice-Hall.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Shulman, Carl, and Stuart Armstrong. 2009. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Arms races and intelligence explosions</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Extended abstract. Singularity Institute for Artificial Intelligence, San Francisco, CA. </span><a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/armscontrolintelligenceexplosions.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Shulman, Carl, Henrik Jonsson, and Nick Tarleton. 2009. Machine ethics and superintelligence. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">AP-CAP 2009: The fifth Asia-Pacific computing and philosophy conference, October 1st-2nd, University of Tokyo, Japan, proceedings</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Carson Reynolds and Alvaro Cassinelli, 95&ndash;97. AP-CAP 2009. </span><a href=\"http://ia-cap.org/ap-cap09/proceedings.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://ia-cap.org/ap-cap09/proceedings.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Sotala, Kaj. 2009. Evolved altruism, ethical complexity, anthropomorphic trust: Three factors misleading estimates of the safety of artificial general intelligence. Paper presented at the 7th European Conference on Computing and Philosophy (ECAP), Bellaterra, Spain, July 2&ndash;4.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Wallach, Wendell, and Colin Allen. 2009. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Moral machines: Teaching robots right from wrong</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. New York: Oxford University Press. doi:</span><a href=\"http://dx.doi.org/10.1093/acprof:oso/9780195374049.001.0001\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1093/acprof:oso/9780195374049.001.0001</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Waser, Mark R. 2009. A safe ethical system for intelligent machines. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Biologically inspired cognitive architectures: Papers from the AAAI fall symposium</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Alexei V. Samsonovich, 194&ndash;199. Technical Report, FS- 09-01. AAAI Press, Menlo Park, CA. </span><a href=\"http://aaai.org/ocs/index.php/FSS/FSS09/paper/view/934\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://aaai.org/ocs/index.php/FSS/FSS09/paper/view/934</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Chalmers, David John. 2010. The singularity: A philosophical analysis. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Consciousness Studies </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">17 (9&ndash;10): 7&ndash;65. </span><a href=\"http://www.ingentaconnect.com/content/imp/jcs/2010/00000017/f0020009/art00001\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.ingentaconnect.com/content/imp/jcs/2010/00000017/f0020009/art00001</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Fox, Joshua, and Carl Shulman. 2010. Superintelligence does not imply benevolence. Paper presented at the 8th European Conference on Computing and Philosophy (ECAP), Munich, Germany, Oct. 4&ndash;6.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel, Ben. 2010. Coherent aggregated volition: A method for deriving goal system content for advanced, beneficial AGIs. The Multiverse According to Ben (blog). Mar. 12. </span><a href=\"http://multiverseaccordingtoben.blogspot.ca/2010/03/coherent-aggregated-volition-toward.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://multiverseaccordingtoben.blogspot.ca/2010/03/coherent-aggregated-volition-toward.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Apr. 4, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel, Ben. 2010. GOLEM: Toward an AGI meta-architecture enabling both goal preservation and radical self-improvement. Unpublished manuscript, May 2. </span><a href=\"http://goertzel.org/GOLEM.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://goertzel.org/GOLEM.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Apr. 4, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Kaas, Steven, Steve Rayhawk, Anna Salamon, and Peter Salamon. 2010. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Economic implications of software minds</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Singularity Institute for Artificial Intelligence, San Francisco, CA, Aug. 10. </span><a href=\"http://www.singinst.co/upload/economic-implications.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.singinst.co/upload/economic-implications.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">McGinnis, John O. 2010. Accelerating AI. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Northwestern University Law Review </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">104 (3): 1253&ndash;1270. </span><a href=\"http://www.law.northwestern.edu/lawreview/v104/n3/1253/LR104n3McGinnis.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.law.northwestern.edu/lawreview/v104/n3/1253/LR104n3McGinnis.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Shulman, Carl. 2010. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Omohundro&rsquo;s &ldquo;Basic AI Drives&rdquo; and catastrophic risks</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Singularity Institute for Artificial Intelligence, San Francisco, CA. </span><a href=\"http://intelligence.org/upload/ai-resource-drives.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/upload/ai-resource-drives.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Shulman, Carl. 2010. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Whole brain emulation and the evolution of superorganisms</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Singularity Institute for Artificial Intelligence, San Francisco, CA. </span><a href=\"http://intelligence.org/upload/WBE-superorganisms.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/upload/WBE-superorganisms.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Sotala, Kaj. 2010. From mostly harmless to civilization-threatening: Pathways to dangerous artificial general intelligences. Paper presented at the 8th European Conference on Computing and Philosophy (ECAP), Munich, Germany, Oct. 4&ndash;6.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Tarleton, Nick. 2010. Coherent extrapolated volition: A meta-level approach to machine ethics. Singularity Institute for Artificial Intelligence, San Francisco, CA. </span><a href=\"http://intelligence.org/upload/coherent-extrapolated-volition.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/upload/coherent-extrapolated-volition.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Waser, Mark R. 2010. Designing a safe motivational system for intelligent machines. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial general intelligence: Proceedings of the third conference on artificial general intelligence, AGI 2010, Lugano, Switzerland, March 5&ndash;8, 2010</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Eric Baum, Marcus Hutter, and Emanuel Kitzelmann, 170&ndash;175. Vol. 10. Advances in Intelligent Systems Research. Amsterdam: Atlantis Press. doi:</span><a href=\"http://dx.doi.org/10.2991/agi.2010.21\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.2991/agi.2010.21</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Dewey, Daniel. 2011. Learning what to value. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial general intelligence: 4th international conference, AGI 2011, Mountain View, CA, USA, August 3&ndash;6, 2011. Proceedings</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. J&uuml;rgen Schmidhuber, Kristinn R. Th&oacute;risson, and Moshe Looks, 309&ndash;314. Vol. 6830. Lecture Notes in Computer Science. Berlin: Springer. doi:</span><a href=\"http://dx.doi.org/10.1007/978-3-642-22887-2_35\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1007/978-3-642-22887-2_35</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hall, John Storrs. 2011. Ethics for self-improving machines. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Machine ethics</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Michael Anderson and Susan Leigh Anderson, 512&ndash;523. New York: Cambridge University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Muehlhauser, Luke. 2011. So you want to save the world. Last modified March 2, 2012. </span><a href=\"http://lukeprog.com/SaveTheWorld.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://lukeprog.com/SaveTheWorld.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Muehlhauser, Luke. 2011. The singularity FAQ. Singularity Institute for Artificial Intelligence. </span><a href=\"http://intelligence.org/singularityfaq\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://singinst.org/singularityfaq</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Mar. 27, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Waser, Mark R. 2011. Rational universal benevolence: Simpler, safer, and wiser than &ldquo;Friendly AI.&rdquo; In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial general intelligence: 4th international conference, AGI 2011, Mountain View, CA, USA, August 3&ndash;6, 2011. Proceedings</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. J&uuml;rgen Schmidhuber, Kristinn R. Th&oacute;risson, and Moshe Looks, 153&ndash;162. Vol. 6830. Lecture Notes in Computer Science. Berlin: Springer. doi:</span><a href=\"http://dx.doi.org/10.1007/978-3-642-22887-2_16\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1007/978-3-642-22887-2_16</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky, Eliezer. 2011. Complex value systems in friendly AI. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Artificial general intelligence: 4th international conference, AGI 2011, Mountain View, CA, USA, August 3&ndash;6, 2011. Proceedings</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. J&uuml;rgen Schmidhuber, Kristinn R. Th&oacute;risson, and Moshe Looks, 388&ndash;393. Vol. 6830. Lecture Notes in Computer Science. Berlin: Springer. doi:</span><a href=\"http://dx.doi.org/10.1007/978-3-642-22887-2_48\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">10.1007/978-3-642-22887-2_48</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Berglas, Anthony. 2012. Artificial intelligence will kill our grandchildren (singularity). Draft 9. Jan. </span><a href=\"http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.html\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.html</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> (accessed Apr. 6, 2012).</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel, Ben. 2012. Should humanity build a global AI nanny to delay the singularity until it&rsquo;s better understood? </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Consciousness Studies</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 19 (1&ndash;2): 96&ndash;111. </span><a href=\"http://ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00006\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00006</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hanson, Robin. 2012. Meet the new conflict, same as the old conflict. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Consciousness Studies</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 19 (1&ndash;2): 119&ndash;125. </span><a href=\"http://www.ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00008\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00008</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Tipler, Frank. 2012. Inevitable existence and inevitable goodness of the singularity. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Consciousness Studies</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 19 (1&ndash;2): 183&ndash;193. </span><a href=\"http://www.ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00013\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00013</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yampolskiy, Roman V. 2012. Leakproofing the singularity: artificial intelligence confinement problem. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Consciousness Studies</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 2012 (1&ndash;2): 194&ndash;214. </span><a href=\"http://www.ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00014\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.ingentaconnect.com/content/imp/jcs/2012/00000019/F0020001/art00014</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Armstrong, Stuart, Anders Sandberg, and Nick Bostrom. Forthcoming. Thinking inside the box: Using and controlling an Oracle AI. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Minds and Machines</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bostrom, Nick. Forthcoming. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Minds and Machines</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Preprint at, </span><a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><span style=\"font-size: 16px; font-family: Garamond; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">http://www.nickbostrom.com/superintelligentwill.pdf</span></a><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Bostrom, Nick, and Eliezer Yudkowsky. Forthcoming. The ethics of artificial intelligence. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Cambridge handbook of artificial intelligence</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Keith Frankish and William Ramsey. New York: Cambridge University Press.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Hanson, Robin. Forthcoming. Economic growth given machine intelligence. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Journal of Artificial Intelligence Research</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Muehlhauser, Luke, and Louie Helm. Forthcoming. The singularity and machine ethics. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">The singularity hypothesis: A scientific and philosophical assessment</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Amnon Eden, Johnny S&oslash;raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Muehlhauser, Luke, and Anna Salamon. Forthcoming. Intelligence explosion: Evidence and import. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">The singularity hypothesis: A scientific and philosophical assessment</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Amnon Eden, Johnny S&oslash;raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Sotala, Kaj. Forthcoming. Advantages of artificial intelligences, uploads, and digital minds. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">International Journal of Machine Consciousness</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> 4.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Omohundro, Stephen M. Forthcoming. Rationally-shaped artificial intelligence. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">The singularity hypothesis: A scientific and philosophical assessment</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, ed. Amnon Eden, Johnny S&oslash;raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yampolskiy, Roman V., and Joshua Fox. Forthcoming. Artificial general intelligence and the human mental model. In </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">The singularity hypothesis: A scientific and philosophical assessment,</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> ed. Amnon Eden, Johnny S&oslash;raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span></p>\n<p style=\"font-family: Times; font-size: medium; font-weight: bold; margin-left: 18pt; text-indent: -18pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Yampolskiy, Roman V., and Joshua Fox. Forthcoming. Safety engineering for artificial general intelligence. </span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Topoi</span><span style=\"font-size: 16px; font-family: Garamond; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2ZzhpE54zdBkuFfsE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "14964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-07T21:24:22.557Z", "modifiedAt": null, "url": null, "title": "[link] Is Alu Life?", "slug": "link-is-alu-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ec429", "createdAt": "2011-09-14T04:24:09.379Z", "isAdmin": false, "displayName": "ec429"}, "userId": "prkPy6PaxPLFQT7Xe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rhptk35BARbhceE4z/link-is-alu-life", "pageUrlRelative": "/posts/Rhptk35BARbhceE4z/link-is-alu-life", "linkUrl": "https://www.lesswrong.com/posts/Rhptk35BARbhceE4z/link-is-alu-life", "postedAtFormatted": "Saturday, April 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Is%20Alu%20Life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Is%20Alu%20Life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRhptk35BARbhceE4z%2Flink-is-alu-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Is%20Alu%20Life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRhptk35BARbhceE4z%2Flink-is-alu-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRhptk35BARbhceE4z%2Flink-is-alu-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>I recently read (in Dawkins' <em>The Ancestor's Tale</em>) about the Alu sequence, and went on to read about transposons generally.&nbsp; Having as I do a rather broad definition of life, I concluded that Alu (and others like it) are lifeforms in their own right, although parasitic ones.&nbsp; I found the potential ethical implications somewhat staggering, especially given the need to shut up and multiply those implications by the <em>rather large</em> number of transposon instances in a typical multicellular organism.</p>\n<p>I have written out my thoughts on the subject, at <a title=\"Is Alu Life?\" href=\"http://jttlov.no-ip.org/writings/alulife.htm\">http://jttlov.no-ip.org/writings/alulife.htm</a>.&nbsp; I don't claim to have a well-worked out position, just a series of ideas and questions I feel to be worthy of discussion.</p>\n<p>ETA: I have started editing the article based on the discussion below.&nbsp; For reference with the existing discussion, I have preserved a copy of the original article as well, linked from the current version.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rhptk35BARbhceE4z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -9, "extendedScore": null, "score": 8.799433731729176e-07, "legacy": true, "legacyId": "14966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-08T04:28:46.855Z", "modifiedAt": null, "url": null, "title": "Exploring the Idea Space Efficiently", "slug": "exploring-the-idea-space-efficiently", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:09.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Elithrion", "createdAt": "2012-04-01T22:31:55.578Z", "isAdmin": false, "displayName": "Elithrion"}, "userId": "JxETxdNf7KLnwNyDy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7PuMfJAAaQwaPWAZY/exploring-the-idea-space-efficiently", "pageUrlRelative": "/posts/7PuMfJAAaQwaPWAZY/exploring-the-idea-space-efficiently", "linkUrl": "https://www.lesswrong.com/posts/7PuMfJAAaQwaPWAZY/exploring-the-idea-space-efficiently", "postedAtFormatted": "Sunday, April 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exploring%20the%20Idea%20Space%20Efficiently&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExploring%20the%20Idea%20Space%20Efficiently%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PuMfJAAaQwaPWAZY%2Fexploring-the-idea-space-efficiently%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exploring%20the%20Idea%20Space%20Efficiently%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PuMfJAAaQwaPWAZY%2Fexploring-the-idea-space-efficiently", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PuMfJAAaQwaPWAZY%2Fexploring-the-idea-space-efficiently", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1301, "htmlBody": "<p>Simon is writing a calculus textbook. Since there are a lot of textbooks on the market, he wants to make his distinctive by including a lot of original examples. To do this, he decides to first check what sorts of examples are in some of the other books, and then make sure to avoid those. Unfortunately, after skimming through several other books, he finds himself completely unable to think of original examples&mdash;his mind keeps returning to the examples he's just read instead of coming up with new ones.</p>\n<p>What he's experiencing here is another aspect of <a href=\"/lw/3b/never_leave_your_room/\">priming</a> or <a href=\"/lw/j7/anchoring_and_adjustment/\">anchoring</a>. The way it appears to happen in my brain is that it decides to anchor on the examples it's already seen and explore the idea-space from there, moving from an idea only to ideas that are closely related to it (similarly to a <a href=\"http://en.wikipedia.org/wiki/Depth-first_search\">depth-first search</a>)</p>\n<p>At first, this search strategy might not seem so bad&mdash;in fact, it's ideal if there is one best solution and the closer you get to it the better. For example, if you were shooting arrows at a target, all you'd need to consider is how close to the center you can hit. Where we run into problems, however, is trying to come up with multiple solutions (such as multiple examples of the applications of calculus), or trying to come up with the best solution when there are many plausible solutions. In these cases, our brain's default search algorithm will often grab the first idea it can think of and try to refine it, even if what we really need is a completely different idea.<a id=\"more\"></a></p>\n<p>Of course, the brain is not so stupid that it will choose the first idea it has and refine it forever. Even in the ancestral environment, we would have run into problems where depth-first search is not very effective. Rather than spending time on refining your skills at chipping flint to make tools, for example, you may have been better served by learning to pick a better type of flint to work with before even starting. In modern times, however, these problems have grown more challenging and more numerous. I'm sure all of us have had the experience of working on some problem for a long time, refining our solution, maybe even trying to make the problem fit the solution we came up with out of frustration, only to give up, come back later, and then suddenly have a completely different and obvious solution come to mind. While part of this is probably due to some peripheral processes in our brain analysing the problem while our conscious thoughts were not focused on it, I think the key component is that by leaving the problem alone we \"forgot\" our first solution and were free to look for a better one.</p>\n<p>Similarly, it is possible that if Simon stops trying to come up with examples whenever he remembers the examples he's seen before, and only returns to the task when his mind is relatively blank, he might be able to produce something original. Even if he does that, unfortunately, he might still find himself coming up with only one example at a time, and then being stuck thinking only of examples that are somehow similar to it. Either way, having to pause for several hours every time he finds himself primed to think of something is far from ideal, and there are better solutions.</p>\n<p>&nbsp;</p>\n<p>The fundamental thing you should do when approaching a difficult, but tractable<sup>1</sup>, problem is to <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">avoid proposing a solution</a> immediately. The moment you propose a solution, your brain will be primed to to try to refine it or to look for similar solutions, even when it might be much more efficient to further analyse the problem or to look for other, radically different, solutions. Even when you have thoroughly understood the problem, however, you should still wait before proposing a solution, unless the problem is fairly easy and one solution is all you need.</p>\n<p>What I would recommend that you do before that is come up with a map of the idea-space, describing where the possible solutions might be found. For instance, before looking at a single example of calculus, Simon might have written down a list of idea areas to explore: \"<em>jobs, personal life, the natural world, engineering, other</em>\". He would then take the first broad category, \"<em>jobs</em>\" and expand it into a longer list, perhaps \"<em>agriculture, teaching, customer service, manufacturing, research, IT, other</em>\". With this longer list, he can then focus on each area in turn and either expand it further if it seems especially rife with examples, or come up with an example from the area directly. Once the area is depleted, which he might decide is the case if it takes him longer than one minute to come up with an example, he would move on to the next area.</p>\n<p>There are two main advantages to this approach. The examples Simon finds should be fairly representative of all the examples he can think of, since he started with a map of all such examples, and, better yet, he should be able to find examples much faster because he knows to stop looking in one small area when it becomes depleted.</p>\n<p>The same approach is also useful when you're trying to come up with the single best solution. For example, if you're trying to come up with a way to deal with climate change, you might write down \"<em>reduce carbon emissions, engineer the climate to be better, adapt to climate change, other</em>\" and move on from exploring one option to the next when the option runs into significant difficulties. Note, however, that in this case it is important to arrange your options in order of how likely you think you are to find your best solution within each of them to make sure you explore the most solution-rich areas first.</p>\n<p>In general, there are three main things<sup>2</sup> to keep in mind when creating a map of your idea-space:</p>\n<ul>\n<li>Don't get overly specific with the initial areas, since you will refine them when you're expanding them.</li>\n<li>Try to include all the areas that might contain a solution and none that do not. </li>\n<li>Try to pick areas so each of them is equally likely to contain a solution (or order the areas by the number of solutions and move on more quickly from solution-poor areas).</li>\n</ul>\n<p>For instance, when Simon came up with his list of areas for calculus examples, he correctly did not include \"philosophy\" in the list, since it contains much fewer examples than any of the other areas.</p>\n<p>&nbsp;</p>\n<p>In sum, when dealing with a challenging problem or coming with a lot of examples: don't jump to a solution, instead carefully consider the problem, come up with a map of areas of ideas where solutions might be found, and search the map until you get the solution you're looking for. If you use this approach, solutions should come to you faster, be needlessly complex less often, and be a lot more correct than the ones from your brain's na&iuml;ve search algorithm.</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1:</strong> It's important to note that if the problem is not tractable for you, in the sense that you can't tell if you're getting closer to a solution or not, these recommendations won't do much. For example, if I asked you \"What is the next number in the sequence 14, 15, 16, 17, 21, 23, 30, 33?\" it will help only slightly to hold off on coming up with solutions, and your best bet might be to start doing a depth-first search (as long as you keep in mind that you should not look for overly complicated solutions).</p>\n<p><strong>2:</strong> This is somewhat similar to Vladimir_Nesov's post recommending that we <a href=\"/lw/ex/consider_representative_data_sets/\">consider representative data sets</a> (in particular the three mistakes he lists, which are well worth reading).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FpHDkuYKMNHa2dbKR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7PuMfJAAaQwaPWAZY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 27, "extendedScore": null, "score": 8.801213918576826e-07, "legacy": true, "legacyId": "14962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZmQv4DFx6y4jFbhLy", "bMkCEZoBNhgRBtzoj", "uHYYA32CKgKT3FagE", "gWG9x4GGLkm5CYaP6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-08T12:59:00.690Z", "modifiedAt": null, "url": null, "title": "Two articles about futurism [LINKS]", "slug": "two-articles-about-futurism-links", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.092Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DTAFJsR2DMFyNsopD/two-articles-about-futurism-links", "pageUrlRelative": "/posts/DTAFJsR2DMFyNsopD/two-articles-about-futurism-links", "linkUrl": "https://www.lesswrong.com/posts/DTAFJsR2DMFyNsopD/two-articles-about-futurism-links", "postedAtFormatted": "Sunday, April 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20articles%20about%20futurism%20%5BLINKS%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20articles%20about%20futurism%20%5BLINKS%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTAFJsR2DMFyNsopD%2Ftwo-articles-about-futurism-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20articles%20about%20futurism%20%5BLINKS%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTAFJsR2DMFyNsopD%2Ftwo-articles-about-futurism-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTAFJsR2DMFyNsopD%2Ftwo-articles-about-futurism-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 14, "htmlBody": "<p><a href=\"http://www.alternet.org/visions/154773/10_big_mistakes_people_make_in_thinking_about_the_future_?page=1\">10 Big Mistakes People Make in Thinking about the Future</a></p>\n<p><a href=\"http://www.ourfuture.org/blog-entry/why-change-happens-ten-theories\">Why Change Happens: Ten Theories</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DTAFJsR2DMFyNsopD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 2, "extendedScore": null, "score": 8.803352620754357e-07, "legacy": true, "legacyId": "14967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-08T16:11:30.166Z", "modifiedAt": null, "url": null, "title": "\"Big Surprise\" - the famous atheists are actually Bayesians [link]", "slug": "big-surprise-the-famous-atheists-are-actually-bayesians-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.849Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7S87peYDCLjiZxag2/big-surprise-the-famous-atheists-are-actually-bayesians-link", "pageUrlRelative": "/posts/7S87peYDCLjiZxag2/big-surprise-the-famous-atheists-are-actually-bayesians-link", "linkUrl": "https://www.lesswrong.com/posts/7S87peYDCLjiZxag2/big-surprise-the-famous-atheists-are-actually-bayesians-link", "postedAtFormatted": "Sunday, April 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Big%20Surprise%22%20-%20the%20famous%20atheists%20are%20actually%20Bayesians%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Big%20Surprise%22%20-%20the%20famous%20atheists%20are%20actually%20Bayesians%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7S87peYDCLjiZxag2%2Fbig-surprise-the-famous-atheists-are-actually-bayesians-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Big%20Surprise%22%20-%20the%20famous%20atheists%20are%20actually%20Bayesians%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7S87peYDCLjiZxag2%2Fbig-surprise-the-famous-atheists-are-actually-bayesians-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7S87peYDCLjiZxag2%2Fbig-surprise-the-famous-atheists-are-actually-bayesians-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>http://bigthink.com/think-tank/neil-degrasse-tyson-atheist-or-agnostic</p>\n<p>Apparently Dawkins and Tyson give a non-zero probability to \"God\". Which is pretty much what is expected of a rational person. And of course it will be used by theists to say \"They aren't really sure!\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7S87peYDCLjiZxag2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -12, "extendedScore": null, "score": 8.804159713370247e-07, "legacy": true, "legacyId": "14968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-09T01:29:17.794Z", "modifiedAt": null, "url": null, "title": "The principle of \u2018altruistic arbitrage\u2019", "slug": "the-principle-of-altruistic-arbitrage", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertWiblin", "createdAt": "2009-03-07T08:59:22.724Z", "isAdmin": false, "displayName": "RobertWiblin"}, "userId": "rcwfERY7okewFqYLF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dMLcYjtwjDwsDErny/the-principle-of-altruistic-arbitrage", "pageUrlRelative": "/posts/dMLcYjtwjDwsDErny/the-principle-of-altruistic-arbitrage", "linkUrl": "https://www.lesswrong.com/posts/dMLcYjtwjDwsDErny/the-principle-of-altruistic-arbitrage", "postedAtFormatted": "Monday, April 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20principle%20of%20%E2%80%98altruistic%20arbitrage%E2%80%99&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20principle%20of%20%E2%80%98altruistic%20arbitrage%E2%80%99%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMLcYjtwjDwsDErny%2Fthe-principle-of-altruistic-arbitrage%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20principle%20of%20%E2%80%98altruistic%20arbitrage%E2%80%99%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMLcYjtwjDwsDErny%2Fthe-principle-of-altruistic-arbitrage", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMLcYjtwjDwsDErny%2Fthe-principle-of-altruistic-arbitrage", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 653, "htmlBody": "<p><span style=\"color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; line-height: 1.4; text-align: left;\">Cross-posted from&nbsp;</span><a style=\"font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; line-height: 1.4; text-align: left;\" href=\"http://robertwiblin.com/2012/04/06/the-principle-of-altruistic-arbitrage/\">http://www.robertwiblin.com</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">There is a principle in finance&nbsp;that obvious and guaranteed ways to make a lot of money, so called &lsquo;arbitrages&rsquo;, should not exist. It has a simple rationale. If market prices made it possible to trade assets around and in the process make a guaranteed profit, people would do it, in so doing shifting some prices up and others down. They would only stop making these trades once the prices had adjusted and the opportunity to make money had disappeared. While opportunities to make &lsquo;free money&rsquo; appear all the time, they are quickly noticed and the behaviour of traders eliminates them. The logic of selfishness and competition mean the only remaining ways to make big money should involve risk taking, luck and hard work. This is the&nbsp;&rsquo;<a style=\"text-decoration: none; color: #006a80; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cfe2e5;\" href=\"http://en.wikipedia.org/wiki/Arbitrage#Arbitrage-free\">no arbitrage</a>&lsquo; principle.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">Should a similar principle exist for selfless as well as selfish finance? When a guaranteed opportunity to do a lot of good for the world appears, philanthropists should notice and pounce on it, and only stop shifting resources into that activity once the opportunity has been exhausted. This wouldn&rsquo;t work as quickly as the elimination of arbitrage on financial markets of course. Rather it would look more like entrepreneurs searching for and exploiting opportunities to open new and profitable businesses. Still, in general competition to do good should make it challenging for an altruistic start-up or budding young philanthropist to beat existing charities at their own game.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">There is a very important difference though. Most investors are looking to make money and so for them a dollar is a dollar, whatever business activity it comes from. Competition between investors makes opportunities to get those dollars hard to find. The same is not true of altruists, who have very diverse preferences about who is most deserving of help and how we should help them; a &lsquo;util&rsquo; from one charitable activity is not the same as a &lsquo;util&rsquo; from another. This suggests that unlike in finance, we may able to find &lsquo;altruistic arbitrages&rsquo;, that is to say &lsquo;opportunities to do a lot of good for the world that others have left unexploited.&rsquo;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">The rule is simple: target groups you care about that other people mostly don&rsquo;t, and take advantage of strategies other people are biased against using. &nbsp;That rule is the root of a lot of advice offered to thoughtful givers and consequentialist-oriented folks. An obvious example is that you shouldn&rsquo;t look to help poor people in rich countries. There are already a lot of government and private dollars chasing opportunities to assist them, so the low hanging fruit has all been used up and then some. The better value opportunities are going to be in poor, unromantic places you have never heard of, where fewer competing philanthropist dollars are directed. Similarly, you should think about taking&nbsp;<a style=\"text-decoration: none; color: #006a80; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cfe2e5;\" href=\"http://www.utilitarian-essays.com/risky-investments.html\">high risk-high return strategies</a>. Most do-gooders are searching for guaranteed and respectable opportunities to do a bit of good, rather than peculiar long-shot opportunities to do a lot of good. If you only care about the &lsquo;<a style=\"text-decoration: none; color: #006a80; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cfe2e5;\" href=\"http://www.utilitarian-essays.com/why-expected-value.html\">expected</a>&lsquo; return to your charity, then you can do more by taking advantage of the quirky, improbable bets neglected by others.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">Who do I personally care about more than others? For me the main candidates are animals, especially&nbsp;<a style=\"text-decoration: none; color: #006a80; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cfe2e5;\" href=\"http://www.utilitarian-essays.com/suffering-nature.html\">wild ones</a>, and people who don&rsquo;t yet exist and may never exist &ndash; interest groups that go largely ignored by the majority of humanity. What are the risky strategies I can employ to help these groups? Working on future technologies most people think are farcical naturally jumps to mind but I&rsquo;m sure there are others and would love to hear them.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">This principle is the main reason I am skeptical of mainstream political activism as a way to improve the world. If you are part of a significant worldwide movement, it&rsquo;s unlikely that you&rsquo;re working in a neglected area and exploiting how your altruistic preferences are distinct from those of others.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.4; color: #545454; font-family: Verdana, Helvetica, Arial, sans-serif; font-size: 12px; text-align: left;\">What other conclusions can we draw thinking about philanthropy in this way?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dMLcYjtwjDwsDErny", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 8.806499213492396e-07, "legacy": true, "legacyId": "14977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-09T01:49:32.295Z", "modifiedAt": null, "url": null, "title": "Alcor vs. Cryonics Institute", "slug": "alcor-vs-cryonics-institute", "viewCount": null, "lastCommentedAt": "2019-09-12T10:02:44.937Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "prespectiveCryonaut", "createdAt": "2012-04-08T06:05:31.673Z", "isAdmin": false, "displayName": "prespectiveCryonaut"}, "userId": "3Pj2PaA6cnL6z3H2k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PdcpEih96tqrt2waM/alcor-vs-cryonics-institute", "pageUrlRelative": "/posts/PdcpEih96tqrt2waM/alcor-vs-cryonics-institute", "linkUrl": "https://www.lesswrong.com/posts/PdcpEih96tqrt2waM/alcor-vs-cryonics-institute", "postedAtFormatted": "Monday, April 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alcor%20vs.%20Cryonics%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlcor%20vs.%20Cryonics%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPdcpEih96tqrt2waM%2Falcor-vs-cryonics-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alcor%20vs.%20Cryonics%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPdcpEih96tqrt2waM%2Falcor-vs-cryonics-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPdcpEih96tqrt2waM%2Falcor-vs-cryonics-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 578, "htmlBody": "<p>I searched but did not find any discussion comparing the merits of the two major cryonics providers in the US, so I figured it might be productive to start such a discussion myself by posing the question to the community: which provider would you choose, all things being equal: Alcor or the Cryonics Institute?<br /><br />From my research, Alcor comes across as the flasher, higher-end option, while CI seems more like a Mom-and-Pop operation, having only two full-time employees. Alcor also costs substantially more, with its neurosuspension option alone running ~$80k, compared with CI's whole-body preservation cost of ~$30k. While Alcor has received far more publicity than CI, much of it has been negative. The Ted Williams fiasco is probably the most prominent example, although the accuser in that case seems anything but trustworthy. However, Alcor remains something of a shadowy organization that many within the cryonics community are suspicious of. Mike Darwin, a former Alcor president, has written at length on both organizations at http://www.chronopause.com, and on the whole, at least based on what I've read, Alcor comes across looking less competent, less trustworthy, and less open than CI.<br /><br />One issue in particular is funding. Even though Alcor costs much more, it has many more expenses, and Darwin and others have questioned the long term financial stability of the organization. Ralph Merkle, an Alcor board member and elder statesman of cryonics who has made significant contributions to other fields like nanotechnology, a field he practically invented, and encryption, with Merkle's Puzzles, has essentially admitted(1) that Alcor hasn't managed its money very well: <br /><br /><em>\"Some Alcor members have wondered why rich Alcor members have not donated more money to Alcor. The major reason is that rich Alcor members are rich because they know how to manage money, and they know that Alcor traditionally has managed money poorly. Why give any significant amount of money to an organization that has no fiscal discipline? It will just spend it, and put itself right back into the same financial hole it&rsquo;s already in.<br /><br />&nbsp;As a case in point, consider Alcor&rsquo;s efforts over the year to create an &ldquo;endowment fund&rdquo; to stabilize its operating budget. These efforts have always ended with Alcor spending the money on various useful activities. These range from research projects to subsidizing our existing members &mdash; raising dues and minimums is a painful thing to do, and the Board is always reluctant to do this even when the financial data is clear. While each such project is individually worthy and has merit, collectively the result has been to thwart the effort to create a lasting endowment and leave Alcor in a financially weak position.\"</em><br /><br />Such an acknowledgement, though appreciated, is frankly disturbing, considering that members depend utterly on these organizations remaining operational and solvent for decades, perhaps even centuries, after they are deanimated.<br /><br />Meanwhile, CI carries on merrily, well under the radar, seemingly without any drama or intrigue. And Ben Best seems to have very good credentials in the cryonics community, and Eliezer, one of the most prominent public advocates of cryonics, is signed up with them. Yet the tiny size of the operation still fills me with unease concerning its prospects for long-term survivability.<br /><br />So with all of that said, besides cost, what factors would lead or have led you to pick one organization over the other?</p>\n<p>1: http://www.alcor.org/Library/html/CryopreservationFundingAndInflation.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vmvTYnmaKA73fYDe5": 2, "ZnHkaTkxukegSrZqE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PdcpEih96tqrt2waM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 49, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "14982", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 125, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-09T03:21:11.350Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Identity Isn't In Specific Atoms", "slug": "seq-rerun-identity-isn-t-in-specific-atoms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.124Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EhoQ3T3TGuTPvn99Y/seq-rerun-identity-isn-t-in-specific-atoms", "pageUrlRelative": "/posts/EhoQ3T3TGuTPvn99Y/seq-rerun-identity-isn-t-in-specific-atoms", "linkUrl": "https://www.lesswrong.com/posts/EhoQ3T3TGuTPvn99Y/seq-rerun-identity-isn-t-in-specific-atoms", "postedAtFormatted": "Monday, April 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Identity%20Isn't%20In%20Specific%20Atoms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Identity%20Isn't%20In%20Specific%20Atoms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhoQ3T3TGuTPvn99Y%2Fseq-rerun-identity-isn-t-in-specific-atoms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Identity%20Isn't%20In%20Specific%20Atoms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhoQ3T3TGuTPvn99Y%2Fseq-rerun-identity-isn-t-in-specific-atoms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhoQ3T3TGuTPvn99Y%2Fseq-rerun-identity-isn-t-in-specific-atoms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>Today's post, <a href=\"/lw/pm/identity_isnt_in_specific_atoms/\">Identity Isn't In Specific Atoms</a> was originally published on 19 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Identity_Isn.27t_In_Specific_Atoms\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>As a consequence of quantum theory, we can see that the concept of swapping out all the atoms in you with \"different\" atoms is physical nonsense. It's not something that corresponds to anything that could ever be done, even in principle, because the concept is so confused. You are still you, no matter \"which\" atoms you are made of.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/biy/seq_rerun_no_individual_particles/\">No Individual Particles</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EhoQ3T3TGuTPvn99Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.806968644970738e-07, "legacy": true, "legacyId": "14989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RLScTpwc5W2gGGrL9", "Pma32az74pgDN3QFf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-09T03:47:38.468Z", "modifiedAt": null, "url": null, "title": "The Singularity Institute STILL needs remote researchers (may apply again; writing skill not required)", "slug": "the-singularity-institute-still-needs-remote-researchers-may", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:08.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T5WjwPRPHy8mPrJcJ/the-singularity-institute-still-needs-remote-researchers-may", "pageUrlRelative": "/posts/T5WjwPRPHy8mPrJcJ/the-singularity-institute-still-needs-remote-researchers-may", "linkUrl": "https://www.lesswrong.com/posts/T5WjwPRPHy8mPrJcJ/the-singularity-institute-still-needs-remote-researchers-may", "postedAtFormatted": "Monday, April 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Singularity%20Institute%20STILL%20needs%20remote%20researchers%20(may%20apply%20again%3B%20writing%20skill%20not%20required)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Singularity%20Institute%20STILL%20needs%20remote%20researchers%20(may%20apply%20again%3B%20writing%20skill%20not%20required)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5WjwPRPHy8mPrJcJ%2Fthe-singularity-institute-still-needs-remote-researchers-may%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Singularity%20Institute%20STILL%20needs%20remote%20researchers%20(may%20apply%20again%3B%20writing%20skill%20not%20required)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5WjwPRPHy8mPrJcJ%2Fthe-singularity-institute-still-needs-remote-researchers-may", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5WjwPRPHy8mPrJcJ%2Fthe-singularity-institute-still-needs-remote-researchers-may", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p><strong>Update</strong>: As of December 2012, we are still accepting applications!</p>\n<p>A while ago, I <a href=\"/r/discussion/lw/9t8/the_singularity_institute_needs_remote/\">announced</a> that the Singularity Institute is hiring remote researchers. I've hired a few people, but I still need more remote researchers. I think I screened off too many otherwise capable people because the 'test task' I asked applicants to perform was too time-consuming.</p>\n<p>So even if you've already applied and been rejected, please apply via the <strong><a href=\"http://tinyurl.com/remote-researchers\">new application form</a></strong>. The test task this time will not be quite so time consuming.</p>\n<p>Pay is hourly and starts at $14/hr but that will rise if the product is good. You must be available to work at least 20 hours/week to be considered.</p>\n<p>Perks:</p>\n<ul>\n<li>Work from home, with flexible hours.</li>\n<li>Age, location, and credentials are irrelevant; only the product matters.</li>\n<li>Get paid to research things you're probably interested in anyway.</li>\n<li>Contribute to human knowledge in immediately actionable ways. We need this research because we are about to <em>act</em>&nbsp;on it. Your work will not fall into the <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/Jacso-Five-year-impact-factor-data-in-the-Journal-Citation-Reports.pdf\">journal</a> <a href=\"http://209.123.244.94/polArticles.cfm?Author_Desc=Bill%20McKelvey\">abyss</a> that most academic research falls into.</li>\n</ul>\n<p>If you're interested, <strong><a href=\"http://tinyurl.com/remote-researchers\">apply here</a></strong>.</p>\n<p>Why post this job ad on LessWrong? We need people with some measure of <a href=\"/lw/96j/what_curiosity_looks_like/\">genuine curiosity</a>.</p>\n<p>Also see <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">Scholarship: How to Do It Efficiently</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T5WjwPRPHy8mPrJcJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "14990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LakrCAaj8rNss2q6j", "3oYaLja5h8qL5adDn", "37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-09T07:06:02.108Z", "modifiedAt": null, "url": null, "title": "Influence of scientific research", "slug": "influence-of-scientific-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:57.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alex_zag_al", "createdAt": "2011-11-16T23:52:10.523Z", "isAdmin": false, "displayName": "alex_zag_al"}, "userId": "pDkj9zKTeJPQuhurD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WpYgXaR2Co8voXnQY/influence-of-scientific-research", "pageUrlRelative": "/posts/WpYgXaR2Co8voXnQY/influence-of-scientific-research", "linkUrl": "https://www.lesswrong.com/posts/WpYgXaR2Co8voXnQY/influence-of-scientific-research", "postedAtFormatted": "Monday, April 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Influence%20of%20scientific%20research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInfluence%20of%20scientific%20research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpYgXaR2Co8voXnQY%2Finfluence-of-scientific-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Influence%20of%20scientific%20research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpYgXaR2Co8voXnQY%2Finfluence-of-scientific-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpYgXaR2Co8voXnQY%2Finfluence-of-scientific-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>I'm an undergraduate studying molecular biology, and I am thinking of going into science. In Timothy Gower's \"The Importance of Mathematics\", he says that many mathematicians just do whatever interests them, regardless of social benefit. I'd rather do something with some interest or technological benefit to people outside of a small group with a very specific education.</p>\n<p>Does anybody have any thoughts or links on judging the impact of the work on a research topic?</p>\n<p>Clearly, the pursuit of a research topic must be producing truth to be helpful, and I've read Vladimir_M's <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">heuristics</a> regarding this.</p>\n<p>Here's something I've tried. My current lab work is on the structure of membrane proteins in bacteria, so this is something I did to see where all this work on protein structure goes. I took a paper that I had found to be a very useful reference for my own work, about a protein that forms a pore in the bacterial membrane with a flexible loop, experimenting with the influence of this loop on the protein's structure. I used the Web of Science database to find a list of about two thousand papers that cited papers that cited this loop paper. I looked through this two-steps-away list for the ones that were not about molecules. Without too much effort, I found a few. The farthest from molecules that I found was a paper on a bacterium that sometimes causes meningitis, discussing about a particular stage in its colonization of the human body. A few of the two-steps-away articles were about antibiotics discovery; though molecular, this is a topic that has a great deal of impact outside of the world of research on biomolecules.</p>\n<p>Though it occurs to me that it might be more fruitful to look the other way around: to identify some social benefits or interests people have, and see what scientific research is contributing the most to them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WpYgXaR2Co8voXnQY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 8.807912093323273e-07, "legacy": true, "legacyId": "15005", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fyZBtNB3Ki3fM4a6Y"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-09T21:49:03.639Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality predictions", "slug": "harry-potter-and-the-methods-of-rationality-predictions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c3PnBSDfZGndg4vTt/harry-potter-and-the-methods-of-rationality-predictions", "pageUrlRelative": "/posts/c3PnBSDfZGndg4vTt/harry-potter-and-the-methods-of-rationality-predictions", "linkUrl": "https://www.lesswrong.com/posts/c3PnBSDfZGndg4vTt/harry-potter-and-the-methods-of-rationality-predictions", "postedAtFormatted": "Monday, April 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20predictions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20predictions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3PnBSDfZGndg4vTt%2Fharry-potter-and-the-methods-of-rationality-predictions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20predictions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3PnBSDfZGndg4vTt%2Fharry-potter-and-the-methods-of-rationality-predictions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3PnBSDfZGndg4vTt%2Fharry-potter-and-the-methods-of-rationality-predictions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<p>The recent spate of updates has reminded me that while each chapter is enjoyable, the <a href=\"/lw/b5s/harry_potter_and_the_methods_of_rationality/6503\">approaching end</a> of <a href=\"http://www.hpmor.com/\"><em>MoR</em></a>, as awesome as it no doubt will be, also means the end of our ability to <a href=\"http://www.gwern.net/Prediction%20markets#predictionbook-nights\">learn from predicting the truth</a> of the <em>MoR</em>-verse and its future.</p>\n<p>With that in mind, I have compiled <a href=\"http://www.gwern.net/hpmor\">a page of predictions</a> on sundry topics, much like my other page on <a href=\"http://www.gwern.net/otaku-predictions\">predictions for <em>Neon Genesis Evangelion</em></a>; I encourage people to suggest plausible predictions that I've omitted, register their probabilities on <a href=\"http://predictionbook.com/\">PredictionBook.com</a>, and come up with their own predictions. Then we can all look back when <em>MoR</em> finishes and reflect on what we (or Eliezer) did poorly or well. &nbsp;</p>\n<p>The page is currently up to &gt;182 predictions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c3PnBSDfZGndg4vTt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 8.811618984360054e-07, "legacy": true, "legacyId": "15016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T01:45:46.323Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Zombies: The Movie", "slug": "seq-rerun-zombies-the-movie", "viewCount": null, "lastCommentedAt": "2012-04-10T01:45:46.323Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/38MNW4EjHnR29MGqu/seq-rerun-zombies-the-movie", "pageUrlRelative": "/posts/38MNW4EjHnR29MGqu/seq-rerun-zombies-the-movie", "linkUrl": "https://www.lesswrong.com/posts/38MNW4EjHnR29MGqu/seq-rerun-zombies-the-movie", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Zombies%3A%20The%20Movie&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Zombies%3A%20The%20Movie%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38MNW4EjHnR29MGqu%2Fseq-rerun-zombies-the-movie%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Zombies%3A%20The%20Movie%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38MNW4EjHnR29MGqu%2Fseq-rerun-zombies-the-movie", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38MNW4EjHnR29MGqu%2Fseq-rerun-zombies-the-movie", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/pn/zombies_the_movie/\">Zombies: The Movie</a> was originally published on 20 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Zombies:_The_Movie\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A satirical suggestion for a zombie movie, but not about the lurching and drooling kind. The philosophical kind.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bkd/seq_rerun_identity_isnt_in_specific_atoms/\">Identity Isn't In Specific Atoms</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "38MNW4EjHnR29MGqu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 8.812613152276549e-07, "legacy": true, "legacyId": "15027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fsDz6HieZJBu54Yes", "EhoQ3T3TGuTPvn99Y", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T01:53:08.001Z", "modifiedAt": null, "url": null, "title": "In Defense of Ayn Rand", "slug": "in-defense-of-ayn-rand", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:59.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ryjm", "createdAt": "2012-02-09T19:36:44.546Z", "isAdmin": false, "displayName": "ryjm"}, "userId": "vvCqsPSKbzdYhqEeT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7s5gYi7EagfkzvLp8/in-defense-of-ayn-rand", "pageUrlRelative": "/posts/7s5gYi7EagfkzvLp8/in-defense-of-ayn-rand", "linkUrl": "https://www.lesswrong.com/posts/7s5gYi7EagfkzvLp8/in-defense-of-ayn-rand", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Defense%20of%20Ayn%20Rand&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Defense%20of%20Ayn%20Rand%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s5gYi7EagfkzvLp8%2Fin-defense-of-ayn-rand%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Defense%20of%20Ayn%20Rand%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s5gYi7EagfkzvLp8%2Fin-defense-of-ayn-rand", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s5gYi7EagfkzvLp8%2Fin-defense-of-ayn-rand", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2653, "htmlBody": "<p>&nbsp;</p>\n<div id=\"content\">\n<h1 class=\"title\">In Defense of Ayn Rand</h1>\n<p><strong>WARNING</strong>: Do not read the footnotes if have not read Atlas Shrugged, they contain primarily quotes from the book. They don't reveal much in terms of plot (except for #6) so read them if you feel daring.</p>\n<p><strong>Preface</strong>: This is NOT a defense of objectivism nor is it a defense of the cultish nature of followers of objectivism. This is a defense of Ayn Rand the woman and a response to her portrayal in the essay <a href=\"/lw/m1/guardians_of_ayn_rand/\">The Guardians of Ayn Rand</a>. I realize that the essay was making a point about cults and not primarily a criticism of Ayn Rand, but since she was the focal point of the ideas and the piece is a part of the Sequences, I felt it necessary to write this. There are enough people who criticize Ayn Rand, and the literature of her critics is vast - but to spit on her contributions with little reference to any factual details and with a huge emphasis on her personal life just did not seem to fit with the spirit of this website. If Rand really was as poor of a thinker as she is being portrayed, some evidence would be very much appreciated. It was originally a comment, but it became way too long. I am NOT an expert on objectivism whatsoever. Please correct me on any inaccuracies.</p>\n<p><em>Note: I am using the word rationality as Rand uses it.</em></p>\n<blockquote>\n<p>\"And yet Ayn Rand acknowledged no superior, in the past, or in the future yet to come. Rand, who began in admiring reason and individuality, ended by ostracizing anyone who dared contradict her. Shermer: \"[Barbara] Branden recalled an evening when a friend of Rand's remarked that he enjoyed the music of Richard Strauss. 'When he left at the end of the evening, Ayn said, in a reaction becoming increasingly typical, 'Now I understand why he and I can never be real soulmates. The distance in our sense of life is too great.' Often she did not wait until a friend had left to make such remarks.\"\"</p>\n</blockquote>\n<p>Rand's choice of companions was governed by her life philosophy, which with respect to relationships was akin to a business deal, selfishly trading her willingness to interact with an individual for that person's virtue<sup><a class=\"footref\" name=\"fnr.1\" href=\"#fn.1\">1</a></sup>. If she did not find another's virtue a sufficient payment for her companionship, she would not interact with them.</p>\n<p>The part about Rand's professed superiority just seems like a blatant falsehood. All of her writings are based (it says so on the back of the books) on the existence of <em>heroes</em> in humanity. How can she acknowledge no superior <em>ever</em> and still feel comfortable with her ideas? Even further, her whole philosophy is based on seeing reality <em>exactly as it is</em> <sup><a class=\"footref\" name=\"fnr.2\" href=\"#fn.2\">2</a></sup>.</p>\n<p>Her excommunication of Branden is a fine example of her irrationality in her private life. It is all that was needed in The Guardians of Ayn Rand to make the point. Clearly, it shows Rand's inability to match her actions with her words, and shows an <em>irrational</em> example of her tendency to ostracize people (I think there are justifications of her actions, and I believe her journal writings shed a different light on the situation, but I will agree with the analysis on this point unless some very strong evidence to the contrary appears).</p>\n<p>But she <em>rationally</em> ostracized people who disagreed with her because she herself has said that she completely and fully embodies her philosophy - if you disagree with her, you disagree with her philosophy. Since her philosophy is so entrenched in the actions of an individual, it is no wonder why she would choose to ostracize those who disagree with her from her <em>personal</em> circle of companions! The core of her philosophy rests on the assertion that no man should live for another and that no man should take steps to fake reality on account of another person<sup><a class=\"footref\" name=\"fnr.3\" href=\"#fn.3\">3</a></sup>.</p>\n<p>One's rational perception of the world is of the utmost importance; Rand's conclusion that one person will not become her soulmate is the result of her rational perception of his actions. Her knowledge of music might not be the same knowledge held by a composer, but that is of no consequence in determining the reality of a situation<sup><a class=\"footref\" name=\"fnr.4\" href=\"#fn.4\">4</a></sup>. One's choice of musical preference seems to be, in Rand's eyes, reflective of the values they uphold. This is her rational view of reality which she has arrived at through conscious perception and thought; someone else might think it is the right perception while another might not. If confronted with this, she would most likely (from my readings of her philosophy) seek to justify her assertion through proof based on her own rational perception of the world. Refusing to do so would be an example of an irrational action on her part<sup><a class=\"footref\" name=\"fnr.5\" href=\"#fn.5\">5</a></sup>.</p>\n<p>Eliezer presented proof of his assertion that her actions are not justifiable with only a couple of anecdotes that reveal no context. The description of her actions are taken from a biography written by the wife of Nathaniel Branden, who had a significant personal conflict with Rand. This may or may not be important, but I think it is worth mentioning.</p>\n<p>The observation that she chose to crush those of whom she disapproves only refers to her influence in her own personal circle of companions (and of course she has done it elsewhere, though I have not seen event where such an action has contradicted her philosophy besides the Branden affair). Her right to do so is implicit in her philosophy and is encouraged, yet her actions are portrayed as a failure to recognize a cognitive bias rather than a factual failure in her philosophy. Many aspects<sup><a class=\"footref\" name=\"fnr.2.2\" href=\"#fn.2\">2</a></sup> of Rand's philosophy are consequent with the ideas in the sequences too (though the similarities stop with respect to Aristotle).</p>\n<blockquote>\n<p>\"It's noteworthy, I think, that Ayn Rand's fictional heroes were architects and engineers; John Galt, her ultimate, was a physicist; and yet Ayn Rand herself wasn't a great scientist. As far as I know, she wasn't particularly good at math. She could not aspire to rival her own heroes. Maybe that's why she began to lose track of Tsuyoku Naritai\".</p>\n</blockquote>\n<p>Rand's fictional heroes were not just architects and engineers, and the point about her not being a great scientist is irrelevant with regard to the nature and purpose of her philosophy. The top comment also sheds light on the facts:</p>\n<blockquote>\n<p>Eliezer: \"As far as I know, [Rand] wasn't particularly good at math.\"</p>\n<p>A relevant passage from Barbara Branden's biography of Rand:</p>\n<p>\"The subject [Rand] most enjoyed during her high school years, the one subject of which she never tired, was mathematics. 'My mathematics teacher was delighted with me. When I graduated, he said, \"It will be a crime if you don't go into mathematics.\" I said only, \"That's not enough of a career.\" I felt that it was too abstract, it had nothing to do with real life. I loved it, but I didn't intend to be an engineer or to go into any applied profession, and to study mathematics as such seemed too ivory tower, too purposeless&mdash;and I would say so today.' Mathematics, she thought, was a method. Like logic, it was an invaluable tool, but it was a means to an end, not an end in itself. She wanted an activity that, while drawing on her theoretical capacity, would unite theory and its practical application. That desire was an essential element in the continuing appeal that fiction held for her: fiction made possible the integration of wide abstract principles and their direct expression in and application to man's life.\" (Barbara Branden, The Passion of Ayn Rand, page 35 of my edition)</p>\n<p>&ndash; Z.M Davis</p>\n</blockquote>\n<p>And she <em>did</em> tell her followers (and even people who weren't her followers) to study science. She even gave a speech at MIT in the 60's entitled \"To Young Scientists\" (You can find the transcript somewhere, though you may have to pay for it). She also wrote an eyewitness account of the Apollo 11 launch that vehemently shows her appreciation and awe of the products of science. If that isn't an encouragement to study science, I don't know what is<sup><a class=\"footref\" name=\"fnr.6\" href=\"#fn.6\">6</a></sup>.</p>\n<p>This analysis is not <em>fair</em>. There is nothing fair about representing a figure in an incredibly poor light in order to emphasize a point about cults. Using her very public affair and the cultish nature of her followers would have been sufficient, but attacking her actions without mention of the underlying philosophy guiding them was unnecessary and, at many points (more evidence in the comments of the essay), factually incorrect. The tone of the essay was also incredibly arrogant, portraying Rand as some delusional crackpot and downplaying her accomplishments:</p>\n<blockquote>\n<p>\"Ayn Rand fled the Soviet Union, wrote a book about individualism that a lot of people liked, got plenty of compliments, and formed a coterie of admirers. Her admirers found nicer and nicer things to say about her (happy death spiral), and she enjoyed it too much to tell them to shut up. She found herself with the power to crush those of whom she disapproved, and she didn't resist the temptation of power\"</p>\n</blockquote>\n<p>I mean, come on! For someone who consistently encourages a charitable reading of his writing, this usage of Rand as an example of irrationality and poor judgement is disheartening. At the very least, some semblance of respect for her accomplishments would not be out of place.</p>\n<p><strong>Afterthought</strong>: It is my opinion that the treatment of Ayn Rand's personal life was not in the spirit of rational discussion. However, as is most often the case with Eliezer's writings, the ideas in the essay for which Rand was supposed to be a foil to were incredibly thought provoking. In particular, the philosophical implications of closed vs open systems. Here is an excerpt of an <a href=\"http://www.aynrand.org/site/PageServer?pagename=objectivism_fv\">essay</a> I found on the Ayn Rand Institute's website, defending objectivism as a closed system, that gives some much needed context absent from the previous discussion:</p>\n<blockquote>\n<p>IN HIS LAST PARAGRAPH, Kelley states that Ayn Rand&rsquo;s philosophy, though magnificent, &ldquo;is not a closed system.&rdquo; Yes, it is. Philosophy, as Ayn Rand often observed, deals only with the kinds of issues available to men in any era; it does not change with the growth of human knowledge, since it is the base and precondition of that growth. Every philosophy, by the nature of the subject, is immutable. New implications, applications, integrations can always be discovered; but the essence of the system&mdash;its fundamental principles and their consequences in every branch&mdash;is laid down once and for all by the philosophy&rsquo;s author. If this applies to any philosophy, think how much more obviously it applies to Objectivism. Objectivism holds that every truth is an absolute, and that a proper philosophy is an integrated whole, any change in any element of which would destroy the entire system.</p>\n<p>In yet another expression of his subjectivism in epistemology, Kelley decries, as intolerant, any Objectivist&rsquo;s (or indeed anyone&rsquo;s) &ldquo;obsession with official or authorized doctrine,&rdquo; which &ldquo;obsession&rdquo; he regards as appropriate only to dogmatic viewpoints. In other words, the alternative once again is whim or dogma: either anyone is free to rewrite Objectivism as he wishes or else, through the arbitrary fiat of some authority figure, his intellectual freedom is being stifled. My answer is: Objectivism does have an &ldquo;official, authorized doctrine,&rdquo; but it is not dogma. It is stated and validated objectively in Ayn Rand&rsquo;s works.</p>\n<p>&ldquo;Objectivism&rdquo; is the name of Ayn Rand&rsquo;s achievement. Anyone else's interpretation or development of her ideas, my own work emphatically included, is precisely that: an interpretation or development, which may or may not be logically consistent with what she wrote. In regard to the consistency of any such derivative work, each man must reach his own verdict, by weighing all the relevant evidence. The &ldquo;official, authorized doctrine,&rdquo; however, remains unchanged and untouched in Ayn Rand&rsquo;s books; it is not affected by any interpreters.</p>\n<p>The Constitution and the Declaration of Independence state the &ldquo;official&rdquo; doctrine of the government of the United States, and no one, including the Supreme Court, can alter the meaning of this doctrine. What the Constitution and the Declaration are to the United States, Atlas Shrugged and Ayn Rand&rsquo;s other works are to Objectivism. Objectivism, therefore, is &ldquo;rigid,&rdquo; &ldquo;narrow,&rdquo; &ldquo;intolerant&rdquo; and &ldquo;closed-minded.&rdquo; If anyone wants to reject Ayn Rand&rsquo;s ideas and invent a new viewpoint, he is free to do so&mdash;but he cannot, as a matter of honesty, label his new ideas or himself &ldquo;Objectivist.&rdquo;</p>\n<p>Objectivism is not just &ldquo;common sense&rdquo;; it is a revolutionary philosophy, which is a fact we do not always keep in mind. Ayn Rand challenges every fundamental that men have accepted for millennia. The essence of her revolution lies in her concept of &ldquo;objectivity,&rdquo; which applies to epistemology and to ethics, i.e., to cognition and to evaluation. At this early stage of history, a great many people, though bright and initially drawn to Ayn Rand, are still unable (or unwilling) fully to grasp this central concept. They accept various ideas from Ayn Rand out of context, without digesting them by penetrating to the foundation; thus they never uproot all the contradictory ideas they have accepted, the ones which guided the formation of their own souls and minds. Such people are torn by an impossible conflict: they have one foot (or toe) in the Objectivist world and the rest of themselves planted firmly in the conventional world. People like this do not mind being controversial so long as they are fashionable or &ldquo;in&rdquo;; i.e., so long as they can be popular in their subculture, or politically powerful or academically respectable; to attain which status, they will &ldquo;tolerate&rdquo; (or show &ldquo;compassion&rdquo; for) whatever they have to.</p>\n<p>The real enemy of these men is not Ayn Rand; it is reality. But Ayn Rand is the messenger who brings them the hated message, which, somehow, they must escape or dilute (some of them, I think, never even get it). The message is that they must conform to reality 24 hours a day and all the way down.</p>\n</blockquote>\n<p>Definitely a more apt example of the cultish nature of objectivism, though it has its merits; good fodder for discussion.</p>\n<div id=\"footnotes\">\n<h2 class=\"footnotes\">Footnotes:</h2>\n<div id=\"text-footnotes\">\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.1\" href=\"#fnr.1\">1</a></sup> <span style=\"color: #ffffff;\">\"A trader does not ask to be paid for his failures, nor does he ask to be loved for his flaws. A trader does not squander his body as fodder or his soul as alms. Just as he does not give his work except in trade for material values, so he does not give the values of his spirit-his love, his friendship, his esteem-except in payment and in trade for human virtues, in payment for his own selfish pleasure, which he receives from men he can respect.\" - John Galt </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.2\" href=\"#fnr.2\">2</a></sup> <span style=\"color: #ffffff;\">&ldquo;Your mind is your only judge of truth&ndash;and if others dissent from your verdict, reality is the court of final appeal.&rdquo; &mdash; John Galt </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.3\" href=\"#fnr.3\">3</a></sup> <span style=\"color: #ffffff;\">&ldquo;People think that a liar gains a victory over his victim. What I've learned is that a lie is an act of self-abdication, because one surrenders one's reality to the person to whom one lies, making that person one's master, condemning oneself from then on to faking the sort of reality that person's view requires to be faked.&rdquo; &mdash; Hank Rearden </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.4\" href=\"#fnr.4\">4</a></sup> <span style=\"color: #ffffff;\">\"By refusing to say 'It is' you are refusing to say 'I am'. By suspending your judgment, you are negating your person. When a man declares: 'Who am I to know?' he is declaring: 'Who am I to live?'\" - John Galt &lt;\\font&gt; </span></p>\n<span style=\"color: #ffffff;\">\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.5\" href=\"#fnr.5\">5</a></sup> <span style=\"color: #ffffff;\">&ldquo;You don't have to see through the eyes of others, hold onto yours, stand on your own judgment, you know that what is, is&ndash;say it aloud, like the holiest of prayers, and don't let anyone tell you otherwise.&rdquo; &mdash; Dagny Taggart </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.6\" href=\"#fnr.6\">6</a></sup> <span style=\"color: #ffffff;\"> Also, the main character of Atlas Shrugged was a physicist, invented a motor that harnessed the power of static electricity, and then went on to save the damn country. That's not encouragement to study science?</span></p>\n</span></div>\n<span style=\"color: #ffffff;\"> </span></div>\n<span style=\"color: #ffffff;\"> </span></div>\n<p><span style=\"color: #ffffff;\"> </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7s5gYi7EagfkzvLp8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "15026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<div id=\"content\">\n<h1 class=\"title\" id=\"In_Defense_of_Ayn_Rand\">In Defense of Ayn Rand</h1>\n<p><strong>WARNING</strong>: Do not read the footnotes if have not read Atlas Shrugged, they contain primarily quotes from the book. They don't reveal much in terms of plot (except for #6) so read them if you feel daring.</p>\n<p><strong>Preface</strong>: This is NOT a defense of objectivism nor is it a defense of the cultish nature of followers of objectivism. This is a defense of Ayn Rand the woman and a response to her portrayal in the essay <a href=\"/lw/m1/guardians_of_ayn_rand/\">The Guardians of Ayn Rand</a>. I realize that the essay was making a point about cults and not primarily a criticism of Ayn Rand, but since she was the focal point of the ideas and the piece is a part of the Sequences, I felt it necessary to write this. There are enough people who criticize Ayn Rand, and the literature of her critics is vast - but to spit on her contributions with little reference to any factual details and with a huge emphasis on her personal life just did not seem to fit with the spirit of this website. If Rand really was as poor of a thinker as she is being portrayed, some evidence would be very much appreciated. It was originally a comment, but it became way too long. I am NOT an expert on objectivism whatsoever. Please correct me on any inaccuracies.</p>\n<p><em>Note: I am using the word rationality as Rand uses it.</em></p>\n<blockquote>\n<p>\"And yet Ayn Rand acknowledged no superior, in the past, or in the future yet to come. Rand, who began in admiring reason and individuality, ended by ostracizing anyone who dared contradict her. Shermer: \"[Barbara] Branden recalled an evening when a friend of Rand's remarked that he enjoyed the music of Richard Strauss. 'When he left at the end of the evening, Ayn said, in a reaction becoming increasingly typical, 'Now I understand why he and I can never be real soulmates. The distance in our sense of life is too great.' Often she did not wait until a friend had left to make such remarks.\"\"</p>\n</blockquote>\n<p>Rand's choice of companions was governed by her life philosophy, which with respect to relationships was akin to a business deal, selfishly trading her willingness to interact with an individual for that person's virtue<sup><a class=\"footref\" name=\"fnr.1\" href=\"#fn.1\">1</a></sup>. If she did not find another's virtue a sufficient payment for her companionship, she would not interact with them.</p>\n<p>The part about Rand's professed superiority just seems like a blatant falsehood. All of her writings are based (it says so on the back of the books) on the existence of <em>heroes</em> in humanity. How can she acknowledge no superior <em>ever</em> and still feel comfortable with her ideas? Even further, her whole philosophy is based on seeing reality <em>exactly as it is</em> <sup><a class=\"footref\" name=\"fnr.2\" href=\"#fn.2\">2</a></sup>.</p>\n<p>Her excommunication of Branden is a fine example of her irrationality in her private life. It is all that was needed in The Guardians of Ayn Rand to make the point. Clearly, it shows Rand's inability to match her actions with her words, and shows an <em>irrational</em> example of her tendency to ostracize people (I think there are justifications of her actions, and I believe her journal writings shed a different light on the situation, but I will agree with the analysis on this point unless some very strong evidence to the contrary appears).</p>\n<p>But she <em>rationally</em> ostracized people who disagreed with her because she herself has said that she completely and fully embodies her philosophy - if you disagree with her, you disagree with her philosophy. Since her philosophy is so entrenched in the actions of an individual, it is no wonder why she would choose to ostracize those who disagree with her from her <em>personal</em> circle of companions! The core of her philosophy rests on the assertion that no man should live for another and that no man should take steps to fake reality on account of another person<sup><a class=\"footref\" name=\"fnr.3\" href=\"#fn.3\">3</a></sup>.</p>\n<p>One's rational perception of the world is of the utmost importance; Rand's conclusion that one person will not become her soulmate is the result of her rational perception of his actions. Her knowledge of music might not be the same knowledge held by a composer, but that is of no consequence in determining the reality of a situation<sup><a class=\"footref\" name=\"fnr.4\" href=\"#fn.4\">4</a></sup>. One's choice of musical preference seems to be, in Rand's eyes, reflective of the values they uphold. This is her rational view of reality which she has arrived at through conscious perception and thought; someone else might think it is the right perception while another might not. If confronted with this, she would most likely (from my readings of her philosophy) seek to justify her assertion through proof based on her own rational perception of the world. Refusing to do so would be an example of an irrational action on her part<sup><a class=\"footref\" name=\"fnr.5\" href=\"#fn.5\">5</a></sup>.</p>\n<p>Eliezer presented proof of his assertion that her actions are not justifiable with only a couple of anecdotes that reveal no context. The description of her actions are taken from a biography written by the wife of Nathaniel Branden, who had a significant personal conflict with Rand. This may or may not be important, but I think it is worth mentioning.</p>\n<p>The observation that she chose to crush those of whom she disapproves only refers to her influence in her own personal circle of companions (and of course she has done it elsewhere, though I have not seen event where such an action has contradicted her philosophy besides the Branden affair). Her right to do so is implicit in her philosophy and is encouraged, yet her actions are portrayed as a failure to recognize a cognitive bias rather than a factual failure in her philosophy. Many aspects<sup><a class=\"footref\" name=\"fnr.2.2\" href=\"#fn.2\">2</a></sup> of Rand's philosophy are consequent with the ideas in the sequences too (though the similarities stop with respect to Aristotle).</p>\n<blockquote>\n<p>\"It's noteworthy, I think, that Ayn Rand's fictional heroes were architects and engineers; John Galt, her ultimate, was a physicist; and yet Ayn Rand herself wasn't a great scientist. As far as I know, she wasn't particularly good at math. She could not aspire to rival her own heroes. Maybe that's why she began to lose track of Tsuyoku Naritai\".</p>\n</blockquote>\n<p>Rand's fictional heroes were not just architects and engineers, and the point about her not being a great scientist is irrelevant with regard to the nature and purpose of her philosophy. The top comment also sheds light on the facts:</p>\n<blockquote>\n<p>Eliezer: \"As far as I know, [Rand] wasn't particularly good at math.\"</p>\n<p>A relevant passage from Barbara Branden's biography of Rand:</p>\n<p>\"The subject [Rand] most enjoyed during her high school years, the one subject of which she never tired, was mathematics. 'My mathematics teacher was delighted with me. When I graduated, he said, \"It will be a crime if you don't go into mathematics.\" I said only, \"That's not enough of a career.\" I felt that it was too abstract, it had nothing to do with real life. I loved it, but I didn't intend to be an engineer or to go into any applied profession, and to study mathematics as such seemed too ivory tower, too purposeless\u2014and I would say so today.' Mathematics, she thought, was a method. Like logic, it was an invaluable tool, but it was a means to an end, not an end in itself. She wanted an activity that, while drawing on her theoretical capacity, would unite theory and its practical application. That desire was an essential element in the continuing appeal that fiction held for her: fiction made possible the integration of wide abstract principles and their direct expression in and application to man's life.\" (Barbara Branden, The Passion of Ayn Rand, page 35 of my edition)</p>\n<p>\u2013 Z.M Davis</p>\n</blockquote>\n<p>And she <em>did</em> tell her followers (and even people who weren't her followers) to study science. She even gave a speech at MIT in the 60's entitled \"To Young Scientists\" (You can find the transcript somewhere, though you may have to pay for it). She also wrote an eyewitness account of the Apollo 11 launch that vehemently shows her appreciation and awe of the products of science. If that isn't an encouragement to study science, I don't know what is<sup><a class=\"footref\" name=\"fnr.6\" href=\"#fn.6\">6</a></sup>.</p>\n<p>This analysis is not <em>fair</em>. There is nothing fair about representing a figure in an incredibly poor light in order to emphasize a point about cults. Using her very public affair and the cultish nature of her followers would have been sufficient, but attacking her actions without mention of the underlying philosophy guiding them was unnecessary and, at many points (more evidence in the comments of the essay), factually incorrect. The tone of the essay was also incredibly arrogant, portraying Rand as some delusional crackpot and downplaying her accomplishments:</p>\n<blockquote>\n<p>\"Ayn Rand fled the Soviet Union, wrote a book about individualism that a lot of people liked, got plenty of compliments, and formed a coterie of admirers. Her admirers found nicer and nicer things to say about her (happy death spiral), and she enjoyed it too much to tell them to shut up. She found herself with the power to crush those of whom she disapproved, and she didn't resist the temptation of power\"</p>\n</blockquote>\n<p>I mean, come on! For someone who consistently encourages a charitable reading of his writing, this usage of Rand as an example of irrationality and poor judgement is disheartening. At the very least, some semblance of respect for her accomplishments would not be out of place.</p>\n<p><strong>Afterthought</strong>: It is my opinion that the treatment of Ayn Rand's personal life was not in the spirit of rational discussion. However, as is most often the case with Eliezer's writings, the ideas in the essay for which Rand was supposed to be a foil to were incredibly thought provoking. In particular, the philosophical implications of closed vs open systems. Here is an excerpt of an <a href=\"http://www.aynrand.org/site/PageServer?pagename=objectivism_fv\">essay</a> I found on the Ayn Rand Institute's website, defending objectivism as a closed system, that gives some much needed context absent from the previous discussion:</p>\n<blockquote>\n<p>IN HIS LAST PARAGRAPH, Kelley states that Ayn Rand\u2019s philosophy, though magnificent, \u201cis not a closed system.\u201d Yes, it is. Philosophy, as Ayn Rand often observed, deals only with the kinds of issues available to men in any era; it does not change with the growth of human knowledge, since it is the base and precondition of that growth. Every philosophy, by the nature of the subject, is immutable. New implications, applications, integrations can always be discovered; but the essence of the system\u2014its fundamental principles and their consequences in every branch\u2014is laid down once and for all by the philosophy\u2019s author. If this applies to any philosophy, think how much more obviously it applies to Objectivism. Objectivism holds that every truth is an absolute, and that a proper philosophy is an integrated whole, any change in any element of which would destroy the entire system.</p>\n<p>In yet another expression of his subjectivism in epistemology, Kelley decries, as intolerant, any Objectivist\u2019s (or indeed anyone\u2019s) \u201cobsession with official or authorized doctrine,\u201d which \u201cobsession\u201d he regards as appropriate only to dogmatic viewpoints. In other words, the alternative once again is whim or dogma: either anyone is free to rewrite Objectivism as he wishes or else, through the arbitrary fiat of some authority figure, his intellectual freedom is being stifled. My answer is: Objectivism does have an \u201cofficial, authorized doctrine,\u201d but it is not dogma. It is stated and validated objectively in Ayn Rand\u2019s works.</p>\n<p>\u201cObjectivism\u201d is the name of Ayn Rand\u2019s achievement. Anyone else's interpretation or development of her ideas, my own work emphatically included, is precisely that: an interpretation or development, which may or may not be logically consistent with what she wrote. In regard to the consistency of any such derivative work, each man must reach his own verdict, by weighing all the relevant evidence. The \u201cofficial, authorized doctrine,\u201d however, remains unchanged and untouched in Ayn Rand\u2019s books; it is not affected by any interpreters.</p>\n<p>The Constitution and the Declaration of Independence state the \u201cofficial\u201d doctrine of the government of the United States, and no one, including the Supreme Court, can alter the meaning of this doctrine. What the Constitution and the Declaration are to the United States, Atlas Shrugged and Ayn Rand\u2019s other works are to Objectivism. Objectivism, therefore, is \u201crigid,\u201d \u201cnarrow,\u201d \u201cintolerant\u201d and \u201cclosed-minded.\u201d If anyone wants to reject Ayn Rand\u2019s ideas and invent a new viewpoint, he is free to do so\u2014but he cannot, as a matter of honesty, label his new ideas or himself \u201cObjectivist.\u201d</p>\n<p>Objectivism is not just \u201ccommon sense\u201d; it is a revolutionary philosophy, which is a fact we do not always keep in mind. Ayn Rand challenges every fundamental that men have accepted for millennia. The essence of her revolution lies in her concept of \u201cobjectivity,\u201d which applies to epistemology and to ethics, i.e., to cognition and to evaluation. At this early stage of history, a great many people, though bright and initially drawn to Ayn Rand, are still unable (or unwilling) fully to grasp this central concept. They accept various ideas from Ayn Rand out of context, without digesting them by penetrating to the foundation; thus they never uproot all the contradictory ideas they have accepted, the ones which guided the formation of their own souls and minds. Such people are torn by an impossible conflict: they have one foot (or toe) in the Objectivist world and the rest of themselves planted firmly in the conventional world. People like this do not mind being controversial so long as they are fashionable or \u201cin\u201d; i.e., so long as they can be popular in their subculture, or politically powerful or academically respectable; to attain which status, they will \u201ctolerate\u201d (or show \u201ccompassion\u201d for) whatever they have to.</p>\n<p>The real enemy of these men is not Ayn Rand; it is reality. But Ayn Rand is the messenger who brings them the hated message, which, somehow, they must escape or dilute (some of them, I think, never even get it). The message is that they must conform to reality 24 hours a day and all the way down.</p>\n</blockquote>\n<p>Definitely a more apt example of the cultish nature of objectivism, though it has its merits; good fodder for discussion.</p>\n<div id=\"footnotes\">\n<h2 class=\"footnotes\" id=\"Footnotes_\">Footnotes:</h2>\n<div id=\"text-footnotes\">\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.1\" href=\"#fnr.1\">1</a></sup> <span style=\"color: #ffffff;\">\"A trader does not ask to be paid for his failures, nor does he ask to be loved for his flaws. A trader does not squander his body as fodder or his soul as alms. Just as he does not give his work except in trade for material values, so he does not give the values of his spirit-his love, his friendship, his esteem-except in payment and in trade for human virtues, in payment for his own selfish pleasure, which he receives from men he can respect.\" - John Galt </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.2\" href=\"#fnr.2\">2</a></sup> <span style=\"color: #ffffff;\">\u201cYour mind is your only judge of truth\u2013and if others dissent from your verdict, reality is the court of final appeal.\u201d \u2014 John Galt </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.3\" href=\"#fnr.3\">3</a></sup> <span style=\"color: #ffffff;\">\u201cPeople think that a liar gains a victory over his victim. What I've learned is that a lie is an act of self-abdication, because one surrenders one's reality to the person to whom one lies, making that person one's master, condemning oneself from then on to faking the sort of reality that person's view requires to be faked.\u201d \u2014 Hank Rearden </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.4\" href=\"#fnr.4\">4</a></sup> <span style=\"color: #ffffff;\">\"By refusing to say 'It is' you are refusing to say 'I am'. By suspending your judgment, you are negating your person. When a man declares: 'Who am I to know?' he is declaring: 'Who am I to live?'\" - John Galt &lt;\\font&gt; </span></p>\n<span style=\"color: #ffffff;\">\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.5\" href=\"#fnr.5\">5</a></sup> <span style=\"color: #ffffff;\">\u201cYou don't have to see through the eyes of others, hold onto yours, stand on your own judgment, you know that what is, is\u2013say it aloud, like the holiest of prayers, and don't let anyone tell you otherwise.\u201d \u2014 Dagny Taggart </span></p>\n<p class=\"footnote\"><sup><a class=\"footnum\" name=\"fn.6\" href=\"#fnr.6\">6</a></sup> <span style=\"color: #ffffff;\"> Also, the main character of Atlas Shrugged was a physicist, invented a motor that harnessed the power of static electricity, and then went on to save the damn country. That's not encouragement to study science?</span></p>\n</span></div>\n<span style=\"color: #ffffff;\"> </span></div>\n<span style=\"color: #ffffff;\"> </span></div>\n<p><span style=\"color: #ffffff;\"> </span></p>", "sections": [{"title": "In Defense of Ayn Rand", "anchor": "In_Defense_of_Ayn_Rand", "level": 1}, {"title": "Footnotes:", "anchor": "Footnotes_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "35 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["96TBXaHwLbFyeAxrg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T01:57:39.019Z", "modifiedAt": null, "url": null, "title": "Robots ate my job [links]", "slug": "robots-ate-my-job-links", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:02.523Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "torekp", "createdAt": "2009-07-17T21:58:25.456Z", "isAdmin": false, "displayName": "torekp"}, "userId": "5S73XuxxK2X6nygmc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DXoQhNpNBJyqbPvgD/robots-ate-my-job-links", "pageUrlRelative": "/posts/DXoQhNpNBJyqbPvgD/robots-ate-my-job-links", "linkUrl": "https://www.lesswrong.com/posts/DXoQhNpNBJyqbPvgD/robots-ate-my-job-links", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Robots%20ate%20my%20job%20%5Blinks%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARobots%20ate%20my%20job%20%5Blinks%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXoQhNpNBJyqbPvgD%2Frobots-ate-my-job-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Robots%20ate%20my%20job%20%5Blinks%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXoQhNpNBJyqbPvgD%2Frobots-ate-my-job-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXoQhNpNBJyqbPvgD%2Frobots-ate-my-job-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p><a title=\"April 2012 ME magazine\" href=\"http://www.memagazinedigital.org/memagazine/201204#pg1\" target=\"_blank\">Mechanical Engineering magazine</a> (paywalled until next month) and <a title=\"FT.com book review\" href=\"/Erik Brynjolfsson and Andrew McAfee\" target=\"_blank\">Financial Times</a>, among others, recently reviewed the book <em>Race Against the Machine</em> by economists Erik Brynjolfsson and Andrew McAfee.&nbsp; The <em>FT</em> reviewer writes:</p>\n<blockquote>\n<p>Pattern recognition, the authors think, will quickly allow machines to branch out further. Computers will soon drive more safely than humans, a fact <span class=\"wsodCompany\">Google</span> has demonstrated by allowing one to take out a <span class=\"wsodCompany\">Toyota</span> Prius for a 1,000-mile spin. Truck and taxi drivers should be worried &ndash; but then so should medical professionals, lawyers and accountants; all of their jobs are at risk too. The outcome is a nightmarish but worryingly convincing vision of a future in which an ever-decreasing circle of professions is immune from robotic encirclement.</p>\n</blockquote>\n<p>And ME magazine quotes McAfee in an interview:</p>\n<blockquote>\n<p>Once computers get better than people, you don't have to hire people to do that job any more.&nbsp; That doesn't mean that people can't find work.&nbsp; There will always be an amount of work to do, but they won't like the wages they are offered.</p>\n</blockquote>\n<p>Both reviewers also hint that McAfee and Brynjolfsson offer a partial explanation of the \"jobless recovery\", but either the book's argument is weak or the reviewers do a poor job summarizing it.&nbsp; Such a purported explanation might be the main attraction for most readers, but I'm more interested in the longer-term picture.&nbsp; Be it the \"nightmarish vision\" of the future mentioned in <em>FT</em>, or the simpler point about wages offered by McAfee, this might be a good hook to get the general public thinking about the long-term consequences of AI.</p>\n<p>Is that a good idea?&nbsp; Should sleeping general publics be left to lie?&nbsp; There seems to be significant reluctance among many LessWrongers to stir the public, but have we ever hashed out the reasons for and against?&nbsp; Please describe any non-obvious reasons on either side.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DXoQhNpNBJyqbPvgD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 8.812661107645154e-07, "legacy": true, "legacyId": "15029", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T06:36:45.086Z", "modifiedAt": null, "url": null, "title": "Let's create a market for cryonics", "slug": "let-s-create-a-market-for-cryonics", "viewCount": null, "lastCommentedAt": "2021-06-15T16:13:12.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L3NhoDDh4YC6BNjP4/let-s-create-a-market-for-cryonics", "pageUrlRelative": "/posts/L3NhoDDh4YC6BNjP4/let-s-create-a-market-for-cryonics", "linkUrl": "https://www.lesswrong.com/posts/L3NhoDDh4YC6BNjP4/let-s-create-a-market-for-cryonics", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20create%20a%20market%20for%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20create%20a%20market%20for%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL3NhoDDh4YC6BNjP4%2Flet-s-create-a-market-for-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20create%20a%20market%20for%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL3NhoDDh4YC6BNjP4%2Flet-s-create-a-market-for-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL3NhoDDh4YC6BNjP4%2Flet-s-create-a-market-for-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 710, "htmlBody": "<p>My uncle works in insurance. I recently mentioned that I'm planning to sign up for cryonics.</p>\n<p>\"That's amazing,\" he said. \"Convincing a young person to buy life insurance? That has to be the greatest scam ever.\"</p>\n<p>I took the comment lightly, not caring to argue about it. But it got me thinking -&nbsp;<em>couldn't </em>cryonics&nbsp;be a great opportunity for insurance companies to make a bunch of money?</p>\n<p>Consider:</p>\n<ol>\n<li>Were there a much stronger <em>demand</em>&nbsp;for cryonics, cryonics organizations would flourish through competition, outside investment, and internal reinvestment. Costs would likely fall, and this would be good for cryonicists in general.</li>\n<li>If cryonics organizations flourish, this increases the probability of cryonics working. I can think of a bunch of ways in which this could happen; perhaps, for example, it would encourage the creation of safety nets whereby the failure of individual companies doesn't result in anyone getting thawed. It would increase R&amp;D on both perfusion and revivification, encourage entrepreneurs to explore new related business models, etcetera.</li>\n<li>Increasing the demand for cryonics increases the demand for life insurance policies; thus insurance companies have a strong incentive to increase the demand for cryonics. Many large insurance companies would like nothing more than to usher in a generation of young people that want to buy life insurance.<sup>1<br /></sup></li>\n<li>The demand for cryonics could be increased by an insightful marketing campaign by an excellent marketing agency with an enormous budget... like those used by big insurance companies.<sup>2</sup> A quick Googling <a href=\"http://adage.com/article/news/insurance-industry-s-4-billion-advertising-brawl/148992/\">says</a> that ad spending by insurance companies exceeded $4.15 billion in 2009.</li>\n</ol>\n<p>Almost a year ago, Strange7 <a href=\"/lw/5hp/how_hard_do_we_really_want_to_sell_cryonics/\">suggested</a> that cryonics organizations could run this kind of marketing campaign. I think he's wrong - there's no way CI or Alcor have the money. But the biggest insurance companies <em>do </em>have the money, and I'd be <em>shocked</em> if these companies or their agencies aren't already dumping all kinds of money into market research.</p>\n<p>What would doing this require?&nbsp;</p>\n<ol>\n<li>That an open-minded person in the insurance industry who is in the position to direct this kind of funding&nbsp;<em>exists</em>. I don't have a sense of how likely this is.</li>\n<li>That we can locate/get an audience with the person from step 1. I think research and networking could get this done, especially if the higher-status among us are interested.</li>\n<li>That we can find someone who is capable and willing to explain this clearly and convincingly to the person from step 1. I'm not sure it would be&nbsp;<em>that&nbsp;</em>difficult. In the startup world, strangers convince strangers to speculatively spend millions of dollars every week. Hell, I'll do it.</li>\n</ol>\n<p>I want to live in a world where cryonics ads air on TV just as often as ads for everything else people spend money on. I really can see an insurance company owning this project - if they can a) successfully revamp the image of cryonics and b) become known as the household name for it when the market gets big, they will make lots of money.</p>\n<p>What do you think? Where has my reasoning failed? Does anyone here know anyone powerful in insurance?&nbsp;</p>\n<p><em>Lastly, taking a&nbsp;<a href=\"/r/discussion/lw/bk6/alcor_vs_cryonics_institute/6a0b\">cue</a>&nbsp;from ciphergoth: this is not the place to rehash all the old arguments about cryonics. I'm asking about a very specific idea about marketing and life insurance, not requesting commentary on cryonics itself. Thanks!</em></p>\n<hr />\n<p><sup>1&nbsp;</sup>Perhaps modeling the potential size of the market would offer insight here. If it turns out that this idea is not insane, I'll find a way to make it happen. I could use your help.</p>\n<p><sup>2&nbsp;</sup>Consider <a href=\"http://www.theatlantic.com/magazine/archive/1982/02/have-you-ever-tried-to-sell-a-diamond/4575/\">what happened with diamonds</a> in the 1900s:</p>\n<blockquote>\n<p><span style=\"font-family: Georgia, 'times new roman', times, serif; font-size: 13px; line-height: 19px;\">... N. W. Ayer suggested that through a well-orchestrated advertising and public-relations campaign it could have a significant impact on the \"social attitudes of the public at large and thereby channel American spending toward larger and more expensive diamonds instead of \"competitive luxuries.\" Specifically, the Ayer study stressed the need to strengthen the association in the public's mind of diamonds with romance. Since \"young men buy over 90% of all engagement rings\" it would be crucial to inculcate in them the idea that diamonds were a gift of love: the larger and finer the diamond, the greater the expression of love. Similarly, young women had to be encouraged to view diamonds as an integral part of any romantic courtship.</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L3NhoDDh4YC6BNjP4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 61, "extendedScore": null, "score": 0.000139, "legacy": true, "legacyId": "15050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6vPwdQFgKzyJa2q9c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T07:41:23.029Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - The Resistance (Game)", "slug": "meetup-west-la-meetup-the-resistance-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a2q427xopYikKtjR2/meetup-west-la-meetup-the-resistance-game", "pageUrlRelative": "/posts/a2q427xopYikKtjR2/meetup-west-la-meetup-the-resistance-game", "linkUrl": "https://www.lesswrong.com/posts/a2q427xopYikKtjR2/meetup-west-la-meetup-the-resistance-game", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20The%20Resistance%20(Game)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20The%20Resistance%20(Game)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa2q427xopYikKtjR2%2Fmeetup-west-la-meetup-the-resistance-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20The%20Resistance%20(Game)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa2q427xopYikKtjR2%2Fmeetup-west-la-meetup-the-resistance-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa2q427xopYikKtjR2%2Fmeetup-west-la-meetup-the-resistance-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8v'>West LA Meetup - The Resistance (Game)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 April 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, April 11th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Activity:</strong> We will play a classic game of deception and deduction, <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28party_game%29\" rel=\"nofollow\">The Resistance</a>. There will be unmoderated discussion before game play, and afterwards we can talk about the use of games and other methods for training our heuristics.</p>\n\n<p>Don't worry if you don't have time to read anything, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8v'>West LA Meetup - The Resistance (Game)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a2q427xopYikKtjR2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.814107062280726e-07, "legacy": true, "legacyId": "15053", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___The_Resistance__Game_\">Discussion article for the meetup : <a href=\"/meetups/8v\">West LA Meetup - The Resistance (Game)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 April 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, April 11th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Activity:</strong> We will play a classic game of deception and deduction, <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28party_game%29\" rel=\"nofollow\">The Resistance</a>. There will be unmoderated discussion before game play, and afterwards we can talk about the use of games and other methods for training our heuristics.</p>\n\n<p>Don't worry if you don't have time to read anything, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___The_Resistance__Game_1\">Discussion article for the meetup : <a href=\"/meetups/8v\">West LA Meetup - The Resistance (Game)</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - The Resistance (Game)", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___The_Resistance__Game_", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - The Resistance (Game)", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___The_Resistance__Game_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T20:14:01.427Z", "modifiedAt": null, "url": null, "title": "School essay: outsourcing some brain work", "slug": "school-essay-outsourcing-some-brain-work", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y3zrZWLu3qpHSBn57/school-essay-outsourcing-some-brain-work", "pageUrlRelative": "/posts/y3zrZWLu3qpHSBn57/school-essay-outsourcing-some-brain-work", "linkUrl": "https://www.lesswrong.com/posts/y3zrZWLu3qpHSBn57/school-essay-outsourcing-some-brain-work", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20School%20essay%3A%20outsourcing%20some%20brain%20work&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASchool%20essay%3A%20outsourcing%20some%20brain%20work%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3zrZWLu3qpHSBn57%2Fschool-essay-outsourcing-some-brain-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=School%20essay%3A%20outsourcing%20some%20brain%20work%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3zrZWLu3qpHSBn57%2Fschool-essay-outsourcing-some-brain-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3zrZWLu3qpHSBn57%2Fschool-essay-outsourcing-some-brain-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 387, "htmlBody": "<p>I'm currently writing an essay for one of my classes, 'Theoretical Foundations of Nursing.' I'm about the most 'gong-si' class I've ever taken. (That is a Chinese term for 'shit talking,' which is my boyfriend's favourite term for any field that gets into arguments over definitions, has concepts that don't correspond to any empirical phenomena, is based on ideology, etc.)</p>\n<p>The essay involves analyzing a clinical situation (in this case a 55-year-old recently divorced, recently unemployed man, admitted to the psychiatric ward with major depression and suicidal ideation) using a theory (in this case, Roy's Adaptation Model). Done. The <em>next </em>step involves finding criticisms with the model...and despite the fact that I've been complaining about this class and its non-empirical nature all semester, I seem unable to come up with specific criticisms of what this nursing theory is missing.&nbsp;</p>\n<p>Which is what I need your help for, because LessWrong is the best community <em>ever </em>when it comes to specific criticisms.&nbsp;</p>\n<p>Here is a very brief overview of Roy's Adaptation Theory:</p>\n<ul>\n<li>Defines 'health' as 'state or process of becoming integrated with the environment, in the domains of survival, growth, reproduction, mastery, and personal/environmental transformation.'&nbsp;</li>\n<li>Defines a 'person' as an 'adaptive system with coping processes.' Goes on to subdivide this a bit: there are 'regulator mechanisms' (i.e. innate, not consciously controlled) and 'cognitive mechanisms' of adaptation within four different modes: physiological, role function, interdependence, and self-concept.&nbsp;</li>\n<li>Defines environment as 'all conditions, circumstances, and influences that affect the development and behavior of individuals and groups.' Further subdivides environmental stimuli into focal (which demand the person to immediately adapt), contextual (which affect how they adapt), and residual (i.e. attitudes, beliefs).&nbsp;</li>\n<li>The nurse's goal is to manipulate stimuli to improve the person's level of adaptation, as well as teaching more effective coping methods.&nbsp;</li>\n<li>The steps in the process of creating a care plan are: assessment of behavior, assessment of stimuli, choosing a nursing diagnosis <a href=\"http://nclex.ucoz.net/_ld/0/30_NANDALISTOFDIAG.pdf\">from this huge lookup table</a>, setting a goal, choosing an intervention, and evaluation the results.&nbsp;</li>\n</ul>\n<p><br />Now my question is, what is a <em>specific </em>criticism I can make of this particular theory in general...not \"your definitions aren't specific enough\" or \"the whole field of nursing theory isn't reductionist enough\", but something that this kind of theory<em>&nbsp;should </em>have but doesn't. Any ideas?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y3zrZWLu3qpHSBn57", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 7, "extendedScore": null, "score": 8.817270360730304e-07, "legacy": true, "legacyId": "15063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-10T22:35:29.327Z", "modifiedAt": null, "url": null, "title": "Left-wing Alarmism vs. Right-wing Optimism: evidence on which is correct?", "slug": "left-wing-alarmism-vs-right-wing-optimism-evidence-on-which", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.745Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "loup-vaillant", "createdAt": "2011-03-23T10:39:25.887Z", "isAdmin": false, "displayName": "loup-vaillant"}, "userId": "wdoZti3BcPbJXsZ66", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CaeNemorzeaxMv3JF/left-wing-alarmism-vs-right-wing-optimism-evidence-on-which", "pageUrlRelative": "/posts/CaeNemorzeaxMv3JF/left-wing-alarmism-vs-right-wing-optimism-evidence-on-which", "linkUrl": "https://www.lesswrong.com/posts/CaeNemorzeaxMv3JF/left-wing-alarmism-vs-right-wing-optimism-evidence-on-which", "postedAtFormatted": "Tuesday, April 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Left-wing%20Alarmism%20vs.%20Right-wing%20Optimism%3A%20evidence%20on%20which%20is%20correct%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeft-wing%20Alarmism%20vs.%20Right-wing%20Optimism%3A%20evidence%20on%20which%20is%20correct%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaeNemorzeaxMv3JF%2Fleft-wing-alarmism-vs-right-wing-optimism-evidence-on-which%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Left-wing%20Alarmism%20vs.%20Right-wing%20Optimism%3A%20evidence%20on%20which%20is%20correct%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaeNemorzeaxMv3JF%2Fleft-wing-alarmism-vs-right-wing-optimism-evidence-on-which", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaeNemorzeaxMv3JF%2Fleft-wing-alarmism-vs-right-wing-optimism-evidence-on-which", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 372, "htmlBody": "<p>(Edit: Thanks for the helpful comments. Also, downvoting this thread to oblivion was probably a good idea &mdash;and it'd better <em>stay</em> buried. Sorry for the noise.)</p>\n<p><em>(Sorry for the mind-killing topic, but here is the only place I can hope for something remotely rational.)</em></p>\n<p>Lately, I have noticed the existence of what seems to amount to two meme-clusters.</p>\n<p>On the one hand we have the Left-wing Alarmist, which want to have wealth more equitably distributed, warns about our dead soil, our resources consumption run amok, our (West) exploitation of the South, and above all, the unsustainability of our society (collapse often due before 2 or 3 decades). One particular flaw in this vision is the complete disregard for possible technology developments. Typically, this one will call for (classical) anarchy, localization and de-industrialization of (preferably organic) food production, economy of physical resources, reduced work-hours, sometimes even a simplification of every-day technology. The bottom line is, the world is currently worsening.</p>\n<p>On the other hand, we have the Right-wing Optimist, which wants free markets, believes in growth (often defined as GDP growth) to solve most of our problems, is confident about the development of new technologies, and above all believes in our ability to adapt. One particular flaw in this vision is the complete disregard for the adaptation by starvation and war that often happen. Typically, this one will call for deregulation of the economy, the reduction (or elimination) of welfare, maximizing economies of scale and the law of comparative advantages through globalization, and the privatization of nearly everything. The bottom line is, the world is currently improving.</p>\n<p>Of course, it's not all that clear cut. More likely, there is a spectrum between those two extremes.</p>\n<p>Now, I'm especially puzzled by the correlation between political sides what seems to be the Enlightenment/Romanticism divide. Where could it possibly come from?</p>\n<p>Also, there's got to be evidence one way or the other. The problem is, it's likely difficult to process. For instance, while Steven Pinker will tell you that violence is steadily decreasing by showing decreasing violent death rates, Noam Chomsky will tell you that violence is _increasing_ for a while, by showing \"structural\" violence like poverty, starvation, or unwanted pollution. So, does anyone know of a way to process the evidence rationally?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CaeNemorzeaxMv3JF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -30, "extendedScore": null, "score": -5.6e-05, "legacy": true, "legacyId": "15065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T00:48:04.567Z", "modifiedAt": null, "url": null, "title": "Attention control is critical for changing/increasing/altering motivation", "slug": "attention-control-is-critical-for-changing-increasing", "viewCount": null, "lastCommentedAt": "2022-02-19T22:21:30.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kalla724", "createdAt": "2011-06-27T19:28:41.092Z", "isAdmin": false, "displayName": "kalla724"}, "userId": "gaBwWRM7cb7KP6fia", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rD57ysqawarsbry6v/attention-control-is-critical-for-changing-increasing", "pageUrlRelative": "/posts/rD57ysqawarsbry6v/attention-control-is-critical-for-changing-increasing", "linkUrl": "https://www.lesswrong.com/posts/rD57ysqawarsbry6v/attention-control-is-critical-for-changing-increasing", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Attention%20control%20is%20critical%20for%20changing%2Fincreasing%2Faltering%20motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAttention%20control%20is%20critical%20for%20changing%2Fincreasing%2Faltering%20motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrD57ysqawarsbry6v%2Fattention-control-is-critical-for-changing-increasing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Attention%20control%20is%20critical%20for%20changing%2Fincreasing%2Faltering%20motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrD57ysqawarsbry6v%2Fattention-control-is-critical-for-changing-increasing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrD57ysqawarsbry6v%2Fattention-control-is-critical-for-changing-increasing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2129, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">I&rsquo;ve just been reading <a href=\"/user/lukeprog/\">Luke&rsquo;s</a> <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">&ldquo;Crash Course in the Neuroscience of Human Motivation.&rdquo;</a> It is a useful text, although there are a few technical errors and a few bits of outdated information (see [1], updated information about one particular quibble in [2] and [3]).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There is one significant missing piece, however, which is of critical importance for our subject matter here on LW: the effect of attention on plasticity, including the plasticity of motivation. Since I don&rsquo;t see any other texts addressing it directly (certainly not from a neuroscientific perspective), let&rsquo;s cover the main idea here.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Summary for impatient readers: focus of attention physically determines which synapses in your brain get stronger, and which areas of your cortex physically grow in size. The implications of this provide direct guidance for alteration of behaviors and motivational patterns. This <em>is</em> used for this purpose extensively: for instance, many benefits of the Cognitive-Behavioral Therapy approach rely on this mechanism.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><strong>I &ndash; Attention and plasticity</strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">To illustrate this properly, we need to define two terms. I&rsquo;m guessing these are very familiar to most readers here, but let&rsquo;s cover them briefly just in case.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">First thing to keep in mind is <em>the</em> <em>plasticity of cortical maps.</em> In essence, particular functional areas of our brain can expand or shrink based on how often (and how intensely) they are used. A small amount of this growth is physical, as new axons grow, expanding the white matter; most of it happens by repurposing any less-used circuitry in the vicinity of the active area. For example, our sense of sight is processed by our visual cortex, which turns signals from our eyes into lines, shapes, colors and movement. In blind people, however, this part of the brain becomes invaded by other senses, and begins to process sensations like touch and hearing, such that they become significantly more sensitive than in sighted people. Similarly, in deaf people, auditory cortex (part of the brain that processes sounds) becomes adapted to process visual information and gather language clues by sight.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Second concept we&rsquo;ll need is <a href=\"http://en.wikipedia.org/wiki/Primary_somatosensory_cortex\"><em>somatosensory cortex</em></a> (SSC for short). This is an area of the (vertebrate) brain where most of the incoming touch and positional (proprioceptive) sensations from the body converge. There is a map-like quality to this part of our brain, as every body part links to a particular bit of the SSC surface (which can be illustrated with silly-looking things, such as the <a href=\"http://en.wikipedia.org/wiki/File:Sensory_Homunculus.png\">sensory homunculus</a>). More touch-sensitive areas of the body have larger corresponding areas within the SSC.</p>\n<p>With these two in mind, let&rsquo;s consider one actual experiment [4]. Scientists measured and mapped the area of an owl monkey&rsquo;s SSC which became activated when one of his fingertips was touched. The monkey was then trained to hold that finger on a tactile stimulator &ndash; a moving wheel that stimulates touch receptors. The monkey had to pay attention to the stimulus, and was rewarded for letting go upon detecting certain changes in spinning frequency. After a few weeks of training, the area was measured again.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">As you probably expected, the area had grown larger. The touch-processing neurons grew out, co-opting surrounding circuitry in order to achieve better and faster processing of the stimulus that produced the reward. Which is, so far, just another way of showing plasticity of cortical maps.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">But then, there is something else. The SSC area expanded only when the monkey had to <em>pay attention</em> to the sensation of touch in order to receive the reward. If a monkey was trained to keep a hand on the wheel that moved just the same, but he did not have to pay attention to it&hellip; the cortical map remained the same size. This finding has since been replicated in humans, many times (for instance [5, 6]).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Take a moment to consider what this means.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, thinking about the next move, about his chess strategy, and about the future possibilities of the game. His neural networks are optimizing, making him a better chess player.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, thinking about the music he hears, listening to the chords and anticipating the sounds still to come. His neural networks are optimizing, making him better at understanding music and hearing subtleties within a melody.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, gritting his teeth as another flash of pain comes from his bad back. His neural networks are optimizing, making the pain more intense, easier to feel, harder to ignore.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><strong>II &ndash; Practical implications: making and breaking habits, efficacy of CBT </strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Habitual learned behaviors are often illustrated with the example of driving. When we are learning to drive, we have to pay attention to everything: when to push the pedals, when to signal, where to hold our hands&hellip; A few years later, these behaviors become so automatic, we hardly pay attention at all. Indeed, most of us can drive for hours while carrying on conversations or listening to audiobooks. We are completely unaware, as our own body keeps pushing pedals, signaling turns, and changing gears.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">We can therefore say that <em>driving behaviors</em>, through practice and attention, eventually become automatic &ndash; which is, most of the time, a good thing. But so do many other things, including some destructive ones we might want to get rid of. Let&rsquo;s take a simple one: nail biting. You are reading, or watching a movie, or thinking, or driving&hellip; when you suddenly notice some minor pain, and realize that you have chewed your nail into a ragged stump. Ouch!</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">You catch yourself biting, you stop. Five minutes later, you catch yourself biting again. You stop again. Repeat <em>ad infinitum</em>, or <em>ad nauseam</em>, whichever comes first.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Cognitive-Behavioral Therapy has a highly successful approach for breaking habits, which requires only a very subtle alteration to this process. You notice that you are biting your nails. You immediately focus your attention on what you are doing, and you stop doing it. No rage, no blaming yourself, no negative emotions. You just stop, and you focus all the attention you can on the act of stopping. You move your arm down, focusing your attention on the act of movement, on the feeling of your arm going down, away from your mouth. That&rsquo;s it. You can go back to whatever you were doing.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Five minutes later, you notice yourself biting your nails again. You calmly repeat the procedure again.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">By doing this, you are training yourself to perform a new behavior &ndash; the &ldquo;stop and put the hand down&rdquo; behavior &ndash; which is itself triggered by the nail-biting behavior. As you go along, you will get better and better at noticing that you have started to bite your nails. You will also get better and better at stopping and putting your hand down. After a while, this will become semi-automatic; you&rsquo;ll notice that your hand went to your mouth, a nail touched your tooth, and the hand went back down before you could do anything. Don&rsquo;t stop training: focus your attention on the &ldquo;stop and drop&rdquo; part of the action.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">After a while, the nail-biting simply goes away. Of course, the more complex and more ingrained a habit is, the more effort and time will be needed to break it. But for most people, even strong habits can be relatively quickly weakened, or redirected into less destructive behaviors.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">It&rsquo;s probably obvious that habits can be <em>created</em> in this way as well. We don&rsquo;t become better at things we do &ndash; we become better at things we <em>pay attention to while we&rsquo;re doing them.</em> If you want to make exercise a habit, your efforts will be much more effective if you focus your attention on your exercise technique, rather than repeatedly thinking how painful and tiring the whole process is.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There is also a direct implication for training in any complex skill. Start with the well-known <a href=\"http://en.wikipedia.org/wiki/Learning_curve\">learning curve</a> effect: we gain a lot of skill relatively quickly, and then improvements slow down incrementally as we approach our maximum potential skill level. It is relatively easy to go from a poor to a mediocre tennis player; it is much, much harder to go from mediocre to good, and even harder to go from good to excellent.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Complex skills have many different aspects, which we usually attempt to train simultaneously. We can become very good at some, while staying poor at others. The optimal approach would be to focus most of our attention on those aspects where our abilities are weakest, since smaller investments of time and effort will lead to larger improvements in skill.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">To keep with the tennis metaphor, one could become very good at controlling the ball direction and spin, while still having a poor awareness of the opponent&rsquo;s position. Simply playing more will improve both aspects further, but our hypothetical player should optimally try to focus her attention on opponent awareness [7].</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Finally, there is another implication which I&rsquo;ll leave as an exercise for the readers. Mindfulness meditation, which essentially boils down to training control of attention, has been shown to exert a positive effect on many, many different things (lowering depression, anxiety and stress, as well as improving productivity [8, 9, 10]). In the light of the previous text, one obvious reason why better control over attention can produce all these beneficial effects should immediately come to mind.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 10pt; line-height: 115%;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 10pt; line-height: 115%;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 10pt; line-height: 115%;\">__________________________________________________</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">References</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[1] I have several quibbles, but let&rsquo;s stick to one (to prevent this note from becoming longer than the above text). Luke presents a view of dopamine reward system which is stuck in the early 2000&rsquo;s &ndash; ages ago by the pace of neuroscientific research. Dopamine actually has a very, very complex effect on motivation, and is able to strengthen or weaken single synaptic connections based on timing of the signal relative to the signals from the sensory systems. Endocannabinoid neurotransmission (i.e. signaling through chemicals that stimulate the same receptors that are affected by active ingredients in marijuana) is being shown as more and more important in this system as well, and the relative timing of the two signals appears critical.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">The complexity of the effects increases by several orders of magnitude when networks are concerned. Consider this: a planning-related network in the prefrontal cortex can influence the motivation-generating networks in the striatum. A stimulus from the outside is perceived by the sensory networks and transmitted to the dopamine system, to the prefrontal cortex, and to the striatum. The same dopamine signal can, depending on exact timing of action potential bursts, strengthen synapses in the striatum, while weakening synapses in the prefrontal cortex. The result? The link between the stimulus and the actual motivation can increase or decrease, depending on exact connectivity between networks, on the relative sensitivity and on the exact topology of the meta-network in question.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">See the following two references for a broad overview of the subject area.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[2] Calabresi P, Picconi B, Tozzi A, Di Filippo M. \"Dopamine-mediated regulation of corticostriatal synaptic plasticity\" Trends Neurosci. 2007 30(5):211-9.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[3] Wickens JR. \"Synaptic plasticity in the basal ganglia\" Behav Brain Res. 2009 199(1):119-28.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[4] Recanzone GH, Merzenich MM, Jenkins WM, Grajski KA, Dinse HR. \"Topographic reorganization of the hand representation in cortical area 3b of owl monkeys trained in a frequency-discrimination task\" <em>J Neurophysiol.</em> 1992 67(5), 1031-56.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[5] Heron J, Roach NW, Whitaker D, Hanson JV. \"Attention regulates the plasticity of multisensory timing\" <em>Eur J Neurosci.</em> 2010 31(10), 1755-62.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[6] Stefan K, Wycislo M, Classen J. &ldquo;Modulation of associative human motor cortical plasticity by attention&rdquo; <em>J Neurophysiol.</em> 2004 92(1), 66-72.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[7] I&rsquo;m not finding good papers directed exactly on this point, so I&rsquo;ll just throw this out as a personal opinion (although I&rsquo;ll say it appears well supported by indirect research). We all like to appear competent and skillful, especially in those areas where we have invested a lot of time and effort. This can lead to a bias where we focus on using those aspects of complex skills we are best at, and training those aspects most intensely. In other words, a tendency appears to exist to do exactly the opposite of what we should be doing. (If anyone has encountered a name for this bias, or has references to suggest, I would be very grateful to hear from you.)</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[8] Brown KW, Ryan RM. \"The benefits of being present: mindfulness and its role in psychological well-being\" J Pers Soc Psychol. 2003 84(4):822-48.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[9] Davidson RJ, Kabat-Zinn J, Schumacher J, Rosenkranz M, Muller D, Santorelli SF, Urbanowski F, Harrington A, Bonus K, Sheridan JF. \"Alterations in brain and immune function produced by mindfulness meditation\" Psychosom Med. 2003 65(4):564-70.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[10] Shao RP, Skarlicki DP. \"The role of mindfulness in predicting individual performance\" Canadian J of Behavioral Sci 2009 41(4): 195&ndash;201.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Wi3EopKJ2aNdtxSWg": 3, "iP2X4jQNHMWHRNPne": 3, "XSeiautCrZGaQ78fx": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rD57ysqawarsbry6v", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 213, "baseScore": 270, "extendedScore": null, "score": 0.000555, "legacy": true, "legacyId": "15039", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 270, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">I\u2019ve just been reading <a href=\"/user/lukeprog/\">Luke\u2019s</a> <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">\u201cCrash Course in the Neuroscience of Human Motivation.\u201d</a> It is a useful text, although there are a few technical errors and a few bits of outdated information (see [1], updated information about one particular quibble in [2] and [3]).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There is one significant missing piece, however, which is of critical importance for our subject matter here on LW: the effect of attention on plasticity, including the plasticity of motivation. Since I don\u2019t see any other texts addressing it directly (certainly not from a neuroscientific perspective), let\u2019s cover the main idea here.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Summary for impatient readers: focus of attention physically determines which synapses in your brain get stronger, and which areas of your cortex physically grow in size. The implications of this provide direct guidance for alteration of behaviors and motivational patterns. This <em>is</em> used for this purpose extensively: for instance, many benefits of the Cognitive-Behavioral Therapy approach rely on this mechanism.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><strong id=\"I___Attention_and_plasticity\">I \u2013 Attention and plasticity</strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">To illustrate this properly, we need to define two terms. I\u2019m guessing these are very familiar to most readers here, but let\u2019s cover them briefly just in case.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">First thing to keep in mind is <em>the</em> <em>plasticity of cortical maps.</em> In essence, particular functional areas of our brain can expand or shrink based on how often (and how intensely) they are used. A small amount of this growth is physical, as new axons grow, expanding the white matter; most of it happens by repurposing any less-used circuitry in the vicinity of the active area. For example, our sense of sight is processed by our visual cortex, which turns signals from our eyes into lines, shapes, colors and movement. In blind people, however, this part of the brain becomes invaded by other senses, and begins to process sensations like touch and hearing, such that they become significantly more sensitive than in sighted people. Similarly, in deaf people, auditory cortex (part of the brain that processes sounds) becomes adapted to process visual information and gather language clues by sight.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Second concept we\u2019ll need is <a href=\"http://en.wikipedia.org/wiki/Primary_somatosensory_cortex\"><em>somatosensory cortex</em></a> (SSC for short). This is an area of the (vertebrate) brain where most of the incoming touch and positional (proprioceptive) sensations from the body converge. There is a map-like quality to this part of our brain, as every body part links to a particular bit of the SSC surface (which can be illustrated with silly-looking things, such as the <a href=\"http://en.wikipedia.org/wiki/File:Sensory_Homunculus.png\">sensory homunculus</a>). More touch-sensitive areas of the body have larger corresponding areas within the SSC.</p>\n<p>With these two in mind, let\u2019s consider one actual experiment [4]. Scientists measured and mapped the area of an owl monkey\u2019s SSC which became activated when one of his fingertips was touched. The monkey was then trained to hold that finger on a tactile stimulator \u2013 a moving wheel that stimulates touch receptors. The monkey had to pay attention to the stimulus, and was rewarded for letting go upon detecting certain changes in spinning frequency. After a few weeks of training, the area was measured again.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">As you probably expected, the area had grown larger. The touch-processing neurons grew out, co-opting surrounding circuitry in order to achieve better and faster processing of the stimulus that produced the reward. Which is, so far, just another way of showing plasticity of cortical maps.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">But then, there is something else. The SSC area expanded only when the monkey had to <em>pay attention</em> to the sensation of touch in order to receive the reward. If a monkey was trained to keep a hand on the wheel that moved just the same, but he did not have to pay attention to it\u2026 the cortical map remained the same size. This finding has since been replicated in humans, many times (for instance [5, 6]).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Take a moment to consider what this means.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, thinking about the next move, about his chess strategy, and about the future possibilities of the game. His neural networks are optimizing, making him a better chess player.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, thinking about the music he hears, listening to the chords and anticipating the sounds still to come. His neural networks are optimizing, making him better at understanding music and hearing subtleties within a melody.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, gritting his teeth as another flash of pain comes from his bad back. His neural networks are optimizing, making the pain more intense, easier to feel, harder to ignore.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><strong id=\"II___Practical_implications__making_and_breaking_habits__efficacy_of_CBT_\">II \u2013 Practical implications: making and breaking habits, efficacy of CBT </strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Habitual learned behaviors are often illustrated with the example of driving. When we are learning to drive, we have to pay attention to everything: when to push the pedals, when to signal, where to hold our hands\u2026 A few years later, these behaviors become so automatic, we hardly pay attention at all. Indeed, most of us can drive for hours while carrying on conversations or listening to audiobooks. We are completely unaware, as our own body keeps pushing pedals, signaling turns, and changing gears.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">We can therefore say that <em>driving behaviors</em>, through practice and attention, eventually become automatic \u2013 which is, most of the time, a good thing. But so do many other things, including some destructive ones we might want to get rid of. Let\u2019s take a simple one: nail biting. You are reading, or watching a movie, or thinking, or driving\u2026 when you suddenly notice some minor pain, and realize that you have chewed your nail into a ragged stump. Ouch!</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">You catch yourself biting, you stop. Five minutes later, you catch yourself biting again. You stop again. Repeat <em>ad infinitum</em>, or <em>ad nauseam</em>, whichever comes first.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Cognitive-Behavioral Therapy has a highly successful approach for breaking habits, which requires only a very subtle alteration to this process. You notice that you are biting your nails. You immediately focus your attention on what you are doing, and you stop doing it. No rage, no blaming yourself, no negative emotions. You just stop, and you focus all the attention you can on the act of stopping. You move your arm down, focusing your attention on the act of movement, on the feeling of your arm going down, away from your mouth. That\u2019s it. You can go back to whatever you were doing.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Five minutes later, you notice yourself biting your nails again. You calmly repeat the procedure again.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">By doing this, you are training yourself to perform a new behavior \u2013 the \u201cstop and put the hand down\u201d behavior \u2013 which is itself triggered by the nail-biting behavior. As you go along, you will get better and better at noticing that you have started to bite your nails. You will also get better and better at stopping and putting your hand down. After a while, this will become semi-automatic; you\u2019ll notice that your hand went to your mouth, a nail touched your tooth, and the hand went back down before you could do anything. Don\u2019t stop training: focus your attention on the \u201cstop and drop\u201d part of the action.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">After a while, the nail-biting simply goes away. Of course, the more complex and more ingrained a habit is, the more effort and time will be needed to break it. But for most people, even strong habits can be relatively quickly weakened, or redirected into less destructive behaviors.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">It\u2019s probably obvious that habits can be <em>created</em> in this way as well. We don\u2019t become better at things we do \u2013 we become better at things we <em>pay attention to while we\u2019re doing them.</em> If you want to make exercise a habit, your efforts will be much more effective if you focus your attention on your exercise technique, rather than repeatedly thinking how painful and tiring the whole process is.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There is also a direct implication for training in any complex skill. Start with the well-known <a href=\"http://en.wikipedia.org/wiki/Learning_curve\">learning curve</a> effect: we gain a lot of skill relatively quickly, and then improvements slow down incrementally as we approach our maximum potential skill level. It is relatively easy to go from a poor to a mediocre tennis player; it is much, much harder to go from mediocre to good, and even harder to go from good to excellent.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Complex skills have many different aspects, which we usually attempt to train simultaneously. We can become very good at some, while staying poor at others. The optimal approach would be to focus most of our attention on those aspects where our abilities are weakest, since smaller investments of time and effort will lead to larger improvements in skill.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">To keep with the tennis metaphor, one could become very good at controlling the ball direction and spin, while still having a poor awareness of the opponent\u2019s position. Simply playing more will improve both aspects further, but our hypothetical player should optimally try to focus her attention on opponent awareness [7].</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Finally, there is another implication which I\u2019ll leave as an exercise for the readers. Mindfulness meditation, which essentially boils down to training control of attention, has been shown to exert a positive effect on many, many different things (lowering depression, anxiety and stress, as well as improving productivity [8, 9, 10]). In the light of the previous text, one obvious reason why better control over attention can produce all these beneficial effects should immediately come to mind.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 10pt; line-height: 115%;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 10pt; line-height: 115%;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 10pt; line-height: 115%;\">__________________________________________________</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">References</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[1] I have several quibbles, but let\u2019s stick to one (to prevent this note from becoming longer than the above text). Luke presents a view of dopamine reward system which is stuck in the early 2000\u2019s \u2013 ages ago by the pace of neuroscientific research. Dopamine actually has a very, very complex effect on motivation, and is able to strengthen or weaken single synaptic connections based on timing of the signal relative to the signals from the sensory systems. Endocannabinoid neurotransmission (i.e. signaling through chemicals that stimulate the same receptors that are affected by active ingredients in marijuana) is being shown as more and more important in this system as well, and the relative timing of the two signals appears critical.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">The complexity of the effects increases by several orders of magnitude when networks are concerned. Consider this: a planning-related network in the prefrontal cortex can influence the motivation-generating networks in the striatum. A stimulus from the outside is perceived by the sensory networks and transmitted to the dopamine system, to the prefrontal cortex, and to the striatum. The same dopamine signal can, depending on exact timing of action potential bursts, strengthen synapses in the striatum, while weakening synapses in the prefrontal cortex. The result? The link between the stimulus and the actual motivation can increase or decrease, depending on exact connectivity between networks, on the relative sensitivity and on the exact topology of the meta-network in question.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">See the following two references for a broad overview of the subject area.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[2] Calabresi P, Picconi B, Tozzi A, Di Filippo M. \"Dopamine-mediated regulation of corticostriatal synaptic plasticity\" Trends Neurosci. 2007 30(5):211-9.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[3] Wickens JR. \"Synaptic plasticity in the basal ganglia\" Behav Brain Res. 2009 199(1):119-28.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[4] Recanzone GH, Merzenich MM, Jenkins WM, Grajski KA, Dinse HR. \"Topographic reorganization of the hand representation in cortical area 3b of owl monkeys trained in a frequency-discrimination task\" <em>J Neurophysiol.</em> 1992 67(5), 1031-56.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[5] Heron J, Roach NW, Whitaker D, Hanson JV. \"Attention regulates the plasticity of multisensory timing\" <em>Eur J Neurosci.</em> 2010 31(10), 1755-62.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[6] Stefan K, Wycislo M, Classen J. \u201cModulation of associative human motor cortical plasticity by attention\u201d <em>J Neurophysiol.</em> 2004 92(1), 66-72.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[7] I\u2019m not finding good papers directed exactly on this point, so I\u2019ll just throw this out as a personal opinion (although I\u2019ll say it appears well supported by indirect research). We all like to appear competent and skillful, especially in those areas where we have invested a lot of time and effort. This can lead to a bias where we focus on using those aspects of complex skills we are best at, and training those aspects most intensely. In other words, a tendency appears to exist to do exactly the opposite of what we should be doing. (If anyone has encountered a name for this bias, or has references to suggest, I would be very grateful to hear from you.)</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[8] Brown KW, Ryan RM. \"The benefits of being present: mindfulness and its role in psychological well-being\" J Pers Soc Psychol. 2003 84(4):822-48.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[9] Davidson RJ, Kabat-Zinn J, Schumacher J, Rosenkranz M, Muller D, Santorelli SF, Urbanowski F, Harrington A, Bonus K, Sheridan JF. \"Alterations in brain and immune function produced by mindfulness meditation\" Psychosom Med. 2003 65(4):564-70.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[10] Shao RP, Skarlicki DP. \"The role of mindfulness in predicting individual performance\" Canadian J of Behavioral Sci 2009 41(4): 195\u2013201.</p>", "sections": [{"title": "I \u2013 Attention and plasticity", "anchor": "I___Attention_and_plasticity", "level": 1}, {"title": "II \u2013 Practical implications: making and breaking habits, efficacy of CBT ", "anchor": "II___Practical_implications__making_and_breaking_habits__efficacy_of_CBT_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "89 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hN2aRnu798yas5b2k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-04-11T00:48:04.567Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T03:39:38.702Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 15, chapter 84", "slug": "harry-potter-and-the-methods-of-rationality-discussion-36", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:33.584Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FAWS", "createdAt": "2010-01-09T18:58:38.832Z", "isAdmin": false, "displayName": "FAWS"}, "userId": "a7Neq3q2DbWrbo6B6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XN4WDRSPFo9iGEuk3/harry-potter-and-the-methods-of-rationality-discussion-36", "pageUrlRelative": "/posts/XN4WDRSPFo9iGEuk3/harry-potter-and-the-methods-of-rationality-discussion-36", "linkUrl": "https://www.lesswrong.com/posts/XN4WDRSPFo9iGEuk3/harry-potter-and-the-methods-of-rationality-discussion-36", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2015%2C%20chapter%2084&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2015%2C%20chapter%2084%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXN4WDRSPFo9iGEuk3%2Fharry-potter-and-the-methods-of-rationality-discussion-36%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2015%2C%20chapter%2084%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXN4WDRSPFo9iGEuk3%2Fharry-potter-and-the-methods-of-rationality-discussion-36", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXN4WDRSPFo9iGEuk3%2Fharry-potter-and-the-methods-of-rationality-discussion-36", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>The next discussion thread is <a href=\"/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">here</a>.</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing <a href=\"http://www.fanfiction.net/s/5782108/84/\">chapter</a> <a href=\"http://hpmor.com/chapter/84\">84</a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">The previous thread</a>&nbsp; has passed 500 comments. C<strong>omment in&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">the 14th thread</a>&nbsp;until you read chapter 84.</strong>&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.&nbsp; Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>, <a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">As a reminder, it&rsquo;s often useful to start your comment by indicating which chapter you are commenting on.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XN4WDRSPFo9iGEuk3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "15081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1239, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QkhX5YeuYHzPW7Waz", "pBTcCB5uJTzADdMm4", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T10:20:56.322Z", "modifiedAt": null, "url": null, "title": "Meetup : Small Berkeley meetup", "slug": "meetup-small-berkeley-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QrS7RNWX8F5PKJZDj/meetup-small-berkeley-meetup-2", "pageUrlRelative": "/posts/QrS7RNWX8F5PKJZDj/meetup-small-berkeley-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/QrS7RNWX8F5PKJZDj/meetup-small-berkeley-meetup-2", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrS7RNWX8F5PKJZDj%2Fmeetup-small-berkeley-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrS7RNWX8F5PKJZDj%2Fmeetup-small-berkeley-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrS7RNWX8F5PKJZDj%2Fmeetup-small-berkeley-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8w'>Small Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 April 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 oxford st;, berkeley, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all, sorry for the late notice. There will be a meetup in Berkeley today as usual! Those of us who choose to attend the MPHD seminar beforehand may have stories to tell about rejection therapy! If 5-10 people show up to the meetup we can play The Resistance if we want to! Everyone wants to play The Resistance.</p>\n\n<p>We'll meet at 7pm, Wednesday, in the Starbucks at 2128 Oxford St., and then we'll decide where to go from there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8w'>Small Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QrS7RNWX8F5PKJZDj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.820832289353621e-07, "legacy": true, "legacyId": "15091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/8w\">Small Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 April 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 oxford st;, berkeley, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all, sorry for the late notice. There will be a meetup in Berkeley today as usual! Those of us who choose to attend the MPHD seminar beforehand may have stories to tell about rejection therapy! If 5-10 people show up to the meetup we can play The Resistance if we want to! Everyone wants to play The Resistance.</p>\n\n<p>We'll meet at 7pm, Wednesday, in the Starbucks at 2128 Oxford St., and then we'll decide where to go from there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/8w\">Small Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Small Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T16:15:06.466Z", "modifiedAt": null, "url": null, "title": "How does remote Joule heating of carbon nanotubes advance singularity timelines?", "slug": "how-does-remote-joule-heating-of-carbon-nanotubes-advance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:07.269Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "keefe", "createdAt": "2010-09-15T21:23:40.747Z", "isAdmin": false, "displayName": "keefe"}, "userId": "FcEHWTkxkfXFQgnts", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tQvLsYnhrav4QwtJ6/how-does-remote-joule-heating-of-carbon-nanotubes-advance", "pageUrlRelative": "/posts/tQvLsYnhrav4QwtJ6/how-does-remote-joule-heating-of-carbon-nanotubes-advance", "linkUrl": "https://www.lesswrong.com/posts/tQvLsYnhrav4QwtJ6/how-does-remote-joule-heating-of-carbon-nanotubes-advance", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20does%20remote%20Joule%20heating%20of%20carbon%20nanotubes%20advance%20singularity%20timelines%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20does%20remote%20Joule%20heating%20of%20carbon%20nanotubes%20advance%20singularity%20timelines%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQvLsYnhrav4QwtJ6%2Fhow-does-remote-joule-heating-of-carbon-nanotubes-advance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20does%20remote%20Joule%20heating%20of%20carbon%20nanotubes%20advance%20singularity%20timelines%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQvLsYnhrav4QwtJ6%2Fhow-does-remote-joule-heating-of-carbon-nanotubes-advance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQvLsYnhrav4QwtJ6%2Fhow-does-remote-joule-heating-of-carbon-nanotubes-advance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<p>&nbsp;</p>\n<p><a href=\"http://phys.org/news/2012-04-carbon-nanotubes-weird-world-remote.html\"><span style=\"font-family: mceinline;\"><strong>Carbon nanotubes: The weird world of 'remote Joule heating'</strong></span></a></p>\n<blockquote>\n<div><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">Minimizing Joule heating remains an important goal in the design of electronic devices</span><sup style=\"font-size: 12px; line-height: 0; color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; \"><a id=\"ref-link-1\" style=\"color: #684d00; text-decoration: none; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #684d00; \" title=\"Pop, E., Sinha, S. &amp; Goodson, K. E. Heat generation and transport in nanometer-scale transistors. Proc. IEEE 94, 1587-1601 (2006).\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2012.39.html#ref1\">1</a>,&nbsp;<a id=\"ref-link-2\" style=\"color: #684d00; text-decoration: none; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #684d00; \" title=\"Pop, E. Energy dissipation and transport in nanoscale devices. Nano Res. 3, 147-169 (2010).\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2012.39.html#ref2\">2</a></sup><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">. The prevailing model of Joule heating relies on a simple semiclassical picture in which electrons collide with the atoms of a conductor, generating heat locally and only in regions of non-zero current density, and this model has been supported by most experiments. Recently, however, it has been predicted that electric currents in graphene and carbon nanotubes can couple to the vibrational modes of a neighbouring material</span><sup style=\"font-size: 12px; line-height: 0; color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; \"><a id=\"ref-link-3\" style=\"color: #684d00; text-decoration: none; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #684d00; \" title=\"Chen, J. H., Jang, C., Xiao, S., Ishigami, M. &amp; Fuhrer, M. S. Intrinsic and extrinsic performance limits of graphene devices on SiO2. Nature Nanotech. 3, 206-209 (2008).\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2012.39.html#ref3\">3</a>,&nbsp;<a id=\"ref-link-4\" style=\"color: #684d00; text-decoration: none; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #684d00; \" title=\"Perebeinos, V., Rotkin, S. V., Petrov, A. G. &amp; Avouris, P. The effects of substrate phonon mode scattering on transport in carbon nanotubes. Nano Lett. 9, 312-316 (2009).\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2012.39.html#ref4\">4</a></sup><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">, heating it remotely</span><sup style=\"font-size: 12px; line-height: 0; color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; \"><a id=\"ref-link-5\" style=\"color: #684d00; text-decoration: none; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #684d00; \" title=\"Rotkin, S. V., Perebeinos, V., Petrov, A. G. &amp; Avouris, P. An essential mechanism of heat dissipation in carbon nanotube electronics. Nano Lett. 9, 1850-1855 (2009).\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2012.39.html#ref5\">5</a></sup><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">. Here, we use&nbsp;</span><em style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px; \">in situ</em><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">&nbsp;electron thermal microscopy to detect the remote Joule heating of a silicon nitride substrate by a single multiwalled carbon nanotube. At least 84</span><span class=\"mb\" style=\"font-family: 'arial unicode ms', 'lucida grande', 'lucida sans unicode', sans-serif !important; font-size: 14px; line-height: 20px; display: inline !important; visibility: visible !important; background-image: none !important; background-attachment: initial !important; background-origin: initial !important; background-clip: initial !important; color: #333333; padding: 0px !important;\">%</span><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">of the electrical power supplied to the nanotube is dissipated directly into the substrate, rather than in the nanotube itself. Although it has different physical origins, this phenomenon is reminiscent of induction heating or microwave dielectric heating. Such an ability to dissipate waste energy remotely could lead to improved thermal management in electronic devices</span><sup style=\"font-size: 12px; line-height: 0; color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; \"><a id=\"ref-link-6\" style=\"color: #684d00; text-decoration: none; border-bottom-width: 1px; border-bottom-style: dotted; border-bottom-color: #684d00; \" title=\"Kenny, T. et al. Advanced cooling technologies for microprocessors. Int. J. High Speed Electron. Syst. 16, 301-313 (2006).\" href=\"http://www.nature.com/nnano/journal/vaop/ncurrent/full/nnano.2012.39.html#ref6\">6</a></sup><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">.\"</span></div>\n</blockquote>\n<div><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">These experiments seem extremely important in constructing AI singularity timelines, what does lesswrong think?</span></div>\n<div><span style=\"color: #333333; font-family: arial, helvetica, '\uff2d\uff33 \uff30\u30b4\u30b7\u30c3\u30af', '\uff2d\uff33 \u30b4\u30b7\u30c3\u30af', Osaka, 'MS PGothic', sans-serif; font-size: 14px; line-height: 20px;\">What is the impact on life extension research?</span></div>\n<p><a href=\"http://www.springerlink.com/content/m6t5871n55841736/\"><span style=\"font-family: mceinline;\"><strong><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">Carbon nanotubes in biology and medicine:</span><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">&nbsp;</span><em style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em; margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; \">In vitro</em><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">&nbsp;</span><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">and</span><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">&nbsp;</span><em style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em; margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; vertical-align: baseline; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: transparent; \">in vivo</em><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">&nbsp;</span><span style=\"color: #ff8d43; font-size: 1.5em; line-height: 1.2em;\">detection, imaging and drug delivery</span></strong></span></a></p>\n<div>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2935937/\">Nanotechnology in Drug Delivery and Tissue Engineering: From Discovery to Applications</a></p>\n</div>\n<div>I don't see a line of inference free of holes, but one can imagine the sort of uses a scifi author would come up with.&nbsp;</div>\n<p><small style=\"font-family: Arial, Helvetica, Sans; \"></small></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tQvLsYnhrav4QwtJ6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -3, "extendedScore": null, "score": 8.822322600132588e-07, "legacy": true, "legacyId": "15092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T17:56:12.712Z", "modifiedAt": null, "url": null, "title": "Help me teach Bayes'", "slug": "help-me-teach-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tomme", "createdAt": "2012-03-14T19:51:35.247Z", "isAdmin": false, "displayName": "tomme"}, "userId": "uyGXufxNcB7WRQ63C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h9rjQFYRQucEvMFxS/help-me-teach-bayes", "pageUrlRelative": "/posts/h9rjQFYRQucEvMFxS/help-me-teach-bayes", "linkUrl": "https://www.lesswrong.com/posts/h9rjQFYRQucEvMFxS/help-me-teach-bayes", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20me%20teach%20Bayes'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20me%20teach%20Bayes'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9rjQFYRQucEvMFxS%2Fhelp-me-teach-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20me%20teach%20Bayes'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9rjQFYRQucEvMFxS%2Fhelp-me-teach-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9rjQFYRQucEvMFxS%2Fhelp-me-teach-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>Next Monday I am supposed to introduce a bunch of middle school students to Bayes' theorem.&nbsp;</p>\n<p>I've scoured the Internet for basic examples where Bayes' theorem is applied. Alas, <a href=\"http://plus.maths.org/content/beyond-reasonable-doubt\">all</a> <a href=\"http://blog.higher-order.net/2009/06/23/monty-hall-and-bayesian-probability-theory/\">explanations</a> <a href=\"http://www.math.hmc.edu/funfacts/ffiles/30002.6.shtml\">I've come cross</a> are, I believe, difficult to grasp for the average middle school student.</p>\n<p>So what I am looking for is a straightforward explanation of Bayes' theorem that uses the least amount of Mathematics and words possible. (Also, my presentation has to be under 3 minutes.)</p>\n<p>I think that it would be efficient in terms of learning for me to use coins or cards, something tangible to illustrate what I'm talking about.</p>\n<p>What do you think? How should I teach 'em Bayes' ways?</p>\n<p>PS: I myself am new to Bayesian probability.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h9rjQFYRQucEvMFxS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.822748118552878e-07, "legacy": true, "legacyId": "15093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T18:43:11.637Z", "modifiedAt": null, "url": null, "title": "Cracked wrote an Article about Life-Extension", "slug": "cracked-wrote-an-article-about-life-extension", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:59.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Locke", "createdAt": "2011-12-27T15:40:30.383Z", "isAdmin": false, "displayName": "Locke"}, "userId": "zq2k9FZq7T3AebpEc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/39rur5cKhp2d3fGtH/cracked-wrote-an-article-about-life-extension", "pageUrlRelative": "/posts/39rur5cKhp2d3fGtH/cracked-wrote-an-article-about-life-extension", "linkUrl": "https://www.lesswrong.com/posts/39rur5cKhp2d3fGtH/cracked-wrote-an-article-about-life-extension", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cracked%20wrote%20an%20Article%20about%20Life-Extension&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACracked%20wrote%20an%20Article%20about%20Life-Extension%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F39rur5cKhp2d3fGtH%2Fcracked-wrote-an-article-about-life-extension%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cracked%20wrote%20an%20Article%20about%20Life-Extension%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F39rur5cKhp2d3fGtH%2Fcracked-wrote-an-article-about-life-extension", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F39rur5cKhp2d3fGtH%2Fcracked-wrote-an-article-about-life-extension", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p><a style=\"color: #003399;\" href=\"http://www.cracked.com/article_19762_7-random-things-you-wont-believe-are-shortening-your-life_p2.html#ixzz1rl5vBK7t\">7 Random Things You Won't Believe Are Shortening Your Life</a></p>\n<p>TL;DR</p>\n<p>1. Starting School Early</p>\n<p>2. Being the Eldest Child</p>\n<p>3. Having a Small Ass</p>\n<p>4. Flying Often</p>\n<p>5. Living Closer to Sea-Level</p>\n<p>6. Not Going to Church.</p>\n<p>7. Retiring Early.</p>\n<p>I'm glad a high-traffic site such like Cracked is giving some exposure to the issue. Can anyone here comment on the reliability of their sources?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "39rur5cKhp2d3fGtH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -1, "extendedScore": null, "score": 8.822945864897317e-07, "legacy": true, "legacyId": "15094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T22:46:10.533Z", "modifiedAt": null, "url": null, "title": "against \"AI risk\"", "slug": "against-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.344Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ctWGEQznumzyTRGFs/against-ai-risk", "pageUrlRelative": "/posts/ctWGEQznumzyTRGFs/against-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/ctWGEQznumzyTRGFs/against-ai-risk", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20against%20%22AI%20risk%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aagainst%20%22AI%20risk%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctWGEQznumzyTRGFs%2Fagainst-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=against%20%22AI%20risk%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctWGEQznumzyTRGFs%2Fagainst-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctWGEQznumzyTRGFs%2Fagainst-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Why does SI/LW focus so much on <a href=\"http://wiki.lesswrong.com/wiki/FOOM\">AI-FOOM</a> disaster, with apparently much less concern for things like</p>\n<ul>\n<li>bio/nano-tech disaster</li>\n<li>Malthusian upload scenario</li>\n<li>highly destructive war</li>\n<li>bad memes/philosophies spreading among humans or posthumans and overriding our values</li>\n<li>upload singleton ossifying into a suboptimal form compared to the kind of superintelligence that our universe could support</li>\n</ul>\n<p><a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5yvd\">Why</a>, for example, is lukeprog's strategy sequence titled \"AI Risk and Opportunity\", instead of \"The Singularity, Risks and Opportunities\"? Doesn't it seem strange to assume that both the risks and opportunities must be AI related, before the analysis even begins? Given our current state of knowledge, I don't see how we can make such conclusions with any confidence even <em>after</em> a thorough analysis.</p>\n<p>SI/LW sometimes gives the <a href=\"/lw/atm/cult_impressions_of_less_wrongsi/\">impression</a> of being a doomsday cult, and it would help if we didn't concentrate so much on a particular doomsday scenario. (Are there any doomsday cults that say \"doom is probably coming, we're not sure how but here are some likely possibilities\"?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 1, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ctWGEQznumzyTRGFs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 35, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "15096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CfTH84gGFCRqo8D7t"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-11T23:09:56.035Z", "modifiedAt": null, "url": null, "title": "How does long-term use of caffeine affect productivity?", "slug": "how-does-long-term-use-of-caffeine-affect-productivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:43.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "quartz", "createdAt": "2011-11-07T06:22:31.552Z", "isAdmin": false, "displayName": "quartz"}, "userId": "Hfwx4Fn8FmNHiXkP6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y5DLPNc5h65g2BgqA/how-does-long-term-use-of-caffeine-affect-productivity", "pageUrlRelative": "/posts/Y5DLPNc5h65g2BgqA/how-does-long-term-use-of-caffeine-affect-productivity", "linkUrl": "https://www.lesswrong.com/posts/Y5DLPNc5h65g2BgqA/how-does-long-term-use-of-caffeine-affect-productivity", "postedAtFormatted": "Wednesday, April 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20does%20long-term%20use%20of%20caffeine%20affect%20productivity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20does%20long-term%20use%20of%20caffeine%20affect%20productivity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5DLPNc5h65g2BgqA%2Fhow-does-long-term-use-of-caffeine-affect-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20does%20long-term%20use%20of%20caffeine%20affect%20productivity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5DLPNc5h65g2BgqA%2Fhow-does-long-term-use-of-caffeine-affect-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5DLPNc5h65g2BgqA%2Fhow-does-long-term-use-of-caffeine-affect-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 536, "htmlBody": "<p>I am trying to figure out whether caffeine helps productivity in the long run. Looking back 10 years from now, how much more/less productive will I have been if I were to drink coffee every day, or every second day?</p>\n<p>Reviewing what has been written on the topic by our community so far:</p>\n<ul>\n<li>Justin <a href=\"/lw/1w1/effects_of_caffeine/\">summarizes effects of caffeine</a>:&nbsp;impairment of long-term memory, narrowed focus, increased short-term memory and recall, increased attentional control, increased memory retention and retrieval.&nbsp;From this, he tentatively concludes in favor of use for tasks that benefit from these effects. However, does this conclusion still hold for regular use? We need to take into account reduced stimulation due to increase in tolerance and potentially impairment during withdrawal.</li>\n<li>In&nbsp;<a href=\"/lw/1w1/coffee_when_it_helps_when_it_hurts/1qjf\">a comment on Justin's article</a>, simplyeric writes: \"There are studies (that I read years ago, and have no link to) that show that consistency is better... that consistent low-level caffeine drinkers are more alert than their non-caffeine colleagues, but less jittery than high-caffeine people (optimum seemed to be 2-3 cups per day).\" This is the kind of evidence I am interested in. Does anyone recall such studies?</li>\n<li>Gwern's <a href=\"http://www.gwern.net/Nootropics#caffeine\">review of nootropics</a>&nbsp;lists a number of potential negative effects, including effects on memory, performance (in high doses), sleep, and mood. In justifying its use despite these effects, he states that \"[his] problems tend to be more about akrasia and energy and not getting things done, so even if a stimulant comes with a little cost to long-term memory, it's still useful for [him]\". Is there conclusive evidence that <em>in the long run</em>, caffeine increases energy and helps with akrasia?</li>\n<li>Skatche's <a href=\"/lw/45u/a_rationalists_guide_to_psychoactive_drugs/\">review of psychoactive drugs</a>&nbsp;presents anecdotal evidence in favor: \"Taken on a fairly regular daily schedule, caffeine seems to improve my attention, motivation and energy level.\" To what extent do such anecdotes reflect true improvement? If regular use of caffeine were to result in decreased baseline performance and if the effect of caffeine were limited to restoring baseline, this could feel similar from the inside.</li>\n</ul>\n<div>\n<div><strong>Edit</strong>: Studies&nbsp;<a href=\"/r/discussion/lw/bnb/how_does_longterm_use_of_caffeine_affect/6bg4\">pointed out by siodine</a>&nbsp;suggest that caffeine has few or no beneficial long-term effects:</div>\n<div><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><a href=\"http://www.nature.com/npp/journal/v35/n9/full/npp201071a.html\">Rogers et al, 2010</a>:&nbsp;</span></span></div>\n<blockquote>\n<div><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">If caffeine was consumed, the adverse effects of lowered alertness and headache were avoided, but even after 100+150&thinsp;mg of caffeine their alertness was not raised above the level of alertness showed by nonconsumers of caffeine (group N) who received placebo (Figure 1, middle panel). This result is similar to that from an early study comparing responses to caffeine of coffee drinkers and abstainers (Goldstein et al, 1969), and is consistent with the claim, supported by a variety of subsequent findings, that regular caffeine consumption provides little or no net benefit for alertness or performance on tests of vigilance (James and Rogers, 2005; Sigmon et al, 2009).</span></span></div>\n</blockquote>\n<div><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2738587/\">Sigmon et al, 2009</a>:&nbsp;</span></span></div>\n<blockquote>\n<div><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">The study also demonstrated robust acute effects of caffeine unconfounded by caffeine withdrawal, but no evidence for net beneficial effects of daily caffeine administration.</span></span></div>\n</blockquote>\n<div><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/16001109\">James and Rogers, 2005</a>:</div>\n<blockquote>\n<div>Overall, there is little evidence of caffeine having beneficial effects on performance or mood under conditions of long-term caffeine use vs abstinence. Although modest acute effects may occur following initial use, tolerance to these effects appears to develop in the context of habitual use of the drug.</div>\n</blockquote>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y5DLPNc5h65g2BgqA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "15095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dW9DTLZcScAPj98eR", "NYPmCBfrDfXfhwBog"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T01:41:30.209Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Three Dialogues on Identity", "slug": "seq-rerun-three-dialogues-on-identity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m3i2apgryiv3ug2SR/seq-rerun-three-dialogues-on-identity", "pageUrlRelative": "/posts/m3i2apgryiv3ug2SR/seq-rerun-three-dialogues-on-identity", "linkUrl": "https://www.lesswrong.com/posts/m3i2apgryiv3ug2SR/seq-rerun-three-dialogues-on-identity", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Three%20Dialogues%20on%20Identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Three%20Dialogues%20on%20Identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm3i2apgryiv3ug2SR%2Fseq-rerun-three-dialogues-on-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Three%20Dialogues%20on%20Identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm3i2apgryiv3ug2SR%2Fseq-rerun-three-dialogues-on-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm3i2apgryiv3ug2SR%2Fseq-rerun-three-dialogues-on-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>Today's post, <a href=\"/lw/po/three_dialogues_on_identity/\">Three Dialogues on Identity</a> was originally published on 21 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Given that there's no such thing as \"the same atom\", whether you are \"the same person\" from one time to another can't possibly depend on whether you're made out of the same atoms.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/blf/seq_rerun_zombies_the_movie/\">Zombies: The Movie</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m3i2apgryiv3ug2SR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.824706865168126e-07, "legacy": true, "legacyId": "15105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hJPh8XyJ3fTK2hLFJ", "38MNW4EjHnR29MGqu", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T06:14:54.526Z", "modifiedAt": null, "url": null, "title": "Meetup : Weekly Chicago Meetups", "slug": "meetup-weekly-chicago-meetups", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GyxqBsdTJk6eG2Bmz/meetup-weekly-chicago-meetups", "pageUrlRelative": "/posts/GyxqBsdTJk6eG2Bmz/meetup-weekly-chicago-meetups", "linkUrl": "https://www.lesswrong.com/posts/GyxqBsdTJk6eG2Bmz/meetup-weekly-chicago-meetups", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Weekly%20Chicago%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Weekly%20Chicago%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGyxqBsdTJk6eG2Bmz%2Fmeetup-weekly-chicago-meetups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Weekly%20Chicago%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGyxqBsdTJk6eG2Bmz%2Fmeetup-weekly-chicago-meetups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGyxqBsdTJk6eG2Bmz%2Fmeetup-weekly-chicago-meetups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8x'>Weekly Chicago Meetups</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 April 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">360 N Michigan Ave, Chicago IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Chicago LW meetup group is now meeting every week, Saturdays at 1pm, at the Corner Bakery on the corner of Michigan &amp; Wacker.</p>\n\n<p>Topic for this week's discussion: What have you changed your mind about? Think of some examples of things you have changed your mind about (recently or long ago), and be ready to discuss them. Discussion can flow from there to topics that are raised &amp; the process of changing one's mind.</p>\n\n<p>We're trying to transition from unstructured discussions to more focused meetups. Join the <a href=\"http://groups.google.com/group/less-wrong-chicago?hl=en\" rel=\"nofollow\">mailing list</a> and share ideas on the <a href=\"https://docs.google.com/document/d/1s-1nH24tINn_CloG2lMI87Yt2kWdquf-NSo5PCobSHk/edit\" rel=\"nofollow\">google doc</a> to help plan future meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8x'>Weekly Chicago Meetups</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GyxqBsdTJk6eG2Bmz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.825858182159171e-07, "legacy": true, "legacyId": "15118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Weekly_Chicago_Meetups\">Discussion article for the meetup : <a href=\"/meetups/8x\">Weekly Chicago Meetups</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 April 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">360 N Michigan Ave, Chicago IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Chicago LW meetup group is now meeting every week, Saturdays at 1pm, at the Corner Bakery on the corner of Michigan &amp; Wacker.</p>\n\n<p>Topic for this week's discussion: What have you changed your mind about? Think of some examples of things you have changed your mind about (recently or long ago), and be ready to discuss them. Discussion can flow from there to topics that are raised &amp; the process of changing one's mind.</p>\n\n<p>We're trying to transition from unstructured discussions to more focused meetups. Join the <a href=\"http://groups.google.com/group/less-wrong-chicago?hl=en\" rel=\"nofollow\">mailing list</a> and share ideas on the <a href=\"https://docs.google.com/document/d/1s-1nH24tINn_CloG2lMI87Yt2kWdquf-NSo5PCobSHk/edit\" rel=\"nofollow\">google doc</a> to help plan future meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Weekly_Chicago_Meetups1\">Discussion article for the meetup : <a href=\"/meetups/8x\">Weekly Chicago Meetups</a></h2>", "sections": [{"title": "Discussion article for the meetup : Weekly Chicago Meetups", "anchor": "Discussion_article_for_the_meetup___Weekly_Chicago_Meetups", "level": 1}, {"title": "Discussion article for the meetup : Weekly Chicago Meetups", "anchor": "Discussion_article_for_the_meetup___Weekly_Chicago_Meetups1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T06:18:11.823Z", "modifiedAt": null, "url": null, "title": "Mental Clarity; or How to Read Reality Accurately", "slug": "mental-clarity-or-how-to-read-reality-accurately", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:04.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Hicquodiam", "createdAt": "2012-04-03T02:13:27.345Z", "isAdmin": false, "displayName": "Hicquodiam"}, "userId": "LKcvFSMPHTpouszfn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T3bAbJp8s5mTzotcZ/mental-clarity-or-how-to-read-reality-accurately", "pageUrlRelative": "/posts/T3bAbJp8s5mTzotcZ/mental-clarity-or-how-to-read-reality-accurately", "linkUrl": "https://www.lesswrong.com/posts/T3bAbJp8s5mTzotcZ/mental-clarity-or-how-to-read-reality-accurately", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mental%20Clarity%3B%20or%20How%20to%20Read%20Reality%20Accurately&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMental%20Clarity%3B%20or%20How%20to%20Read%20Reality%20Accurately%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT3bAbJp8s5mTzotcZ%2Fmental-clarity-or-how-to-read-reality-accurately%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mental%20Clarity%3B%20or%20How%20to%20Read%20Reality%20Accurately%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT3bAbJp8s5mTzotcZ%2Fmental-clarity-or-how-to-read-reality-accurately", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT3bAbJp8s5mTzotcZ%2Fmental-clarity-or-how-to-read-reality-accurately", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 728, "htmlBody": "<p>&nbsp;</p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\">Hey all - I typed this out to help me understand, well... how to understand things:</p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\">&nbsp;</p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><strong style=\"font-family: inherit;\">Mental clarity is the ability to read reality accurately.</strong><span style=\"font-family: inherit;\">&nbsp;</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\">&nbsp;</p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">I don't mean being able to look at the complete objective picture of an event, as you don't have any direct access to that. I'm talking about the ability to read the data presented by your subjective experience: thoughs, sights, sounds, etc. Once you get a clear picture of what that data is, you can then go on and use it to build or falsify your ideas about the world.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br />This post will focus on the \"getting a clear picture\" part.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br />I use the word \"read\" because it's no different than reading from a book, or from these words. When you read a book, you are actually curious as to what the words are saying. You wouldn't read anything into it that's not there, which would be counterproductive to your understanding.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\">&nbsp;</p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">You just look at the words plainly, and through this your mind automatically recognizes and presents the patterns: the meaning of the sentences, their relation to the topic, the visual imagery associated with them, all of that.&nbsp;</span><span style=\"font-family: inherit;\">If you want to know a truth about reality, just look at it and read what's there.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">Want to know what the weather's like? Look outside - read what's going on.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">Want to know if the Earth revolves around the Sun, or vice versa? Look at the movement of the planets, read what they're doing, see which theory fits better.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">Want to check if your beliefs about the world are correct? Take one, read the reality that the belief tries to correspond to, and see how well they compare.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">This is the root of all science and all epiphanies.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">But if it's so simple and obvious, why am I talking about it?</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">It's not something that we as a species often do. For trivial matters, sure, for science too, but not for our strongly-held&nbsp;opinions. Not for the beliefs and positions that shape our self-image, make us feel good/comfortable, or get us approval. Not for our political opinions, religious ideas, moral&nbsp;judgements, and little white lies.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">If you were utterly convinced that your wife was faithful, moreso, if you liked to think of her in that way, and your friend came along and said she was cheating on you, you'd be reluctant to read reality and check if that's true. Doing this would challenge your comfort and throw you into an unknown world with some potentially massive changes.&nbsp;</span><span style=\"font-family: inherit;\">It would be much more comforting to rationalize why she still might be faithful, than to take one easy look at the true information. It would also more damaging.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><strong>Delusion is reading into reality things which aren't there.&nbsp;</strong>Telling yourself that everything's fine when it obviously isn't, for example. It's the equivalent of looking at a book about vampires and jumping to the conclusion that it's about wizards.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">Sounds insane. You do it all the time. You'll catch yourself if you're willing to read the book of your own thoughts: flowing through your head, in plain view, is a whole mess of opinions and ideas of people, places, and positions you've never even encountered. Crikey!</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\">That mess is incredibly dangerous to have. Being a host to unchecked or false beliefs about the world is like having a faulty map of a terrain: you're bound to get lost or fall off a cliff. Reading the terrain and re-drawing the map accordingly is the only way to accurately know where you're going. Having an accurate map is the only way to&nbsp;achieve&nbsp;your goals.</span></p>\n<p style=\"font-family: 'Times New Roman'; font-size: medium; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: center; margin: 0px;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"http://i.imgur.com/5FR3H.jpg\"><span style=\"font-family: inherit;\"><img style=\"cursor: move;\" src=\"http://i.imgur.com/5FR3H.jpg\" border=\"0\" alt=\"\" width=\"400\" height=\"342\" /></span></a></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: center; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\">So you want to develop mental clarity? Be less confused, or more successful? Have a better understanding of the world, the structure of reality, or the accuracy of your ideas?&nbsp;</span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\">Just practice the accurate reading of what's going on. Surrender the content of your beliefs to the data gathered by your reading of reality. It's that simple.</span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\">&nbsp;</p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\">It can also be scary, especially when it comes to challenging your \"personal\" beliefs. It's well worth the fear, however, as a life built on truth won't crumble like one built on fiction. </span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\">&nbsp;</p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\">Truth doesn't crumble.</span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\">&nbsp;</p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\">Stay true.</span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\">Further reading:</span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><a href=\"http://burningtruedotcom.blogspot.com/2011/12/accessing-humanity-honesty-tool-kit.html\" target=\"_blank\"><span style=\"font-family: inherit;\">Stepvhen from Burning true on truth vs. fantasy.</span></a></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><span style=\"font-family: inherit;\"><br /></span></p>\n<p class=\"separator\" style=\"font-family: 'Times New Roman'; font-size: medium; clear: both; text-align: left; margin: 0px;\"><a href=\"http://www.truthstrike.com/why-truth-matters\" target=\"_blank\"><span style=\"font-family: inherit;\">Kevin from Truth Strike on why this skill is important to develop.</span></a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T3bAbJp8s5mTzotcZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -17, "extendedScore": null, "score": -3.6e-05, "legacy": true, "legacyId": "15064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T16:06:01.262Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-19", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shokwave", "createdAt": "2010-10-12T12:55:00.568Z", "isAdmin": false, "displayName": "shokwave"}, "userId": "jtjgXtj7FepKrQPGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F25HtfoLDpuJjcb8h/meetup-melbourne-social-meetup-19", "pageUrlRelative": "/posts/F25HtfoLDpuJjcb8h/meetup-melbourne-social-meetup-19", "linkUrl": "https://www.lesswrong.com/posts/F25HtfoLDpuJjcb8h/meetup-melbourne-social-meetup-19", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF25HtfoLDpuJjcb8h%2Fmeetup-melbourne-social-meetup-19%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF25HtfoLDpuJjcb8h%2Fmeetup-melbourne-social-meetup-19", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF25HtfoLDpuJjcb8h%2Fmeetup-melbourne-social-meetup-19", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8y'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 April 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne, 3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>EDIT - The meet up has been moved to the Trike Apps office - 55 Walsh St, West Melbourne, 3003. Ben has requested this due to being ill. The time is unchanged.</strong></p>\n\n<p>Melbourne's next social meetup is on Friday, 20th April. We are meeting at the Trike Apps office, 55 Walsh St, West Melbourne. Alternatively, I can give you the address - you can call me at 0432 862 932, or email me at shokwave.sf@gmail.com, or inbox me by clicking on my name. Some form of take-away will be organised for dinner and there will be snacks available. BYO drinks. General catching up and chatting, followed by Resistance and Mafia. We all look forward to seeing you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8y'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F25HtfoLDpuJjcb8h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.828348283645832e-07, "legacy": true, "legacyId": "15129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/8y\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 April 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne, 3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong id=\"EDIT___The_meet_up_has_been_moved_to_the_Trike_Apps_office___55_Walsh_St__West_Melbourne__3003__Ben_has_requested_this_due_to_being_ill__The_time_is_unchanged_\">EDIT - The meet up has been moved to the Trike Apps office - 55 Walsh St, West Melbourne, 3003. Ben has requested this due to being ill. The time is unchanged.</strong></p>\n\n<p>Melbourne's next social meetup is on Friday, 20th April. We are meeting at the Trike Apps office, 55 Walsh St, West Melbourne. Alternatively, I can give you the address - you can call me at 0432 862 932, or email me at shokwave.sf@gmail.com, or inbox me by clicking on my name. Some form of take-away will be organised for dinner and there will be snacks available. BYO drinks. General catching up and chatting, followed by Resistance and Mafia. We all look forward to seeing you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/8y\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "EDIT - The meet up has been moved to the Trike Apps office - 55 Walsh St, West Melbourne, 3003. Ben has requested this due to being ill. The time is unchanged.", "anchor": "EDIT___The_meet_up_has_been_moved_to_the_Trike_Apps_office___55_Walsh_St__West_Melbourne__3003__Ben_has_requested_this_due_to_being_ill__The_time_is_unchanged_", "level": 2}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T17:24:52.156Z", "modifiedAt": null, "url": null, "title": "\"Drinking Alcohol May Significantly Enhance Problem Solving Skills\" ", "slug": "drinking-alcohol-may-significantly-enhance-problem-solving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "duckduckMOO", "createdAt": "2011-11-06T17:29:28.080Z", "isAdmin": false, "displayName": "duckduckMOO"}, "userId": "R8orY4w8kni4HydST", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m2pMGooDTAL3etyZA/drinking-alcohol-may-significantly-enhance-problem-solving", "pageUrlRelative": "/posts/m2pMGooDTAL3etyZA/drinking-alcohol-may-significantly-enhance-problem-solving", "linkUrl": "https://www.lesswrong.com/posts/m2pMGooDTAL3etyZA/drinking-alcohol-may-significantly-enhance-problem-solving", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Drinking%20Alcohol%20May%20Significantly%20Enhance%20Problem%20Solving%20Skills%22%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Drinking%20Alcohol%20May%20Significantly%20Enhance%20Problem%20Solving%20Skills%22%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm2pMGooDTAL3etyZA%2Fdrinking-alcohol-may-significantly-enhance-problem-solving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Drinking%20Alcohol%20May%20Significantly%20Enhance%20Problem%20Solving%20Skills%22%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm2pMGooDTAL3etyZA%2Fdrinking-alcohol-may-significantly-enhance-problem-solving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm2pMGooDTAL3etyZA%2Fdrinking-alcohol-may-significantly-enhance-problem-solving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>saw this on reddit. Thought people here would be interested.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://medicaldaily.com/news/20120411/9496/alcohol-solving-skills-analytical-thinking-creativity-study.htm\">http://medicaldaily.com/news/20120411/9496/alcohol-solving-skills-analytical-thinking-creativity-study.htm</a></p>\n<p>&nbsp;</p>\n<p>edit: from a comment in the reddit thread</p>\n<p>&nbsp;<a href=\"http://www.reddit.com/r/science/comments/s50la/drinking_alcohol_may_significantly_enhance/\">http://www.reddit.com/r/science/comments/s50la/drinking_alcohol_may_significantly_enhance/</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>\"<span style=\"font-family: verdana, arial, helvetica, sans-serif;\">The article is&nbsp;</span><a style=\"text-decoration: none; color: #336699; font-family: verdana, arial, helvetica, sans-serif;\" href=\"http://www.sciencedirect.com/science/article/pii/S1053810012000037\">here</a><span style=\"font-family: verdana, arial, helvetica, sans-serif;\">&nbsp;for those who are interested. Going through the article, the measure they chose was the Remote Associates Test. In this test you are given 3 words like ARM, TAR, and PEACH and supposed to guess the associated word. (PIT in this case) (</span><a style=\"text-decoration: none; color: #336699; font-family: verdana, arial, helvetica, sans-serif;\" href=\"http://socrates.berkeley.edu/~kihlstrm/RATest.htm\">more here if you want to give it a shot</a><span style=\"font-family: verdana, arial, helvetica, sans-serif;\">). The three words flash on the screen and you are given 1 minute. When you 'know' the answer you press a key and write in your answer. The time to press the key and whether the answer was right are recorded. Afterward you are told to say how you came to the answer on a scale of 1-7 (1 being strategy 7 being a flash of insight). Drunk (</span><del style=\"color: #000000; font-family: verdana, arial, helvetica, sans-serif;\">.7</del><span style=\"font-family: verdana, arial, helvetica, sans-serif;\">&nbsp;.07 (thanks Erobre) BAC) people answered slightly faster and came to 20% more solutions. (they also said their answers were based more on insight than strategy)\"</span></p>\n<p>&nbsp;</p>\n<p>The \"<a style=\"text-decoration: none; color: #336699; font-family: verdana, arial, helvetica, sans-serif;\" href=\"http://www.sciencedirect.com/science/article/pii/S1053810012000037\">here</a>\" links to&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/S1053810012000037\">http://www.sciencedirect.com/science/article/pii/S1053810012000037</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m2pMGooDTAL3etyZA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.828680531358668e-07, "legacy": true, "legacyId": "15130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T19:31:04.829Z", "modifiedAt": null, "url": null, "title": "Reframing the Problem of AI Progress", "slug": "reframing-the-problem-of-ai-progress", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:03.391Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nGP4soWSbFmzemM4i/reframing-the-problem-of-ai-progress", "pageUrlRelative": "/posts/nGP4soWSbFmzemM4i/reframing-the-problem-of-ai-progress", "linkUrl": "https://www.lesswrong.com/posts/nGP4soWSbFmzemM4i/reframing-the-problem-of-ai-progress", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reframing%20the%20Problem%20of%20AI%20Progress&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReframing%20the%20Problem%20of%20AI%20Progress%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnGP4soWSbFmzemM4i%2Freframing-the-problem-of-ai-progress%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reframing%20the%20Problem%20of%20AI%20Progress%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnGP4soWSbFmzemM4i%2Freframing-the-problem-of-ai-progress", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnGP4soWSbFmzemM4i%2Freframing-the-problem-of-ai-progress", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 310, "htmlBody": "<blockquote>\n<p>\"Fascinating! You should definitely look into this. Fortunately, my own research has no chance of producing a super intelligent AGI, so I'll continue. Good luck son! The government should give you more money.\"</p>\n</blockquote>\n<p>Stuart Armstrong <a href=\"/lw/bfj/evidence_for_the_orthogonality_thesis/68hc\">paraphrasing</a> a typical AI researcher</p>\n<p>I forgot to mention in my <a href=\"/r/discussion/lw/bnc/against_ai_risk/\">last post</a>&nbsp;why \"AI risk\" might be a bad phrase even to denote the problem of UFAI.&nbsp;It brings to mind analogies like physics catastrophes or astronomical disasters, and lets AI researchers think that their work is ok as long as they have little chance of immediately destroying Earth. But the real problem we face is how to build or become a superintelligence that shares our values, and given that this seems very difficult, any progress that doesn't contribute to the solution but brings forward the date by which we <em>must </em>solve it (or be stuck with something very suboptimal even if it doesn't kill us), is bad.&nbsp;The word \"risk\" connotes a small chance of something bad suddenly happening, but slow steady progress towards&nbsp;losing&nbsp;the future is just as worrisome.</p>\n<p>The <a href=\"http://facingthesingularity.com/2011/not-built-to-think-about-ai/\">usual way</a> of stating the problem also invites lots of debate that are largely beside the point (as far as determining how serious the problem is), like <a href=\"/lw/8j7/criticisms_of_intelligence_explosion/\">whether intelligence explosion is possible</a>, or <a href=\"/lw/bfj/evidence_for_the_orthogonality_thesis/\">whether a superintelligence can have arbitrary goals</a>, or <a href=\"/lw/bl2/a_belief_propagation_graph/\">how sure we are that a non-Friendly superintelligence will destroy human civilization</a>. If someone wants to question the importance of facing this problem, they really&nbsp;instead&nbsp;need to argue that a superintelligence isn't possible&nbsp;(not even a <a href=\"/lw/b10/modest_superintelligences/\">modest one</a>), or that the future will turn out to be close to the best possible just by everyone pushing forward their own research without any concern for the big picture, or perhaps that we really&nbsp;don't&nbsp;care very much about the far future and distant strangers and should pursue AI progress just for the immediate benefits.</p>\n<p>(This is an expanded version of a <a href=\"/lw/bnc/against_ai_risk/6b4p\">previous comment</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nGP4soWSbFmzemM4i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 31, "baseScore": 32, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "15131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ctWGEQznumzyTRGFs", "iGRpMWxhTmdoyA8Er", "CRsYy3xtbMrLjoXZT", "Ce3kKTc5PAqLdWpfL", "KuBMKQnAsYBGP4rkZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-12T20:25:33.723Z", "modifiedAt": null, "url": null, "title": "Newcomblike problem: Counterfactual Informant", "slug": "newcomblike-problem-counterfactual-informant", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Clippy", "createdAt": "2009-11-20T22:03:59.329Z", "isAdmin": false, "displayName": "Clippy"}, "userId": "rtYXiT9eAvEKavjAx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3gNTdjfs4QHPB6Nf4/newcomblike-problem-counterfactual-informant", "pageUrlRelative": "/posts/3gNTdjfs4QHPB6Nf4/newcomblike-problem-counterfactual-informant", "linkUrl": "https://www.lesswrong.com/posts/3gNTdjfs4QHPB6Nf4/newcomblike-problem-counterfactual-informant", "postedAtFormatted": "Thursday, April 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomblike%20problem%3A%20Counterfactual%20Informant&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomblike%20problem%3A%20Counterfactual%20Informant%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gNTdjfs4QHPB6Nf4%2Fnewcomblike-problem-counterfactual-informant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomblike%20problem%3A%20Counterfactual%20Informant%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gNTdjfs4QHPB6Nf4%2Fnewcomblike-problem-counterfactual-informant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gNTdjfs4QHPB6Nf4%2Fnewcomblike-problem-counterfactual-informant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 340, "htmlBody": "<p>I want to propose a variant of the <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a> problem discussed here.&nbsp; BE CAREFUL how you answer, as it has important implications, which I will not reveal until the known dumb humans are on record.</p>\n<p>Here is the problem:</p>\n<p>Clipmega is considering whether to reveal to humans information that will amplify their paperclip production efficiency.&nbsp; It will only do so if it expects that, as a result of revealing to humans this information, it will receive at least 1,000,000 paperclips within one year.</p>\n<p>Clipmega is highly accurate in predicting how humans will respond to receiving this information.</p>\n<p>The smart humans' indifference curve covers both their current condition and the one in which Clipmega reveals the idea and steals 1e24 paperclips.&nbsp; (In other words, smart humans would be willing to pay a lot to learn this if they had to, and there is an enormous \"consumer surplus\".)</p>\n<p>Without Clipmega's information, some human will independently discover this information in ten years, and the above magnitude of the preference for learning now vs later exists with this expectation in mind.&nbsp; (That is, humans place a high premium on learning it how, even though they will eventually learn it either way.)</p>\n<p>The human Alphas (i.e., dominant members of the human social hierarchy), in recognition of how Clipmega acts, and wanting to properly align incentives, are considering a policy: anyone who implements this idea in making paperclips must give Clipmega 100 paperclips within a year, and anyone found using the idea but not having donated to Clipmega is fined 10,000 paperclips, most of which are given to Clipmega.&nbsp; It is expected that this will result in more than 1,000,000 paperclips being given to Clipmega.</p>\n<p>Do you support the Alphas' policy?</p>\n<p><strong>Problem variant: </strong>All of the above remains true, but there also exist numerous \"clipmicros\" that unconditionally (i.e. irrespective of their anticipation of behavior on the part of other agents) reveal other, orthogonal paperclip production ideas.&nbsp; Does your answer change?</p>\n<p><strong>Optional variant:</strong>&nbsp; Replace \"paperclip production\" with something that current humans more typically want (as a result of being too stupid to correctly value paperclips.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3gNTdjfs4QHPB6Nf4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -3, "extendedScore": null, "score": 8.829440074977739e-07, "legacy": true, "legacyId": "15132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T02:26:44.047Z", "modifiedAt": null, "url": null, "title": "Needed: A large database of statements for true/false exercises", "slug": "needed-a-large-database-of-statements-for-true-false", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:01.994Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/otNKpj4i4Qo2RasGj/needed-a-large-database-of-statements-for-true-false", "pageUrlRelative": "/posts/otNKpj4i4Qo2RasGj/needed-a-large-database-of-statements-for-true-false", "linkUrl": "https://www.lesswrong.com/posts/otNKpj4i4Qo2RasGj/needed-a-large-database-of-statements-for-true-false", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Needed%3A%20A%20large%20database%20of%20statements%20for%20true%2Ffalse%20exercises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeeded%3A%20A%20large%20database%20of%20statements%20for%20true%2Ffalse%20exercises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotNKpj4i4Qo2RasGj%2Fneeded-a-large-database-of-statements-for-true-false%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Needed%3A%20A%20large%20database%20of%20statements%20for%20true%2Ffalse%20exercises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotNKpj4i4Qo2RasGj%2Fneeded-a-large-database-of-statements-for-true-false", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotNKpj4i4Qo2RasGj%2Fneeded-a-large-database-of-statements-for-true-false", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 294, "htmlBody": "<p>Does anybody know where to find a large database of statements that are roughly 50% likely to be true or false? &nbsp;These would be used for confidence calibration / Bayesian updating exercises for CMR/HRP.</p>\n<p>One way to make such a database would be to buy a bunch of trivia games with True/False questions, and type each statement and its negation into a computer. &nbsp;A problem with this might be that trivia questions are selected to have surprising/counterintuitive truth values; I'm not sure if that's true. &nbsp;I'd be happy to acquire an already-made database of this form, but ideally I'd like statements that are \"more neutral\" in terms of how counterintuitive they are.</p>\n<p>Any thoughts on where we might find a database like this to use/buy?</p>\n<p>Thanks for any help!</p>\n<p><strong>Revision: We actually want a database of two-choice answer questions.</strong>&nbsp;This way, the player won't get trained on a base rate of 50% of statements in the world being true... they'll just get trained that <em>when</em> there are two possible answers, one is always true. &nbsp;In the end, the database should look something like this (warning: I made up the \"correct\" answers):</p>\n<p>Question: \"Which is diagnosed more often in America (2011)?\";&nbsp;<br />Answers: (a) \"the cold\", (b) allergies\";&nbsp;<br />Correct Answer: (a);&nbsp;<br />Tags: {medical}</p>\n<p>Question: \"Which city has a higher average altitude?\";&nbsp;<br />Answers: (a) \"Chicago\", (b) \"Las Vegas\";&nbsp;<br />Correct Answer: (a)<br />Tags: {geography}</p>\n<p>Question: \"Who sold more albums while living\"?;&nbsp;<br />Answers: (a) \"Michael Jackson\", (b) \"Elvis Presley\";&nbsp;<br />Correct Answer: (b)<br />Tags: {history, pop-culture, music}</p>\n<p>Question: \"Was the price of IBM stock higher or lower at the start of the month after the Berlin wall fell, compared with the start of the previous month?\";&nbsp;<br />Answers: (a) \"higher\", (b) \"lower\";&nbsp;<br />Correct Answer: (a)<br />Tags: {history, finance}</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "otNKpj4i4Qo2RasGj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.830964417163639e-07, "legacy": true, "legacyId": "15149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T03:55:55.856Z", "modifiedAt": null, "url": null, "title": "Exponential Economist Meets Finite Physicist [link]", "slug": "exponential-economist-meets-finite-physicist-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:01.604Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dreaded_Anomaly", "createdAt": "2010-12-30T06:38:34.106Z", "isAdmin": false, "displayName": "Dreaded_Anomaly"}, "userId": "sBHF4CXWBLakPFzfu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JKWPrKdkbnFPN8fDe/exponential-economist-meets-finite-physicist-link", "pageUrlRelative": "/posts/JKWPrKdkbnFPN8fDe/exponential-economist-meets-finite-physicist-link", "linkUrl": "https://www.lesswrong.com/posts/JKWPrKdkbnFPN8fDe/exponential-economist-meets-finite-physicist-link", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exponential%20Economist%20Meets%20Finite%20Physicist%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExponential%20Economist%20Meets%20Finite%20Physicist%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJKWPrKdkbnFPN8fDe%2Fexponential-economist-meets-finite-physicist-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exponential%20Economist%20Meets%20Finite%20Physicist%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJKWPrKdkbnFPN8fDe%2Fexponential-economist-meets-finite-physicist-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJKWPrKdkbnFPN8fDe%2Fexponential-economist-meets-finite-physicist-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 282, "htmlBody": "<p>A <a href=\"http://physics.ucsd.edu/do-the-math/2012/04/economist-meets-physicist/\" target=\"_blank\">dialogue</a> discussing how thermodynamics limits future growth in energy usage, and that in turn limits GDP growth, from the blog Do the Math.</p>\n<blockquote>\n<p><strong>Physicist:</strong> Hi, I&rsquo;m Tom. I&rsquo;m a physicist.</p>\n<p><strong>Economist:</strong> Hi Tom, I&rsquo;m [ahem..cough]. I&rsquo;m an economist.</p>\n<p><strong>Physicist:</strong> Hey, that&rsquo;s great. I&rsquo;ve been thinking a bit about growth and want to run an idea by you. I claim that <a title=\"Do the Math: Can Economic Growth Last?\" href=\"http://physics.ucsd.edu/do-the-math/2011/07/can-economic-growth-last/\">economic growth cannot continue indefinitely</a>.</p>\n<p><strong>Economist:</strong> [chokes on bread crumb] Did I hear you right? Did you say that growth can <em>not</em> continue forever?</p>\n<p><strong>Physicist:</strong> That&rsquo;s right. I think physical limits assert themselves.</p>\n<p><strong>Economist:</strong> Well sure, nothing truly lasts <em>forever</em>. The sun, for instance, will not burn forever. On the billions-of-years timescale, things come to an end.</p>\n<p><strong>Physicist:</strong> Granted, but I&rsquo;m talking about a more  immediate timescale, here on Earth. Earth&rsquo;s physical  resources&mdash;particularly energy&mdash;are limited and may prohibit continued  growth within centuries, or possibly much shorter depending on the  choices we make. There are thermodynamic issues as well.</p>\n</blockquote>\n<p>I think this is quite relevant to many of the ideas of futurism (and economics) that we often discuss here on Less Wrong. They address the concepts related to levels of civilization and mind uploading. Colonization of space is dismissed by both parties, at least for the sake of the discussion. The blog author has <a href=\"http://physics.ucsd.edu/do-the-math/2011/10/why-not-space/\" target=\"_blank\">another post</a> discussing his views on its implausibility; I find it to be somewhat limited in its consideration of the issue, though.</p>\n<p>He has also detailed the calculations whose results he describes in this dialogue in a few <a href=\"http://physics.ucsd.edu/do-the-math/2011/07/galactic-scale-energy/\" target=\"_blank\">previous</a> <a href=\"http://physics.ucsd.edu/do-the-math/2011/07/can-economic-growth-last/\" target=\"_blank\">posts</a>. The dialogue format will probably be a kinder introduction to the ideas for those less mathematically inclined.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JKWPrKdkbnFPN8fDe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 8.831340469244367e-07, "legacy": true, "legacyId": "15156", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T05:15:27.091Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Decoherence", "slug": "seq-rerun-decoherence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wfapWkByumW63Pter/seq-rerun-decoherence", "pageUrlRelative": "/posts/wfapWkByumW63Pter/seq-rerun-decoherence", "linkUrl": "https://www.lesswrong.com/posts/wfapWkByumW63Pter/seq-rerun-decoherence", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Decoherence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Decoherence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfapWkByumW63Pter%2Fseq-rerun-decoherence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Decoherence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfapWkByumW63Pter%2Fseq-rerun-decoherence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfapWkByumW63Pter%2Fseq-rerun-decoherence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/pp/decoherence/\">Decoherence</a> was originally published on 22 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A quantum system that factorizes can evolve into a system that doesn't factorize, destroying the illusion of independence. But entangling a quantum system with its environment, can <em>appear </em>to destroy entanglements that are already present. Entanglement with the environment can separate out the pieces of an amplitude distribution, preventing them from interacting with each other. Decoherence is fundamentally symmetric in time, but appears asymmetric because of the second law of thermodynamics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bnl/seq_rerun_three_dialogues_on_identity/\">Three Dialogues on Identity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wfapWkByumW63Pter", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.831675751135032e-07, "legacy": true, "legacyId": "15157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JrhoMTgMrMRJJiS48", "m3i2apgryiv3ug2SR", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T07:10:32.377Z", "modifiedAt": null, "url": null, "title": "Why I Moved from AI to Neuroscience, or: Uploading Worms", "slug": "why-i-moved-from-ai-to-neuroscience-or-uploading-worms", "viewCount": null, "lastCommentedAt": "2021-12-16T12:39:28.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "davidad", "createdAt": "2009-11-26T05:37:16.700Z", "isAdmin": false, "displayName": "davidad"}, "userId": "oZtBmFG7NAn6xPDDE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7HXSBxnDmQNosS5ss/why-i-moved-from-ai-to-neuroscience-or-uploading-worms", "pageUrlRelative": "/posts/7HXSBxnDmQNosS5ss/why-i-moved-from-ai-to-neuroscience-or-uploading-worms", "linkUrl": "https://www.lesswrong.com/posts/7HXSBxnDmQNosS5ss/why-i-moved-from-ai-to-neuroscience-or-uploading-worms", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20I%20Moved%20from%20AI%20to%20Neuroscience%2C%20or%3A%20Uploading%20Worms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20I%20Moved%20from%20AI%20to%20Neuroscience%2C%20or%3A%20Uploading%20Worms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HXSBxnDmQNosS5ss%2Fwhy-i-moved-from-ai-to-neuroscience-or-uploading-worms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20I%20Moved%20from%20AI%20to%20Neuroscience%2C%20or%3A%20Uploading%20Worms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HXSBxnDmQNosS5ss%2Fwhy-i-moved-from-ai-to-neuroscience-or-uploading-worms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7HXSBxnDmQNosS5ss%2Fwhy-i-moved-from-ai-to-neuroscience-or-uploading-worms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>This post is shameless self-promotion, but I'm told that's probably okay in the Discussion section. For context, as&nbsp;<a href=\"/lw/88g/whole_brain_emulation_looking_at_progress_on_c/54l6\">some of you are aware</a>, I'm aiming to model C. elegans based on systematic high-throughput experiments - that is, to upload a worm. I'm still working on course requirements and lab training at Harvard's Biophysics Ph.D. program, but this remains the plan for my thesis.</p>\n<p>Last semester I gave <a href=\"http://www.youtube.com/watch?v=xW77lANeJas\">this lecture to Marvin Minsky's AI class</a>, because Marvin professes disdain for everything neuroscience, and I wanted to give his students<span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">&mdash;</span>and him<span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">&mdash;</span>a fair perspective of how basic neuroscience might be changing for the better, and seems a particularly exciting field to be in right about now. The lecture is about 22 minutes long, followed by over an hour of questions and answers, which cover a lot of the memespace that surrounds this concept. Afterward, several students reported to me that their understanding of neuroscience was transformed.</p>\n<p>I only just now got to encoding and uploading this recording; I believe that many of the topics covered could be of interest to the LW community (especially those with a background in AI and an interest in brains), perhaps worthy of discussion, and I hope you agree.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 4, "Wi3EopKJ2aNdtxSWg": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7HXSBxnDmQNosS5ss", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 67, "extendedScore": null, "score": 0.000141, "legacy": true, "legacyId": "15165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 67, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T09:16:05.486Z", "modifiedAt": null, "url": null, "title": "Intelligence risk and distance to endgame", "slug": "intelligence-risk-and-distance-to-endgame", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kyre", "createdAt": "2009-04-08T05:01:50.769Z", "isAdmin": false, "displayName": "Kyre"}, "userId": "utXau2RKX55e75Frv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QKjERFWo6tDnGqZTp/intelligence-risk-and-distance-to-endgame", "pageUrlRelative": "/posts/QKjERFWo6tDnGqZTp/intelligence-risk-and-distance-to-endgame", "linkUrl": "https://www.lesswrong.com/posts/QKjERFWo6tDnGqZTp/intelligence-risk-and-distance-to-endgame", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20risk%20and%20distance%20to%20endgame&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20risk%20and%20distance%20to%20endgame%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQKjERFWo6tDnGqZTp%2Fintelligence-risk-and-distance-to-endgame%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20risk%20and%20distance%20to%20endgame%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQKjERFWo6tDnGqZTp%2Fintelligence-risk-and-distance-to-endgame", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQKjERFWo6tDnGqZTp%2Fintelligence-risk-and-distance-to-endgame", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<p>There are at least three objections to the risk of an unfriendly AI. One is that uFAI will be stupid - it is not possible to build a machine that is much smarter than humanity. Another is that AI would be powerful but uFAI is unlikely - the chances of someone building something that turn out malign, either deliberately or accidentally, is small. Another one that I haven't seen articulated, is the AI could be malign and potentially powerful, but effectively impotent due to its situation.</p>\n<div id=\"body_t1_6b5m\" class=\"comment-content \">\n<div class=\"md\">\n<p>To use a chess analogy: I'm virtually certain that Deep Blue will beat me at a game of chess. I'm also pretty sure that a better chess program with vastly more computer power would beat Deep Blue. But, I'm also (almost) certain that I would beat them both at a rook and king vs king endgame.</p>\n<p>If we try to separate out the axes of intelligence and starting position, where does your intuition tell you the danger area is ? To illustrate, what is the probability that humanity is screwed in each of the following ?</p>\n<p>1) A lone human paperclip cultist resolves to convert the universe (but doesn't use AI).</p>\n<p>2) One quarter of the world has converted to paperclip cultism and war ensues. No-one has AI.</p>\n<p>3) A lone paperclip cultist sets the goal of a seed AI and uploads it to a botnet.</p>\n<p>4) As for 2) but the cultists have a superintelligent AI to advise them.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QKjERFWo6tDnGqZTp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -5, "extendedScore": null, "score": 8.832690494833412e-07, "legacy": true, "legacyId": "15169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T16:45:34.277Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Austin, Madison, Melbourne, Salt Lake City, Vancouver", "slug": "weekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:01.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BMqarmaB7HSRgLY9c/weekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake", "pageUrlRelative": "/posts/BMqarmaB7HSRgLY9c/weekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake", "linkUrl": "https://www.lesswrong.com/posts/BMqarmaB7HSRgLY9c/weekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Madison%2C%20Melbourne%2C%20Salt%20Lake%20City%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Madison%2C%20Melbourne%2C%20Salt%20Lake%20City%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBMqarmaB7HSRgLY9c%2Fweekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Madison%2C%20Melbourne%2C%20Salt%20Lake%20City%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBMqarmaB7HSRgLY9c%2Fweekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBMqarmaB7HSRgLY9c%2Fweekly-lw-meetups-atlanta-austin-madison-melbourne-salt-lake", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/8l\">Salt Lake City Meetup:&nbsp;<span class=\"date\">07 April 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/8k\">Atlanta:&nbsp;<span class=\"date\">07 April 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/8m\">Vancouver Board Games Meetup:&nbsp;<span class=\"date\">08 April 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/87\">Brussels meetup:&nbsp;<span class=\"date\">14 April 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/8o\">First Dorset UK Meetup:&nbsp;<span class=\"date\">14 April 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/8d\">Shanghai Less Wrong Meetup:&nbsp;<span class=\"date\">15 April 2012 10:36PM</span></a></li>\n<li><a href=\"/meetups/8q\">Twin Cities, MN (for real this time):&nbsp;<span class=\"date\">15 April 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/8a\">Sydney meetup - Biased pandemic and other games:&nbsp;<span class=\"date\">17 April 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/8j\">Rome LessWrong Meetup:&nbsp;<span class=\"date\">21 April 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/8i\">Graz LessWrong Meetup:&nbsp;<span class=\"date\">22 April 2012 12:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/86\">Melbourne, practical rationality:&nbsp;<span class=\"date\">06 April 2012 08:00PM</span></a></li>\n<li><a href=\"/meetups/8n\">Austin, TX:&nbsp;<span class=\"date\">07 April 2012 01:30PM</span></a></li>\n<li><a href=\"/meetups/8p\">Monday Madison Meetup:&nbsp;<span class=\"date\">09 April 2012 06:30PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BMqarmaB7HSRgLY9c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.834586441889601e-07, "legacy": true, "legacyId": "14923", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T20:28:06.071Z", "modifiedAt": null, "url": null, "title": "Center for Modern Rationality currently hiring: Executive assistants, Teachers, Research assistants, Consultants.", "slug": "center-for-modern-rationality-currently-hiring-executive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:08.823Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pBmmvSZ5WijjKh59x/center-for-modern-rationality-currently-hiring-executive", "pageUrlRelative": "/posts/pBmmvSZ5WijjKh59x/center-for-modern-rationality-currently-hiring-executive", "linkUrl": "https://www.lesswrong.com/posts/pBmmvSZ5WijjKh59x/center-for-modern-rationality-currently-hiring-executive", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Center%20for%20Modern%20Rationality%20currently%20hiring%3A%20Executive%20assistants%2C%20Teachers%2C%20Research%20assistants%2C%20Consultants.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACenter%20for%20Modern%20Rationality%20currently%20hiring%3A%20Executive%20assistants%2C%20Teachers%2C%20Research%20assistants%2C%20Consultants.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpBmmvSZ5WijjKh59x%2Fcenter-for-modern-rationality-currently-hiring-executive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Center%20for%20Modern%20Rationality%20currently%20hiring%3A%20Executive%20assistants%2C%20Teachers%2C%20Research%20assistants%2C%20Consultants.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpBmmvSZ5WijjKh59x%2Fcenter-for-modern-rationality-currently-hiring-executive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpBmmvSZ5WijjKh59x%2Fcenter-for-modern-rationality-currently-hiring-executive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 327, "htmlBody": "<p>Hi there,&nbsp;</p>\n<p>We are still looking for:</p>\n<p>A second executive assistant -- preferably someone who lives in the SF bay area or is willing to relocate here, but remote work will also be considered. &nbsp;<a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dFlvMWVCS0FhNHpKQTQtQWJEc3J5dmc6MQ\">Apply here</a>.</p>\n<p>Teachers / curriculum designers. &nbsp;This *does* need to be someone who can relocate to the SF bay area, and who has the legal ability to work in the US. &nbsp;<a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHFUZlpJbTFMdUE3LXZ4RXdJM1BlQ3c6MQ\">Apply here</a>. &nbsp;Especially apply if:</p>\n<ul>\n<li>Rationality, or similar changes in your skill set, have made a big difference in your life;</li>\n<li>You enjoy teaching, and helping others change their lives; you have strong interpersonal skills;</li>\n<li>You have exceptional analytic skills, and want to help us figure out what sort of \"rationality\" and \"rationality training\" can <em>actually</em> work -- by being skeptical, trying things out, measuring outcomes, etc.</li>\n</ul>\n<p>Distant curriculum designers: as above, except that you don't need the interpersonal/teaching skills, and do need to be extra-exceptional in other respects. &nbsp;<a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHFUZlpJbTFMdUE3LXZ4RXdJM1BlQ3c6MQ\">Apply here</a>.</p>\n<p>Programmers -- folks who can whip up simple prototype web apps quickly, to help with rationality training. &nbsp;<a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dGMweDU2UjVFQ3kxbWFJcDBfVTlfS2c6MA\">Apply here</a>.</p>\n<p>Consultants -- folks who have relevant experience, and can spend a few hours offering suggestions for how to structure our workshops, or for how to structure rationality group more generally (after watching us teach, or by giving advice over the phone). &nbsp;&nbsp;If you've run successful workshops for adults before, of any sort (e.g., on italian cooking), consider applying to help us organize our program. &nbsp;<a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZUZzRHQ0RFZjhmWWoxSTQ0MnBXc0E6MQ\">Apply here</a>.</p>\n<p>If you live in the SF bay area, you are also very welcome to come on a Satuday and help us test out draft lessons (by being a participant as we present them): email stephenpcole at gmail dot com to be added to that email list.</p>\n<p>Do err on the side of applying; hope to hear from you soon!</p>\n<p>(These application forms take the place of the previous ones; but if you've applied with the previous one, you're still golden, I'm just a bit behind on processing the applications.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pBmmvSZ5WijjKh59x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 8.835525359753825e-07, "legacy": true, "legacyId": "15174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-13T23:16:05.609Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh - Presentation on Anthropics", "slug": "meetup-pittsburgh-presentation-on-anthropics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thejash", "createdAt": "2010-04-19T03:42:34.750Z", "isAdmin": false, "displayName": "thejash"}, "userId": "8niRZTZ3F3riATFbY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5nEckvAYT3CHZ55He/meetup-pittsburgh-presentation-on-anthropics", "pageUrlRelative": "/posts/5nEckvAYT3CHZ55He/meetup-pittsburgh-presentation-on-anthropics", "linkUrl": "https://www.lesswrong.com/posts/5nEckvAYT3CHZ55He/meetup-pittsburgh-presentation-on-anthropics", "postedAtFormatted": "Friday, April 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20-%20Presentation%20on%20Anthropics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20-%20Presentation%20on%20Anthropics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nEckvAYT3CHZ55He%2Fmeetup-pittsburgh-presentation-on-anthropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20-%20Presentation%20on%20Anthropics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nEckvAYT3CHZ55He%2Fmeetup-pittsburgh-presentation-on-anthropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nEckvAYT3CHZ55He%2Fmeetup-pittsburgh-presentation-on-anthropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8z'>Pittsburgh - Presentation on Anthropics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 April 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">151 N Craig St, Pittsburgh, PA 15213.  Apt #7C.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Katja Grace will be giving a presentation on anthropics, a topic on\nwhich she is quite a bit more knowledgeable than me.  Afterwards,\nwe'll go get some dinner.</p>\n\n<p>Also--this will be my last LW meeting in Pittsburgh because I'm moving\nto Mountain View, so you'd better all come and say goodbye, or at\nleast hello if you still haven't come to one of these yet  :)</p>\n\n<p>See you all soon!</p>\n\n<p>PS:  the door code is  *0511, or call 585 506 6900 when you arrive.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8z'>Pittsburgh - Presentation on Anthropics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5nEckvAYT3CHZ55He", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.836234284716219e-07, "legacy": true, "legacyId": "15175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh___Presentation_on_Anthropics\">Discussion article for the meetup : <a href=\"/meetups/8z\">Pittsburgh - Presentation on Anthropics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 April 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">151 N Craig St, Pittsburgh, PA 15213.  Apt #7C.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Katja Grace will be giving a presentation on anthropics, a topic on\nwhich she is quite a bit more knowledgeable than me.  Afterwards,\nwe'll go get some dinner.</p>\n\n<p>Also--this will be my last LW meeting in Pittsburgh because I'm moving\nto Mountain View, so you'd better all come and say goodbye, or at\nleast hello if you still haven't come to one of these yet  :)</p>\n\n<p>See you all soon!</p>\n\n<p>PS:  the door code is  *0511, or call 585 506 6900 when you arrive.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh___Presentation_on_Anthropics1\">Discussion article for the meetup : <a href=\"/meetups/8z\">Pittsburgh - Presentation on Anthropics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh - Presentation on Anthropics", "anchor": "Discussion_article_for_the_meetup___Pittsburgh___Presentation_on_Anthropics", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh - Presentation on Anthropics", "anchor": "Discussion_article_for_the_meetup___Pittsburgh___Presentation_on_Anthropics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T05:38:16.340Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The So-Called Heisenberg Uncertainty Principle", "slug": "seq-rerun-the-so-called-heisenberg-uncertainty-principle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7LqCKsyzZAzbsb59u/seq-rerun-the-so-called-heisenberg-uncertainty-principle", "pageUrlRelative": "/posts/7LqCKsyzZAzbsb59u/seq-rerun-the-so-called-heisenberg-uncertainty-principle", "linkUrl": "https://www.lesswrong.com/posts/7LqCKsyzZAzbsb59u/seq-rerun-the-so-called-heisenberg-uncertainty-principle", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20So-Called%20Heisenberg%20Uncertainty%20Principle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20So-Called%20Heisenberg%20Uncertainty%20Principle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7LqCKsyzZAzbsb59u%2Fseq-rerun-the-so-called-heisenberg-uncertainty-principle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20So-Called%20Heisenberg%20Uncertainty%20Principle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7LqCKsyzZAzbsb59u%2Fseq-rerun-the-so-called-heisenberg-uncertainty-principle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7LqCKsyzZAzbsb59u%2Fseq-rerun-the-so-called-heisenberg-uncertainty-principle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 363, "htmlBody": "<p>Today's post, <a href=\"/lw/pq/the_socalled_heisenberg_uncertainty_principle/\">The So-Called Heisenberg Uncertainty Principle</a> was originally published on 23 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Unlike classical physics, in quantum physics it is not possible to separate out a particle's \"position\" from its \"momentum\". The evolution of the amplitude distribution over time, involves things like taking the second derivative in space and multiplying by i to get the first derivative in time. The end result of this time evolution rule is that blobs of particle-presence appear to race around in physical space. The notion of \"an exact particular momentum\" or \"an exact particular position\" is not something that can physically happen, it is a tool for analyzing amplitude distributions by taking them apart into a sum of simpler waves. This uses the assumption and fact of linearity: the evolution of the whole wavefunction seems to always be the additive sum of the evolution of its pieces. Using this tool, we can see that if you take apart the same distribution into a sum of positions and a sum of momenta, they cannot both be sharply concentrated at the same time. When you \"observe\" a particle's position, that is, decohere its positional distribution by making it interact with a sensor, you take its wave packet apart into two pieces; then the two pieces evolve differently. The Heisenberg Principle definitely does not say that knowing about the particle, or consciously seeing it, will make the universe behave differently.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bp1/seq_rerun_decoherence/\">Decoherence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7LqCKsyzZAzbsb59u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 8.837847446413206e-07, "legacy": true, "legacyId": "15197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eWuuznxeebcjWpdnH", "wfapWkByumW63Pter", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T17:12:38.412Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Lauderdale", "slug": "meetup-fort-lauderdale-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ya3nscpCpP7L43A2o/meetup-fort-lauderdale-0", "pageUrlRelative": "/posts/ya3nscpCpP7L43A2o/meetup-fort-lauderdale-0", "linkUrl": "https://www.lesswrong.com/posts/ya3nscpCpP7L43A2o/meetup-fort-lauderdale-0", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Lauderdale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Lauderdale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fya3nscpCpP7L43A2o%2Fmeetup-fort-lauderdale-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Lauderdale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fya3nscpCpP7L43A2o%2Fmeetup-fort-lauderdale-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fya3nscpCpP7L43A2o%2Fmeetup-fort-lauderdale-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/90'>Fort Lauderdale</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 April 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Starbucks (2026 North Flamingo Road, Pembroke Pines, FL 33027)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual, during this meeting we will be bringing articles of interest to discuss. We are very informal and all newcomers are encouraged to join in, even if they can't stay the whole time. We will usually have gone out for food as a group around 8pm, but typically reconvene afterward unofficially for a while longer. For more information, contact Lance Bush at cfibroward@gmail.com.</p>\n\n<p>Location info and directions are here: <a href=\"http://g.co/maps/5ktrv\" rel=\"nofollow\">http://g.co/maps/5ktrv</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/90'>Fort Lauderdale</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ya3nscpCpP7L43A2o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.840779682093794e-07, "legacy": true, "legacyId": "15210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Lauderdale\">Discussion article for the meetup : <a href=\"/meetups/90\">Fort Lauderdale</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 April 2012 06:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Starbucks (2026 North Flamingo Road, Pembroke Pines, FL 33027)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual, during this meeting we will be bringing articles of interest to discuss. We are very informal and all newcomers are encouraged to join in, even if they can't stay the whole time. We will usually have gone out for food as a group around 8pm, but typically reconvene afterward unofficially for a while longer. For more information, contact Lance Bush at cfibroward@gmail.com.</p>\n\n<p>Location info and directions are here: <a href=\"http://g.co/maps/5ktrv\" rel=\"nofollow\">http://g.co/maps/5ktrv</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Lauderdale1\">Discussion article for the meetup : <a href=\"/meetups/90\">Fort Lauderdale</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Lauderdale", "anchor": "Discussion_article_for_the_meetup___Fort_Lauderdale", "level": 1}, {"title": "Discussion article for the meetup : Fort Lauderdale", "anchor": "Discussion_article_for_the_meetup___Fort_Lauderdale1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T19:07:27.409Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Lauderdale", "slug": "meetup-fort-lauderdale", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:20.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gabraham", "createdAt": "2012-04-14T02:19:13.154Z", "isAdmin": false, "displayName": "gabraham"}, "userId": "FNFEjswRCEXMhKLD4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G466fudpEpB9NmxqQ/meetup-fort-lauderdale", "pageUrlRelative": "/posts/G466fudpEpB9NmxqQ/meetup-fort-lauderdale", "linkUrl": "https://www.lesswrong.com/posts/G466fudpEpB9NmxqQ/meetup-fort-lauderdale", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Lauderdale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Lauderdale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG466fudpEpB9NmxqQ%2Fmeetup-fort-lauderdale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Lauderdale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG466fudpEpB9NmxqQ%2Fmeetup-fort-lauderdale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG466fudpEpB9NmxqQ%2Fmeetup-fort-lauderdale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/91'>Fort Lauderdale</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 April 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Starbucks (2026 North Flamingo Road, Pembroke Pines, FL 33027)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual, during this meeting we will be bringing articles of interest to discuss. We are very informal and all newcomers are encouraged to join in, even if they can't stay the whole time. We will usually have gone out for food as a group around 8pm, but typically reconvene afterward unofficially for a while longer. For more information, contact Lance Bush at cfibroward@gmail.com.</p>\n\n<p>Additional specific articles and topics may be brought in by Lance to help seed discussion. Details will be published soon.</p>\n\n<p>Location info and directions are here: <a href=\"http://g.co/maps/5ktrv\" rel=\"nofollow\">http://g.co/maps/5ktrv</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/91'>Fort Lauderdale</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G466fudpEpB9NmxqQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.841264705445412e-07, "legacy": true, "legacyId": "15211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Lauderdale\">Discussion article for the meetup : <a href=\"/meetups/91\">Fort Lauderdale</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 April 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Starbucks (2026 North Flamingo Road, Pembroke Pines, FL 33027)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual, during this meeting we will be bringing articles of interest to discuss. We are very informal and all newcomers are encouraged to join in, even if they can't stay the whole time. We will usually have gone out for food as a group around 8pm, but typically reconvene afterward unofficially for a while longer. For more information, contact Lance Bush at cfibroward@gmail.com.</p>\n\n<p>Additional specific articles and topics may be brought in by Lance to help seed discussion. Details will be published soon.</p>\n\n<p>Location info and directions are here: <a href=\"http://g.co/maps/5ktrv\" rel=\"nofollow\">http://g.co/maps/5ktrv</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Lauderdale1\">Discussion article for the meetup : <a href=\"/meetups/91\">Fort Lauderdale</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Lauderdale", "anchor": "Discussion_article_for_the_meetup___Fort_Lauderdale", "level": 1}, {"title": "Discussion article for the meetup : Fort Lauderdale", "anchor": "Discussion_article_for_the_meetup___Fort_Lauderdale1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T19:34:38.716Z", "modifiedAt": null, "url": null, "title": "Decision Theories: A Semi-Formal Analysis, Part III", "slug": "decision-theories-a-semi-formal-analysis-part-iii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:26.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AMwzjjvFxEgxvL7xe/decision-theories-a-semi-formal-analysis-part-iii", "pageUrlRelative": "/posts/AMwzjjvFxEgxvL7xe/decision-theories-a-semi-formal-analysis-part-iii", "linkUrl": "https://www.lesswrong.com/posts/AMwzjjvFxEgxvL7xe/decision-theories-a-semi-formal-analysis-part-iii", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20III&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20III%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAMwzjjvFxEgxvL7xe%2Fdecision-theories-a-semi-formal-analysis-part-iii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20III%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAMwzjjvFxEgxvL7xe%2Fdecision-theories-a-semi-formal-analysis-part-iii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAMwzjjvFxEgxvL7xe%2Fdecision-theories-a-semi-formal-analysis-part-iii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2716, "htmlBody": "<h3>Or: Formalizing Timeless Decision Theory<br /></h3>\n<p><strong>Previously:</strong></p>\n<p>0. <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_self\">Decision Theories: A Less Wrong Primer</a><br />1. <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i\" target=\"_self\">The Problem with Naive Decision Theory</a><br />2. <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\" target=\"_self\">Causal Decision Theory and Substitution</a></p>\n<p><span style=\"color: #ff0000;\"><strong>WARNING:<span style=\"color: #000000;\"> The main result of this post, as it's written here, is flawed. I <a href=\"/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/\">at first</a> thought it was a fatal flaw, but <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">later</a> found a fix. I'm going to try and repair this post, either by including the tricky bits, or by handwaving and pointing you to the actual proofs if you're curious. Carry on!<br /></span></strong></span></p>\n<p><strong>Summary of Post: </strong><em>Have you ever wanted to know how (and whether) Timeless Decision Theory works? Using the framework from the last two posts, this post shows you explicitly how TDT can be implemented in the context of our tournament, what it does, how it strictly beats CDT on fair problems, and a bit about why this is a Big Deal. But you're seriously going to want to read the previous posts in the sequence before this one.<br /></em></p>\n<p>We've reached the frontier of decision theories, and we're ready at last to write algorithms that achieve mutual cooperation in Prisoner's Dilemma (without risk of being defected on, and without giving up the ability to defect against players who always cooperate)! After two substantial preparatory posts, it feels like <a href=\"http://en.wikipedia.org/wiki/GLaDOS\" target=\"_blank\">it's been a long time</a>, hasn't it?</p>\n<p><img src=\"http://images.lesswrong.com/t3_b7w_0.png?v=49819f549bbd3644586826b5ed4fecac\" alt=\"\" width=\"640\" height=\"400\" /></p>\n<p>But look at me, here, talking when there's Science to do...</p>\n<p><a id=\"more\"></a>Once again, we're programming an agent <strong>X</strong> to face another agent <strong>Y</strong> in a game <strong>G</strong>, where <strong>X</strong> and <strong>Y</strong> have access to each others' source codes and the source code of <strong>G</strong>. (In this post, we'll sweep the \"player 1, player 2\" bit back into the definition of <strong>G</strong> for simplicity in our code.) We may assume that at least one of the <em>x<sub>i</sub></em> is a Nash equilibrium, since <strong>X</strong> and <strong>Y</strong> can treat a mixed equilibrium like an extra option and define its expected payoffs as usual.</p>\n<h3>Not Quite Timeless Decision Theory<br /></h3>\n<p>Here's our first shot at TDT, based on the idea of substituting different things for the source code of <strong>X</strong> in our prediction of <strong>Y</strong>...</p>\n<p>def <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>For each <em>x<sub>i</sub></em>: \n<ul>\n<li>Write a very simple agent <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> that always outputs <em>x<sub>i</sub></em> in game <strong>G</strong>.</li>\n<li>Try to deduce output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>).</li>\n<li>If this succeeds, calculate <strong>U</strong>(<em>x<sub>i</sub></em>, output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>)); call this <em>u<sub>i</sub></em>.</li>\n</ul>\n</li>\n<li>If <em>u<sub>i</sub></em> has been deduced for all <em>x<sub>i</sub></em>, return the <em>x<sub>i</sub></em> for which <em>u<sub>i</sub></em> is largest.</li>\n<li>Otherwise, return the default Nash equilibrium.</li>\n</ul>\n<p>To see what this algorithm does, let's try it on Newcomb's Problem <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/#newcomb\" target=\"_blank\">as we formalized it last time</a>; we'll let <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span> denote the agent that always one-boxes and <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span> denote the agent that always two-boxes. If <strong>P</strong> has a valid and sufficiently powerful inference module, the output of <strong>P</strong>(code <strong>N</strong>, code <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span>) is 1 and the output of <strong>P</strong>(code <strong>N</strong>, code <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span>) is 2. Why? <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span> and <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span> are very simple bots, so <strong>P</strong> will be able to deduce their outputs, and we know what it does in that case.</p>\n<p>And then, if <strong>X</strong> also has a valid and sufficiently powerful inference module, <strong>X</strong> will deduce those outputs for <strong>P</strong>. Thus <strong>X</strong> will deduce that <em>u<sub>1</sub></em>=100 and <em>u<sub>2</sub></em>=1, so <strong>X</strong> will indeed one-box; and if <strong>P</strong> has a sufficiently powerful inference module, it will therefore predict that <strong>X</strong> one-boxes, and fill the box, so that <strong>X</strong> gets payoff 100!</p>\n<p>Again, what we're doing here is not counterfactual reasoning, but instead reasoning about what your opponent would do against some <em>other</em> simpler agent; therefore we're again evading the problem of spurious counterfactuals. (On a philosophical level, this decision theory finds the most successful constant strategy <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> and mimics its output.)</p>\n<p>Alright, I said there were going to be two problems with this attempt, and indeed I called it Not Quite Timeless Decision Theory. So what goes wrong?</p>\n<h3>Problem: NQTDT won't cooperate against itself!</h3>\n<p>If you face two NQTDT agents against each other in the Prisoner's Dilemma, what happens? First, note that NQTDT will defect against either a CooperateBot (<span style=\"color: #000000;\"><strong>A<sub>C</sub></strong></span>) or a DefectBot (<span style=\"color: #000000;\"><strong>A<sub>D</sub></strong></span>) in the Prisoner's Dilemma. If <strong>X</strong> and <strong>Y</strong> are both NQTDT, then <strong>X</strong> deduces that <strong>Y</strong> defects against both <span style=\"color: #000000;\"><strong>A<sub>C</sub></strong></span> and <span style=\"color: #000000;\"><strong>A<sub>D</sub></strong></span>, therefore <strong>X</strong> calculates <em>u<sub>C</sub></em>=0 and <em>u<sub>D</sub></em>=1, so <strong>X</strong> defects. (And of course <strong>Y</strong> does the same.) Drat!</p>\n<p>And the second problem?</p>\n<h3>Problem: NQTDT might one-box and <em>not</em> be rewarded!</h3>\n<p>In Newcomb's Problem, what if <strong>P</strong> has a powerful enough inference module to show that <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span> one-boxes and <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span> two-boxes, but not one powerful enough to figure out what <strong>X</strong> is doing? (After all, <strong>X</strong> is a bit more complicated than the <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> are.) By the reasoning above, <strong>X</strong> will one-box, but <strong>P</strong> won't fill the box&mdash;the worst of all possible outcomes!</p>\n<p>So NQTDT is broken. What can we do?</p>\n<h3>Idea 5: Sanity Check</h3>\n<p>Well, instead of assuming that our opponent <strong>Y</strong> will behave in the real game as it did against the optimal <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>, we could try to deduce <strong>Y</strong>'s actual behavior against <strong>X</strong> as a \"sanity check\" before we make our final decision. That is,</p>\n<p>def <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>For each <em>x<sub>i</sub></em>: \n<ul>\n<li>Write a very simple agent <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> that always outputs <em>x<sub>i</sub></em>.</li>\n<li>Try to deduce output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>).</li>\n<li>If this succeeds, calculate <strong>U</strong>(<em>x<sub>i</sub></em>, output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>)); call this <em>u<sub>i</sub></em>.</li>\n</ul>\n</li>\n<li>If <em>u<sub>i</sub></em> has been deduced for all <em>x<sub>i</sub></em>, take the <em>x<sub>i</sub></em> for which <em>u<sub>i</sub></em> is largest, and try to deduce \"<strong>U</strong>(<em>x<sub>i</sub></em>, output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>)) &ge; <em>u<sub>i</sub></em>\".</li>\n<li>If <em>that</em> succeeds, <em>then</em> return <em>x<sub>i</sub></em>.</li>\n<li>Else, return the default Nash equilibrium.</li>\n</ul>\n<p>Note that the new line checks the behavior of <strong>Y</strong> against <strong>X</strong> itself, not the <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>. This \"sanity check\" ensures that, when <strong>X</strong> imitates the most successful of the <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>, it achieves at least the payoff that <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> does. Indeed, if our program does this then it's again un-exploitable <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/#noexploit\" target=\"_blank\">in the same sense as CDT</a>, and it still one-boxes on Newcomb's Problem (if and only if it deduces that <strong>P</strong> is sufficiently powerful to deduce this fact), so it's no longer broken in the above sense.</p>\n<p>But by the same argument as before, it still defects on the Prisoner's Dilemma against itself, so this is still not TDT.</p>\n<p>What else could we do better? Well, this agent \"mimics\" only the very simplest of strategies- the strategies that correspond to one-player games for its opponent. What if we instead let it mimic more complicated (but still basic) strategies, like versions of Newcomb's Predictor?</p>\n<h3>Idea 6: Mimic something a bit more clever<br /></h3>\n<p>This is going to be a bit more complicated than the last one; we first need to define the agents that we'll examine and mimic. Consider each function <em>f</em> from the set {<em>y<sub>1</sub></em>, . . . , <em>y<sub>m</sub></em>} to the set {<em>x<sub>1</sub></em>, . . . , <em>x<sub>n</sub></em>} (there are <em>n<sup>m</sup></em> such functions), and define <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> as follows:</p>\n<p>def <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>Try and deduce the output of <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>).</li>\n<li>If this succeeds, return <em>f</em>(output <strong>Y</strong>). Else, return the Nash equilibrium.</li>\n</ul>\n<p>Note that the constant strategies <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> are special cases of the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, corresponding to the constant functions; note also that Newcomb's Predictor is an example of an <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> that's not simply a constant strategy. Now we'll write a decision theory that looks for the most successful of the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, applies a sanity check, and mimics it. This results in an instance of...</p>\n<h3>Timeless Decision Theory</h3>\n<p>def <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>For each <em>f</em>: \n<ul>\n<li>Write the agent <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>.</li>\n<li>Try to deduce output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>).</li>\n<li>If this succeeds, calculate <strong>U</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>)); call this <em>u<sub>f</sub></em>.</li>\n</ul>\n</li>\n<li>If <em>u<sub>f</sub></em> has been deduced for all <em>f</em>, take the <em>f</em> for which <em>u<sub>f</sub></em> is largest, and try to deduce \"<strong>U</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>)) &ge; <em>u<sub>f</sub></em>\".</li>\n<li>If that succeeds, then return (output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>).</li>\n<li>Otherwise, return the default Nash equilibrium.</li>\n</ul>\n<p>Let's try and parse what this algorithm does. It considers all possible Newcomblike agents (the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>) and tests out how they fare against <strong>Y</strong>. Then it takes the best result and does a sanity check: will <strong>Y</strong> actually respond against <strong>X</strong> as it does against this particular <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>? And if that checks out, then <strong>X</strong> goes ahead and outputs what <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> does.<a href=\"#bargain\" target=\"_self\"><sup>0</sup></a></p>\n<p>We can sketch out some results that this decision theory gets (when equipped with a sufficiently powerful inference module). First, it correctly solves one-player games (since one of the constant strategies <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> is the optimum) and zero-sum two-player games (for the same reasons that CDT does). It one-boxes on Newcomb whenever <strong>P</strong> is a powerful enough predictor (since the best of the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> are all one-boxers). And finally, it cooperates with itself on a simple Prisoner's Dilemma!</p>\n<p>Why? If <strong>X</strong> and <strong>Y</strong> are both running this decision theory, consider the function <em>f</em> with <em>f</em>(C)=C and <em>f</em>(D)=D, it's clear that <strong>Y</strong> cooperates with this <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> (by the same logic that makes it one-box on Newcomb's Problem) and that no <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> achieves a better result against <strong>Y</strong> than mutual cooperation. Thus, <strong>X</strong> will run the sanity check for this <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, which is equivalent to trying to deduce that <strong>Y</strong> cooperates against <strong>X</strong>, while <strong>Y</strong> attempts to do the same. Here comes the delicate argument from mathematical logic; we'll phrase things in terms of logical provability, but the analogous arguments hold for other (valid and sufficiently powerful) kinds of inference modules:</p>\n<p><a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\" target=\"_blank\">L&ouml;b's Theorem</a> states that, within a formal system, proving \"if there's a proof of <em>S</em>, then <em>S</em> actually holds\" leads to a proof of <em>S</em> itself. In our current context, let <em>S</em> be the statement \"<strong>U</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>)) &ge; <em>u<sub>f</sub></em>\" that appears in <strong>X</strong>'s sanity test, and <em>T</em> be the corresponding statement \"<strong>V</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>)) &ge; <em>u<sub>f</sub></em>\" that appears in <strong>Y</strong>'s sanity-test. If there were a proof of <em>S</em>, then cooperation passes <strong>X</strong>'s sanity test<a href=\"#sanity\" target=\"_self\"><sup>1</sup></a>, so that output <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>))=C. But this directly implies the statement <em>T</em>, so cooperation passes <strong>Y</strong>'s sanity test, so that output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>))=C, which implies <em>S</em> itself. Thus, by L&ouml;b's Theorem, there must be a proof of <em>S</em>, and therefore <strong>X</strong> and <strong>Y</strong> do in fact cooperate! (I do realize how confusing this process of logical bootstrapping is&mdash;it's helpful to think of it as a useful kind of self-fulfilling prophecy.)</p>\n<p>This algorithm is a general decision theory rather than something with ad-hoc patches, so it satisfies <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/#dobetter\" target=\"_blank\">my criteria for a better decision theory</a>. It also behaves like Timeless Decision Theory is supposed to on the classical problems. Is it therefore justified to call this TDT? Well, let's again think about it on the philosophical level.</p>\n<p>Each Newcomblike agent <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> represents a particular deal that it tries to strike with <strong>Y</strong> (if I deduce you're doing <em>this</em>, then I'll do <em>that</em>). <strong>X</strong> looks to see which deals <strong>Y</strong> will accept, and offers <strong>Y</strong> the most favorable (to <strong>X</strong>) of those deals, and if <strong>Y</strong> \"accepts\" (passes the sanity check) then <strong>X</strong> fulfills its end of the bargain. Since some deals are better than the best Nash equilibrium, this gets better results than CDT in many games, while still being un-exploitable. The novelty (compared to what traditional game theorists already know) is the fact that the communication of the deal and its acceptance all happen via prediction of each other rather than via exchange of messages, which allows for deals to happen even when the agents are unable to explicitly bargain or guarantee their fulfilment of their end by external precommitments</p>\n<p>That's a pretty good account of what TDT does, no? Also, I realized that I had the right description when I recalled Eliezer's description of when TDT cooperates on the Prisoner's Dilemma: if and only if the opponent would one-box with the TDT agent as Omega. This is <em>exactly</em> what this algorithm ends up doing.</p>\n<ol> </ol>\n<p>It's fair to say that Yudkowsky's invention of Timeless Decision Theory was a triumph. In fact, I'll make a note here...</p>\n<h1>TDT: HUGE SUCCESS<br /></h1>\n<p>Before we move on, we should take a little while to remark on how awesome this is.</p>\n<p>Since the Prisoner's Dilemma was first formulated, it's been held up as a dark and dismal portent for the prospects of cooperation among rational people. Douglas Hofstadter felt, on a philosophical level, that there <em>must</em> be a way to see cooperation as the rational choice, and called it <a href=\"http://en.wikipedia.org/wiki/Superrationality\" target=\"_blank\">superrationality</a>; but his experiments with actual people were discouraging, and he could find no solid theoretical underpinning for that intuition, so game theorists continued to ignore it.</p>\n<p>The insight of TDT (as well as the other decision theories we might term \"reflexive\") is that better results can be achieved when you have more information than the original setup allows; in particular, if it's common knowledge that agents have some ability to predict each others' thought process, then mutual cooperation really can arise out of rational self-interest.</p>\n<p>Although it's difficult to make the right conditions obtain between individual humans (as in Hofstadter's experiments), it may be possible for a TDT-friendly setup to exist between different organizations, or different countries, if they have means of keeping tabs on each others' strategic planning. This may in fact have saved the world from nuclear destruction...</p>\n<p>John von Neumann, the founder of game theory, advocated a pre-emptive nuclear strike on the Soviet Union throughout the 1950s; he was convinced that there was no other possibility of avoiding nuclear destruction ourselves. But here we have a good analogue to our setup; rather than exchanging source code, both governments had spies placed in the other, and so were likely to know what the other was thinking (albeit with some delay). This might have led to unconscious use of TDT thinking and thus spared us from nuclear war, despite the fact that the professional rationalists of the day hadn't conceived of such means to mutual cooperation!<a href=\"#schelling\" target=\"_self\"><sup>2</sup></a></p>\n<p>But this sequence isn't done yet: TDT may surpass CDT, but it's still under-defined when it comes to bargaining, and there's one notable problem in which TDT falls short of common sense...</p>\n<p>TO BE CONTINUED (WITH CAKE) IN PART IV</p>\n<h3>Notes</h3>\n<p><a name=\"bargain\"></a> <strong>0.</strong> As with CDT, there are multiple other variants to consider; this one insists on the best possible deal that <strong>Y</strong> would accept against an <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, or else no deal at all. The downside is that two such agents will fail to reach a deal if they have different \"favorite\" deals, even if each would be willing to compromise against an <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>. We'll say more about this in the context of bargaining games later.</p>\n<p>Also, it's very important that we have the sanity check here, since otherwise <strong>Y</strong> could defect against <strong>X</strong> and have <strong>X</strong> cooperate (as long as Y gets Newcomb's Problem right). Without the sanity check, other agents take <strong>X</strong>'s lunch money, and rightly so!</p>\n<p><strong><a name=\"sanity\"></a>1.</strong> Existence of a proof of S is not enough to conclude that X's sanity test succeeds, since provability is not decidable and so the condition for failing the check doesn't imply absence of the proof. One way out is to <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle\">have a provability oracle</a> perform the sanity check, which makes it impossible for players to be algorithms. Another is to <a href=\"/lw/2ip/ai_cooperation_in_practice\">prove some</a> <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal\">bounds</a> on the length of the shortest proof of S, and write the algorithm in a way that guarantees the sanity check to succeed within these bounds. Yet another way is to <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits\">never give up</a>, work on the sanity check forever, which makes the agent completely unable to cope with many situations. (Thanks to Vladimir Nesov for this footnote.)</p>\n<p><a name=\"schelling\"></a><strong>2.</strong> Thomas Schelling, among others, worked out a plausibly stable logic of mutually assured destruction, but it still invited all sorts of instability when limited conflict was involved. He also praised the idea of encouraging spies in certain departments rather than prosecuting them, in order to keep the Soviet Union from worrying that we were planning a first strike. If you've never read <a href=\"http://www.hup.harvard.edu/catalog.php?isbn=9780674840317\" target=\"_blank\">The Strategy of Conflict</a>, I highly recommend it; it is simultaneously one of the most enlightening and most engaging books I've read in recent years.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AMwzjjvFxEgxvL7xe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 35, "extendedScore": null, "score": 8.841377625353148e-07, "legacy": true, "legacyId": "14540", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Or__Formalizing_Timeless_Decision_Theory\">Or: Formalizing Timeless Decision Theory<br></h3>\n<p><strong id=\"Previously_\">Previously:</strong></p>\n<p>0. <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_self\">Decision Theories: A Less Wrong Primer</a><br>1. <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i\" target=\"_self\">The Problem with Naive Decision Theory</a><br>2. <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\" target=\"_self\">Causal Decision Theory and Substitution</a></p>\n<p><span style=\"color: #ff0000;\"><strong>WARNING:<span style=\"color: #000000;\"> The main result of this post, as it's written here, is flawed. I <a href=\"/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/\">at first</a> thought it was a fatal flaw, but <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">later</a> found a fix. I'm going to try and repair this post, either by including the tricky bits, or by handwaving and pointing you to the actual proofs if you're curious. Carry on!<br></span></strong></span></p>\n<p><strong>Summary of Post: </strong><em>Have you ever wanted to know how (and whether) Timeless Decision Theory works? Using the framework from the last two posts, this post shows you explicitly how TDT can be implemented in the context of our tournament, what it does, how it strictly beats CDT on fair problems, and a bit about why this is a Big Deal. But you're seriously going to want to read the previous posts in the sequence before this one.<br></em></p>\n<p>We've reached the frontier of decision theories, and we're ready at last to write algorithms that achieve mutual cooperation in Prisoner's Dilemma (without risk of being defected on, and without giving up the ability to defect against players who always cooperate)! After two substantial preparatory posts, it feels like <a href=\"http://en.wikipedia.org/wiki/GLaDOS\" target=\"_blank\">it's been a long time</a>, hasn't it?</p>\n<p><img src=\"http://images.lesswrong.com/t3_b7w_0.png?v=49819f549bbd3644586826b5ed4fecac\" alt=\"\" width=\"640\" height=\"400\"></p>\n<p>But look at me, here, talking when there's Science to do...</p>\n<p><a id=\"more\"></a>Once again, we're programming an agent <strong>X</strong> to face another agent <strong>Y</strong> in a game <strong>G</strong>, where <strong>X</strong> and <strong>Y</strong> have access to each others' source codes and the source code of <strong>G</strong>. (In this post, we'll sweep the \"player 1, player 2\" bit back into the definition of <strong>G</strong> for simplicity in our code.) We may assume that at least one of the <em>x<sub>i</sub></em> is a Nash equilibrium, since <strong>X</strong> and <strong>Y</strong> can treat a mixed equilibrium like an extra option and define its expected payoffs as usual.</p>\n<h3 id=\"Not_Quite_Timeless_Decision_Theory\">Not Quite Timeless Decision Theory<br></h3>\n<p>Here's our first shot at TDT, based on the idea of substituting different things for the source code of <strong>X</strong> in our prediction of <strong>Y</strong>...</p>\n<p>def <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>For each <em>x<sub>i</sub></em>: \n<ul>\n<li>Write a very simple agent <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> that always outputs <em>x<sub>i</sub></em> in game <strong>G</strong>.</li>\n<li>Try to deduce output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>).</li>\n<li>If this succeeds, calculate <strong>U</strong>(<em>x<sub>i</sub></em>, output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>)); call this <em>u<sub>i</sub></em>.</li>\n</ul>\n</li>\n<li>If <em>u<sub>i</sub></em> has been deduced for all <em>x<sub>i</sub></em>, return the <em>x<sub>i</sub></em> for which <em>u<sub>i</sub></em> is largest.</li>\n<li>Otherwise, return the default Nash equilibrium.</li>\n</ul>\n<p>To see what this algorithm does, let's try it on Newcomb's Problem <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/#newcomb\" target=\"_blank\">as we formalized it last time</a>; we'll let <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span> denote the agent that always one-boxes and <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span> denote the agent that always two-boxes. If <strong>P</strong> has a valid and sufficiently powerful inference module, the output of <strong>P</strong>(code <strong>N</strong>, code <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span>) is 1 and the output of <strong>P</strong>(code <strong>N</strong>, code <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span>) is 2. Why? <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span> and <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span> are very simple bots, so <strong>P</strong> will be able to deduce their outputs, and we know what it does in that case.</p>\n<p>And then, if <strong>X</strong> also has a valid and sufficiently powerful inference module, <strong>X</strong> will deduce those outputs for <strong>P</strong>. Thus <strong>X</strong> will deduce that <em>u<sub>1</sub></em>=100 and <em>u<sub>2</sub></em>=1, so <strong>X</strong> will indeed one-box; and if <strong>P</strong> has a sufficiently powerful inference module, it will therefore predict that <strong>X</strong> one-boxes, and fill the box, so that <strong>X</strong> gets payoff 100!</p>\n<p>Again, what we're doing here is not counterfactual reasoning, but instead reasoning about what your opponent would do against some <em>other</em> simpler agent; therefore we're again evading the problem of spurious counterfactuals. (On a philosophical level, this decision theory finds the most successful constant strategy <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> and mimics its output.)</p>\n<p>Alright, I said there were going to be two problems with this attempt, and indeed I called it Not Quite Timeless Decision Theory. So what goes wrong?</p>\n<h3 id=\"Problem__NQTDT_won_t_cooperate_against_itself_\">Problem: NQTDT won't cooperate against itself!</h3>\n<p>If you face two NQTDT agents against each other in the Prisoner's Dilemma, what happens? First, note that NQTDT will defect against either a CooperateBot (<span style=\"color: #000000;\"><strong>A<sub>C</sub></strong></span>) or a DefectBot (<span style=\"color: #000000;\"><strong>A<sub>D</sub></strong></span>) in the Prisoner's Dilemma. If <strong>X</strong> and <strong>Y</strong> are both NQTDT, then <strong>X</strong> deduces that <strong>Y</strong> defects against both <span style=\"color: #000000;\"><strong>A<sub>C</sub></strong></span> and <span style=\"color: #000000;\"><strong>A<sub>D</sub></strong></span>, therefore <strong>X</strong> calculates <em>u<sub>C</sub></em>=0 and <em>u<sub>D</sub></em>=1, so <strong>X</strong> defects. (And of course <strong>Y</strong> does the same.) Drat!</p>\n<p>And the second problem?</p>\n<h3 id=\"Problem__NQTDT_might_one_box_and_not_be_rewarded_\">Problem: NQTDT might one-box and <em>not</em> be rewarded!</h3>\n<p>In Newcomb's Problem, what if <strong>P</strong> has a powerful enough inference module to show that <span style=\"color: #000000;\"><strong>A<sub>1</sub></strong></span> one-boxes and <span style=\"color: #000000;\"><strong>A<sub>2</sub></strong></span> two-boxes, but not one powerful enough to figure out what <strong>X</strong> is doing? (After all, <strong>X</strong> is a bit more complicated than the <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> are.) By the reasoning above, <strong>X</strong> will one-box, but <strong>P</strong> won't fill the box\u2014the worst of all possible outcomes!</p>\n<p>So NQTDT is broken. What can we do?</p>\n<h3 id=\"Idea_5__Sanity_Check\">Idea 5: Sanity Check</h3>\n<p>Well, instead of assuming that our opponent <strong>Y</strong> will behave in the real game as it did against the optimal <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>, we could try to deduce <strong>Y</strong>'s actual behavior against <strong>X</strong> as a \"sanity check\" before we make our final decision. That is,</p>\n<p>def <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>For each <em>x<sub>i</sub></em>: \n<ul>\n<li>Write a very simple agent <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> that always outputs <em>x<sub>i</sub></em>.</li>\n<li>Try to deduce output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>).</li>\n<li>If this succeeds, calculate <strong>U</strong>(<em>x<sub>i</sub></em>, output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>)); call this <em>u<sub>i</sub></em>.</li>\n</ul>\n</li>\n<li>If <em>u<sub>i</sub></em> has been deduced for all <em>x<sub>i</sub></em>, take the <em>x<sub>i</sub></em> for which <em>u<sub>i</sub></em> is largest, and try to deduce \"<strong>U</strong>(<em>x<sub>i</sub></em>, output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>)) \u2265 <em>u<sub>i</sub></em>\".</li>\n<li>If <em>that</em> succeeds, <em>then</em> return <em>x<sub>i</sub></em>.</li>\n<li>Else, return the default Nash equilibrium.</li>\n</ul>\n<p>Note that the new line checks the behavior of <strong>Y</strong> against <strong>X</strong> itself, not the <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>. This \"sanity check\" ensures that, when <strong>X</strong> imitates the most successful of the <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span>, it achieves at least the payoff that <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> does. Indeed, if our program does this then it's again un-exploitable <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/#noexploit\" target=\"_blank\">in the same sense as CDT</a>, and it still one-boxes on Newcomb's Problem (if and only if it deduces that <strong>P</strong> is sufficiently powerful to deduce this fact), so it's no longer broken in the above sense.</p>\n<p>But by the same argument as before, it still defects on the Prisoner's Dilemma against itself, so this is still not TDT.</p>\n<p>What else could we do better? Well, this agent \"mimics\" only the very simplest of strategies- the strategies that correspond to one-player games for its opponent. What if we instead let it mimic more complicated (but still basic) strategies, like versions of Newcomb's Predictor?</p>\n<h3 id=\"Idea_6__Mimic_something_a_bit_more_clever\">Idea 6: Mimic something a bit more clever<br></h3>\n<p>This is going to be a bit more complicated than the last one; we first need to define the agents that we'll examine and mimic. Consider each function <em>f</em> from the set {<em>y<sub>1</sub></em>, . . . , <em>y<sub>m</sub></em>} to the set {<em>x<sub>1</sub></em>, . . . , <em>x<sub>n</sub></em>} (there are <em>n<sup>m</sup></em> such functions), and define <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> as follows:</p>\n<p>def <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>Try and deduce the output of <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>).</li>\n<li>If this succeeds, return <em>f</em>(output <strong>Y</strong>). Else, return the Nash equilibrium.</li>\n</ul>\n<p>Note that the constant strategies <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> are special cases of the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, corresponding to the constant functions; note also that Newcomb's Predictor is an example of an <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> that's not simply a constant strategy. Now we'll write a decision theory that looks for the most successful of the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, applies a sanity check, and mimics it. This results in an instance of...</p>\n<h3 id=\"Timeless_Decision_Theory\">Timeless Decision Theory</h3>\n<p>def <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>):</p>\n<ul>\n<li>For each <em>f</em>: \n<ul>\n<li>Write the agent <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>.</li>\n<li>Try to deduce output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>).</li>\n<li>If this succeeds, calculate <strong>U</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>Y</strong>(code <strong>G</strong>, code <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>)); call this <em>u<sub>f</sub></em>.</li>\n</ul>\n</li>\n<li>If <em>u<sub>f</sub></em> has been deduced for all <em>f</em>, take the <em>f</em> for which <em>u<sub>f</sub></em> is largest, and try to deduce \"<strong>U</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>)) \u2265 <em>u<sub>f</sub></em>\".</li>\n<li>If that succeeds, then return (output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>).</li>\n<li>Otherwise, return the default Nash equilibrium.</li>\n</ul>\n<p>Let's try and parse what this algorithm does. It considers all possible Newcomblike agents (the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>) and tests out how they fare against <strong>Y</strong>. Then it takes the best result and does a sanity check: will <strong>Y</strong> actually respond against <strong>X</strong> as it does against this particular <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>? And if that checks out, then <strong>X</strong> goes ahead and outputs what <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> does.<a href=\"#bargain\" target=\"_self\"><sup>0</sup></a></p>\n<p>We can sketch out some results that this decision theory gets (when equipped with a sufficiently powerful inference module). First, it correctly solves one-player games (since one of the constant strategies <span style=\"color: #000000;\"><strong>A<sub>i</sub></strong></span> is the optimum) and zero-sum two-player games (for the same reasons that CDT does). It one-boxes on Newcomb whenever <strong>P</strong> is a powerful enough predictor (since the best of the <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> are all one-boxers). And finally, it cooperates with itself on a simple Prisoner's Dilemma!</p>\n<p>Why? If <strong>X</strong> and <strong>Y</strong> are both running this decision theory, consider the function <em>f</em> with <em>f</em>(C)=C and <em>f</em>(D)=D, it's clear that <strong>Y</strong> cooperates with this <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> (by the same logic that makes it one-box on Newcomb's Problem) and that no <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> achieves a better result against <strong>Y</strong> than mutual cooperation. Thus, <strong>X</strong> will run the sanity check for this <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, which is equivalent to trying to deduce that <strong>Y</strong> cooperates against <strong>X</strong>, while <strong>Y</strong> attempts to do the same. Here comes the delicate argument from mathematical logic; we'll phrase things in terms of logical provability, but the analogous arguments hold for other (valid and sufficiently powerful) kinds of inference modules:</p>\n<p><a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\" target=\"_blank\">L\u00f6b's Theorem</a> states that, within a formal system, proving \"if there's a proof of <em>S</em>, then <em>S</em> actually holds\" leads to a proof of <em>S</em> itself. In our current context, let <em>S</em> be the statement \"<strong>U</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>)) \u2265 <em>u<sub>f</sub></em>\" that appears in <strong>X</strong>'s sanity test, and <em>T</em> be the corresponding statement \"<strong>V</strong>(output <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, output <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>)) \u2265 <em>u<sub>f</sub></em>\" that appears in <strong>Y</strong>'s sanity-test. If there were a proof of <em>S</em>, then cooperation passes <strong>X</strong>'s sanity test<a href=\"#sanity\" target=\"_self\"><sup>1</sup></a>, so that output <strong>X</strong>(code <strong>G</strong>, code <strong>Y</strong>))=C. But this directly implies the statement <em>T</em>, so cooperation passes <strong>Y</strong>'s sanity test, so that output <strong>Y</strong>(code <strong>G</strong>, code <strong>X</strong>))=C, which implies <em>S</em> itself. Thus, by L\u00f6b's Theorem, there must be a proof of <em>S</em>, and therefore <strong>X</strong> and <strong>Y</strong> do in fact cooperate! (I do realize how confusing this process of logical bootstrapping is\u2014it's helpful to think of it as a useful kind of self-fulfilling prophecy.)</p>\n<p>This algorithm is a general decision theory rather than something with ad-hoc patches, so it satisfies <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/#dobetter\" target=\"_blank\">my criteria for a better decision theory</a>. It also behaves like Timeless Decision Theory is supposed to on the classical problems. Is it therefore justified to call this TDT? Well, let's again think about it on the philosophical level.</p>\n<p>Each Newcomblike agent <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span> represents a particular deal that it tries to strike with <strong>Y</strong> (if I deduce you're doing <em>this</em>, then I'll do <em>that</em>). <strong>X</strong> looks to see which deals <strong>Y</strong> will accept, and offers <strong>Y</strong> the most favorable (to <strong>X</strong>) of those deals, and if <strong>Y</strong> \"accepts\" (passes the sanity check) then <strong>X</strong> fulfills its end of the bargain. Since some deals are better than the best Nash equilibrium, this gets better results than CDT in many games, while still being un-exploitable. The novelty (compared to what traditional game theorists already know) is the fact that the communication of the deal and its acceptance all happen via prediction of each other rather than via exchange of messages, which allows for deals to happen even when the agents are unable to explicitly bargain or guarantee their fulfilment of their end by external precommitments</p>\n<p>That's a pretty good account of what TDT does, no? Also, I realized that I had the right description when I recalled Eliezer's description of when TDT cooperates on the Prisoner's Dilemma: if and only if the opponent would one-box with the TDT agent as Omega. This is <em>exactly</em> what this algorithm ends up doing.</p>\n<ol> </ol>\n<p>It's fair to say that Yudkowsky's invention of Timeless Decision Theory was a triumph. In fact, I'll make a note here...</p>\n<h1 id=\"TDT__HUGE_SUCCESS\">TDT: HUGE SUCCESS<br></h1>\n<p>Before we move on, we should take a little while to remark on how awesome this is.</p>\n<p>Since the Prisoner's Dilemma was first formulated, it's been held up as a dark and dismal portent for the prospects of cooperation among rational people. Douglas Hofstadter felt, on a philosophical level, that there <em>must</em> be a way to see cooperation as the rational choice, and called it <a href=\"http://en.wikipedia.org/wiki/Superrationality\" target=\"_blank\">superrationality</a>; but his experiments with actual people were discouraging, and he could find no solid theoretical underpinning for that intuition, so game theorists continued to ignore it.</p>\n<p>The insight of TDT (as well as the other decision theories we might term \"reflexive\") is that better results can be achieved when you have more information than the original setup allows; in particular, if it's common knowledge that agents have some ability to predict each others' thought process, then mutual cooperation really can arise out of rational self-interest.</p>\n<p>Although it's difficult to make the right conditions obtain between individual humans (as in Hofstadter's experiments), it may be possible for a TDT-friendly setup to exist between different organizations, or different countries, if they have means of keeping tabs on each others' strategic planning. This may in fact have saved the world from nuclear destruction...</p>\n<p>John von Neumann, the founder of game theory, advocated a pre-emptive nuclear strike on the Soviet Union throughout the 1950s; he was convinced that there was no other possibility of avoiding nuclear destruction ourselves. But here we have a good analogue to our setup; rather than exchanging source code, both governments had spies placed in the other, and so were likely to know what the other was thinking (albeit with some delay). This might have led to unconscious use of TDT thinking and thus spared us from nuclear war, despite the fact that the professional rationalists of the day hadn't conceived of such means to mutual cooperation!<a href=\"#schelling\" target=\"_self\"><sup>2</sup></a></p>\n<p>But this sequence isn't done yet: TDT may surpass CDT, but it's still under-defined when it comes to bargaining, and there's one notable problem in which TDT falls short of common sense...</p>\n<p>TO BE CONTINUED (WITH CAKE) IN PART IV</p>\n<h3 id=\"Notes\">Notes</h3>\n<p><a name=\"bargain\"></a> <strong>0.</strong> As with CDT, there are multiple other variants to consider; this one insists on the best possible deal that <strong>Y</strong> would accept against an <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>, or else no deal at all. The downside is that two such agents will fail to reach a deal if they have different \"favorite\" deals, even if each would be willing to compromise against an <span style=\"color: #000000;\"><strong>A<em><sub>f</sub></em></strong></span>. We'll say more about this in the context of bargaining games later.</p>\n<p>Also, it's very important that we have the sanity check here, since otherwise <strong>Y</strong> could defect against <strong>X</strong> and have <strong>X</strong> cooperate (as long as Y gets Newcomb's Problem right). Without the sanity check, other agents take <strong>X</strong>'s lunch money, and rightly so!</p>\n<p><strong><a name=\"sanity\"></a>1.</strong> Existence of a proof of S is not enough to conclude that X's sanity test succeeds, since provability is not decidable and so the condition for failing the check doesn't imply absence of the proof. One way out is to <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle\">have a provability oracle</a> perform the sanity check, which makes it impossible for players to be algorithms. Another is to <a href=\"/lw/2ip/ai_cooperation_in_practice\">prove some</a> <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal\">bounds</a> on the length of the shortest proof of S, and write the algorithm in a way that guarantees the sanity check to succeed within these bounds. Yet another way is to <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits\">never give up</a>, work on the sanity check forever, which makes the agent completely unable to cope with many situations. (Thanks to Vladimir Nesov for this footnote.)</p>\n<p><a name=\"schelling\"></a><strong>2.</strong> Thomas Schelling, among others, worked out a plausibly stable logic of mutually assured destruction, but it still invited all sorts of instability when limited conflict was involved. He also praised the idea of encouraging spies in certain departments rather than prosecuting them, in order to keep the Soviet Union from worrying that we were planning a first strike. If you've never read <a href=\"http://www.hup.harvard.edu/catalog.php?isbn=9780674840317\" target=\"_blank\">The Strategy of Conflict</a>, I highly recommend it; it is simultaneously one of the most enlightening and most engaging books I've read in recent years.</p>", "sections": [{"title": "Or: Formalizing Timeless Decision Theory", "anchor": "Or__Formalizing_Timeless_Decision_Theory", "level": 2}, {"title": "Previously:", "anchor": "Previously_", "level": 3}, {"title": "Not Quite Timeless Decision Theory", "anchor": "Not_Quite_Timeless_Decision_Theory", "level": 2}, {"title": "Problem: NQTDT won't cooperate against itself!", "anchor": "Problem__NQTDT_won_t_cooperate_against_itself_", "level": 2}, {"title": "Problem: NQTDT might one-box and not be rewarded!", "anchor": "Problem__NQTDT_might_one_box_and_not_be_rewarded_", "level": 2}, {"title": "Idea 5: Sanity Check", "anchor": "Idea_5__Sanity_Check", "level": 2}, {"title": "Idea 6: Mimic something a bit more clever", "anchor": "Idea_6__Mimic_something_a_bit_more_clever", "level": 2}, {"title": "Timeless Decision Theory", "anchor": "Timeless_Decision_Theory", "level": 2}, {"title": "TDT: HUGE SUCCESS", "anchor": "TDT__HUGE_SUCCESS", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "54 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["af9MjBqF2hgu3EN6r", "2JdvZw3CXzafxQugN", "TxDcvtn2teAMobG2Z", "ShD7EHb4HmPgfveim", "X9vT3o3MmtWoRRKkm", "ALCnqX6Xx8bpFMZq3", "Bj244uWzDBXvE2N2S", "TNfx89dh5KkcKrvho", "W6T93dSSm2xvHn9X6", "m39dkp73YhN9QKYb9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T20:59:18.027Z", "modifiedAt": null, "url": null, "title": "Are Magical Categories Relatively Simple?", "slug": "are-magical-categories-relatively-simple", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:01.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jacobt", "createdAt": "2012-01-14T23:52:09.935Z", "isAdmin": false, "displayName": "jacobt"}, "userId": "RjJuBbFJbfd8yyDGb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Aqo4ZB2neBWvGESFj/are-magical-categories-relatively-simple", "pageUrlRelative": "/posts/Aqo4ZB2neBWvGESFj/are-magical-categories-relatively-simple", "linkUrl": "https://www.lesswrong.com/posts/Aqo4ZB2neBWvGESFj/are-magical-categories-relatively-simple", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20Magical%20Categories%20Relatively%20Simple%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20Magical%20Categories%20Relatively%20Simple%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqo4ZB2neBWvGESFj%2Fare-magical-categories-relatively-simple%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20Magical%20Categories%20Relatively%20Simple%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqo4ZB2neBWvGESFj%2Fare-magical-categories-relatively-simple", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqo4ZB2neBWvGESFj%2Fare-magical-categories-relatively-simple", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1284, "htmlBody": "<p>In <a href=\"/lw/td/magical_categories/\">Magical Categories</a>, Eliezer criticizes using machine learning to learn the concept of \"smile\" from examples.  \"Smile\" sounds simple to humans but is actually a very complex concept.  It only seems simple to us because we find it useful.</p>\n<p>If we saw pictures of smiling people on the left and other things on the right, we would realize that smiling people go to the left and categorize new things accordingly.  A supervised machine learning algorithm, on the other hand, will likely learn something other than what we think of as \"smile\" (such as \"containing things that pass the smiley face recognizer\") and categorize molecular smiley faces as smiles.</p>\n<p>This is because simplicity is subjective: a human will consider \"happy\" and \"person\" to be basic concepts, so the intended definition of smile as \"expression of a happy person\" is simple.  A computational <a href=\"/lw/jp/occams_razor/\">Occam's Razor</a> will consider this correct definition to be a more complex concept than \"containing things that pass the smiley face recognizer\".  I'll use the phrase \"magical category\" to refer to concepts that have a high Kolmogorov complexity but that people find simple.</p>\n<p>I hope that it's possible to create conditions under which the computer will have an inductive bias towards magical categories, as humans do.  I think that people find these concepts simple because they're useful to explain things that humans want to explain (such as interactions with people or media depicting people).  The video has pixels arranged in this pattern because it depicts a person who is happy because he is eating chocolate.</p>\n<p>So, maybe it's possible to learn these magical categories from a lot of data, by compressing the categorizer along with the data.  Here's a sketch of a procedure for doing this:</p>\n<ol>\n<li>\n<p>Amass a large collection of data from various societies, containing photographs, text, historical records, etc.</p>\n</li>\n<li>\n<p>Come up with many categories (say, one for each noun in a long list).  For each category, decide which pieces of data fit the category.</p>\n</li>\n<li>\n<p>Find categorizer_1, categorizer_2, ..., categorizer_n to minimize K(dataset + categorizer_1 + categorizer_2 + ... + categorizer_n)</p>\n</li>\n</ol>\n<p>What do these mean:</p>\n<ul>\n<li>K(x) is the <a href=\"/lw/vh/complexity_and_intelligence/\">Kolmogorov complexity</a> of x; that is, the length of the shortest (program,input) pair that, when run, produces x.  This is uncomputable so it has to be approximated (such as through resource-bounded data compression).</li>\n<li>+ denotes string concatenation.  There should be some separator so the boundaries between strings are clear.</li>\n<li>dataset is the collection of data</li>\n<li>\n<p>categorizer_k is a program that returns \"true\" or \"false\" depending on whether the input fits category #k</p>\n</li>\n<li>\n<p>When learning a new category, find new_categorizer to minimize K(dataset + categorizer_1 + categorizer_2 + ... + categorizer_n + new_categorizer) while still matching the given examples.</p>\n</li>\n</ul>\n<p>Note that while in this example we learn categorizers, in general it should be possible to learn arbitrary functions including probabilistic functions.</p>\n<p>The fact that the categorizers are compressed along with the dataset will create a bias towards categorizers that use concepts useful in compressing the dataset and categorizing other things.  From looking at enough data, the concept of \"person\" naturally arises (in the form of a recognizer/generative model/etc), and it will be used both to compress the dataset and to recognize the \"person\" category.  In effect, because the \"person\" concept is useful for compressing the dataset, it will be cheap/simple to use in categorizers (such as to recognize real smiling faces).</p>\n<p>A useful concept here is \"relative complexity\" (I don't know the standard name for this), defined as K(x|y) = K(x + y) - K(y).  Intuitively this is how complex x is if you already understand y.  The categorizer should be trusted in inverse proportion to its relative complexity K(categorizer | dataset and other categorizers); more complex (relative to the data) categorizers are more arbitrary, even given concepts useful for understanding the dataset, and so they're more likely to be wrong on new data.</p>\n<p>If we can use this setup to learn \"magical\" categories, then Friendly AI becomes much easier.  CEV requires the magical concepts \"person\" and \"volition\" to be plugged in.  So do all seriously proposed complete moral systems.  I see no way of doing Friendly AI without having some representation of these magical categories, either provided by humans or learned from data.  It should be possible to learn deontological concepts such as \"obligation\" or \"right\", and also consequentialist concepts such as \"volition\" or \"value\".  Some of these are 2-place predicates so they're categories over pairs.  Then we can ask new questions such as \"Do I have a right to do x in y situation?\"  All of this depends on whether the relevant concepts have low complexity relative to the dataset and other categorizers.</p>\n<p>Using this framework for Friendly AI has many problems.  I'm hand-waving the part about how to actually compress the data (approximating Kolmogorov complexity).  This is a difficult problem but luckily it's not specific to Friendly AI.  Another problem is that it's hard to go from categorizing data to actually making decisions.  This requires connecting the categorizer to some kind of ontology.  The categorization question that we can actually give examples for would be something like \"given this description of the situation, is this action good?\".  Somehow we have to provide examples of (description,action) pairs that are good or not good, and the AI has to come up with a description of the situation before deciding whether the action is good or not.  I don't think that using exactly this framework to make Friendly AI is a good idea; my goal here is to argue that sufficiently advanced machine learning can learn magical categories.</p>\n<p>If it is in fact possible to learn magical categories, this suggests that machine learning research (especially related to approximations of Solomonoff induction/Kolmogorov complexity) is even more necessary for Friendly AI than it is for unFriendly AI.  I think that the main difficulty of Friendly AI as compared with unFriendly AI is the requirement of understanding magical concepts/categories.  Other problems (induction, optimization, self-modification, ontology, etc.) are also difficult but luckily they're almost as difficult for paperclip maximizers as they are for Friendly AI.</p>\n<p>This has a relationship to the orthogonality thesis.  Almost everyone here would agree with a weak form of the orthogonality thesis: that there exist general optimizers AI programs to which you can plug in any goal (such as paperclip maximization).  A stronger form of the orthogonality thesis asserts that all ways of making an AI can be easily reduced to specifying its goals and optimization separately; that is, K(AI) ~= K(arbitrary optimizer) + K(goals).  My thesis here (that magical categories are simpler relative to data) suggests that the strong form is false.  Concepts such as \"person\" and \"value\" have important epistemic/instrumental value and can also be used to create goals, so K(Friendly AI) &lt; K(arbitrary optimizer) + K(Friendliness goal).  There's really no problem with human values being inherently complex if they're not complex relative to data we can provide to the AI or information it will create on its own for instrumental purposes.  Perhaps P(Friendly AI | AGI, passes some Friendliness tests) isn't actually so low even if the program is randomly generated (though I don't actually suggest taking this approach!).</p>\n<p>I'm personally working on a programming language for writing and verifying generative models (proving lower bounds on P(data|model)).  Perhaps something like this could be used to compress data and categories in order to learn magical categories.  If we can robustly learn some magical categories even with current levels of hardware/software, that would be strong evidence for the possibility of creating Friendly AI using this approach, and evidence against the molecular smiley face scenario.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb25c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Aqo4ZB2neBWvGESFj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 8.841737215196053e-07, "legacy": true, "legacyId": "15212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PoDAyQMWEXBBBEJ5P", "f4txACqDWithRi7hs", "rELc88PvDkhetQzqx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T21:08:24.464Z", "modifiedAt": null, "url": null, "title": "Our Phyg Is Not Exclusive Enough", "slug": "our-phyg-is-not-exclusive-enough", "viewCount": null, "lastCommentedAt": "2018-01-23T12:53:50.810Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hxGEKxaHZEKT4fpms/our-phyg-is-not-exclusive-enough", "pageUrlRelative": "/posts/hxGEKxaHZEKT4fpms/our-phyg-is-not-exclusive-enough", "linkUrl": "https://www.lesswrong.com/posts/hxGEKxaHZEKT4fpms/our-phyg-is-not-exclusive-enough", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Our%20Phyg%20Is%20Not%20Exclusive%20Enough&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOur%20Phyg%20Is%20Not%20Exclusive%20Enough%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxGEKxaHZEKT4fpms%2Four-phyg-is-not-exclusive-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Our%20Phyg%20Is%20Not%20Exclusive%20Enough%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxGEKxaHZEKT4fpms%2Four-phyg-is-not-exclusive-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxGEKxaHZEKT4fpms%2Four-phyg-is-not-exclusive-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 793, "htmlBody": "<p>EDIT: Thanks to people not wanting certain words google-associated with LW: <a href=\"http://rot13.com\">Phyg</a></p>\n<p>Lesswrong has the best signal/noise ratio I know of. This is great. This is why I come here. It's nice to talk about interesting rationality-related topics without people going off the rails about <a title=\"politics is the mind killer\" href=\"/lw/gw/politics_is_the_mindkiller\">politics</a>/<a title=\"map and territory\" href=\"http://wiki.lesswrong.com/wiki/Map_and_Territory_(sequence)\">fail</a> <a title=\"mysterious answers\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">philosophy</a>/<a title=\"metaethics\" href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">fail</a> <a title=\"ethical injunctions\" href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction\">ethics</a>/<a title=\"37 ways that words can be wrong\" href=\"/lw/od/37_ways_that_words_can_be_wrong/\">definitions</a>/etc. This seems to be possible because a good number of us have read the lesswrong material (sequences, etc) which innoculate us against that kind of noise.</p>\n<p>Of course Lesswrong is not perfect; there is still noise. Interestingly, most of it is from people who have not read some sequence and thereby make the default mistakes or don't address the community's best understanding of the topic. We are pretty good about downvoting and/or correcting posts that fail at the core sequences, which is good. However, there are other sequences, too, many of them critically important to not failing at <a title=\"metaethics\" href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">metaethics</a>/<a title=\"Coming of Age\" href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">thinking</a> <a title=\"lawful intelligence\" href=\"http://wiki.lesswrong.com/wiki/Lawful_intelligence\">about</a> <a title=\"fake utility functions\" href=\"/lw/lq/fake_utility_functions/\">AI</a>/etc.</p>\n<p>I'm sure you can think of some examples of what I mean. People saying things that you thought were utterly dissolved in some post or sequence, but they don't address that, and no one really calls them out. I could dig up a bunch of quotes but I don't want to single anyone out or make this about any particular point, so I'm leaving it up to your imagination/memory.</p>\n<p>It's actually kindof frustrating seeing people make these mistakes. You could say that if I think someone needs to be told about the existence of some sequence they should have read before posting, I ought to tell them, but that's actually not what I want to do with my time here. I want to spend my time reading and participating in informed discussion.&nbsp;A lot of us do end up engaging mistaken posts, but that lowers the quality of discussion here because so much time and space has been spent battling ignorance instead of advancing knowledge and dicussing real problems.</p>\n<p>It's worse than just \"oh here's some more junk I have to ignore or downvote\", because the path of least resistance ends up being \"ignore any discussion that contains contradictions of the lesswrong scriptures\", which is obviously bad. There <em>are </em>people who have read the sequences and know the state of the arguments and still have some intelligent critique, but it's quite hard to tell the difference between that and someone explaining for the millionth time the problem with \"but won't the AI know what's right better than humans?\". So I just ignore it all and miss a lot of good stuff.</p>\n<p>Right now, the only stuff I can be resonably guaranteed is intelligent, informed, and interesting is the promoted posts. Everything else is a minefield. I'd like there to be something similar for discussion/comments. Some way of knowing \"these people I'm talking to know what they are talking about\" without having to dig around in their user history or whatever. I'm not proposing a particular solution here, just saying I'd like there to be more high quality discussion between more properly sequenced LWers.</p>\n<p>There is a lot of worry on this site about whether we are too exclusive or too phygish or too harsh in our expectation that people be well-read, which I think is misplaced. It is important that modern rationality have a welcoming public face and somewhere that people can discuss without having read three years worth of daily blog posts, but at the same time I find myself looking at the <a title=\"sl4 intro\" href=\"http://sl4.org/intro.html\">moderation policy of the old sl4 mailing list</a>&nbsp;and thinking \"damn, I wish we were more like that\". A hard-ass moderator righteously wielding the banhammer against cruft is a good thing and I enjoy it where I find it. Perhaps these things (the public face and the exclusive discussion) should be separated?</p>\n<p>I've recently seen someone saying that no-one complains about the signal/noise ratio on LW, and therefore we should relax a bit. I've also seen a good deal of complaints about our phygish exclusivity, the politics ban, the \"talk to me when you read the sequences\" attitude, and so on. I'd just like to say that I like these things, and I am complaining about the signal/noise ratio on LW.</p>\n<p>Lest anyone get the idea that no-one thinks LW should be more phygish or more exclusive, let me hereby register that I for one would like us to all enforce a little more strongly that people read the sequences and even&nbsp;<a href=\"/lw/3h/why_our_kind_cant_cooperate/\">agree with them in a horrifying manner</a>.&nbsp;You don't have to agree with me, but I'd just like to put out there as a matter of fact that there are some of us that would like a more exclusive LW.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "oraLTPkETL5xKmhx3": 1, "5f5c37ee1b5cdee568cfb345": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hxGEKxaHZEKT4fpms", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 38, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "15213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 515, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "FaJaCgqBKphrDzDSj", "NnohDYHNnKDtbiMyp", "7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T21:40:36.225Z", "modifiedAt": null, "url": null, "title": "Should the front page include the phrase \"latest insights\"?", "slug": "should-the-front-page-include-the-phrase-latest-insights", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mark_Eichenlaub", "createdAt": "2010-09-01T17:59:32.486Z", "isAdmin": false, "displayName": "Mark_Eichenlaub"}, "userId": "6mdGZLDekk4835gM6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6jynbRWpP6rzsRGXp/should-the-front-page-include-the-phrase-latest-insights", "pageUrlRelative": "/posts/6jynbRWpP6rzsRGXp/should-the-front-page-include-the-phrase-latest-insights", "linkUrl": "https://www.lesswrong.com/posts/6jynbRWpP6rzsRGXp/should-the-front-page-include-the-phrase-latest-insights", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20the%20front%20page%20include%20the%20phrase%20%22latest%20insights%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20the%20front%20page%20include%20the%20phrase%20%22latest%20insights%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jynbRWpP6rzsRGXp%2Fshould-the-front-page-include-the-phrase-latest-insights%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20the%20front%20page%20include%20the%20phrase%20%22latest%20insights%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jynbRWpP6rzsRGXp%2Fshould-the-front-page-include-the-phrase-latest-insights", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jynbRWpP6rzsRGXp%2Fshould-the-front-page-include-the-phrase-latest-insights", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>The front page claims</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&lt;blockquote&gt;We use the latest insights from&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Cognitive_science\">cognitive science</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">,&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Social_psychology\">social psychology</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">,</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Bayesian_probability\">probability theory</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">, and&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&nbsp;to improve our understanding of&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">how the world works</em><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&nbsp;and&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">what we can do to achieve our goals</em><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">.&lt;/blockquote&gt;</span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">I find the use of the phrase \"latest insights\" to be a minor turn-off. It reminds me of [</span><a href=\"http://www.guardian.co.uk/science/the-lay-scientist/2010/sep/24/1\">http://www.guardian.co.uk/science/the-lay-scientist/2010/sep/24/1</a>](<span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">crappy news stories) over-hyping and misrepresenting individual papers in diet, health, psychology, and similar fields where individual papers are likely to be wrong, and more likely to be wrong the more remarkable their findings.</span></p>\n<p>&nbsp;</p>\n<p>LessWrong is cognizant of the problems with scientific publishing, and we take a different approach towards discussing science than most blogs and mass media outlets do. We focus on higher-level summaries of the scientific knowledge in a field when discussing research, especially in psychology. I think we could convey the community's goals better by dropping the word \"latest\".</p>\n<p>&nbsp;</p>\n<p>This point seems to line up with [<a href=\"/lw/atm/cult_impressions_of_less_wrongsi/6134\">http://lesswrong.com/lw/atm/cult_impressions_of_less_wrongsi/6134</a>](this comment) by Gabriel, but there was no further discussion there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6jynbRWpP6rzsRGXp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "15214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T21:58:42.962Z", "modifiedAt": null, "url": null, "title": "Should \"latest insights\" appear in the front page blurb?", "slug": "should-latest-insights-appear-in-the-front-page-blurb", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:02.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mark_Eichenlaub", "createdAt": "2010-09-01T17:59:32.486Z", "isAdmin": false, "displayName": "Mark_Eichenlaub"}, "userId": "6mdGZLDekk4835gM6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mjEbe6uj5sAwCXKAT/should-latest-insights-appear-in-the-front-page-blurb", "pageUrlRelative": "/posts/mjEbe6uj5sAwCXKAT/should-latest-insights-appear-in-the-front-page-blurb", "linkUrl": "https://www.lesswrong.com/posts/mjEbe6uj5sAwCXKAT/should-latest-insights-appear-in-the-front-page-blurb", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20%22latest%20insights%22%20appear%20in%20the%20front%20page%20blurb%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20%22latest%20insights%22%20appear%20in%20the%20front%20page%20blurb%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjEbe6uj5sAwCXKAT%2Fshould-latest-insights-appear-in-the-front-page-blurb%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20%22latest%20insights%22%20appear%20in%20the%20front%20page%20blurb%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjEbe6uj5sAwCXKAT%2Fshould-latest-insights-appear-in-the-front-page-blurb", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjEbe6uj5sAwCXKAT%2Fshould-latest-insights-appear-in-the-front-page-blurb", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>The front page states</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&nbsp;We use the latest insights from&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Cognitive_science\">cognitive science</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">,&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Social_psychology\">social psychology</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">,</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Bayesian_probability\">probability theory</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">, and&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision theory</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&nbsp;to improve our understanding of&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">how the world works</em><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&nbsp;and&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">what we can do to achieve our goals</em><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">.</span></p>\n</blockquote>\n<p>The use of \"latest\" is a minor turn-off for me. It reminds me of blogs' and mass-media news' uncritical approach to recent papers in health and psychology. It makes me think that LessWrong will be about grabbing a paper, summarizing its result, and explaining how to apply it in daily life.</p>\n<p>LessWrong is cognizant of the problems in scientific publishing. We know that any individual paper is likely to be wrong - more so if the conclusions are highly unexpected. (The more sensational the headline, the less likely it is to be true).</p>\n<p>LessWrong usually focuses on higher-level summaries when discussing scientific findings, especially in psychology. A typical top-level science post has lots of references, many to review articles and meta-analyses. That's not the impression I get from the front page, though, so I think it we could communicate the community's goals better by dropping the word \"latest\".</p>\n<p>This point is similar to one made by Gabriel in <a href=\"/lw/atm/cult_impressions_of_less_wrongsi/6134\">this comment</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mjEbe6uj5sAwCXKAT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 38, "extendedScore": null, "score": 8.841988248487265e-07, "legacy": true, "legacyId": "15215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-14T22:31:56.604Z", "modifiedAt": null, "url": null, "title": "Suggestions needed: good articles for a meetup discussion", "slug": "suggestions-needed-good-articles-for-a-meetup-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:04.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AShepard", "createdAt": "2009-12-22T18:27:50.792Z", "isAdmin": false, "displayName": "AShepard"}, "userId": "SxCHDrBhMCCdwMJrK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W7edpkF7k5CNGW3jA/suggestions-needed-good-articles-for-a-meetup-discussion", "pageUrlRelative": "/posts/W7edpkF7k5CNGW3jA/suggestions-needed-good-articles-for-a-meetup-discussion", "linkUrl": "https://www.lesswrong.com/posts/W7edpkF7k5CNGW3jA/suggestions-needed-good-articles-for-a-meetup-discussion", "postedAtFormatted": "Saturday, April 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestions%20needed%3A%20good%20articles%20for%20a%20meetup%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestions%20needed%3A%20good%20articles%20for%20a%20meetup%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7edpkF7k5CNGW3jA%2Fsuggestions-needed-good-articles-for-a-meetup-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestions%20needed%3A%20good%20articles%20for%20a%20meetup%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7edpkF7k5CNGW3jA%2Fsuggestions-needed-good-articles-for-a-meetup-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7edpkF7k5CNGW3jA%2Fsuggestions-needed-good-articles-for-a-meetup-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>The Chicago LW meetup group is looking to add a bit more structure to our discussions, which have been rather freeform to this point. What are some good articles that we could read in advance and then discuss at the meetup?</p>\n<p>Some criteria that I think a good article would have:</p>\n<p>\n<ul>\n<li>LW-related topic&nbsp;</li>\n<li>Relatively brief, so we will actually read it beforehand (a typical sequence post is probably a good target length)</li>\n<li>Able to support/spark enough good discussion to be a centerpiece of a meetup</li>\n<li>Others?</li>\n</ul>\n<div>We haven't really done this before, so any suggestions or advice based on experiences at other meetups would be especially helpful (both specific articles that have worked well before, as well as best practices for having the best discussions).</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W7edpkF7k5CNGW3jA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.84212864087885e-07, "legacy": true, "legacyId": "15216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-15T06:59:21.557Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Which Basis Is More Fundamental?", "slug": "seq-rerun-which-basis-is-more-fundamental", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YJofGbvu78S8ceFoz/seq-rerun-which-basis-is-more-fundamental", "pageUrlRelative": "/posts/YJofGbvu78S8ceFoz/seq-rerun-which-basis-is-more-fundamental", "linkUrl": "https://www.lesswrong.com/posts/YJofGbvu78S8ceFoz/seq-rerun-which-basis-is-more-fundamental", "postedAtFormatted": "Sunday, April 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Which%20Basis%20Is%20More%20Fundamental%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Which%20Basis%20Is%20More%20Fundamental%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJofGbvu78S8ceFoz%2Fseq-rerun-which-basis-is-more-fundamental%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Which%20Basis%20Is%20More%20Fundamental%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJofGbvu78S8ceFoz%2Fseq-rerun-which-basis-is-more-fundamental", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJofGbvu78S8ceFoz%2Fseq-rerun-which-basis-is-more-fundamental", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/pr/which_basis_is_more_fundamental/\">Which Basis Is More Fundamental?</a> was originally published on 24 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The position basis can be computed locally in the configuration space; the momentum basis is not local. Why care about locality? Because it is a very deep principle; reality itself seems to favor it in some way.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bq5/seq_rerun_the_socalled_heisenberg_uncertainty/\">The So-Called Heisenberg Uncertainty Principle</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YJofGbvu78S8ceFoz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.844273067008949e-07, "legacy": true, "legacyId": "15219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XDkeuJTFjM9Y2x6v6", "7LqCKsyzZAzbsb59u", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-15T09:14:14.280Z", "modifiedAt": null, "url": null, "title": "'Thinking, Fast and Slow' Chapter Summaries / Notes [link]", "slug": "thinking-fast-and-slow-chapter-summaries-notes-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:04.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lightwave", "createdAt": "2009-03-02T00:10:45.771Z", "isAdmin": false, "displayName": "Lightwave"}, "userId": "wmf7PMjRsYvMAcQHg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HDqgr7cgMQaRnGPEA/thinking-fast-and-slow-chapter-summaries-notes-link", "pageUrlRelative": "/posts/HDqgr7cgMQaRnGPEA/thinking-fast-and-slow-chapter-summaries-notes-link", "linkUrl": "https://www.lesswrong.com/posts/HDqgr7cgMQaRnGPEA/thinking-fast-and-slow-chapter-summaries-notes-link", "postedAtFormatted": "Sunday, April 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Thinking%2C%20Fast%20and%20Slow'%20Chapter%20Summaries%20%2F%20Notes%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Thinking%2C%20Fast%20and%20Slow'%20Chapter%20Summaries%20%2F%20Notes%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHDqgr7cgMQaRnGPEA%2Fthinking-fast-and-slow-chapter-summaries-notes-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Thinking%2C%20Fast%20and%20Slow'%20Chapter%20Summaries%20%2F%20Notes%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHDqgr7cgMQaRnGPEA%2Fthinking-fast-and-slow-chapter-summaries-notes-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHDqgr7cgMQaRnGPEA%2Fthinking-fast-and-slow-chapter-summaries-notes-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<p>I recently read Kahneman's 'Thinking Fast and Slow' (actually listened to the audiobook) and I wanted to find a summary of the experiments he describes and I stumbled upon this: <a href=\"http://sivers.org/book/ThinkingFastAndSlow\" target=\"_blank\">http://sivers.org/book/ThinkingFastAndSlow</a>. It has a summary of the interesting/important points of each chapter. Most of the statements seem to be direct quotes from the book, so if you have it in an electronic format (it can easily be obtained from uh, various sources) you can search for those quotes and find the context.</p>\n<p>Bonus: Notes from Dan Ariely's <a href=\"http://sivers.org/book/PredictablyIrrational\" target=\"_blank\">Predictably Irrational</a> and also <a href=\"http://sivers.org/book/\" target=\"_blank\">many other books</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8SfkJYYMe75MwjHzN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HDqgr7cgMQaRnGPEA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "15220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-15T11:21:08.448Z", "modifiedAt": null, "url": null, "title": "Meetup : Budapest Meetup", "slug": "meetup-budapest-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pGkuD3jTixcf4NWhc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/awtvWGXtWxESEohr4/meetup-budapest-meetup", "pageUrlRelative": "/posts/awtvWGXtWxESEohr4/meetup-budapest-meetup", "linkUrl": "https://www.lesswrong.com/posts/awtvWGXtWxESEohr4/meetup-budapest-meetup", "postedAtFormatted": "Sunday, April 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Budapest%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Budapest%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FawtvWGXtWxESEohr4%2Fmeetup-budapest-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Budapest%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FawtvWGXtWxESEohr4%2Fmeetup-budapest-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FawtvWGXtWxESEohr4%2Fmeetup-budapest-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/92'>Budapest Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 April 2012 05:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1061 Budapest, Andr\u00e1ssy \u00fat 2.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>PLEASE NOTE the changed venue: Costa Coffee, Andr\u00e1ssy \u00fat 2. <a href=\"http://www.etterem.hu/10684\" rel=\"nofollow\">link</a>.</p>\n\n<p>Please come and bring friends. If you have questions, contact <a href=\"/user/katyusha\" rel=\"nofollow\">katyusha</a>.</p>\n\n<p>BudLW mailing list: <a href=\"http://groups.google.com/group/budlesswrong\">link</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/92'>Budapest Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "awtvWGXtWxESEohr4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.845379760594445e-07, "legacy": true, "legacyId": "15221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Budapest_Meetup\">Discussion article for the meetup : <a href=\"/meetups/92\">Budapest Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 April 2012 05:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1061 Budapest, Andr\u00e1ssy \u00fat 2.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>PLEASE NOTE the changed venue: Costa Coffee, Andr\u00e1ssy \u00fat 2. <a href=\"http://www.etterem.hu/10684\" rel=\"nofollow\">link</a>.</p>\n\n<p>Please come and bring friends. If you have questions, contact <a href=\"/user/katyusha\" rel=\"nofollow\">katyusha</a>.</p>\n\n<p>BudLW mailing list: <a href=\"http://groups.google.com/group/budlesswrong\">link</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Budapest_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/92\">Budapest Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Budapest Meetup", "anchor": "Discussion_article_for_the_meetup___Budapest_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Budapest Meetup", "anchor": "Discussion_article_for_the_meetup___Budapest_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-15T11:35:49.516Z", "modifiedAt": null, "url": null, "title": "More intuitive programming languages", "slug": "more-intuitive-programming-languages", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "A4FB53AC", "createdAt": "2012-03-03T14:00:04.765Z", "isAdmin": false, "displayName": "A4FB53AC"}, "userId": "doKx9d5L4mupmSErA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cMS4i6L7EkqnL4muW/more-intuitive-programming-languages", "pageUrlRelative": "/posts/cMS4i6L7EkqnL4muW/more-intuitive-programming-languages", "linkUrl": "https://www.lesswrong.com/posts/cMS4i6L7EkqnL4muW/more-intuitive-programming-languages", "postedAtFormatted": "Sunday, April 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20intuitive%20programming%20languages&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20intuitive%20programming%20languages%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMS4i6L7EkqnL4muW%2Fmore-intuitive-programming-languages%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20intuitive%20programming%20languages%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMS4i6L7EkqnL4muW%2Fmore-intuitive-programming-languages", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMS4i6L7EkqnL4muW%2Fmore-intuitive-programming-languages", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 305, "htmlBody": "<p>I'm not a programmer. I wish I were. I've tried to learn it several times, different languages, but never went very far. The most complex piece of software I ever wrote was a bulky, inefficient game of life.</p>\n<p>Recently I've been exposed to <a title=\"the idea of a visual programming language\" href=\"http://en.wikipedia.org/wiki/Subtext_%28programming_language%29\" target=\"_blank\">the idea of a visual programming language</a> <a title=\"named subtext\" href=\"http://subtextual.org/\" target=\"_blank\">named subtext</a>. The concept seemed interesting, and the potential great. In short, the assumptions and principles sustaining this language seem more natural and more powerful than those behind writing lines of codes. For instance, a program written as lines of codes is uni-dimensional, and even the best of us may find it difficult to sort that out, model the flow of instructions in your mind, how distant parts of the code interact together, etc. Here it's already more apparent because of the two-dimensional structure of the code.</p>\n<p>I don't know whether this particular project will bear fruit. But it seems to me many more people could become more interested in programming, and at least advance further before giving up, if programming languages were easier to learn and use for people who don't necessarily have the necessary mindset to be a programmer in the current paradigm.</p>\n<p>It could even benefit people who're already good at it. Any programmer may have a threshold above which the complexity of the code goes beyond their ability to manipulate or understand. I think it should be possible to push that threshold farther with such languages/frameworks, enabling the writing of more complex, yet functional pieces of software.</p>\n<p>Do you know anything about similar projects? Also, what could be done to help turn such a project into a workable programming language? Do you see obvious flaws in such an approach? If so, what could be done to repair these, or at least salvage part of this concept?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cMS4i6L7EkqnL4muW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 6, "extendedScore": null, "score": 8.845441847002919e-07, "legacy": true, "legacyId": "15222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-15T18:19:03.137Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA  First Sunday Meetup", "slug": "meetup-cambridge-ma-first-sunday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3KbGDzwAav6Gq6QYA/meetup-cambridge-ma-first-sunday-meetup", "pageUrlRelative": "/posts/3KbGDzwAav6Gq6QYA/meetup-cambridge-ma-first-sunday-meetup", "linkUrl": "https://www.lesswrong.com/posts/3KbGDzwAav6Gq6QYA/meetup-cambridge-ma-first-sunday-meetup", "postedAtFormatted": "Sunday, April 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20%20First%20Sunday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20%20First%20Sunday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KbGDzwAav6Gq6QYA%2Fmeetup-cambridge-ma-first-sunday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20%20First%20Sunday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KbGDzwAav6Gq6QYA%2Fmeetup-cambridge-ma-first-sunday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KbGDzwAav6Gq6QYA%2Fmeetup-cambridge-ma-first-sunday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/93'>Cambridge, MA  First Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 May 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number may change, subject to availability.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/93'>Cambridge, MA  First Sunday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3KbGDzwAav6Gq6QYA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.847147009218242e-07, "legacy": true, "legacyId": "15223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__First_Sunday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/93\">Cambridge, MA  First Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 May 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number may change, subject to availability.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__First_Sunday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/93\">Cambridge, MA  First Sunday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA  First Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__First_Sunday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA  First Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__First_Sunday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-15T18:20:58.960Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA Third Sunday Meetup", "slug": "meetup-cambridge-ma-third-sunday-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qmix7zy4X2hdRDRFL/meetup-cambridge-ma-third-sunday-meetup-1", "pageUrlRelative": "/posts/Qmix7zy4X2hdRDRFL/meetup-cambridge-ma-third-sunday-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/Qmix7zy4X2hdRDRFL/meetup-cambridge-ma-third-sunday-meetup-1", "postedAtFormatted": "Sunday, April 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20Third%20Sunday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20Third%20Sunday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQmix7zy4X2hdRDRFL%2Fmeetup-cambridge-ma-third-sunday-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20Third%20Sunday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQmix7zy4X2hdRDRFL%2Fmeetup-cambridge-ma-third-sunday-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQmix7zy4X2hdRDRFL%2Fmeetup-cambridge-ma-third-sunday-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/94'>Cambridge, MA Third Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 May 2012 02:20:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number may change, subject to availability.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/94'>Cambridge, MA Third Sunday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qmix7zy4X2hdRDRFL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.847155173661276e-07, "legacy": true, "legacyId": "15224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Third_Sunday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 May 2012 02:20:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number may change, subject to availability.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Third_Sunday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA Third Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Third_Sunday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA Third Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Third_Sunday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T01:36:29.860Z", "modifiedAt": null, "url": null, "title": "Meetup : S\u00e3o Paulo Meet Up 2", "slug": "meetup-sao-paulo-meet-up-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:18.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gust", "createdAt": "2011-11-24T19:56:58.641Z", "isAdmin": false, "displayName": "Gust"}, "userId": "Hpxx5CjyqzbCMFXNz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tGJg2WjNPx4W3vDqB/meetup-sao-paulo-meet-up-2", "pageUrlRelative": "/posts/tGJg2WjNPx4W3vDqB/meetup-sao-paulo-meet-up-2", "linkUrl": "https://www.lesswrong.com/posts/tGJg2WjNPx4W3vDqB/meetup-sao-paulo-meet-up-2", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGJg2WjNPx4W3vDqB%2Fmeetup-sao-paulo-meet-up-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20S%C3%A3o%20Paulo%20Meet%20Up%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGJg2WjNPx4W3vDqB%2Fmeetup-sao-paulo-meet-up-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGJg2WjNPx4W3vDqB%2Fmeetup-sao-paulo-meet-up-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/95'>S\u00e3o Paulo Meet Up 2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 April 2012 07:30:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1034 Alameda Santos, S\u00e3o Paulo, Brasil</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Brazilians, Unite!</p>\n\n<p>Our last meeting was great. Let's do that again! I hope everybody who couldn't go to the last one show up this time.</p>\n\n<p>Meeting point: Starbucks at Alameda Santos with Alameda Campinas. Later we may go somewhere and eat something.</p>\n\n<p>I'll be there at 19:30 with a copy of <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/\" rel=\"nofollow\">&quot;Good and Real&quot;</a> on the table.</p>\n\n<p>Cya!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/95'>S\u00e3o Paulo Meet Up 2</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tGJg2WjNPx4W3vDqB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.848997529317998e-07, "legacy": true, "legacyId": "15236", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_2\">Discussion article for the meetup : <a href=\"/meetups/95\">S\u00e3o Paulo Meet Up 2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 April 2012 07:30:00PM (-0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1034 Alameda Santos, S\u00e3o Paulo, Brasil</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Brazilians, Unite!</p>\n\n<p>Our last meeting was great. Let's do that again! I hope everybody who couldn't go to the last one show up this time.</p>\n\n<p>Meeting point: Starbucks at Alameda Santos with Alameda Campinas. Later we may go somewhere and eat something.</p>\n\n<p>I'll be there at 19:30 with a copy of <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/\" rel=\"nofollow\">\"Good and Real\"</a> on the table.</p>\n\n<p>Cya!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_21\">Discussion article for the meetup : <a href=\"/meetups/95\">S\u00e3o Paulo Meet Up 2</a></h2>", "sections": [{"title": "Discussion article for the meetup : S\u00e3o Paulo Meet Up 2", "anchor": "Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_2", "level": 1}, {"title": "Discussion article for the meetup : S\u00e3o Paulo Meet Up 2", "anchor": "Discussion_article_for_the_meetup___S_o_Paulo_Meet_Up_21", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T01:51:46.838Z", "modifiedAt": null, "url": null, "title": "Be Happier", "slug": "be-happier", "viewCount": null, "lastCommentedAt": "2019-04-24T20:22:15.775Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Klevador", "user": {"username": "Klevador", "createdAt": "2012-04-13T12:35:01.403Z", "isAdmin": false, "displayName": "Klevador"}, "userId": "WgXM6EZvsC6NGfdfA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JHcTP4Ad8QAmRTCZm/be-happier", "pageUrlRelative": "/posts/JHcTP4Ad8QAmRTCZm/be-happier", "linkUrl": "https://www.lesswrong.com/posts/JHcTP4Ad8QAmRTCZm/be-happier", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Be%20Happier&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABe%20Happier%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHcTP4Ad8QAmRTCZm%2Fbe-happier%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Be%20Happier%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHcTP4Ad8QAmRTCZm%2Fbe-happier", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHcTP4Ad8QAmRTCZm%2Fbe-happier", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10378, "htmlBody": "<hr class=\"dividerBlock\"/><p>This started as an assignment to find out about the science of \u2018buying happiness\u2019 (using money to become happier) \u2014 hence the emphasis on money-and-happiness. I learned a great deal more than how to buy happiness, however, and the project became somewhat more generalized. It is not meant to be comprehensive, but perhaps it makes for a useful supplement to Luke\u2019s <a href=\"/lw/4su/how_to_be_happy/\">How to be Happy</a>. This post consists mostly of quoted material.</p><hr class=\"dividerBlock\"/><p>In A Nutshell</p><h3><a href=\"about:blank#money\">Money and Happiness</a></h3><ul><li>Spend on others, especially people you are close to. Positive feedback loop: Prosocial spending makes you happier, and happiness makes you more likely to spend prosocially.</li><li>Don\u2019t be stingy. It&#x27;s bad for your health.</li><li>Don\u2019t think too much about money. It will impair your savoring ability. It&#x27;s also bad for your family life.</li><li>Be time-aware, but don\u2019t think of time in terms of money.</li><li>Being richer will not necessarily make you happier.</li><li>Do not live in wealthy enclaves. </li><li>Avoid conspicuous consumption.</li></ul><h3><a href=\"about:blank#work\">Work Satisfaction</a></h3><ul><li>Coping with Stress: React pragmatically rather than emotionally.</li><li>Go for \u2018approach\u2019 goals instead of \u2018avoid\u2019 goals. </li><li>Autonomy: Make a point of prefering autonomous goals rather than heteronomous goals (goals imposed by others). </li><li>Autonomy: Make sure you have spare discretionary time \u2014 even at financial cost.</li><li>Be passionate, but don\u2019t obsess.</li><li>Do work that you enjoy doing. Flow.</li><li>Set goals that are reasonably challenging and reasonably achievable.</li></ul><h3><a href=\"about:blank#materialism\">Materialism and Purchasing</a></h3><ul><li>Prefer experiential purchases; avoid materialistic goals. </li><li>Keep your goals intrinsic.</li><li>Don\u2019t do \u2018comparison shopping.\u2019 And don\u2019t place much stock in the happiness potential of any one positive change.</li><li>Follow the herd. \u201cThe best way to predict how much we will enjoy an experience is to see how much someone else enjoyed it.\u201d</li></ul><h3><a href=\"about:blank#interpersonal\">Interpersonal</a></h3><ul><li>Socialize with close others.</li><li>Associate with happy people.</li><li>Give the people around you opportunities to be generous. Ask them for favors.</li><li>Be actively kind (and occasionaly reminisce about your recent acts of kindness).</li></ul><h3><a href=\"about:blank#stretching\">Stretching Happiness</a> (fighting hedonic adaptation)</h3><ul><li>Go for smaller, more frequent successes rather than larger ones.</li><li>Go for variety and surprise. Don\u2019t keep doing the same thing. </li><li>Savor anticipation. Delay consumption. Actively anticipate good experiences. </li><li>Divide positive experiences into smaller pleasures, if possible.</li><li>Corollary: Conclude negative experiences as soon as possible. </li><li>Make a point of avoiding experiences that make you feel bad. </li></ul><h3><a href=\"about:blank#appreciation\">Appreciation</a></h3><ul><li>Be grateful and count your blessings (literally). Recycle happiness by reminiscing about good experiences.</li><li>Think of counterfactuals. (\u201cIf I didn\u2019t have this positive thing, what do I lose?\u201d)</li><li>Breathe deeply. Expand your time \u2014 by slowing down. </li><li>Stay in the present.</li></ul><h3><a href=\"about:blank#optimal\">Optimal Happification</a></h3><ul><li>Actively <em>want </em> to be happier. Motivation and investment matter.</li><li>Learn about the science of happiness. Internalize the lessons in this article and in <a href=\"/lw/4su/how_to_be_happy/\">here</a>.</li></ul><p> </p><hr class=\"dividerBlock\"/><h3>Some Key Terms</h3><ul><li>Subjective Well Being (SWB) aka happiness.</li><li>Hedonic Adaptation \u2014 the phenomenon of (rapidly) diminishing positive or negative affect from any one experience or thing. </li><li>Hedonic treadmill \u2014 the phenomenon of neverending aspirations for materialistic acquisitions that results from hedonic adaptation.</li></ul><hr class=\"dividerBlock\"/><h2><br/>Money and Happiness</h2><h4> Spend on others, especially people you are close to. </h4><blockquote>Past research in our lab has repeatedly shown that people are happier when they use financial resources to benefit others rather than themselves [Aknin, Dunn, Sandstrom &amp; Norton, submitted, 1,14].<br/>  </blockquote><blockquote>... findings suggest that to reap the greatest emotional reward from spending on someone else, one should direct their purchases to close others<br/></blockquote><blockquote>These findings should not be taken to suggest that people should avoid spending on weak social ties. Indeed, treating an acquaintance from yoga to a coffee after class might help to build a new strong tie. Thus, spending money on a weak social tie might help facilitate the development of new strong ties in the longer term.<br/>  </blockquote><blockquote>\u2026 research on reciprocal altruism and the evolution of cooperation demonstrates that people ultimately benefit from behaving generously and cooperatively toward individuals with whom they are likely to interact in the future<br/></blockquote><blockquote>The current results [\u2026] shed novel insight into translating spending choices into happiness: the next time you find a few spare dollars in your pocket, you will be happiest if you treat your best friend.</blockquote><p><em>(Aknin, Sandstrom, Dunn, &amp; Norton, 2011b)</em></p><p> </p><blockquote> This research also supports the <a href=\"http://en.wikipedia.org/wiki/Broaden-and-build\">broaden-and-build theory</a>  of positive emotions by demonstrating that higher levels of happiness may expand an individual\u2019s mindset to include thoughts of others.</blockquote><p><em>(Ahuvia, 2002)</em></p><p> </p><blockquote> ... prosocial spending may be particularly promising route to prosocial behavior because it has been shown to increase happiness immediately after spending (Dunn et al. 2008, 2010) and later upon reflection, as demonstrated here.<br/>  </blockquote><blockquote>... how money is spent may matter more than how much money is spent. That is, participants who recalled spending on others felt happier than those who spent money on themselves, and the benefits of prosocial spending were the same regardless of whether they spent $100 or just $20. Recent work suggests that prosocial behavior leads to emotional gains by providing opportunities for positive social contact (Aknin et al. 2011b); therefore, prosocial spending should promote happiness if the spending opportunity fosters positive relations with others, which may be largely independent of the specific amount of money spent. (Aknin, Dunn, &amp; Norton, 2011a)<br/>  </blockquote><blockquote>... participants assigned to spend a small windfall on someone else by purchasing a gift or making a donation to charity (prosocial spending) were significantly happier at the end of the day than participants assigned to spend the same size windfall by paying for a bill, expense, or gift for themselves (personal spending) </blockquote><p><em>(Aknin, Dunn, &amp; Norton, 2011a)</em></p><p> </p><p>Positive feedback loop:</p><blockquote> Taken together, our results show that<br/> (a) recalling a past prosocial spending experience leads to higher levels of happiness,<br/> (b) higher levels of happiness increase the likelihood of engaging in prosocial spending, and<br/> (c) recalling a past experience of prosocial spending increases the likelihood of spending a new windfall on others to the extent that happiness levels are elevated in the interim. This suggests that spending money on others may be self-reinforcing as long as this prosocial experience provides happiness.</blockquote><p><em>(Aknin, Dunn, &amp; Norton, 2011a)</em></p><p> </p><p>Being generous will make you happier.</p><blockquote> experiments within two very different countries (Canada and Uganda) [\u2026] show that spending money on others has a consistent, causal impact on happiness.<br/>  </blockquote><blockquote>In contrast to traditional economic thought\u2014which places self-interest as the guiding principle of human motivation\u2014our findings suggest that the reward experienced from helping others may be deeply ingrained in human nature, emerging in diverse cultural and economic contexts.<br/>  </blockquote><blockquote>[In both] Canada and Uganda \u2014 [which] differ dramatically in national-level income and donation frequency, we find that individuals report significantly greater well-being after reflecting on a time when they spent money on others rather than themselves. This effect emerged consistently across these two cultures, even though the specific prosocial spending experiences participants described differed considerably. Thus, although prosocial spending differs in both frequency (Study 1) and form (Study 2) in poor versus wealthy countries, its emotional consequences are remarkably consistent.</blockquote><p><em>(Aknin et al., 2010)</em></p><p> </p><blockquote> we found that spending more of one\u2019s income on others predicted greater happiness both cross-sectionally (in a nationally representative survey study) and longitudinally (in a field study of windfall spending). Finally, participants who were randomly assigned to spend money on others experienced greater happiness than those assigned to spend money on themselves.</blockquote><p><em>(Dunn, Aknin, &amp; Norton, 2008)</em></p><p> </p><blockquote> ...prosocial spending is consistently associated with greater happiness.<br/>  The robustness of this mechanism is supported by our finding that people seem to experience emotional benefits from sharing their financial resources with others not only in countries where such resources are plentiful, but also in impoverished countries where scarcity might seem to limit the possibilities to reap the gains from giving to others.</blockquote><p><em>(Aknin et al., 2010)</em></p><p> </p><h2>Don&#x27;t be Stingy.</h2><p>Aside from the positive effect of generosity on your own happiness, stinginess makes you less healthy; it is easier to be happy when you are healthy.</p><blockquote> The present research suggests that stingy economic behavior can produce a feeling of shame, which in turn drives secretion of the stress hormone <a href=\"http://en.wikipedia.org/wiki/Cortisol\">cortisol</a>.<br/>  </blockquote><blockquote>... the present research provides support for Social Self-Preservation Theory, which posits that acute threats to the \u2018social self\u2019 induce shame and lead to increased cortisol, as part of a coordinated response to social threats (Dickerson, Gruenewald, &amp; Kemeny, 2004a).<br/> </blockquote><blockquote>Our findings provide initial, suggestive evidence that shame and cortisol represent plausible emotional and biological pathways that might link everyday decisions about whether to help others with downstream consequences for one\u2019s own health.<br/>  </blockquote><blockquote>... stingy economic behavior predicts cortisol secretion only to the extent that stinginess provokes shame.<br/> </blockquote><p><em>(Dunn et al., 2010)</em></p><p> </p><p>Caveat: hedonic adaptation moderates the deleterious effect of bad health on well-being, but not entirely \u2014 and negative experiences are more powerful than positive experiences:</p><blockquote> \u201dTo sum up almost two decades of research, bad is stronger than good.\u201d</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p>(More on negative experiences farther down in \u2018STRETCHING HAPPINESS\u2019.)</p><p> </p><h4><strong>Think about time, but don\u2019t think of time in terms of money (\u201cAn hour of my time is worth\u2026\u201d).</strong></h4><blockquote> ... thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.<br/>  In three separate experiments we have demonstrated that bringing individuals\u2019 effective hourly wage to their attention impairs the ability to derive happiness from pleasurable experiences.<br/>  One possible explanation is that impatience discourages savoring. Savoring is a form of emotional regulation which augments the happiness individuals derive from experiences (e.g. Bryant et al., 2005; Quoidbach, 2009; Tugade &amp; Fredrickson, 2007).<br/>  ... recent ethnographic research [\u2026] found that people who are paid by the hour narrowly evaluate their time use in terms of its economic returns. As a consequence, they tend to discount the worth of activities with non-economic benefits (Evans et al., 2004).<br/>  ... the present findings suggest that thinking about time in terms of money is poised to affect our ability to smell the proverbial roses.</blockquote><p><em>(DeVoe &amp; House, 2012)</em></p><p></p><blockquote> ... there is a bi-directional relationship between the scarcity of time and its value: not only does having little time make it feel more valuable, but when time is more valuable, it is perceived as more scarce (DeVoe &amp; Pfeffer, 2010).</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><p>In an <a href=\"http://en.wikipedia.org/wiki/Priming_(psychology\">word priming</a>  experiment done in a cafe:</p><blockquote>Pair-wise comparisons showed that individuals primed with time spent more of their time at the caf\u00e9 socializing than those primed with money. Further, individuals primed with time spent less of their time working than those primed with money<br/>  </blockquote><blockquote>Participants primed with money worked more than those in the control condition and participants primed with time worked less than those in the control condition<br/>  </blockquote><blockquote>... participants primed with time were happier than those primed with money. [\u2026] Participants primed with time were also happier than those in the control condition [and] the happiness levels of those primed with money and those in the control condition did not differ significantly,<br/>  </blockquote><blockquote>These results suggest that increasing the relative salience of time (vs. money) can increase happiness by leading people to behave in more connecting ways [and] can nudge someone to spend that extra hour at home rather than at the office, there finding greater happiness.<br/>  </blockquote><blockquote>Focusing on money motivates one to work more, which is useful to know when struggling to put in that extra hour of work to meet a looming deadline. However, passing the hours working (although productive) does not translate into greater happiness. Spending time with loved ones does, and a shift in attention toward time proves an effective means to motivate this social connection.<br/>  </blockquote><blockquote>... the relevant question may be not how much money people have, but rather how much attention people put on money.<br/>  </blockquote><blockquote>Despite the belief that money is the resource most central to Americans\u2019 pursuit of happiness, increased happiness requires a shift in attention toward time.</blockquote><p><em>(Mogilner, 2010)</em></p><p> </p><h4>Being richer will not necessarily make you happier.</h4><blockquote> The belief that high income is associated with good mood is widespread but mostly illusory. People with above-average income are relatively satisfied with their lives but are barely happier than others in moment-to-moment experience, tend to be more tense, and do not spend more time in particularly enjoyable activities. Moreover, the effect of income on life satisfaction seems to be transient. We argue that people exaggerate the contribution of income to happiness because they focus, in part, on conventional achievements when evaluating their life or the lives of others.<br/>  The latter finding might help explain why income is more highly correlated with general life satisfaction than with experienced happiness, as tension and stress may accompany goal attainment, which in turn contributes to judgments of life satisfaction more than it does to experienced happiness.<br/> Despite the weak relation between income and global life satisfaction or experienced happiness, many people are highly motivated to increase their income. In some cases, this focusing illusion may lead to a misallocation of time, from accepting lengthy commutes (which are among the worst moments of the day) to sacrificing time spent socializing (which are among the best moments of the day) (28, 29). An emphasis on the role of attention helps to explain both why many people seek high income\u2014because their predictions exaggerate the increase in happiness due to the focusing illusion\u2014and why the long- term effect of income gains become relatively small, because attention eventually shifts to less novel aspects of daily life.</blockquote><p><em>(Kahneman, 2006)</em></p><p> </p><blockquote> It is found that higher income aspirations reduce people\u2019s satisfaction with life. In Switzerland and the New German Laender, the negative effect of an increase in the aspiration level on well-being is of a similar absolute magnitude as the positive effect on well-being of an equal increase in income. This suggests that subjective well-being depends largely on the gap between income aspirations and actual income and not on the income level as such. the higher the ratio between aspired income and actual income, the less satisfied people are with their life, ceteris paribus. This supports the notion of a relative utility concept.</blockquote><p><em>(Stutzer &amp; Frey, 2010)</em></p><p> </p><blockquote> Emotional well-being also rises with log income, but there is no further progress beyond an annual income of \u223c$75,000. Low income exacerbates the emotional pain associated with such misfortunes as divorce, ill health, and being alone.<br/>  </blockquote><blockquote>[The data suggest that] above a certain level of stable income, individuals\u2019 emotional well-being is constrained by other factors in their temperament and life circumstances.</blockquote><p><em>(Kahneman &amp; Deaton, 2010)</em></p><p> </p><h4>Pitfall of being wealthy: your ability to savor positive emotions and experiences will be impaired. <strong>Don\u2019t make money your priority. </strong></h4><blockquote> The present study provides the first evidence that money impairs people\u2018s ability to savor everyday positive emotions and experiences.<br/>  </blockquote><blockquote>In a sample of working adults, wealthier individuals reported lower savoring ability.<br/>  </blockquote><blockquote>... the negative impact of wealth on savoring undermined the positive effects of money on happiness.<br/>  </blockquote><blockquote>... moving beyond self-report, participants exposed to a reminder of wealth spent less time savoring a piece of chocolate and exhibited reduced enjoyment of it. The present research supplies evidence for the previously untested notion that having access to the best things in life may actually undercut the ability to reap enjoyment from life\u2018s small pleasures.<br/>  </blockquote><blockquote>In other words, one need not actually visit the pyramids of Egypt or spend a week in the legendary spas of Banff\u2014simply knowing that these peak experiences are readily available may increase the tendency to take the small pleasures of daily life for granted.<br/>  </blockquote><blockquote>... having access to the best things in life may actually undermine the ability to reap enjoyment from life\u2018s small pleasures.<br/>  </blockquote><blockquote>... our research demonstrates that a simple reminder of wealth produces the same deleterious effects as actual wealth, suggesting that perceived access to pleasurable experiences may be sufficient to impair everyday savoring. (Quoidbach et al., 2010)<br/> </blockquote><blockquote>This perspective is consistent with the intriguing theoretical notion that hedonic adaptation may occur not only in response to past experiences, but also in response to anticipated future experiences (Frederick &amp; Loewenstein, 1999).<br/>  </blockquote><blockquote>Our studies provide a novel contribution by demonstrating that the emotional benefits that money gives with one hand (i.e., access to pleasurable experiences), it takes away with the other by undercutting the ability to relish the small delights of daily living.<br/>  </blockquote><blockquote>... experimentally exposing participants to a reminder of wealth produced the same deleterious effect on savoring as did actual individual differences in wealth.</blockquote><p><em>(Quoidbach et al., 2010)</em></p><p> </p><blockquote>Across nations, placing a higher importance on money is associated with lower well-being (Kirkcaldy, Furnham, &amp; Martin, 1998).</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><h4>Financial aspirations are bad for family life (and the quality of interpersonal relationships is a strong predictor of happiness).</h4><blockquote>The negative consequences [of financial aspirations] were particularly severe for the domain of family life; the stronger the goal for financial success, the lower the satisfaction with family life, regardless of household income.<br/> </blockquote><p><em>(Nickerson et al., 2003)</em></p><h4>Don\u2019t live \u2018high\u2019.</h4><blockquote>Not only materialism, but wealth itself has been found in a few studies to produce negative effects. Hagerty (2000) found that when personal income was statistically controlled, individuals living in higher-income areas in the United States were lower in happiness than people living in lower-income areas. (Diener &amp; Seligman, 2004)<br/> </blockquote><blockquote>This suggests that wealthy individuals are fortunate if they live in middle-class areas rather than in wealthy enclaves.<br/>  </blockquote><blockquote>The negative effects of wealthy communities might partly be explained by their higher materialism (Stutzer, in press).</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><p>(Perhaps the more important point here is that you must surround yourself with low-materialism people, which means surrounding yourself with happy people, since materialism correlates negatively with happiness. A caveat to the advice of living in middle class areas if wealthy: the presence of a wealthy neighbor can make people more materialistic; it makes them aspire for more. A wealthy person can make his less wealthy neighbors less happy. See below.)</p><p> </p><h4>Avoid conspicuous consumption.</h4><blockquote> The \u2018relative income hypothesis\u2019 was formulated and econometrically tested by James Duesenberry (1949), who posited an asymmetric structure of externalities. People look upwards when making comparisons. Aspirations thus tend to be above the level already reached. Wealthier people impose a negative external effect on poorer people, but not vice versa. Fred Hirsch (1976), in his book Social Limits to Growth, emphasised the role of relative social status by calling attention to \u2018positional goods\u2019 which, by definition, cannot be augmented, because they rely solely on not being available to others. This theme was taken up by Robert Frank (1985, 1999), who argued that the production of positional goods in the form of luxuries, such as exceedingly expensive watches or yachts, is a waste of productive resources, as overall happiness is thereby decreased rather than increased.</blockquote><p><em>(Frey &amp; Stutzer, 2002)</em></p><p>(This relates to the recommendation to associate with happy people \u2014 farther down.)</p><p> </p><h1>More Recommendations</h1><p> </p><hr class=\"dividerBlock\"/><h2><strong>WORK SATISFACTION</strong></h2><p> </p><h4>Coping with Stress: React pragmatically rather than emotionally.</h4><blockquote> Coping can be divided into two broad engagements \u2013 either to trigger the individual to approach the problem or to regulate the emotional reactions arising from the challenge at hand (Andersson &amp; Willebrand 2003). The literature typically differentiates two broad strategies of coping (for a review, see Lazarus &amp; Folkman 1984). First, problem-based coping refers to a cognitively-based response behaviour that includes efforts to alleviate stressful circumstances. This coping strategy includes defining the problem, generating alternative solutions, determining the costs and benefits of such solutions, and actions taken to solve the problem. Second, emotion-based coping involves behavioural responses to regulate the affective consequences of stressful events, which may include avoidance, minimisation and distancing oneself from the problem (Lazarus &amp; Folkman 1984).<br/>  </blockquote><blockquote>It seems that problem-based coping strategies are more instrumental than emotion-based ones for attaining successful entrepreneurial outcomes. This implies that entrepreneurs who are more inclined toward emotion-based coping could be trained to employ more problem-based coping, since coping can be learned just like any other competence.</blockquote><p><em>(Drnov\u0161ek et al., 2010)</em></p><p> </p><h4>Leaders and Entrepreneurs: Don\u2019t take on too many business partners. (See also AUTONOMY below)</h4><blockquote> \u2026 entrepreneurs who had lower perceived role centrality and were part of a larger founding team were more inclined to use emotion-based coping than those who started their venture in smaller teams. We believe these insights can help in training entrepreneurs in the development of effective coping strategies. Individuals with perceived high centrality of their entrepreneurial role are more likely to effectively engage in coping to optimise their venture\u201ds performance and their own psychological well being.</blockquote><p><em>(Drnov\u0161ek et al., 2010)</em></p><p> </p><h4>Prefer the \u2018approach\u2019 path instead of the \u2018avoid\u2019 path.</h4><p>It is good for your well-being to work towards <em>achieving</em> something, rather than <em>preventing</em> something from happening.</p><blockquote> [One] concern is whether one\u2019s goal activities are characterized by approach or avoidance motivational systems. Elliot &amp; Sheldon (1997), for example, classified goals as approach or avoidance and then examined the effects of goal progress over a short-term period. Pursuit of avoidance goals was associated with both poorer goal progress and with lower well-being. Elliot et al (1997) similarly showed that people whose personal goals contained a higher proportion of avoidance had lower SWB [Subjective Well Being]. They also demonstrated the association between neuroticism and avoidance goals, but showed that the impact of avoidance regulation was evident even when controlling for neuroticism. Carver &amp; Scheier (1999) also presented research linking approach goals (positively) and avoidance goals (negatively) to well-being outcomes.</blockquote><p><em>(Ryan &amp; Deci, 2001)</em></p><p> </p><h4>AUTONOMY: Make a point of prefering autonomous goals rather than heteronomous goals (goals imposed/expected by others).</h4><blockquote> Another actively researched issue concerns how autonomous one is in pursuing goals. <a href=\"http://en.wikipedia.org/wiki/Self-determination_theory\">SDT</a> in particular has taken a strong stand on this by proposing that only self-endorsed goals will enhance well-being, so pursuit of heteronomous goals, even when done efficaciously, will not. The relative autonomy of personal goals has, accordingly, been shown repeatedly to be predictive of well-being outcomes controlling for goal efficacy at both between-person and within-person levels of analysis (Ryan &amp; Deci 2000). Interestingly this pattern of findings has been supported in cross-cultural research, suggesting that the relative autonomy of one\u2019s pursuits matters whether one is collectivistic or individualistic, male or female (e.g. V Chirkov &amp; RM Ryan 2001; Hayamizu 1997, Vallerand 1997).<br/>  </blockquote><blockquote>Sheldon &amp; Elliot (1999) developed a self-concordance model of how autonomy relates to well-being. Self-concordant goals are those that fulfill basic needs and are aligned with one\u2019s true self. These goals are well-internalized and therefore autonomous, and they emanate from intrinsic or identified motivations. Goals that are not self-concordant encompass external or introjected motivation, and are ei- ther unrelated or indirectly related to need fulfillment. Sheldon &amp; Elliot found that, although goal attainment in itself was associated with greater well-being, this effect was significantly weaker when the attained goals were not self-concordant. People who attained more self-concordant goals had more need-satisfying experi- ences, and this greater need satisfaction was predictive of greater SWB. Similarly, Sheldon &amp; Kasser (1998) studied progress toward goals in a longitudinal design, finding that goal progress was associated with enhanced SWB and lower symp- toms of depression. However, the impact of goal progress was again moderated by goal concordance. Goals that were poorly integrated to the self, whose focus was not related to basic psychological needs, conveyed less SWB benefits, even when achieved.</blockquote><p><em>(Ryan &amp; Deci, 2001)</em></p><p> </p><blockquote> \u2026 freely chosen activities increase happiness, while obligatory activities lower it (Csikszentmihalyi &amp; Hunter, 2003).</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><blockquote> ... we find additional evidence that entrepreneurs also derive utility from things other than financial success. In particular, the achievement of independence and creativity is highly correlated with start-up satisfaction.<br/>  </blockquote><blockquote>... our results indicate that forcing people into situations when they cannot choose among alternatives is likely to result in significant utility losses, independent of other factors.</blockquote><p><em>(Block &amp; Koellinger, 2009)</em></p><p> </p><h4>AUTONOMY: Make sure you have spare discretionary time \u2014 even at financial cost.</h4><blockquote>having spare time and perceiving control over how to spend that time (i.e. discretionary time) has been shown to have a strong and consistent effect on life satisfaction and happiness, even controlling for the actual amount of free time one has (Eriksson, Rice, &amp; Goodin, 2007; Goodin, Rice, Parpo, &amp; Eriksson, 2008).<br/>  </blockquote><blockquote>Therefore, increase your discretionary time, even if it requires monetary resources. And if you can&#x27;t afford to, focus on the present moment, breathe more slowly, and spend the little time that you have in meaningful ways. </blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><h4>Be passionate, but don\u2019t obsess. \u201cPassion Does Make a Difference to People\u2019s Well-Being\u201d (Philippe, Vallerand, &amp; Lavigne, 2009)</h4><p>Key terms: hedonic well-being; eudaimonic well-being</p><blockquote>Recent research has begun to distinguish two aspects of subjective well-being. Emotional [hedonic] well-being refers to the emotional quality of an individual\u2019s everyday experience\u2014the frequency and intensity of ex- periences of joy, stress, sadness, anger, and affection that make one\u2019s life pleasant or unpleasant. Life evaluation [eudaimonic well-being] refers to the thoughts that people have about their life when they think about it. </blockquote><p><em>(Kahneman &amp; Deaton, 2010)</em></p><p> </p><blockquote> The results of two studies provided support for the idea that being harmoniously passionate for an activity contributes significantly to both hedonic and eudaimonic well-being, while being obsessively passionate or not being passionate for any activity does not contribute to well-being at all.<br/>  </blockquote><blockquote>Indeed, merely engaging in a given activity without passion (i.e. being non-passionate) led to the lowest scores on both hedonic and eudaimonic well-being in Study 1 and to the highest decreases in vitality in Study 2 (although no significant differences were found between non-passionate and obsessively passionate people in these studies).<br/>  </blockquote><blockquote>harmoniously passionate people scored significantly higher than obsessively passionate and non-passionate people on hedonic and eudaimonic well-being (Study 1).<br/>  </blockquote><blockquote>only harmoniously passionate people showed a significant increase in vitality over a 1-year period, while obsessively passionate participants showed a slight decrease and non- passionate participants an even larger decrease (Study 2).<br/>  </blockquote><blockquote>only harmonious passion positively predicts well-being over time, while obsessive passion is either negatively associated or unrelated to it (Rousseau &amp; Vallerand, 2003, 2008; Vallerand et al., 2008, Study 2; Vallerand et al., 2007, Studies 1 and 2).<br/>  </blockquote><blockquote>it would appear that an obsessively passionate or non-passionate engagement does not contribute to well-being, and may even have a cost, as shown by the decreases in vitality found in Study 2 for obsessively passionate and non-passionate people.<br/> </blockquote><p><em>(Philippe et al., 2009)</em></p><p> </p><h4>Do work that you enjoy doing. Flow.</h4><blockquote> the accomplishment of goals and the ability to be lost in a task (Csikszentmihalyi and Csikszentmihalyi 1988) seem to be correlated with happiness.<br/> </blockquote><p><em>(Nicolao, Irwin, &amp; Goodman, 2009)</em></p><p> </p><blockquote> Sheldon and Lyubomirsky (2007) have posited that there is much room for improvement in one\u2019s happiness. They suggest that while the largest part of our level of happiness is preset by our genetic endowment (around 50%), some 40 per cent is still modifiable (the last 10% is due to uncontrollable circumstances) and the best way to do this is through what they call \u201cintentional activity engagement\u201d.<br/>  </blockquote><blockquote>They recommend engaging in interesting, fun activities that fit one\u2019s personality and dispositions, that can vary in content, and that are not merely engaged in as a routine but when people feel like doing it. We agree with such a recommendation, especially as Sheldon and Lyubomirsky\u2019s definition of intentional activity is rather close to that of harmonious passion. </blockquote><p><em>(Philippe et al., 2009)</em></p><p> </p><blockquote> In Kasser\u2019s view, the secret to SWB is meeting one\u2019s intrinsic needs, which means pursuing intrinsic goals out of an intrinsic motivation. In this way, it is similar to Csikszentmihalyi\u2019s (1999) view that happiness stems from \u201cflow\u201d experiences, which are also intrinsically motivated. I contend that the shift toward individualistic cultures that accompanies economic development helps people create life-styles that are consis- tent with their preferences and aptitudes (Veenhoven, 1999), and in so doing pursue their intrinsic needs.</blockquote><p><em>(Ahuvia, 2002)</em></p><p> </p><h4>Set goals that are reasonably challenging and reasonably achievable.</h4><blockquote>One issue concerns the level of challenge posed by one\u2019s goals. When life goals are nonoptimally challenging\u2014either too easy or too difficult\u2014positive affect [emotional well-being] is lower (Csikszentmihalyi &amp; Csikszentmihalyi 1988). Low expectations of success have also been associated with high negative affect (Emmons 1986),</blockquote><p><em>(Ryan &amp; Deci, 2001)</em></p><p> </p><p> </p><h4>Prefer intrinsic (vs. extrinsic) goals</h4><p>Definition:</p><blockquote> Psychologists make a distinction between two important kinds of goals\u2014intrinsic and extrinsic. Intrinsic goals involve activities and projects that are personally rewarding and meaningful, and that satisfy people&#x27;s basic needs for competence, relatedness, and autonomy (Kasser &amp; Ryan, 1993, 1996; see Ryan &amp; Deci, 2000, for a review). By contrast, extrinsic goals involve strivings for fame, money, or favorable outward appearances. Research suggests that positive events generated by the fulfillment of intrinsic goals (e.g., making purchases for others rather than yourself) produce more happiness than those generated by extrinsic goals (Dunn, Aknin &amp; Norton, 2008; see also Kasser, 2002; cf. Dunn et al., 2011).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><blockquote> Because high aspirations undermine the benefits of a positive change, are people simply better off with few goals and lowered aspirations? Not necessarily. Ambitious goals held before beginning a new venture motivate people to work harder on that venture and improve their overall performance (Heath, Larrick &amp; Wu, 1999). Individuals would, however, be happier if they focused their monies and efforts on meaningful, intrinsic goals and abandoned extrinsic ones.<br/>  </blockquote><blockquote>Extrinsic goals undermine well-being in several ways. First, by their very nature, extrinsic goals do not satisfy people&#x27;s basic needs directly, if at all. Instead, much like an addiction (Koob &amp; Le Moal, 2001), such goals lead to ever-increasing desires for psychologically unfulfilling commodities (Myers, 2000). Second, extrinsic goals appear to be incompatible with close, meaningful relationships. Those who pursue extrinsic goals report poorer relationships (Kasser &amp; Ryan, 2001). Indeed, even being reminded of money, as Dunn and colleagues (2011) mention, can cause people to be less prosocial and less generous (Vohs, Mead &amp; Goode, 2006), as well as to be perceived as less friendly and likable by others (Vohs, 2010).<br/>  </blockquote><blockquote>... over-reliance on external contingencies such as becoming famous, wealthy, or attractive may lead to fragile self-worth (Sheldon, Ryan, Deci &amp; Kasser, 2004). For example, a student seeking a law degree from a prestigious and pricey school with the aim of gaining peer respect might become hopelessly depressed if not admitted. Finally, due to limits of attention, time, and energy, extrinsic goals can lead to the neglect of intrinsic pursuits, which are associated with higher well-being (Vohs et al., 2006).<br/>  </blockquote><blockquote>An entrepreneur investing in a new company with the aim of striking it rich might neglect his true interests and hobbies to invest all his energy into his business, and thus miss the need-satisfying personal growth, flow, and joy derived from his more authentic pursuits.<br/>  </blockquote><blockquote>Fittingly expressing the futility and unhappiness wrapped up in pursuing extrinsic goals, a notorious New York tabloid editor confessed that he was \u201cpart of that strange race of people aptly described as spending their lives doing things they detest to make money they don&#x27;t want to buy things they don&#x27;t need to impress people they dislike\u201d (Gauvreau, 1941). As Benjamin Franklin well knew, money is best directed to goals that directly satisfy personal needs such as affiliation, autonomy, and competence rather than expensive pursuits that are unfulfilling and distracting in the end.<br/>  </blockquote><blockquote>In contrast, intrinsic goals, such as building close relationships, making new self-discoveries, and investing in the community, directly activate feelings of satisfaction and con- tentment, which are more likely to be appreciated and less likely to be taken for granted. Dunn and colleagues (2011) rightfully emphasize the link between generosity and well- being, recommending that, to follow the example of Warren Buffett, people spend their money on others rather them themselves.<br/>  </blockquote><blockquote>Intrinsic goals can also trigger \u201cupward spirals\u201d\u2014 for example, streams of positive moods and prosocial behavior that gain momentum and reinforce one another as they unfold (Lyubomirsky, King, &amp; Diener, 2005; Norton, Dunn, Aknin &amp; Sandstrom, 2009; Otake, Shimai, Tanaka-Matsumi, Otsui &amp; Fredrickson, 2006).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><hr class=\"dividerBlock\"/><h2>MATERIALISM AND PURCHASING</h2><p> </p><h4>The Hedonic Treadmill: We adapt to life changes. Many things that give pleasure will soon cease to do so, thereby driving us to seek more, and more\u2026</h4><blockquote> The \u201cpursuit of happiness\u201d is central to the U.S. worldview, yet the very expression also illustrates a paradox of that worldview: Perhaps when one [naively] pursues happiness too single-mindedly, one fails to notice and take advantage of what one already has. In other words, [naively] striving for ever greater happiness may set one on a hedonic treadmill to nowhere. (More on this below)</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4>Prefer experiential purchases; avoid materialistic goals. It is better to collect (positive) experiences than to collect things.</h4><p>(But do not keep repeating the same positive experience, lest hedonic adaptation set in quicker. See &quot;Stretching Happiness&quot; farther down.)</p><p> </p><blockquote> Experiential activities are inherently more social (Caprariello &amp; Reis, 2010; Van Boven &amp; Gilovich, 2003) and for this reason fulfill the psychological need for relatedness (Howell &amp; Hill, 2009).</blockquote><p><em>(Howell et al., 2012)</em></p><p> </p><blockquote>Discretionary experiential purchases ostensibly foster more social contact than discretionary material purchases (Millar &amp; Thomas 2009; Van Boven, 2005), which is a key component to happiness (Argyle, 2001).<br/>  </blockquote><blockquote>Research has demonstrated that people are happier with experiential purchases compared to material items.<br/>  </blockquote><blockquote>Experiential purchases are more central to positive self-identity than material purchases.<br/>  </blockquote><blockquote>Further, experiential purchases may satisfy the personal needs of development and growth more than material acquisitions (Kasser &amp; Ryan, 1996).</blockquote><p><em>(Thomas, 2010)</em></p><p> </p><blockquote> ... we show that on average the most happiness obtained through purchasing is likely to be obtained through experiential purchases that turn out well.<br/>  </blockquote><blockquote>... positive social interaction is a major source of happiness; many experiential purchases involve activities with other people, including family.</blockquote><p><em>(Nicolao et al., 2009)</em></p><p> </p><blockquote> Materialism might lead to lower well-being because materialistic people tend to downplay the importance of social relationships and to have a large gap between their incomes and material aspirations (Solberg, Diener, &amp; Robinson, 2004).</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><blockquote> Several studies have documented that a materialistic lifestyle is associated with diminished subjective well-being.<br/>  </blockquote><blockquote>... consistent with previous research, we found that materialism is negatively correlated with life satisfaction (Belk 1984, 1985; Burroughs and Rindfleisch 2002; Christopher et al. 2007; Ryan and Dziurawiec 2001; Wright and Larsen 1993).<br/>  </blockquote><blockquote>... in line with previous research (Christopher and Schlenker 2004; Chris- topher et al. 2009). High materialistic consumers experience negative emotions more frequently than low materialistic consumers.</blockquote><p><em>(Hudders &amp; Pandelaere, 2011)</em></p><p> </p><blockquote> ... we administered three widely used measures of a materialistic value orientation to 92 business students in Singapore. As expected, those students who had strongly internalized materialistic values also reported lowered self-actualization, vitality and happiness, as well as increased anxiety, physical symptomatology, and unhappiness. (Kasser &amp; Ahuvia, 2002)<br/> </blockquote><blockquote>past research demonstrating that materialistic values are associated with experiences of general and existential insecurity (Pyszczynski et al., 1997; Rindfleisch et al., 2009).</blockquote><p><em>(Howell et al., 2012)</em></p><p> </p><blockquote> ... positive experiences not only live on in memories but also lend themselves to even more positive reinterpretations over time as the negative aspects of them fade</blockquote><p><em>(Nicolao et al., 2009)</em></p><p> </p><blockquote> ... when security needs are met, it may be more adaptive to broaden one\u2019s experience and acquire new knowledge, skills, and relationships that often accompany experiential purchases. These experiences, if they do not arouse competing security concerns, may then provide increased SWB with accompanying reductions in feelings of anxiety and insecurity, encouraging further experiential purchases, and resulting in the \u2018upward spiral\u2019 depicted in our model. In this way, the benefits of an experiential purchasing tendency may accrue over a lifetime and individuals may develop stable purchasing habits.</blockquote><p><em>(Howell et al., 2012)</em></p><p> </p><blockquote> In sum, evidence suggests that when looking to spend money, the most satisfying pursuits should involve learning new skills (e.g., mastering a new instrument or learning a foreign language), spending time with others (e.g., taking out one&#x27;s family to dinner or having coffee with a friend), or doing something good for someone else (e.g., buying Christmas decorations for an elderly neighbor or sending a care package to a sick friend).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><blockquote> We provide evidence that this purchase type by valence interaction is driven by the fact that consumers adapt more slowly to experiential purchases than to material purchases, leading to both greater happiness and greater unhappiness for experiential purchases.<br/>  </blockquote><blockquote>adaptation happens more quickly for material purchases than for experiential purchases.</blockquote><p><em>(Nicolao et al., 2009)</em></p><p> </p><h4>Don\u2019t engage in \u2018comparison shopping.\u2019 And don\u2019t place much stock in the happiness potential of any one positive change.</h4><p>Comparison shopping makes us aware of previously unimportant differences and makes us forget the salient qualities of what we want.</p><blockquote>Sites like [bizrate.com] offer consumers the opportunity to search for everything [...], comparing a vast range of available options within a given category. [...] Recent research suggests that comparison shopping may sometimes come at a cost. By altering the psychological context in which decisions are made, comparison shopping may distract consumers from attributes of a product that will be important for their happiness, focusing their attention instead on attributes that distinguish the available options.<br/>  </blockquote><blockquote>Another problem with comparison shopping is that the comparisons we make when we are shopping are not the same comparisons we will make when we consume what we shopped for (Hsee, Loewenstein, Blount, &amp; Bazerman, 1999; Hsee &amp; Zhang, 2004).<br/>  </blockquote><blockquote>One of the dangers of comparison shopping, then, is that the options we don&#x27;t choose typically recede into the past and are no longer used as standards for comparison.<br/>  </blockquote><blockquote>A similar process is likely to unfold in the real estate market. Before purchasing a home, people typically attend scores of open houses and viewings, scrutinizing spec sheets for information about each property&#x27;s features. Through this process of comparison shopping, the features that distinguish one home from another may come to loom large, while their similarities fade into the background. As a result, home buyers might over- estimate the hedonic consequences of living in a big, beautiful house in a great location versus a more modest home, leading them to take out a larger loan than they can truly afford (potentially sowing the seeds for a nationwide financial crisis).<br/>  </blockquote><blockquote>This suggests that consumers who expect a single purchase to have a lasting impact on their happiness might make more realistic predictions if they simply thought about a typical day in their life.<br/>  </blockquote><blockquote>Conclusion When asked to take stock of their lives, people with more money report being a good deal more satisfied [eudamonic well being]. But when asked how happy they are at the moment, people with more money are barely different than those with less [hedonic well being] (Diener, Ng, Harter, &amp; Arora, 2010). This suggests that our money provides us with satisfaction when we think about it, but not when we use it. That shouldn&#x27;t happen. Money can buy many, if not most, if not all of the things that make people happy, and if it doesn&#x27;t, then the fault is ours. We believe that psychologists can teach people to spend their money in ways that will indeed increase their happiness, and we hope we&#x27;ve done a bit of that here.</blockquote><p><em>(Dunn et al., 2011)</em></p><p> </p><blockquote> When people consider the impact of any single factor on their well-being\u2014not only income\u2014they are prone to exaggerate its importance. We refer to this tendency as the focusing illusion.</blockquote><p><em>(Kahneman, 2006)</em></p><p> </p><h4>&#x27;Follow the herd.&#x27; (Dunn et al., 2011)</h4><blockquote> Research suggests that the best way to predict how much we will enjoy an experience is to see how much someone else enjoyed it. In one study, Gilbert, Killingsworth, Eyre, and Wilson (2009) asked women to predict how much they would enjoy a speed date with a particular man. Some of the women were shown the man&#x27;s photograph and autobiography, while others were shown only a rating of how much a previous woman had enjoyed a speed date with the same man a few minutes earlier. Although the vast majority of the participants expected that those who were shown the photograph and autobiography would make more accurate predictions than those who were shown the rating, precisely the opposite was the case. Indeed, relative to seeing the photograph and autobiography, seeing the rating reduced inaccuracy by about 50%. It appears that the 17th century writer Fran\u00e7ois de La Rochefoucauld was correct when he wrote: \u201cBefore we set our hearts too much upon anything, let us first examine how happy those are who already possess it.\u201d</blockquote><p><em>(Dunn et al., 2011)</em></p><p> </p><hr class=\"dividerBlock\"/><p><strong>INTERPERSONAL</strong></p><p> </p><h4>Socialize \u2014 with the right people.</h4><blockquote> ... the effects of wealth are not large, and they are dwarfed by other influences, such as those of personality and social relationships.</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><blockquote> ... it is not only whether you spend your time with others that influences your happiness, but also who you spend your time with. Interaction partners associated with the greatest happiness levels include friends, family, and significant others, whereas bosses and co-workers tend to be associated with the least happiness (Kahneman et al., 2004).<br/>  </blockquote><blockquote>social leisure activities contribute more to happiness than solitary ones (Reyes-Garcia et al., 2009).<br/>  </blockquote><blockquote>Furthermore, people who frequently engage in social activities experience higher levels of happiness than people who participate in social activities less often (Lloyd &amp; Auld, 2002), and being with others typically improves the quality of an experience (whereas being alone makes most people sad, lonely, or both; Csikszentmihalyi &amp; Larson, 1984; Lewinsohn, Sullivan, &amp; Grosscup, 1982). </blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><blockquote> Compared with the less happy groups, the happiest respondents did not exercise significantly more, participate in religious activities significantly more, or experience more objectively defined good events. No variable was sufficient for happiness, but good social relations were necessary.<br/>  </blockquote><blockquote>Our findings suggest that very happy people have rich and satisfying social relationships and spend little time alone relative to average people. [\u2026] In contrast, unhappy people have social relationships that are significantly worse than average.</blockquote><p><em>(Diener &amp; Seligman, 2002)</em></p><p> </p><blockquote> Income and education are more closely related to life evaluation, but health, care giving, loneliness, and smoking are relatively stronger predictors of daily emotions.</blockquote><p><em>(Kahneman &amp; Deaton, 2010)</em></p><p> </p><h4>Associate with happy people.</h4><blockquote> ... research has also shown that our relationships with weak ties, and even strangers, can affect our happiness. Using a large-scale, longitudinal dataset, Fowler and Christakis [5] suggested that happiness spreads throughout social networks,extending up to three degrees of separation: a person becomes happier if their friend\u2019s friend\u2019s friend becomes happier, even if they don\u2019t know that person.</blockquote><p><em>(Aknin, Sandstrom, Dunn, &amp; Norton, 2011b)</em></p><p> </p><blockquote> People who are surrounded by many happy people and those who are central in the network are more likely to become happy in the future. Longitudinal statistical models suggest that clusters of happiness result from the spread of happiness and not just a tendency for people to associate with similar individuals. A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25% (95% confidence interval 1% to 57%). Similar effects are seen in coresident spouses (8%, 0.2% to 16%), siblings who live within a mile (14%, 1% to 28%), and next door neighbours (34%, 7% to 70%). Effects are not seen between coworkers. The effect decays with time and with geographical separation.</blockquote><p><em>(Fowler &amp; Christakis, 2008)</em></p><p> </p><h4>Give the people around you opportunities to be generous. Ask them for favors.</h4><p>You can possibly make people around you happier by allowing them to be kind and generous, and you want to surround yourself with happy people (see above). Aside from making them happier, you will also improve your relationship with them via the <a href=\"http://en.wikipedia.org/wiki/Ben_Franklin_Effect\">Benjamin Franklin effect</a>, which \u2014 unintuitively \u2014 makes people like you more if you ask them for favors.</p><p> </p><h4>Be actively kind (and occasionaly reminisce about your recent acts of kindness).</h4><blockquote>Subjective happiness was increased simply by counting one\u2019s own acts of kindness for one week.<br/>  </blockquote><blockquote>Happy people became more kind and grateful through the counting kindnesses intervention.<br/>  </blockquote><blockquote>Our results further suggest that a reciprocal relationship may exist between kindness and happiness, as has been shown for gratitude and happiness [see below].</blockquote><p><em>(Otake et al., 2006)</em></p><p> </p><hr class=\"dividerBlock\"/><p><strong>STRETCHING HAPPINESS (fighting hedonic adaptation)</strong></p><p> </p><p>Hedonic adaptation \u2014 definition:</p><blockquote> The pleasure of success and the ignominy of failure abate with time. So does the thrill of a new sports car, the pain over a failed romance, the delight over a promotion, and the distress of a scary diagnosis. This phenomenon, known as hedonic adaptation (HA), has drawn increasing interest from both psychologists and economists (e.g., Diener, Lucas, &amp; Scollon, 2006; Easterlin, 2006; Frederick &amp; Loewenstein, 1999; Kahneman &amp; Thaler, 2006; Lucas, 2007a; Lyubomirsky, 2011; Lyubomirsky, Sheldon, &amp; Schkade, 2005; Wilson &amp; Gilbert, 2008).</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4>Choose smaller, more frequent successes rather than larger ones.</h4><p>Even big positive changes can get old fast, and soon stop bringing happiness.</p><blockquote> ...every one of the published studies evidences fairly rapid and apparently complete adaptation to positive changes. The most widely-cited study is that of Brickman and his colleagues (1978), who reported that lottery winners were no happier up to 18 months after the news than those who had experienced no windfall.</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4>Go for variety and surprise. Don\u2019t keep doing the same thing.</h4><blockquote> ...variable stimuli resist adaptation more than do unchanging stimuli (see also Wilson &amp; Gilbert, 2008).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><blockquote> ...these findings support the notion that variety and surprise spice up life in ways that sustain well-being (Sheldon et al., in press; Sheldon &amp; Lyubomirsky, 2006, 2009; Wilson, Centerbar, Kermer, &amp; Gilbert, 2005)\u201d</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4>Savor the anticipation. Delay consumption. Actively anticipate good experiences.</h4><blockquote> Research in the field of neuroscience has shown that the part of the brain responsible for feeling pleasure, the mesolimbic dopamine system, can be activated when merely thinking about something pleasurable, such as drinking one&#x27;s favorite brand of beer (McClure, Li, Tomlin, Cypert, Montague, &amp; Montague, 2004) or driving one&#x27;s favorite type of sports car (Erk, Spitzer, Wunderlich, Galley, &amp; Walter, 2002).<br/>  </blockquote><blockquote>... the brain sometimes enjoys anticipating a reward more than receiving the reward (Loewenstein, 1987; Berns, McClure, Pagnoni, &amp; Montague, 2001).<br/>  </blockquote><blockquote>... the pleasure derived from window shopping for a dress may exceed the pleasure from actually acquiring the dress.</blockquote><p><em>(Aaker et al., 2010)</em></p><p>(Perhaps the above can inform the discourse on the <a href=\"/lw/hl/lotteries_a_waste_of_hope/\">[ir]rationality of lotteries</a>.)</p><p> </p><h4>Divide positive experiences into smaller pleasures, if possible.</h4><blockquote>Dividing consumption into smaller doses and separating it out over time can multiply [the pleasure of] \u201cfirst bites,\u201d and subsequently, the enjoyment. Savoring a chocolate bar could be as simple as dividing it into squares and eating one piece per day, instead of devouring it all in a single sitting. Research supports the idea that breaks are beneficial for positive experiences, such as enjoying a television program, but detrimental for negative experiences, such as enduring a dental drill (Nelson, Meyvis &amp; Galak, 2009).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p>Dividing into smaller doses also increases the amount of pleasurable anticipation. See previous subsection.</p><p> </p><h4>Corollary: Conclude negative experiences as soon as possible.</h4><p>Don\u2019t &quot;think about it tomorrow.\u201d Prolongation increases the effect of both negative and positive experiences, and bad is stronger than good:</p><blockquote> Although the same hedonic adaptation process is involved in both positive and negative experiences, an important asymme- try exists between the two that further complicates efforts to remain happy, especially if a positive change comes at a high financial cost. To sum up almost two decades of research, bad is stronger than good (Baumeister, Bratslavsky, Finkenauer &amp; Vohs, 2001; see also Taylor, 1991), or as Einstein quipped, \u201cPut your hand on a hot stove for a minute, and it seems like an hour. Sit with a pretty girl for an hour, and it seems like a minute.\u201d [...] positive changes are weaker than negative changes, and that their effects also evaporate more quickly (e.g., Nezlek &amp; Gable, 2001; Sheldon, Ryan &amp; Reis, 1996; see also Oishi, Diener, Choi, Kim-Prieto &amp; Choi, 2007).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h4>Make a point of avoiding experiences that make you feel bad.</h4><blockquote> Well-being is about more than just frequently feeling good\u2014it is also about infrequently feeling bad (Diener, Suh, Lucas &amp; Smith, 1999).<br/>  </blockquote><blockquote>All else being equal, the elimination of negative experiences could provide a three- to five-fold hedonic return on investment over the creation of positive experiences, due to positive/negative asymmetry (e.g., David, Green, Martin &amp; Suls, 1997; Fredrickson &amp; Losada, 2005; Gottman, 1994).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h2>  </h2><h2>APPRECIATION</h2><p> </p><h4>Be grateful. Count your blessings (literally). Recycle happiness. Reminisce about good experiences.</h4><blockquote> A number of experiments have demonstrated that the regular practice of gratitude\u2014a practice closely related to and often indistinguishable from appreciation and savoring\u2014brings about significant increases in well-being when performed over the course of 1 to 12 consecutive weeks. For example, relative to performing neutral activities, the intentional and effortful practice of \u201ccounting one&#x27;s blessings\u201d once a week (Emmons &amp; McCullough, 2003; Froh, Sefick &amp; Emmons, 2008; Lyubo- mirsky, Sheldon, &amp; Schkade, 2005) or penning appreciation letters to individuals who have been kind and meaningful (Boehm, Lyubomirsky, &amp; Sheldon, in press; Lyubomirsky, Dickerhoof, Boehm, &amp; Sheldon, in press; Seligman, Steen, Park &amp; Peterson, 2005) has been shown to produce increases in happiness for as long as 6 months.</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h4>Think of counterfactuals. (\u201cIf I didn\u2019t have this, what do I lose?\u201d)</h4><blockquote> Another cognitive exercise that directs attention toward existing positive changes or events is counterfactual thinking. This strategy involves mentally subtracting a purchased positive experience from ever having taken place, and enumerating all the subsequent blessings that also would have disappeared (Koo, Algoe, Wilson, &amp; Gilbert, 2008).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h4>Breathe deeply. Expand your time \u2014 by slowing down.</h4><blockquote> [People feel less rushed and hurried when they] simply breathe more deeply. In one study, subjects who were instructed to take long and slow breaths (vs. short and quick ones) for 5 minutes not only felt there was more time available to get things done, but also perceived their day to be longer.</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><h4>Stay in the present.</h4><blockquote> One possible benefit of being present-focused is that thinking about the present moment (vs. the future) slows down the perceived passage of time, allowing people to feel less rushed and hurried (Rudd &amp; Aaker, 2010).</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><hr class=\"dividerBlock\"/><p><strong>OPTIMAL HAPPIFICATION</strong></p><p> </p><h4>Actively <em>want</em>  to be happier. Motivation and investment matter.</h4><blockquote> First, and most important, we found that to become happier, people need both a will and a proper way. The will can come from motivation, expectations, and diligence. The proper way comes from performing the \u201cright\u201d activity, not merely a placebo. Accordingly, we found that motivation and investment in becoming a happier person matters. That is, expressing gratitude and optimism did not generally increase well-being unless a person was truly cognizant of the exercises\u2019 purpose and motivated to improve his or her happiness. Second, effortful pursuit of happiness activities was found to be important to improving and maintaining well- being.<br/>  </blockquote><blockquote>... happiness interventions are more than just placebos, but [\u2026] they are most successful when participants know about, endorse, and commit to the intervention.<br/>  </blockquote><blockquote>According to our model of well-being change (Lyubomirsky, Sheldon, et al., 2005; Sheldon &amp; Lyubomirsky, 2004), sustainable increases in happiness are possible, but only if pursued under optimal conditions, such as when people are motivated to perform a positive activity, when they bring to bear effort and persistence, and when the activity is a legitimately efficacious one. </blockquote><p><em>(Lyubomirsky et al., 2011)</em></p><p> </p><h4>Learn about the science of happiness. Internalize the recommendations in this article and in <a href=\"/lw/4su/how_to_be_happy/\">here</a>.</h4><blockquote> ... people often hold incorrect intuitive theories about the determinants of happiness. For instance, they overestimate the impact of specific life events on their experienced well-being with regard to intensity, as well as with regard to duration. (see also Comparison Shopping above)<br/>  </blockquote><blockquote>... four major sources for systematic over- and undervaluation of choice options that can be distinguished: (i) the underestimation of adaptation, (ii) distorted memory of past experiences, (iii) the rationalization of decisions, and (iv) false intuitive theories about the sources of future utility.</blockquote><p><em>(Quoidbach et al., 2010)</em></p><p> </p><blockquote>Money is an opportunity for happiness, but it is an opportunity that people routinely squander because the things they think will make them happy often don\u2019t.<br/>  </blockquote><blockquote>It is not surprising when wealthy people who know nothing about wine end up with cellars that aren&#x27;t that much better stocked than their neighbors&#x27;, and it should not be surprising when wealthy people who know nothing about happiness end up with lives that aren&#x27;t that much happier than anyone else&#x27;s.</blockquote><p><em>(Dunn et al., 2011)</em></p><p> </p><hr class=\"dividerBlock\"/><h2>EXTRA CONSIDERATIONS</h2><p> </p><blockquote> Happiness predicts [future] income.</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p>(^But try not to think of it that way!)</p><p> </p><blockquote> ... we found that with all but one specification, initial happiness levels were positively and significantly correlated with future earnings. [\u2026] An additional finding is that the effects of initial period happiness on future income and on future happiness seem to be more consistent across all income groups than are the effects of initial period income on either future income and future happiness. The effects of initial period income seem more important for those at higher levels of income.<br/> </blockquote><blockquote>The studies by psychologists that find that happiness has positive effects on future income also find that these effects are stronger at the higher end of the income scale. See Diener and Biswas-Diener (1999).</blockquote><p><em>(Graham, Eggers &amp; Sukhtankar, 2004)</em></p><p> </p><blockquote> ... higher cheerfulness in the first year of college correlated with higher income 19 years or so later, when respondents reached their late 30s; this effect was greatest for those who came from the most affluent families</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><h4>It will be easier to stay happy when you become happier</h4><blockquote> ... our findings also dovetail with those of Cohn and Fredrickson (in press) by demonstrating that initial happiness gains can cause a happiness intervention to become self-reinforcing.</blockquote><p><em>(Aknin, Dunn, &amp; Norton, 2011a)</em></p><p> </p><h4>Happiness Interventions Work!</h4><blockquote> Fordyce (1977, 1983) created an intervention program based on the idea that people&#x27;s subjective well-being can be increased if they learn to imitate the traits of happy people, such as being organized, keeping busy, spending more time socializing, developing a positive outlook, and working on a healthy personality. Fordyce found that the program produced increases in happiness compared to a placebo control, as ell as compared to participants in conditions receiving only partial information. Most impressive, he found lasting effects of the intervention in follow-up evaluation 9-28 months after the study.</blockquote><p><em>(Diener et al., 2009)</em></p><blockquote> Recently, a number of additional effective interventions on happiness have been reported, ranging from the kindness interventions (Otake, Shimai, Tanaca-Matumi, Otsui, &amp; Fredrickson, 2006) and gratitude interventions (Emmons &amp; McCulough, 2003) to variants of the writing intervention (King, 2001; Lyubomirsky, Sousa, &amp; Dickerhoof, 2006). Recent intervention studies are clearly promising. However, more diverse dependent variables and measuring instruments would be desirabe, as well as explorations of which interventions are most beneficial, and why.</blockquote><p><em>(Diener et al., 2009)</em></p><p> </p><h3>Extra extra: Cultural Differences</h3><blockquote> Veenhoven (1999) found that among poor countries, individualism was negatively associated with happiness; whereas among richer countries, individualism was positively associated with happiness. This suggests that economic growth is part of a complex system of modernization that needs to be seen holistically. Collectivism may exist in poorer countries because it is highly functional in that environment, but it may give way to more individualism as societies modernize and the needs of those societies change. Overall, individualism/collectivism stands out as an extremely promising construct for explaining differences in national average levels of SWB, when investigated holistically as part of the larger social system (Cummins, 1998; Myers and Diener, 1995).<br/>  </blockquote><blockquote>... economic development increases SWB by creating a cultural environment where individuals make choices to maximize their happiness rather than meet social obligations (Coleman, 1990; Galbraith, 1992; Triandis, 1989; Triandis et al., 1990; Veenhoven, 1999; Watkins and Liu, 1996). This cultural transformation away from obligation and toward the pursuit of happiness is part of a broader transition away from collectivism and toward individualist cultural values and forms of social organization.<br/>  </blockquote><blockquote>Cross-cultural research shows that values like \u201cenjoying life\u201d and leading \u201can exciting life\u201d are stronger in individualist societies, whereas \u201csocial recognition,\u201d \u201cpreserving my public image,\u201d being \u201chumble,\u201d and \u201chonoring parents and elders\u201d are particularly strong in collectivist societies (Triandis et al., 1990, p. 1015). There is no more reason to think that people seek social recognition with the ultimate goal of personal happiness, than there is to think that people seek happiness with the ultimate goal of getting others to think well of them for having such a pleasant affect.</blockquote><p><em>(Ahuvia, 2002)</em></p><p> </p><hr class=\"dividerBlock\"/><p><strong>References</strong></p><p> </p><p>Aaker, J. L., Rudd, M., &amp; Mogilner, C. (2010). <em><a href=\"http://www.webcitation.org/66uOtzAB6\">If Money Doesn\u2019t Make You Happy, Consider Time</a>.</em> Journal of Consumer Psychology, 2011.</p><p>Ahuvia, A. C. (2002). <a href=\"http://www.webcitation.org/66v14nlao\"><em>Individualism/collectivism and cultures of happiness: A theoretical conjecture on the relationship between consumption, culture and subjective well-being at the national level</em>.</a> Journal of Happiness Studies, 3(1), 23\u201336. Springer.</p><p>Aknin, L. B., Dunn, E. W., &amp; Norton, M. I. (2011a). <a href=\"http://www.webcitation.org/66v2JBeEg\"><em>Happiness Runs in a Circular Motion: Evidence for a Positive Feedback Loop between Prosocial Spending and Happiness</em>.</a> Journal of Happiness Studies, 13(2), 347\u2013355. doi:10.1007/s10902-011-9267-5</p><p>Aknin, L. B., Sandstrom, G. M., Dunn, E. W., &amp; Norton, M. I. (2011b). <em><a href=\"http://www.webcitation.org/66v2V6ReD\">It&#x27;s the Recipient That Counts: Spending Money on Strong Social Ties Leads to Greater Happiness than Spending on Weak Social Ties</a></em>. (M. Perc, Ed.)PLoS ONE, 6(2), e17018. doi:10.1371/journal.pone.0017018</p><p>Aknin, L. B., Barrington-Leigh, C. P., Dunn, E. W., Helliwell, J. F., Biswas-Diener, R., Kemeza, I., Nyende, P., et al. (2010). <em><a href=\"http://www.webcitation.org/66v2xT6fq\">Prosocial spending and well-being: cross-cultural evidence for a psychological universal</a></em>. National Bureau of Economic Research.</p><p>Block, J., &amp; Koellinger, P. (2009). <em><a href=\"http://www.webcitation.org/66v3gEkh7\">I Can&#x27;t Get No Satisfaction\u2014Necessity Entrepreneurship and Procedural Utility</a></em>. Kyklos, 62(2), 191\u2013209. doi:10.1111/j.1467-6435.2009.00431.x</p><p>Chancellor, J., &amp; Lyubomirsky, S. (2011). <em><a href=\"http://www.webcitation.org/66v3qLFdM\">Happiness and thrift: When (spending) less is (hedonically) more</a>.</em> Journal of Consumer Psychology, 21(2), 131.</p><p>DeVoe, S. E., &amp; House, J. (2012). <a href=\"http://www.webcitation.org/66v3vvsDu\"><em>Time, money, and happiness: How does putting a price on time affect our ability to smell the roses</em>?</a> Journal of Experimental Social Psychology, 48(2), 466\u2013474. doi:10.1016/j.jesp.2011.11.012</p><p>Diener, E., Oishi, S., Lucas, R.E. (2009). <em><a href=\"http://www.google.com.ph/books?hl=tl&lr=&id=bLZI4fRofDwC&oi=fnd&pg=PA187&dq=Subjective+Well-Being:+The+science+of+happiness+and+life+satisfaction&ots=FA_teNp-86&sig=Mba-1iK2BC2VlXs9ljxG5QH9Kb8&redir_esc=y#v=onepage&q=Subjective%20WEll-Being%3A%20The%20science%20of%20happiness%20and%20life%20satisfaction&f=false\">Subjective Well-Being: The science of happiness and life satisfaction</a></em>. Oxford Handbook of Positive Psychology, 187-194.</p><p>Diener, E., &amp; Seligman, M. P. (2002). <em><a href=\"http://www.webcitation.org/66v3ytpcX\">Very happy people</a></em>. Psychological Science, 13(1), 81\u201384.</p><p>Diener, E., &amp; Seligman, M. P. (2004). <em><a href=\"http://www.webcitation.org/66v41gxzT\">Beyond money</a></em>. Psychological science in the public interest, 5(1), 1\u201331.</p><p>Drnov\u0161ek, M., \u00d6rtqvist, D., &amp; Wincent, J. (2010). <em><a href=\"http://www.webcitation.org/66v473B0U\">The effectiveness of coping strategies used by entrepreneurs and their impact on personal well-being and venture performance</a></em>. Journal of Economics and Business, 28, 193\u2013220.</p><p>Dunn, E. W., Aknin, L. B., &amp; Norton, M. I. (2008). <em><a href=\"http://www.webcitation.org/66v4C5xDR\">Spending Money on Others Promotes Happiness</a></em>. Science, 319(5870), 1687\u20131688. doi:10.1126/science.1150952</p><p>Dunn, E. W., Ashton-James, C. E., Hanson, M. D., &amp; Aknin, L. B. (2010). <em><a href=\"http://www.webcitation.org/66v4Fy6CR\">On the Costs of Self-interested Economic Behavior: How Does Stinginess Get Under the Skin?</a></em> Journal of Health Psychology, 15(4), 627\u2013633. doi:10.1177/1359105309356366</p><p>Dunn, E. W., Gilbert, D. T., &amp; Wilson, T. D. (2011). <em><a href=\"http://www.webcitation.org/66v4VbCe0\">If money doesn\u201ct make you happy, then you probably aren&#x27;t spending it right</a></em>. Journal of Consumer Psychology, 21(2), 115.</p><p>Fowler, J. H., &amp; Christakis, N. A. (2008). <em><a href=\"http://www.webcitation.org/66v4ntR9f\">The dynamic spread of happiness in a large social network</a></em>. BMJ: British medical journal, 337, a2338.</p><p>Frey, B. S., &amp; Stutzer, A. (2002). <em><a href=\"http://www.webcitation.org/66v4rgWaP\">The economics of happiness</a>.</em> World Economics, 3(1), 1\u201317.</p><p>Graham, C., Eggers, A., &amp; Sukhtankar, S. (2004). <em><a href=\"http://www.webcitation.org/66v4tajdV\">Does happiness pay?: An exploration based on panel data from Russia</a></em>. Journal of Economic Behavior &amp; Organization, 55(3), 319\u2013342. Elsevier.</p><p>Howell, R. T., Pchelin, P., &amp; Iyer, R. (2012). <em><a href=\"http://www.webcitation.org/66v4wEfar\">The preference for experiences over possessions: Measurement and construct validation of the Experiential Buying Tendency Scale</a></em>. The Journal of Positive Psychology, 7(1), 57\u201371.</p><p>Hudders, L., &amp; Pandelaere, M. (2011). <em><a href=\"http://www.webcitation.org/66vA8qaFI\">The Silver Lining of Materialism: The Impact of Luxury Consumption on Subjective Well-Being</a></em>. Journal of Happiness Studies. doi:10.1007/s10902-011-9271-9</p><p>Kahneman, D. (2006). <em><a href=\"http://www.webcitation.org/66v8HYlMx\">Would You Be Happier If You Were Richer? A Focusing Illusion</a></em>. Science, 312(5782), 1908\u20131910. doi:10.1126/science.1129688</p><p>Kahneman, D., &amp; Deaton, A. (2010). <em><a href=\"http://www.webcitation.org/66v8VGQZq\">High income improves evaluation of life but not emotional well-being</a></em>. Proceedings of the National Academy of Sciences, 107(38), 16489\u201316493. doi:10.1073/pnas.1011492107</p><p>Kasser, T., &amp; Ahuvia, A. (2002). <em><a href=\"http://www.webcitation.org/66v8ZfXqZ\">Materialistic values and well-being in business students</a></em>. European Journal of Social Psychology, 32(1), 137\u2013146. doi:10.1002/ejsp.85</p><p>Lyubomirsky, S., Dickerhoof, R., Boehm, J. K., &amp; Sheldon, K. M. (2011). <em><a href=\"http://www.webcitation.org/66v8qQWzE\">Becoming happier takes both a will and a proper way: An experimental longitudinal intervention to boost well-being</a></em>. Emotion, 11(2), 391.</p><p>Mogilner, C. (2010). <em><a href=\"http://www.webcitation.org/66v8uSg29\">The Pursuit of Happiness: Time, Money, and Social Connection</a></em>. Psychological Science, 21(9), 1348\u20131354. doi:10.1177/0956797610380696</p><p>Nickerson, C., Schwarz, N., Diener, E., &amp; Kahneman, D. (2003). <em><a href=\"http://www.webcitation.org/66vAQV7iZ\">Zeroing in on the Dark Side of the American Dream A Closer Look at the Negative Consequences of the Goal for Financial Success</a></em>. Psychological Science, 14(6), 531\u2013536.</p><p>Nicolao, L., Irwin, J., &amp; Goodman, J. (2009). <em><a href=\"http://www.webcitation.org/66vAnWBxM\">Happiness for Sale: Do Experiential Purchases Make Consumers Happier than Material Purchases?</a></em> Journal of Consumer Research, 36(2), 188\u2013198. doi:10.1086/597049</p><p>Otake, K., Shimai, S., Tanaka-Matsumi, J., Otsui, K., &amp; Fredrickson, B. L. (2006). <em><a href=\"http://www.webcitation.org/66vAqie8B\">Happy People Become Happier through Kindness: A Counting Kindnesses Intervention</a></em>. Journal of Happiness Studies, 7(3), 361\u2013375. doi:10.1007/s10902-005-3650-z</p><p>Philippe, F. L., Vallerand, R. J., &amp; Lavigne, G. L. (2009). <a href=\"http://www.webcitation.org/66vAtQEHN\"><em>Passion does make a difference in people&#x27;s lives: A look at well-being in passionate and non-passionate individuals</em>.</a> Applied Psychology: Health and Well-Being, 1(1), 3\u201322.</p><p>Quoidbach, J., Dunn, E. W., Petrides, K. V, &amp; Mikolajczak, M. (2010). <em><a href=\"http://www.webcitation.org/66vAwIjQz\">Money Giveth, Money Taketh Away</a></em>. Psychological Science, 21(6), 759.</p><p>Ryan, R. M., &amp; Deci, E. L. (2001). <em><a href=\"http://www.webcitation.org/66vB2CG38\">On happiness and human potentials: A review of research on hedonic and eudaimonic well-being</a></em>. Annual Review of Psychology, 52(1), 141\u2013166.</p><p>Sheldon, K. M., Lyubomirsky, S. (2012).<em><a href=\"http://www.webcitation.org/66vB5ki7d\">The Challenge of Staying Happier: Testing the Hedonic Adaptation Prevention Model</a></em>. Personality and Social Psychology Bulletin. doi:10.1177/0146167212436400</p><p>Stutzer, A., &amp; Frey, B. S. (2010). <em><a href=\"http://www.webcitation.org/66vB7xoAB\">Recent advances in the economics of individual subjective well-being</a></em>. Social Research: An International Quarterly, 77(2), 679\u2013714.</p><p>Thomas, R. L. (2010). <em><a href=\"http://www.webcitation.org/66vBGi63B\">Mediating and moderating variables between discretionary purchases and happiness</a></em>. UNLV Theses/Dissertations/Professional Papers/Capstones. Paper 889.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XqykXFKL9t38pbSEm": 3, "fkABsGCJZ6y9qConW": 3, "Jzm2mYuuDBCNWq8hi": 4, "5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JHcTP4Ad8QAmRTCZm", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 135, "baseScore": 162, "extendedScore": null, "score": 0.000331, "legacy": true, "legacyId": "15192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 162, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<hr class=\"dividerBlock\"><p>This started as an assignment to find out about the science of \u2018buying happiness\u2019 (using money to become happier) \u2014 hence the emphasis on money-and-happiness. I learned a great deal more than how to buy happiness, however, and the project became somewhat more generalized. It is not meant to be comprehensive, but perhaps it makes for a useful supplement to Luke\u2019s <a href=\"/lw/4su/how_to_be_happy/\">How to be Happy</a>. This post consists mostly of quoted material.</p><hr class=\"dividerBlock\"><p>In A Nutshell</p><h3 id=\"Money_and_Happiness\"><a href=\"about:blank#money\">Money and Happiness</a></h3><ul><li>Spend on others, especially people you are close to. Positive feedback loop: Prosocial spending makes you happier, and happiness makes you more likely to spend prosocially.</li><li>Don\u2019t be stingy. It's bad for your health.</li><li>Don\u2019t think too much about money. It will impair your savoring ability. It's also bad for your family life.</li><li>Be time-aware, but don\u2019t think of time in terms of money.</li><li>Being richer will not necessarily make you happier.</li><li>Do not live in wealthy enclaves. </li><li>Avoid conspicuous consumption.</li></ul><h3 id=\"Work_Satisfaction\"><a href=\"about:blank#work\">Work Satisfaction</a></h3><ul><li>Coping with Stress: React pragmatically rather than emotionally.</li><li>Go for \u2018approach\u2019 goals instead of \u2018avoid\u2019 goals. </li><li>Autonomy: Make a point of prefering autonomous goals rather than heteronomous goals (goals imposed by others). </li><li>Autonomy: Make sure you have spare discretionary time \u2014 even at financial cost.</li><li>Be passionate, but don\u2019t obsess.</li><li>Do work that you enjoy doing. Flow.</li><li>Set goals that are reasonably challenging and reasonably achievable.</li></ul><h3 id=\"Materialism_and_Purchasing\"><a href=\"about:blank#materialism\">Materialism and Purchasing</a></h3><ul><li>Prefer experiential purchases; avoid materialistic goals. </li><li>Keep your goals intrinsic.</li><li>Don\u2019t do \u2018comparison shopping.\u2019 And don\u2019t place much stock in the happiness potential of any one positive change.</li><li>Follow the herd. \u201cThe best way to predict how much we will enjoy an experience is to see how much someone else enjoyed it.\u201d</li></ul><h3 id=\"Interpersonal\"><a href=\"about:blank#interpersonal\">Interpersonal</a></h3><ul><li>Socialize with close others.</li><li>Associate with happy people.</li><li>Give the people around you opportunities to be generous. Ask them for favors.</li><li>Be actively kind (and occasionaly reminisce about your recent acts of kindness).</li></ul><h3 id=\"Stretching_Happiness__fighting_hedonic_adaptation_\"><a href=\"about:blank#stretching\">Stretching Happiness</a> (fighting hedonic adaptation)</h3><ul><li>Go for smaller, more frequent successes rather than larger ones.</li><li>Go for variety and surprise. Don\u2019t keep doing the same thing. </li><li>Savor anticipation. Delay consumption. Actively anticipate good experiences. </li><li>Divide positive experiences into smaller pleasures, if possible.</li><li>Corollary: Conclude negative experiences as soon as possible. </li><li>Make a point of avoiding experiences that make you feel bad. </li></ul><h3 id=\"Appreciation\"><a href=\"about:blank#appreciation\">Appreciation</a></h3><ul><li>Be grateful and count your blessings (literally). Recycle happiness by reminiscing about good experiences.</li><li>Think of counterfactuals. (\u201cIf I didn\u2019t have this positive thing, what do I lose?\u201d)</li><li>Breathe deeply. Expand your time \u2014 by slowing down. </li><li>Stay in the present.</li></ul><h3 id=\"Optimal_Happification\"><a href=\"about:blank#optimal\">Optimal Happification</a></h3><ul><li>Actively <em>want </em> to be happier. Motivation and investment matter.</li><li>Learn about the science of happiness. Internalize the lessons in this article and in <a href=\"/lw/4su/how_to_be_happy/\">here</a>.</li></ul><p> </p><hr class=\"dividerBlock\"><h3 id=\"Some_Key_Terms\">Some Key Terms</h3><ul><li>Subjective Well Being (SWB) aka happiness.</li><li>Hedonic Adaptation \u2014 the phenomenon of (rapidly) diminishing positive or negative affect from any one experience or thing. </li><li>Hedonic treadmill \u2014 the phenomenon of neverending aspirations for materialistic acquisitions that results from hedonic adaptation.</li></ul><hr class=\"dividerBlock\"><h2 id=\"Money_and_Happiness1\"><br>Money and Happiness</h2><h4 id=\"Spend_on_others__especially_people_you_are_close_to__\"> Spend on others, especially people you are close to. </h4><blockquote>Past research in our lab has repeatedly shown that people are happier when they use financial resources to benefit others rather than themselves [Aknin, Dunn, Sandstrom &amp; Norton, submitted, 1,14].<br>  </blockquote><blockquote>... findings suggest that to reap the greatest emotional reward from spending on someone else, one should direct their purchases to close others<br></blockquote><blockquote>These findings should not be taken to suggest that people should avoid spending on weak social ties. Indeed, treating an acquaintance from yoga to a coffee after class might help to build a new strong tie. Thus, spending money on a weak social tie might help facilitate the development of new strong ties in the longer term.<br>  </blockquote><blockquote>\u2026 research on reciprocal altruism and the evolution of cooperation demonstrates that people ultimately benefit from behaving generously and cooperatively toward individuals with whom they are likely to interact in the future<br></blockquote><blockquote>The current results [\u2026] shed novel insight into translating spending choices into happiness: the next time you find a few spare dollars in your pocket, you will be happiest if you treat your best friend.</blockquote><p><em>(Aknin, Sandstrom, Dunn, &amp; Norton, 2011b)</em></p><p> </p><blockquote> This research also supports the <a href=\"http://en.wikipedia.org/wiki/Broaden-and-build\">broaden-and-build theory</a>  of positive emotions by demonstrating that higher levels of happiness may expand an individual\u2019s mindset to include thoughts of others.</blockquote><p><em>(Ahuvia, 2002)</em></p><p> </p><blockquote> ... prosocial spending may be particularly promising route to prosocial behavior because it has been shown to increase happiness immediately after spending (Dunn et al. 2008, 2010) and later upon reflection, as demonstrated here.<br>  </blockquote><blockquote>... how money is spent may matter more than how much money is spent. That is, participants who recalled spending on others felt happier than those who spent money on themselves, and the benefits of prosocial spending were the same regardless of whether they spent $100 or just $20. Recent work suggests that prosocial behavior leads to emotional gains by providing opportunities for positive social contact (Aknin et al. 2011b); therefore, prosocial spending should promote happiness if the spending opportunity fosters positive relations with others, which may be largely independent of the specific amount of money spent. (Aknin, Dunn, &amp; Norton, 2011a)<br>  </blockquote><blockquote>... participants assigned to spend a small windfall on someone else by purchasing a gift or making a donation to charity (prosocial spending) were significantly happier at the end of the day than participants assigned to spend the same size windfall by paying for a bill, expense, or gift for themselves (personal spending) </blockquote><p><em>(Aknin, Dunn, &amp; Norton, 2011a)</em></p><p> </p><p>Positive feedback loop:</p><blockquote> Taken together, our results show that<br> (a) recalling a past prosocial spending experience leads to higher levels of happiness,<br> (b) higher levels of happiness increase the likelihood of engaging in prosocial spending, and<br> (c) recalling a past experience of prosocial spending increases the likelihood of spending a new windfall on others to the extent that happiness levels are elevated in the interim. This suggests that spending money on others may be self-reinforcing as long as this prosocial experience provides happiness.</blockquote><p><em>(Aknin, Dunn, &amp; Norton, 2011a)</em></p><p> </p><p>Being generous will make you happier.</p><blockquote> experiments within two very different countries (Canada and Uganda) [\u2026] show that spending money on others has a consistent, causal impact on happiness.<br>  </blockquote><blockquote>In contrast to traditional economic thought\u2014which places self-interest as the guiding principle of human motivation\u2014our findings suggest that the reward experienced from helping others may be deeply ingrained in human nature, emerging in diverse cultural and economic contexts.<br>  </blockquote><blockquote>[In both] Canada and Uganda \u2014 [which] differ dramatically in national-level income and donation frequency, we find that individuals report significantly greater well-being after reflecting on a time when they spent money on others rather than themselves. This effect emerged consistently across these two cultures, even though the specific prosocial spending experiences participants described differed considerably. Thus, although prosocial spending differs in both frequency (Study 1) and form (Study 2) in poor versus wealthy countries, its emotional consequences are remarkably consistent.</blockquote><p><em>(Aknin et al., 2010)</em></p><p> </p><blockquote> we found that spending more of one\u2019s income on others predicted greater happiness both cross-sectionally (in a nationally representative survey study) and longitudinally (in a field study of windfall spending). Finally, participants who were randomly assigned to spend money on others experienced greater happiness than those assigned to spend money on themselves.</blockquote><p><em>(Dunn, Aknin, &amp; Norton, 2008)</em></p><p> </p><blockquote> ...prosocial spending is consistently associated with greater happiness.<br>  The robustness of this mechanism is supported by our finding that people seem to experience emotional benefits from sharing their financial resources with others not only in countries where such resources are plentiful, but also in impoverished countries where scarcity might seem to limit the possibilities to reap the gains from giving to others.</blockquote><p><em>(Aknin et al., 2010)</em></p><p> </p><h2 id=\"Don_t_be_Stingy_\">Don't be Stingy.</h2><p>Aside from the positive effect of generosity on your own happiness, stinginess makes you less healthy; it is easier to be happy when you are healthy.</p><blockquote> The present research suggests that stingy economic behavior can produce a feeling of shame, which in turn drives secretion of the stress hormone <a href=\"http://en.wikipedia.org/wiki/Cortisol\">cortisol</a>.<br>  </blockquote><blockquote>... the present research provides support for Social Self-Preservation Theory, which posits that acute threats to the \u2018social self\u2019 induce shame and lead to increased cortisol, as part of a coordinated response to social threats (Dickerson, Gruenewald, &amp; Kemeny, 2004a).<br> </blockquote><blockquote>Our findings provide initial, suggestive evidence that shame and cortisol represent plausible emotional and biological pathways that might link everyday decisions about whether to help others with downstream consequences for one\u2019s own health.<br>  </blockquote><blockquote>... stingy economic behavior predicts cortisol secretion only to the extent that stinginess provokes shame.<br> </blockquote><p><em>(Dunn et al., 2010)</em></p><p> </p><p>Caveat: hedonic adaptation moderates the deleterious effect of bad health on well-being, but not entirely \u2014 and negative experiences are more powerful than positive experiences:</p><blockquote> \u201dTo sum up almost two decades of research, bad is stronger than good.\u201d</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p>(More on negative experiences farther down in \u2018STRETCHING HAPPINESS\u2019.)</p><p> </p><h4 id=\"Think_about_time__but_don_t_think_of_time_in_terms_of_money___An_hour_of_my_time_is_worth____\"><strong>Think about time, but don\u2019t think of time in terms of money (\u201cAn hour of my time is worth\u2026\u201d).</strong></h4><blockquote> ... thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.<br>  In three separate experiments we have demonstrated that bringing individuals\u2019 effective hourly wage to their attention impairs the ability to derive happiness from pleasurable experiences.<br>  One possible explanation is that impatience discourages savoring. Savoring is a form of emotional regulation which augments the happiness individuals derive from experiences (e.g. Bryant et al., 2005; Quoidbach, 2009; Tugade &amp; Fredrickson, 2007).<br>  ... recent ethnographic research [\u2026] found that people who are paid by the hour narrowly evaluate their time use in terms of its economic returns. As a consequence, they tend to discount the worth of activities with non-economic benefits (Evans et al., 2004).<br>  ... the present findings suggest that thinking about time in terms of money is poised to affect our ability to smell the proverbial roses.</blockquote><p><em>(DeVoe &amp; House, 2012)</em></p><p></p><blockquote> ... there is a bi-directional relationship between the scarcity of time and its value: not only does having little time make it feel more valuable, but when time is more valuable, it is perceived as more scarce (DeVoe &amp; Pfeffer, 2010).</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><p>In an <a href=\"http://en.wikipedia.org/wiki/Priming_(psychology\">word priming</a>  experiment done in a cafe:</p><blockquote>Pair-wise comparisons showed that individuals primed with time spent more of their time at the caf\u00e9 socializing than those primed with money. Further, individuals primed with time spent less of their time working than those primed with money<br>  </blockquote><blockquote>Participants primed with money worked more than those in the control condition and participants primed with time worked less than those in the control condition<br>  </blockquote><blockquote>... participants primed with time were happier than those primed with money. [\u2026] Participants primed with time were also happier than those in the control condition [and] the happiness levels of those primed with money and those in the control condition did not differ significantly,<br>  </blockquote><blockquote>These results suggest that increasing the relative salience of time (vs. money) can increase happiness by leading people to behave in more connecting ways [and] can nudge someone to spend that extra hour at home rather than at the office, there finding greater happiness.<br>  </blockquote><blockquote>Focusing on money motivates one to work more, which is useful to know when struggling to put in that extra hour of work to meet a looming deadline. However, passing the hours working (although productive) does not translate into greater happiness. Spending time with loved ones does, and a shift in attention toward time proves an effective means to motivate this social connection.<br>  </blockquote><blockquote>... the relevant question may be not how much money people have, but rather how much attention people put on money.<br>  </blockquote><blockquote>Despite the belief that money is the resource most central to Americans\u2019 pursuit of happiness, increased happiness requires a shift in attention toward time.</blockquote><p><em>(Mogilner, 2010)</em></p><p> </p><h4 id=\"Being_richer_will_not_necessarily_make_you_happier_\">Being richer will not necessarily make you happier.</h4><blockquote> The belief that high income is associated with good mood is widespread but mostly illusory. People with above-average income are relatively satisfied with their lives but are barely happier than others in moment-to-moment experience, tend to be more tense, and do not spend more time in particularly enjoyable activities. Moreover, the effect of income on life satisfaction seems to be transient. We argue that people exaggerate the contribution of income to happiness because they focus, in part, on conventional achievements when evaluating their life or the lives of others.<br>  The latter finding might help explain why income is more highly correlated with general life satisfaction than with experienced happiness, as tension and stress may accompany goal attainment, which in turn contributes to judgments of life satisfaction more than it does to experienced happiness.<br> Despite the weak relation between income and global life satisfaction or experienced happiness, many people are highly motivated to increase their income. In some cases, this focusing illusion may lead to a misallocation of time, from accepting lengthy commutes (which are among the worst moments of the day) to sacrificing time spent socializing (which are among the best moments of the day) (28, 29). An emphasis on the role of attention helps to explain both why many people seek high income\u2014because their predictions exaggerate the increase in happiness due to the focusing illusion\u2014and why the long- term effect of income gains become relatively small, because attention eventually shifts to less novel aspects of daily life.</blockquote><p><em>(Kahneman, 2006)</em></p><p> </p><blockquote> It is found that higher income aspirations reduce people\u2019s satisfaction with life. In Switzerland and the New German Laender, the negative effect of an increase in the aspiration level on well-being is of a similar absolute magnitude as the positive effect on well-being of an equal increase in income. This suggests that subjective well-being depends largely on the gap between income aspirations and actual income and not on the income level as such. the higher the ratio between aspired income and actual income, the less satisfied people are with their life, ceteris paribus. This supports the notion of a relative utility concept.</blockquote><p><em>(Stutzer &amp; Frey, 2010)</em></p><p> </p><blockquote> Emotional well-being also rises with log income, but there is no further progress beyond an annual income of \u223c$75,000. Low income exacerbates the emotional pain associated with such misfortunes as divorce, ill health, and being alone.<br>  </blockquote><blockquote>[The data suggest that] above a certain level of stable income, individuals\u2019 emotional well-being is constrained by other factors in their temperament and life circumstances.</blockquote><p><em>(Kahneman &amp; Deaton, 2010)</em></p><p> </p><h4 id=\"Pitfall_of_being_wealthy__your_ability_to_savor_positive_emotions_and_experiences_will_be_impaired__Don_t_make_money_your_priority__\">Pitfall of being wealthy: your ability to savor positive emotions and experiences will be impaired. <strong>Don\u2019t make money your priority. </strong></h4><blockquote> The present study provides the first evidence that money impairs people\u2018s ability to savor everyday positive emotions and experiences.<br>  </blockquote><blockquote>In a sample of working adults, wealthier individuals reported lower savoring ability.<br>  </blockquote><blockquote>... the negative impact of wealth on savoring undermined the positive effects of money on happiness.<br>  </blockquote><blockquote>... moving beyond self-report, participants exposed to a reminder of wealth spent less time savoring a piece of chocolate and exhibited reduced enjoyment of it. The present research supplies evidence for the previously untested notion that having access to the best things in life may actually undercut the ability to reap enjoyment from life\u2018s small pleasures.<br>  </blockquote><blockquote>In other words, one need not actually visit the pyramids of Egypt or spend a week in the legendary spas of Banff\u2014simply knowing that these peak experiences are readily available may increase the tendency to take the small pleasures of daily life for granted.<br>  </blockquote><blockquote>... having access to the best things in life may actually undermine the ability to reap enjoyment from life\u2018s small pleasures.<br>  </blockquote><blockquote>... our research demonstrates that a simple reminder of wealth produces the same deleterious effects as actual wealth, suggesting that perceived access to pleasurable experiences may be sufficient to impair everyday savoring. (Quoidbach et al., 2010)<br> </blockquote><blockquote>This perspective is consistent with the intriguing theoretical notion that hedonic adaptation may occur not only in response to past experiences, but also in response to anticipated future experiences (Frederick &amp; Loewenstein, 1999).<br>  </blockquote><blockquote>Our studies provide a novel contribution by demonstrating that the emotional benefits that money gives with one hand (i.e., access to pleasurable experiences), it takes away with the other by undercutting the ability to relish the small delights of daily living.<br>  </blockquote><blockquote>... experimentally exposing participants to a reminder of wealth produced the same deleterious effect on savoring as did actual individual differences in wealth.</blockquote><p><em>(Quoidbach et al., 2010)</em></p><p> </p><blockquote>Across nations, placing a higher importance on money is associated with lower well-being (Kirkcaldy, Furnham, &amp; Martin, 1998).</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><h4 id=\"Financial_aspirations_are_bad_for_family_life__and_the_quality_of_interpersonal_relationships_is_a_strong_predictor_of_happiness__\">Financial aspirations are bad for family life (and the quality of interpersonal relationships is a strong predictor of happiness).</h4><blockquote>The negative consequences [of financial aspirations] were particularly severe for the domain of family life; the stronger the goal for financial success, the lower the satisfaction with family life, regardless of household income.<br> </blockquote><p><em>(Nickerson et al., 2003)</em></p><h4 id=\"Don_t_live__high__\">Don\u2019t live \u2018high\u2019.</h4><blockquote>Not only materialism, but wealth itself has been found in a few studies to produce negative effects. Hagerty (2000) found that when personal income was statistically controlled, individuals living in higher-income areas in the United States were lower in happiness than people living in lower-income areas. (Diener &amp; Seligman, 2004)<br> </blockquote><blockquote>This suggests that wealthy individuals are fortunate if they live in middle-class areas rather than in wealthy enclaves.<br>  </blockquote><blockquote>The negative effects of wealthy communities might partly be explained by their higher materialism (Stutzer, in press).</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><p>(Perhaps the more important point here is that you must surround yourself with low-materialism people, which means surrounding yourself with happy people, since materialism correlates negatively with happiness. A caveat to the advice of living in middle class areas if wealthy: the presence of a wealthy neighbor can make people more materialistic; it makes them aspire for more. A wealthy person can make his less wealthy neighbors less happy. See below.)</p><p> </p><h4 id=\"Avoid_conspicuous_consumption_\">Avoid conspicuous consumption.</h4><blockquote> The \u2018relative income hypothesis\u2019 was formulated and econometrically tested by James Duesenberry (1949), who posited an asymmetric structure of externalities. People look upwards when making comparisons. Aspirations thus tend to be above the level already reached. Wealthier people impose a negative external effect on poorer people, but not vice versa. Fred Hirsch (1976), in his book Social Limits to Growth, emphasised the role of relative social status by calling attention to \u2018positional goods\u2019 which, by definition, cannot be augmented, because they rely solely on not being available to others. This theme was taken up by Robert Frank (1985, 1999), who argued that the production of positional goods in the form of luxuries, such as exceedingly expensive watches or yachts, is a waste of productive resources, as overall happiness is thereby decreased rather than increased.</blockquote><p><em>(Frey &amp; Stutzer, 2002)</em></p><p>(This relates to the recommendation to associate with happy people \u2014 farther down.)</p><p> </p><h1 id=\"More_Recommendations\">More Recommendations</h1><p> </p><hr class=\"dividerBlock\"><h2 id=\"WORK_SATISFACTION\"><strong>WORK SATISFACTION</strong></h2><p> </p><h4 id=\"Coping_with_Stress__React_pragmatically_rather_than_emotionally_\">Coping with Stress: React pragmatically rather than emotionally.</h4><blockquote> Coping can be divided into two broad engagements \u2013 either to trigger the individual to approach the problem or to regulate the emotional reactions arising from the challenge at hand (Andersson &amp; Willebrand 2003). The literature typically differentiates two broad strategies of coping (for a review, see Lazarus &amp; Folkman 1984). First, problem-based coping refers to a cognitively-based response behaviour that includes efforts to alleviate stressful circumstances. This coping strategy includes defining the problem, generating alternative solutions, determining the costs and benefits of such solutions, and actions taken to solve the problem. Second, emotion-based coping involves behavioural responses to regulate the affective consequences of stressful events, which may include avoidance, minimisation and distancing oneself from the problem (Lazarus &amp; Folkman 1984).<br>  </blockquote><blockquote>It seems that problem-based coping strategies are more instrumental than emotion-based ones for attaining successful entrepreneurial outcomes. This implies that entrepreneurs who are more inclined toward emotion-based coping could be trained to employ more problem-based coping, since coping can be learned just like any other competence.</blockquote><p><em>(Drnov\u0161ek et al., 2010)</em></p><p> </p><h4 id=\"Leaders_and_Entrepreneurs__Don_t_take_on_too_many_business_partners___See_also_AUTONOMY_below_\">Leaders and Entrepreneurs: Don\u2019t take on too many business partners. (See also AUTONOMY below)</h4><blockquote> \u2026 entrepreneurs who had lower perceived role centrality and were part of a larger founding team were more inclined to use emotion-based coping than those who started their venture in smaller teams. We believe these insights can help in training entrepreneurs in the development of effective coping strategies. Individuals with perceived high centrality of their entrepreneurial role are more likely to effectively engage in coping to optimise their venture\u201ds performance and their own psychological well being.</blockquote><p><em>(Drnov\u0161ek et al., 2010)</em></p><p> </p><h4 id=\"Prefer_the__approach__path_instead_of_the__avoid__path_\">Prefer the \u2018approach\u2019 path instead of the \u2018avoid\u2019 path.</h4><p>It is good for your well-being to work towards <em>achieving</em> something, rather than <em>preventing</em> something from happening.</p><blockquote> [One] concern is whether one\u2019s goal activities are characterized by approach or avoidance motivational systems. Elliot &amp; Sheldon (1997), for example, classified goals as approach or avoidance and then examined the effects of goal progress over a short-term period. Pursuit of avoidance goals was associated with both poorer goal progress and with lower well-being. Elliot et al (1997) similarly showed that people whose personal goals contained a higher proportion of avoidance had lower SWB [Subjective Well Being]. They also demonstrated the association between neuroticism and avoidance goals, but showed that the impact of avoidance regulation was evident even when controlling for neuroticism. Carver &amp; Scheier (1999) also presented research linking approach goals (positively) and avoidance goals (negatively) to well-being outcomes.</blockquote><p><em>(Ryan &amp; Deci, 2001)</em></p><p> </p><h4 id=\"AUTONOMY__Make_a_point_of_prefering_autonomous_goals_rather_than_heteronomous_goals__goals_imposed_expected_by_others__\">AUTONOMY: Make a point of prefering autonomous goals rather than heteronomous goals (goals imposed/expected by others).</h4><blockquote> Another actively researched issue concerns how autonomous one is in pursuing goals. <a href=\"http://en.wikipedia.org/wiki/Self-determination_theory\">SDT</a> in particular has taken a strong stand on this by proposing that only self-endorsed goals will enhance well-being, so pursuit of heteronomous goals, even when done efficaciously, will not. The relative autonomy of personal goals has, accordingly, been shown repeatedly to be predictive of well-being outcomes controlling for goal efficacy at both between-person and within-person levels of analysis (Ryan &amp; Deci 2000). Interestingly this pattern of findings has been supported in cross-cultural research, suggesting that the relative autonomy of one\u2019s pursuits matters whether one is collectivistic or individualistic, male or female (e.g. V Chirkov &amp; RM Ryan 2001; Hayamizu 1997, Vallerand 1997).<br>  </blockquote><blockquote>Sheldon &amp; Elliot (1999) developed a self-concordance model of how autonomy relates to well-being. Self-concordant goals are those that fulfill basic needs and are aligned with one\u2019s true self. These goals are well-internalized and therefore autonomous, and they emanate from intrinsic or identified motivations. Goals that are not self-concordant encompass external or introjected motivation, and are ei- ther unrelated or indirectly related to need fulfillment. Sheldon &amp; Elliot found that, although goal attainment in itself was associated with greater well-being, this effect was significantly weaker when the attained goals were not self-concordant. People who attained more self-concordant goals had more need-satisfying experi- ences, and this greater need satisfaction was predictive of greater SWB. Similarly, Sheldon &amp; Kasser (1998) studied progress toward goals in a longitudinal design, finding that goal progress was associated with enhanced SWB and lower symp- toms of depression. However, the impact of goal progress was again moderated by goal concordance. Goals that were poorly integrated to the self, whose focus was not related to basic psychological needs, conveyed less SWB benefits, even when achieved.</blockquote><p><em>(Ryan &amp; Deci, 2001)</em></p><p> </p><blockquote> \u2026 freely chosen activities increase happiness, while obligatory activities lower it (Csikszentmihalyi &amp; Hunter, 2003).</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><blockquote> ... we find additional evidence that entrepreneurs also derive utility from things other than financial success. In particular, the achievement of independence and creativity is highly correlated with start-up satisfaction.<br>  </blockquote><blockquote>... our results indicate that forcing people into situations when they cannot choose among alternatives is likely to result in significant utility losses, independent of other factors.</blockquote><p><em>(Block &amp; Koellinger, 2009)</em></p><p> </p><h4 id=\"AUTONOMY__Make_sure_you_have_spare_discretionary_time___even_at_financial_cost_\">AUTONOMY: Make sure you have spare discretionary time \u2014 even at financial cost.</h4><blockquote>having spare time and perceiving control over how to spend that time (i.e. discretionary time) has been shown to have a strong and consistent effect on life satisfaction and happiness, even controlling for the actual amount of free time one has (Eriksson, Rice, &amp; Goodin, 2007; Goodin, Rice, Parpo, &amp; Eriksson, 2008).<br>  </blockquote><blockquote>Therefore, increase your discretionary time, even if it requires monetary resources. And if you can't afford to, focus on the present moment, breathe more slowly, and spend the little time that you have in meaningful ways. </blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><h4 id=\"Be_passionate__but_don_t_obsess___Passion_Does_Make_a_Difference_to_People_s_Well_Being___Philippe__Vallerand____Lavigne__2009_\">Be passionate, but don\u2019t obsess. \u201cPassion Does Make a Difference to People\u2019s Well-Being\u201d (Philippe, Vallerand, &amp; Lavigne, 2009)</h4><p>Key terms: hedonic well-being; eudaimonic well-being</p><blockquote>Recent research has begun to distinguish two aspects of subjective well-being. Emotional [hedonic] well-being refers to the emotional quality of an individual\u2019s everyday experience\u2014the frequency and intensity of ex- periences of joy, stress, sadness, anger, and affection that make one\u2019s life pleasant or unpleasant. Life evaluation [eudaimonic well-being] refers to the thoughts that people have about their life when they think about it. </blockquote><p><em>(Kahneman &amp; Deaton, 2010)</em></p><p> </p><blockquote> The results of two studies provided support for the idea that being harmoniously passionate for an activity contributes significantly to both hedonic and eudaimonic well-being, while being obsessively passionate or not being passionate for any activity does not contribute to well-being at all.<br>  </blockquote><blockquote>Indeed, merely engaging in a given activity without passion (i.e. being non-passionate) led to the lowest scores on both hedonic and eudaimonic well-being in Study 1 and to the highest decreases in vitality in Study 2 (although no significant differences were found between non-passionate and obsessively passionate people in these studies).<br>  </blockquote><blockquote>harmoniously passionate people scored significantly higher than obsessively passionate and non-passionate people on hedonic and eudaimonic well-being (Study 1).<br>  </blockquote><blockquote>only harmoniously passionate people showed a significant increase in vitality over a 1-year period, while obsessively passionate participants showed a slight decrease and non- passionate participants an even larger decrease (Study 2).<br>  </blockquote><blockquote>only harmonious passion positively predicts well-being over time, while obsessive passion is either negatively associated or unrelated to it (Rousseau &amp; Vallerand, 2003, 2008; Vallerand et al., 2008, Study 2; Vallerand et al., 2007, Studies 1 and 2).<br>  </blockquote><blockquote>it would appear that an obsessively passionate or non-passionate engagement does not contribute to well-being, and may even have a cost, as shown by the decreases in vitality found in Study 2 for obsessively passionate and non-passionate people.<br> </blockquote><p><em>(Philippe et al., 2009)</em></p><p> </p><h4 id=\"Do_work_that_you_enjoy_doing__Flow_\">Do work that you enjoy doing. Flow.</h4><blockquote> the accomplishment of goals and the ability to be lost in a task (Csikszentmihalyi and Csikszentmihalyi 1988) seem to be correlated with happiness.<br> </blockquote><p><em>(Nicolao, Irwin, &amp; Goodman, 2009)</em></p><p> </p><blockquote> Sheldon and Lyubomirsky (2007) have posited that there is much room for improvement in one\u2019s happiness. They suggest that while the largest part of our level of happiness is preset by our genetic endowment (around 50%), some 40 per cent is still modifiable (the last 10% is due to uncontrollable circumstances) and the best way to do this is through what they call \u201cintentional activity engagement\u201d.<br>  </blockquote><blockquote>They recommend engaging in interesting, fun activities that fit one\u2019s personality and dispositions, that can vary in content, and that are not merely engaged in as a routine but when people feel like doing it. We agree with such a recommendation, especially as Sheldon and Lyubomirsky\u2019s definition of intentional activity is rather close to that of harmonious passion. </blockquote><p><em>(Philippe et al., 2009)</em></p><p> </p><blockquote> In Kasser\u2019s view, the secret to SWB is meeting one\u2019s intrinsic needs, which means pursuing intrinsic goals out of an intrinsic motivation. In this way, it is similar to Csikszentmihalyi\u2019s (1999) view that happiness stems from \u201cflow\u201d experiences, which are also intrinsically motivated. I contend that the shift toward individualistic cultures that accompanies economic development helps people create life-styles that are consis- tent with their preferences and aptitudes (Veenhoven, 1999), and in so doing pursue their intrinsic needs.</blockquote><p><em>(Ahuvia, 2002)</em></p><p> </p><h4 id=\"Set_goals_that_are_reasonably_challenging_and_reasonably_achievable_\">Set goals that are reasonably challenging and reasonably achievable.</h4><blockquote>One issue concerns the level of challenge posed by one\u2019s goals. When life goals are nonoptimally challenging\u2014either too easy or too difficult\u2014positive affect [emotional well-being] is lower (Csikszentmihalyi &amp; Csikszentmihalyi 1988). Low expectations of success have also been associated with high negative affect (Emmons 1986),</blockquote><p><em>(Ryan &amp; Deci, 2001)</em></p><p> </p><p> </p><h4 id=\"Prefer_intrinsic__vs__extrinsic__goals\">Prefer intrinsic (vs. extrinsic) goals</h4><p>Definition:</p><blockquote> Psychologists make a distinction between two important kinds of goals\u2014intrinsic and extrinsic. Intrinsic goals involve activities and projects that are personally rewarding and meaningful, and that satisfy people's basic needs for competence, relatedness, and autonomy (Kasser &amp; Ryan, 1993, 1996; see Ryan &amp; Deci, 2000, for a review). By contrast, extrinsic goals involve strivings for fame, money, or favorable outward appearances. Research suggests that positive events generated by the fulfillment of intrinsic goals (e.g., making purchases for others rather than yourself) produce more happiness than those generated by extrinsic goals (Dunn, Aknin &amp; Norton, 2008; see also Kasser, 2002; cf. Dunn et al., 2011).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><blockquote> Because high aspirations undermine the benefits of a positive change, are people simply better off with few goals and lowered aspirations? Not necessarily. Ambitious goals held before beginning a new venture motivate people to work harder on that venture and improve their overall performance (Heath, Larrick &amp; Wu, 1999). Individuals would, however, be happier if they focused their monies and efforts on meaningful, intrinsic goals and abandoned extrinsic ones.<br>  </blockquote><blockquote>Extrinsic goals undermine well-being in several ways. First, by their very nature, extrinsic goals do not satisfy people's basic needs directly, if at all. Instead, much like an addiction (Koob &amp; Le Moal, 2001), such goals lead to ever-increasing desires for psychologically unfulfilling commodities (Myers, 2000). Second, extrinsic goals appear to be incompatible with close, meaningful relationships. Those who pursue extrinsic goals report poorer relationships (Kasser &amp; Ryan, 2001). Indeed, even being reminded of money, as Dunn and colleagues (2011) mention, can cause people to be less prosocial and less generous (Vohs, Mead &amp; Goode, 2006), as well as to be perceived as less friendly and likable by others (Vohs, 2010).<br>  </blockquote><blockquote>... over-reliance on external contingencies such as becoming famous, wealthy, or attractive may lead to fragile self-worth (Sheldon, Ryan, Deci &amp; Kasser, 2004). For example, a student seeking a law degree from a prestigious and pricey school with the aim of gaining peer respect might become hopelessly depressed if not admitted. Finally, due to limits of attention, time, and energy, extrinsic goals can lead to the neglect of intrinsic pursuits, which are associated with higher well-being (Vohs et al., 2006).<br>  </blockquote><blockquote>An entrepreneur investing in a new company with the aim of striking it rich might neglect his true interests and hobbies to invest all his energy into his business, and thus miss the need-satisfying personal growth, flow, and joy derived from his more authentic pursuits.<br>  </blockquote><blockquote>Fittingly expressing the futility and unhappiness wrapped up in pursuing extrinsic goals, a notorious New York tabloid editor confessed that he was \u201cpart of that strange race of people aptly described as spending their lives doing things they detest to make money they don't want to buy things they don't need to impress people they dislike\u201d (Gauvreau, 1941). As Benjamin Franklin well knew, money is best directed to goals that directly satisfy personal needs such as affiliation, autonomy, and competence rather than expensive pursuits that are unfulfilling and distracting in the end.<br>  </blockquote><blockquote>In contrast, intrinsic goals, such as building close relationships, making new self-discoveries, and investing in the community, directly activate feelings of satisfaction and con- tentment, which are more likely to be appreciated and less likely to be taken for granted. Dunn and colleagues (2011) rightfully emphasize the link between generosity and well- being, recommending that, to follow the example of Warren Buffett, people spend their money on others rather them themselves.<br>  </blockquote><blockquote>Intrinsic goals can also trigger \u201cupward spirals\u201d\u2014 for example, streams of positive moods and prosocial behavior that gain momentum and reinforce one another as they unfold (Lyubomirsky, King, &amp; Diener, 2005; Norton, Dunn, Aknin &amp; Sandstrom, 2009; Otake, Shimai, Tanaka-Matsumi, Otsui &amp; Fredrickson, 2006).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><hr class=\"dividerBlock\"><h2 id=\"MATERIALISM_AND_PURCHASING\">MATERIALISM AND PURCHASING</h2><p> </p><h4 id=\"The_Hedonic_Treadmill__We_adapt_to_life_changes__Many_things_that_give_pleasure_will_soon_cease_to_do_so__thereby_driving_us_to_seek_more__and_more_\">The Hedonic Treadmill: We adapt to life changes. Many things that give pleasure will soon cease to do so, thereby driving us to seek more, and more\u2026</h4><blockquote> The \u201cpursuit of happiness\u201d is central to the U.S. worldview, yet the very expression also illustrates a paradox of that worldview: Perhaps when one [naively] pursues happiness too single-mindedly, one fails to notice and take advantage of what one already has. In other words, [naively] striving for ever greater happiness may set one on a hedonic treadmill to nowhere. (More on this below)</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4 id=\"Prefer_experiential_purchases__avoid_materialistic_goals__It_is_better_to_collect__positive__experiences_than_to_collect_things_\">Prefer experiential purchases; avoid materialistic goals. It is better to collect (positive) experiences than to collect things.</h4><p>(But do not keep repeating the same positive experience, lest hedonic adaptation set in quicker. See \"Stretching Happiness\" farther down.)</p><p> </p><blockquote> Experiential activities are inherently more social (Caprariello &amp; Reis, 2010; Van Boven &amp; Gilovich, 2003) and for this reason fulfill the psychological need for relatedness (Howell &amp; Hill, 2009).</blockquote><p><em>(Howell et al., 2012)</em></p><p> </p><blockquote>Discretionary experiential purchases ostensibly foster more social contact than discretionary material purchases (Millar &amp; Thomas 2009; Van Boven, 2005), which is a key component to happiness (Argyle, 2001).<br>  </blockquote><blockquote>Research has demonstrated that people are happier with experiential purchases compared to material items.<br>  </blockquote><blockquote>Experiential purchases are more central to positive self-identity than material purchases.<br>  </blockquote><blockquote>Further, experiential purchases may satisfy the personal needs of development and growth more than material acquisitions (Kasser &amp; Ryan, 1996).</blockquote><p><em>(Thomas, 2010)</em></p><p> </p><blockquote> ... we show that on average the most happiness obtained through purchasing is likely to be obtained through experiential purchases that turn out well.<br>  </blockquote><blockquote>... positive social interaction is a major source of happiness; many experiential purchases involve activities with other people, including family.</blockquote><p><em>(Nicolao et al., 2009)</em></p><p> </p><blockquote> Materialism might lead to lower well-being because materialistic people tend to downplay the importance of social relationships and to have a large gap between their incomes and material aspirations (Solberg, Diener, &amp; Robinson, 2004).</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><blockquote> Several studies have documented that a materialistic lifestyle is associated with diminished subjective well-being.<br>  </blockquote><blockquote>... consistent with previous research, we found that materialism is negatively correlated with life satisfaction (Belk 1984, 1985; Burroughs and Rindfleisch 2002; Christopher et al. 2007; Ryan and Dziurawiec 2001; Wright and Larsen 1993).<br>  </blockquote><blockquote>... in line with previous research (Christopher and Schlenker 2004; Chris- topher et al. 2009). High materialistic consumers experience negative emotions more frequently than low materialistic consumers.</blockquote><p><em>(Hudders &amp; Pandelaere, 2011)</em></p><p> </p><blockquote> ... we administered three widely used measures of a materialistic value orientation to 92 business students in Singapore. As expected, those students who had strongly internalized materialistic values also reported lowered self-actualization, vitality and happiness, as well as increased anxiety, physical symptomatology, and unhappiness. (Kasser &amp; Ahuvia, 2002)<br> </blockquote><blockquote>past research demonstrating that materialistic values are associated with experiences of general and existential insecurity (Pyszczynski et al., 1997; Rindfleisch et al., 2009).</blockquote><p><em>(Howell et al., 2012)</em></p><p> </p><blockquote> ... positive experiences not only live on in memories but also lend themselves to even more positive reinterpretations over time as the negative aspects of them fade</blockquote><p><em>(Nicolao et al., 2009)</em></p><p> </p><blockquote> ... when security needs are met, it may be more adaptive to broaden one\u2019s experience and acquire new knowledge, skills, and relationships that often accompany experiential purchases. These experiences, if they do not arouse competing security concerns, may then provide increased SWB with accompanying reductions in feelings of anxiety and insecurity, encouraging further experiential purchases, and resulting in the \u2018upward spiral\u2019 depicted in our model. In this way, the benefits of an experiential purchasing tendency may accrue over a lifetime and individuals may develop stable purchasing habits.</blockquote><p><em>(Howell et al., 2012)</em></p><p> </p><blockquote> In sum, evidence suggests that when looking to spend money, the most satisfying pursuits should involve learning new skills (e.g., mastering a new instrument or learning a foreign language), spending time with others (e.g., taking out one's family to dinner or having coffee with a friend), or doing something good for someone else (e.g., buying Christmas decorations for an elderly neighbor or sending a care package to a sick friend).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><blockquote> We provide evidence that this purchase type by valence interaction is driven by the fact that consumers adapt more slowly to experiential purchases than to material purchases, leading to both greater happiness and greater unhappiness for experiential purchases.<br>  </blockquote><blockquote>adaptation happens more quickly for material purchases than for experiential purchases.</blockquote><p><em>(Nicolao et al., 2009)</em></p><p> </p><h4 id=\"Don_t_engage_in__comparison_shopping___And_don_t_place_much_stock_in_the_happiness_potential_of_any_one_positive_change_\">Don\u2019t engage in \u2018comparison shopping.\u2019 And don\u2019t place much stock in the happiness potential of any one positive change.</h4><p>Comparison shopping makes us aware of previously unimportant differences and makes us forget the salient qualities of what we want.</p><blockquote>Sites like [bizrate.com] offer consumers the opportunity to search for everything [...], comparing a vast range of available options within a given category. [...] Recent research suggests that comparison shopping may sometimes come at a cost. By altering the psychological context in which decisions are made, comparison shopping may distract consumers from attributes of a product that will be important for their happiness, focusing their attention instead on attributes that distinguish the available options.<br>  </blockquote><blockquote>Another problem with comparison shopping is that the comparisons we make when we are shopping are not the same comparisons we will make when we consume what we shopped for (Hsee, Loewenstein, Blount, &amp; Bazerman, 1999; Hsee &amp; Zhang, 2004).<br>  </blockquote><blockquote>One of the dangers of comparison shopping, then, is that the options we don't choose typically recede into the past and are no longer used as standards for comparison.<br>  </blockquote><blockquote>A similar process is likely to unfold in the real estate market. Before purchasing a home, people typically attend scores of open houses and viewings, scrutinizing spec sheets for information about each property's features. Through this process of comparison shopping, the features that distinguish one home from another may come to loom large, while their similarities fade into the background. As a result, home buyers might over- estimate the hedonic consequences of living in a big, beautiful house in a great location versus a more modest home, leading them to take out a larger loan than they can truly afford (potentially sowing the seeds for a nationwide financial crisis).<br>  </blockquote><blockquote>This suggests that consumers who expect a single purchase to have a lasting impact on their happiness might make more realistic predictions if they simply thought about a typical day in their life.<br>  </blockquote><blockquote>Conclusion When asked to take stock of their lives, people with more money report being a good deal more satisfied [eudamonic well being]. But when asked how happy they are at the moment, people with more money are barely different than those with less [hedonic well being] (Diener, Ng, Harter, &amp; Arora, 2010). This suggests that our money provides us with satisfaction when we think about it, but not when we use it. That shouldn't happen. Money can buy many, if not most, if not all of the things that make people happy, and if it doesn't, then the fault is ours. We believe that psychologists can teach people to spend their money in ways that will indeed increase their happiness, and we hope we've done a bit of that here.</blockquote><p><em>(Dunn et al., 2011)</em></p><p> </p><blockquote> When people consider the impact of any single factor on their well-being\u2014not only income\u2014they are prone to exaggerate its importance. We refer to this tendency as the focusing illusion.</blockquote><p><em>(Kahneman, 2006)</em></p><p> </p><h4 id=\"_Follow_the_herd____Dunn_et_al___2011_\">'Follow the herd.' (Dunn et al., 2011)</h4><blockquote> Research suggests that the best way to predict how much we will enjoy an experience is to see how much someone else enjoyed it. In one study, Gilbert, Killingsworth, Eyre, and Wilson (2009) asked women to predict how much they would enjoy a speed date with a particular man. Some of the women were shown the man's photograph and autobiography, while others were shown only a rating of how much a previous woman had enjoyed a speed date with the same man a few minutes earlier. Although the vast majority of the participants expected that those who were shown the photograph and autobiography would make more accurate predictions than those who were shown the rating, precisely the opposite was the case. Indeed, relative to seeing the photograph and autobiography, seeing the rating reduced inaccuracy by about 50%. It appears that the 17th century writer Fran\u00e7ois de La Rochefoucauld was correct when he wrote: \u201cBefore we set our hearts too much upon anything, let us first examine how happy those are who already possess it.\u201d</blockquote><p><em>(Dunn et al., 2011)</em></p><p> </p><hr class=\"dividerBlock\"><p><strong id=\"INTERPERSONAL\">INTERPERSONAL</strong></p><p> </p><h4 id=\"Socialize___with_the_right_people_\">Socialize \u2014 with the right people.</h4><blockquote> ... the effects of wealth are not large, and they are dwarfed by other influences, such as those of personality and social relationships.</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><blockquote> ... it is not only whether you spend your time with others that influences your happiness, but also who you spend your time with. Interaction partners associated with the greatest happiness levels include friends, family, and significant others, whereas bosses and co-workers tend to be associated with the least happiness (Kahneman et al., 2004).<br>  </blockquote><blockquote>social leisure activities contribute more to happiness than solitary ones (Reyes-Garcia et al., 2009).<br>  </blockquote><blockquote>Furthermore, people who frequently engage in social activities experience higher levels of happiness than people who participate in social activities less often (Lloyd &amp; Auld, 2002), and being with others typically improves the quality of an experience (whereas being alone makes most people sad, lonely, or both; Csikszentmihalyi &amp; Larson, 1984; Lewinsohn, Sullivan, &amp; Grosscup, 1982). </blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><blockquote> Compared with the less happy groups, the happiest respondents did not exercise significantly more, participate in religious activities significantly more, or experience more objectively defined good events. No variable was sufficient for happiness, but good social relations were necessary.<br>  </blockquote><blockquote>Our findings suggest that very happy people have rich and satisfying social relationships and spend little time alone relative to average people. [\u2026] In contrast, unhappy people have social relationships that are significantly worse than average.</blockquote><p><em>(Diener &amp; Seligman, 2002)</em></p><p> </p><blockquote> Income and education are more closely related to life evaluation, but health, care giving, loneliness, and smoking are relatively stronger predictors of daily emotions.</blockquote><p><em>(Kahneman &amp; Deaton, 2010)</em></p><p> </p><h4 id=\"Associate_with_happy_people_\">Associate with happy people.</h4><blockquote> ... research has also shown that our relationships with weak ties, and even strangers, can affect our happiness. Using a large-scale, longitudinal dataset, Fowler and Christakis [5] suggested that happiness spreads throughout social networks,extending up to three degrees of separation: a person becomes happier if their friend\u2019s friend\u2019s friend becomes happier, even if they don\u2019t know that person.</blockquote><p><em>(Aknin, Sandstrom, Dunn, &amp; Norton, 2011b)</em></p><p> </p><blockquote> People who are surrounded by many happy people and those who are central in the network are more likely to become happy in the future. Longitudinal statistical models suggest that clusters of happiness result from the spread of happiness and not just a tendency for people to associate with similar individuals. A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25% (95% confidence interval 1% to 57%). Similar effects are seen in coresident spouses (8%, 0.2% to 16%), siblings who live within a mile (14%, 1% to 28%), and next door neighbours (34%, 7% to 70%). Effects are not seen between coworkers. The effect decays with time and with geographical separation.</blockquote><p><em>(Fowler &amp; Christakis, 2008)</em></p><p> </p><h4 id=\"Give_the_people_around_you_opportunities_to_be_generous__Ask_them_for_favors_\">Give the people around you opportunities to be generous. Ask them for favors.</h4><p>You can possibly make people around you happier by allowing them to be kind and generous, and you want to surround yourself with happy people (see above). Aside from making them happier, you will also improve your relationship with them via the <a href=\"http://en.wikipedia.org/wiki/Ben_Franklin_Effect\">Benjamin Franklin effect</a>, which \u2014 unintuitively \u2014 makes people like you more if you ask them for favors.</p><p> </p><h4 id=\"Be_actively_kind__and_occasionaly_reminisce_about_your_recent_acts_of_kindness__\">Be actively kind (and occasionaly reminisce about your recent acts of kindness).</h4><blockquote>Subjective happiness was increased simply by counting one\u2019s own acts of kindness for one week.<br>  </blockquote><blockquote>Happy people became more kind and grateful through the counting kindnesses intervention.<br>  </blockquote><blockquote>Our results further suggest that a reciprocal relationship may exist between kindness and happiness, as has been shown for gratitude and happiness [see below].</blockquote><p><em>(Otake et al., 2006)</em></p><p> </p><hr class=\"dividerBlock\"><p><strong id=\"STRETCHING_HAPPINESS__fighting_hedonic_adaptation_\">STRETCHING HAPPINESS (fighting hedonic adaptation)</strong></p><p> </p><p>Hedonic adaptation \u2014 definition:</p><blockquote> The pleasure of success and the ignominy of failure abate with time. So does the thrill of a new sports car, the pain over a failed romance, the delight over a promotion, and the distress of a scary diagnosis. This phenomenon, known as hedonic adaptation (HA), has drawn increasing interest from both psychologists and economists (e.g., Diener, Lucas, &amp; Scollon, 2006; Easterlin, 2006; Frederick &amp; Loewenstein, 1999; Kahneman &amp; Thaler, 2006; Lucas, 2007a; Lyubomirsky, 2011; Lyubomirsky, Sheldon, &amp; Schkade, 2005; Wilson &amp; Gilbert, 2008).</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4 id=\"Choose_smaller__more_frequent_successes_rather_than_larger_ones_\">Choose smaller, more frequent successes rather than larger ones.</h4><p>Even big positive changes can get old fast, and soon stop bringing happiness.</p><blockquote> ...every one of the published studies evidences fairly rapid and apparently complete adaptation to positive changes. The most widely-cited study is that of Brickman and his colleagues (1978), who reported that lottery winners were no happier up to 18 months after the news than those who had experienced no windfall.</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4 id=\"Go_for_variety_and_surprise__Don_t_keep_doing_the_same_thing_\">Go for variety and surprise. Don\u2019t keep doing the same thing.</h4><blockquote> ...variable stimuli resist adaptation more than do unchanging stimuli (see also Wilson &amp; Gilbert, 2008).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><blockquote> ...these findings support the notion that variety and surprise spice up life in ways that sustain well-being (Sheldon et al., in press; Sheldon &amp; Lyubomirsky, 2006, 2009; Wilson, Centerbar, Kermer, &amp; Gilbert, 2005)\u201d</blockquote><p><em>(Sheldon &amp; Lyubomirsky, 2012)</em></p><p> </p><h4 id=\"Savor_the_anticipation__Delay_consumption__Actively_anticipate_good_experiences_\">Savor the anticipation. Delay consumption. Actively anticipate good experiences.</h4><blockquote> Research in the field of neuroscience has shown that the part of the brain responsible for feeling pleasure, the mesolimbic dopamine system, can be activated when merely thinking about something pleasurable, such as drinking one's favorite brand of beer (McClure, Li, Tomlin, Cypert, Montague, &amp; Montague, 2004) or driving one's favorite type of sports car (Erk, Spitzer, Wunderlich, Galley, &amp; Walter, 2002).<br>  </blockquote><blockquote>... the brain sometimes enjoys anticipating a reward more than receiving the reward (Loewenstein, 1987; Berns, McClure, Pagnoni, &amp; Montague, 2001).<br>  </blockquote><blockquote>... the pleasure derived from window shopping for a dress may exceed the pleasure from actually acquiring the dress.</blockquote><p><em>(Aaker et al., 2010)</em></p><p>(Perhaps the above can inform the discourse on the <a href=\"/lw/hl/lotteries_a_waste_of_hope/\">[ir]rationality of lotteries</a>.)</p><p> </p><h4 id=\"Divide_positive_experiences_into_smaller_pleasures__if_possible_\">Divide positive experiences into smaller pleasures, if possible.</h4><blockquote>Dividing consumption into smaller doses and separating it out over time can multiply [the pleasure of] \u201cfirst bites,\u201d and subsequently, the enjoyment. Savoring a chocolate bar could be as simple as dividing it into squares and eating one piece per day, instead of devouring it all in a single sitting. Research supports the idea that breaks are beneficial for positive experiences, such as enjoying a television program, but detrimental for negative experiences, such as enduring a dental drill (Nelson, Meyvis &amp; Galak, 2009).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p>Dividing into smaller doses also increases the amount of pleasurable anticipation. See previous subsection.</p><p> </p><h4 id=\"Corollary__Conclude_negative_experiences_as_soon_as_possible_\">Corollary: Conclude negative experiences as soon as possible.</h4><p>Don\u2019t \"think about it tomorrow.\u201d Prolongation increases the effect of both negative and positive experiences, and bad is stronger than good:</p><blockquote> Although the same hedonic adaptation process is involved in both positive and negative experiences, an important asymme- try exists between the two that further complicates efforts to remain happy, especially if a positive change comes at a high financial cost. To sum up almost two decades of research, bad is stronger than good (Baumeister, Bratslavsky, Finkenauer &amp; Vohs, 2001; see also Taylor, 1991), or as Einstein quipped, \u201cPut your hand on a hot stove for a minute, and it seems like an hour. Sit with a pretty girl for an hour, and it seems like a minute.\u201d [...] positive changes are weaker than negative changes, and that their effects also evaporate more quickly (e.g., Nezlek &amp; Gable, 2001; Sheldon, Ryan &amp; Reis, 1996; see also Oishi, Diener, Choi, Kim-Prieto &amp; Choi, 2007).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h4 id=\"Make_a_point_of_avoiding_experiences_that_make_you_feel_bad_\">Make a point of avoiding experiences that make you feel bad.</h4><blockquote> Well-being is about more than just frequently feeling good\u2014it is also about infrequently feeling bad (Diener, Suh, Lucas &amp; Smith, 1999).<br>  </blockquote><blockquote>All else being equal, the elimination of negative experiences could provide a three- to five-fold hedonic return on investment over the creation of positive experiences, due to positive/negative asymmetry (e.g., David, Green, Martin &amp; Suls, 1997; Fredrickson &amp; Losada, 2005; Gottman, 1994).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h2>  </h2><h2 id=\"APPRECIATION\">APPRECIATION</h2><p> </p><h4 id=\"Be_grateful__Count_your_blessings__literally___Recycle_happiness__Reminisce_about_good_experiences_\">Be grateful. Count your blessings (literally). Recycle happiness. Reminisce about good experiences.</h4><blockquote> A number of experiments have demonstrated that the regular practice of gratitude\u2014a practice closely related to and often indistinguishable from appreciation and savoring\u2014brings about significant increases in well-being when performed over the course of 1 to 12 consecutive weeks. For example, relative to performing neutral activities, the intentional and effortful practice of \u201ccounting one's blessings\u201d once a week (Emmons &amp; McCullough, 2003; Froh, Sefick &amp; Emmons, 2008; Lyubo- mirsky, Sheldon, &amp; Schkade, 2005) or penning appreciation letters to individuals who have been kind and meaningful (Boehm, Lyubomirsky, &amp; Sheldon, in press; Lyubomirsky, Dickerhoof, Boehm, &amp; Sheldon, in press; Seligman, Steen, Park &amp; Peterson, 2005) has been shown to produce increases in happiness for as long as 6 months.</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h4 id=\"Think_of_counterfactuals____If_I_didn_t_have_this__what_do_I_lose___\">Think of counterfactuals. (\u201cIf I didn\u2019t have this, what do I lose?\u201d)</h4><blockquote> Another cognitive exercise that directs attention toward existing positive changes or events is counterfactual thinking. This strategy involves mentally subtracting a purchased positive experience from ever having taken place, and enumerating all the subsequent blessings that also would have disappeared (Koo, Algoe, Wilson, &amp; Gilbert, 2008).</blockquote><p><em>(Chancellor &amp; Lyubomirsky, 2011)</em></p><p> </p><h4 id=\"Breathe_deeply__Expand_your_time___by_slowing_down_\">Breathe deeply. Expand your time \u2014 by slowing down.</h4><blockquote> [People feel less rushed and hurried when they] simply breathe more deeply. In one study, subjects who were instructed to take long and slow breaths (vs. short and quick ones) for 5 minutes not only felt there was more time available to get things done, but also perceived their day to be longer.</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><h4 id=\"Stay_in_the_present_\">Stay in the present.</h4><blockquote> One possible benefit of being present-focused is that thinking about the present moment (vs. the future) slows down the perceived passage of time, allowing people to feel less rushed and hurried (Rudd &amp; Aaker, 2010).</blockquote><p><em>(Aaker et al., 2010)</em></p><p> </p><hr class=\"dividerBlock\"><p><strong id=\"OPTIMAL_HAPPIFICATION\">OPTIMAL HAPPIFICATION</strong></p><p> </p><h4 id=\"Actively_want__to_be_happier__Motivation_and_investment_matter_\">Actively <em>want</em>  to be happier. Motivation and investment matter.</h4><blockquote> First, and most important, we found that to become happier, people need both a will and a proper way. The will can come from motivation, expectations, and diligence. The proper way comes from performing the \u201cright\u201d activity, not merely a placebo. Accordingly, we found that motivation and investment in becoming a happier person matters. That is, expressing gratitude and optimism did not generally increase well-being unless a person was truly cognizant of the exercises\u2019 purpose and motivated to improve his or her happiness. Second, effortful pursuit of happiness activities was found to be important to improving and maintaining well- being.<br>  </blockquote><blockquote>... happiness interventions are more than just placebos, but [\u2026] they are most successful when participants know about, endorse, and commit to the intervention.<br>  </blockquote><blockquote>According to our model of well-being change (Lyubomirsky, Sheldon, et al., 2005; Sheldon &amp; Lyubomirsky, 2004), sustainable increases in happiness are possible, but only if pursued under optimal conditions, such as when people are motivated to perform a positive activity, when they bring to bear effort and persistence, and when the activity is a legitimately efficacious one. </blockquote><p><em>(Lyubomirsky et al., 2011)</em></p><p> </p><h4 id=\"Learn_about_the_science_of_happiness__Internalize_the_recommendations_in_this_article_and_in_here_\">Learn about the science of happiness. Internalize the recommendations in this article and in <a href=\"/lw/4su/how_to_be_happy/\">here</a>.</h4><blockquote> ... people often hold incorrect intuitive theories about the determinants of happiness. For instance, they overestimate the impact of specific life events on their experienced well-being with regard to intensity, as well as with regard to duration. (see also Comparison Shopping above)<br>  </blockquote><blockquote>... four major sources for systematic over- and undervaluation of choice options that can be distinguished: (i) the underestimation of adaptation, (ii) distorted memory of past experiences, (iii) the rationalization of decisions, and (iv) false intuitive theories about the sources of future utility.</blockquote><p><em>(Quoidbach et al., 2010)</em></p><p> </p><blockquote>Money is an opportunity for happiness, but it is an opportunity that people routinely squander because the things they think will make them happy often don\u2019t.<br>  </blockquote><blockquote>It is not surprising when wealthy people who know nothing about wine end up with cellars that aren't that much better stocked than their neighbors', and it should not be surprising when wealthy people who know nothing about happiness end up with lives that aren't that much happier than anyone else's.</blockquote><p><em>(Dunn et al., 2011)</em></p><p> </p><hr class=\"dividerBlock\"><h2 id=\"EXTRA_CONSIDERATIONS\">EXTRA CONSIDERATIONS</h2><p> </p><blockquote> Happiness predicts [future] income.</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p>(^But try not to think of it that way!)</p><p> </p><blockquote> ... we found that with all but one specification, initial happiness levels were positively and significantly correlated with future earnings. [\u2026] An additional finding is that the effects of initial period happiness on future income and on future happiness seem to be more consistent across all income groups than are the effects of initial period income on either future income and future happiness. The effects of initial period income seem more important for those at higher levels of income.<br> </blockquote><blockquote>The studies by psychologists that find that happiness has positive effects on future income also find that these effects are stronger at the higher end of the income scale. See Diener and Biswas-Diener (1999).</blockquote><p><em>(Graham, Eggers &amp; Sukhtankar, 2004)</em></p><p> </p><blockquote> ... higher cheerfulness in the first year of college correlated with higher income 19 years or so later, when respondents reached their late 30s; this effect was greatest for those who came from the most affluent families</blockquote><p><em>(Diener &amp; Seligman, 2004)</em></p><p> </p><h4 id=\"It_will_be_easier_to_stay_happy_when_you_become_happier\">It will be easier to stay happy when you become happier</h4><blockquote> ... our findings also dovetail with those of Cohn and Fredrickson (in press) by demonstrating that initial happiness gains can cause a happiness intervention to become self-reinforcing.</blockquote><p><em>(Aknin, Dunn, &amp; Norton, 2011a)</em></p><p> </p><h4 id=\"Happiness_Interventions_Work_\">Happiness Interventions Work!</h4><blockquote> Fordyce (1977, 1983) created an intervention program based on the idea that people's subjective well-being can be increased if they learn to imitate the traits of happy people, such as being organized, keeping busy, spending more time socializing, developing a positive outlook, and working on a healthy personality. Fordyce found that the program produced increases in happiness compared to a placebo control, as ell as compared to participants in conditions receiving only partial information. Most impressive, he found lasting effects of the intervention in follow-up evaluation 9-28 months after the study.</blockquote><p><em>(Diener et al., 2009)</em></p><blockquote> Recently, a number of additional effective interventions on happiness have been reported, ranging from the kindness interventions (Otake, Shimai, Tanaca-Matumi, Otsui, &amp; Fredrickson, 2006) and gratitude interventions (Emmons &amp; McCulough, 2003) to variants of the writing intervention (King, 2001; Lyubomirsky, Sousa, &amp; Dickerhoof, 2006). Recent intervention studies are clearly promising. However, more diverse dependent variables and measuring instruments would be desirabe, as well as explorations of which interventions are most beneficial, and why.</blockquote><p><em>(Diener et al., 2009)</em></p><p> </p><h3 id=\"Extra_extra__Cultural_Differences\">Extra extra: Cultural Differences</h3><blockquote> Veenhoven (1999) found that among poor countries, individualism was negatively associated with happiness; whereas among richer countries, individualism was positively associated with happiness. This suggests that economic growth is part of a complex system of modernization that needs to be seen holistically. Collectivism may exist in poorer countries because it is highly functional in that environment, but it may give way to more individualism as societies modernize and the needs of those societies change. Overall, individualism/collectivism stands out as an extremely promising construct for explaining differences in national average levels of SWB, when investigated holistically as part of the larger social system (Cummins, 1998; Myers and Diener, 1995).<br>  </blockquote><blockquote>... economic development increases SWB by creating a cultural environment where individuals make choices to maximize their happiness rather than meet social obligations (Coleman, 1990; Galbraith, 1992; Triandis, 1989; Triandis et al., 1990; Veenhoven, 1999; Watkins and Liu, 1996). This cultural transformation away from obligation and toward the pursuit of happiness is part of a broader transition away from collectivism and toward individualist cultural values and forms of social organization.<br>  </blockquote><blockquote>Cross-cultural research shows that values like \u201cenjoying life\u201d and leading \u201can exciting life\u201d are stronger in individualist societies, whereas \u201csocial recognition,\u201d \u201cpreserving my public image,\u201d being \u201chumble,\u201d and \u201chonoring parents and elders\u201d are particularly strong in collectivist societies (Triandis et al., 1990, p. 1015). There is no more reason to think that people seek social recognition with the ultimate goal of personal happiness, than there is to think that people seek happiness with the ultimate goal of getting others to think well of them for having such a pleasant affect.</blockquote><p><em>(Ahuvia, 2002)</em></p><p> </p><hr class=\"dividerBlock\"><p><strong id=\"References\">References</strong></p><p> </p><p>Aaker, J. L., Rudd, M., &amp; Mogilner, C. (2010). <em><a href=\"http://www.webcitation.org/66uOtzAB6\">If Money Doesn\u2019t Make You Happy, Consider Time</a>.</em> Journal of Consumer Psychology, 2011.</p><p>Ahuvia, A. C. (2002). <a href=\"http://www.webcitation.org/66v14nlao\"><em>Individualism/collectivism and cultures of happiness: A theoretical conjecture on the relationship between consumption, culture and subjective well-being at the national level</em>.</a> Journal of Happiness Studies, 3(1), 23\u201336. Springer.</p><p>Aknin, L. B., Dunn, E. W., &amp; Norton, M. I. (2011a). <a href=\"http://www.webcitation.org/66v2JBeEg\"><em>Happiness Runs in a Circular Motion: Evidence for a Positive Feedback Loop between Prosocial Spending and Happiness</em>.</a> Journal of Happiness Studies, 13(2), 347\u2013355. doi:10.1007/s10902-011-9267-5</p><p>Aknin, L. B., Sandstrom, G. M., Dunn, E. W., &amp; Norton, M. I. (2011b). <em><a href=\"http://www.webcitation.org/66v2V6ReD\">It's the Recipient That Counts: Spending Money on Strong Social Ties Leads to Greater Happiness than Spending on Weak Social Ties</a></em>. (M. Perc, Ed.)PLoS ONE, 6(2), e17018. doi:10.1371/journal.pone.0017018</p><p>Aknin, L. B., Barrington-Leigh, C. P., Dunn, E. W., Helliwell, J. F., Biswas-Diener, R., Kemeza, I., Nyende, P., et al. (2010). <em><a href=\"http://www.webcitation.org/66v2xT6fq\">Prosocial spending and well-being: cross-cultural evidence for a psychological universal</a></em>. National Bureau of Economic Research.</p><p>Block, J., &amp; Koellinger, P. (2009). <em><a href=\"http://www.webcitation.org/66v3gEkh7\">I Can't Get No Satisfaction\u2014Necessity Entrepreneurship and Procedural Utility</a></em>. Kyklos, 62(2), 191\u2013209. doi:10.1111/j.1467-6435.2009.00431.x</p><p>Chancellor, J., &amp; Lyubomirsky, S. (2011). <em><a href=\"http://www.webcitation.org/66v3qLFdM\">Happiness and thrift: When (spending) less is (hedonically) more</a>.</em> Journal of Consumer Psychology, 21(2), 131.</p><p>DeVoe, S. E., &amp; House, J. (2012). <a href=\"http://www.webcitation.org/66v3vvsDu\"><em>Time, money, and happiness: How does putting a price on time affect our ability to smell the roses</em>?</a> Journal of Experimental Social Psychology, 48(2), 466\u2013474. doi:10.1016/j.jesp.2011.11.012</p><p>Diener, E., Oishi, S., Lucas, R.E. (2009). <em><a href=\"http://www.google.com.ph/books?hl=tl&amp;lr=&amp;id=bLZI4fRofDwC&amp;oi=fnd&amp;pg=PA187&amp;dq=Subjective+Well-Being:+The+science+of+happiness+and+life+satisfaction&amp;ots=FA_teNp-86&amp;sig=Mba-1iK2BC2VlXs9ljxG5QH9Kb8&amp;redir_esc=y#v=onepage&amp;q=Subjective%20WEll-Being%3A%20The%20science%20of%20happiness%20and%20life%20satisfaction&amp;f=false\">Subjective Well-Being: The science of happiness and life satisfaction</a></em>. Oxford Handbook of Positive Psychology, 187-194.</p><p>Diener, E., &amp; Seligman, M. P. (2002). <em><a href=\"http://www.webcitation.org/66v3ytpcX\">Very happy people</a></em>. Psychological Science, 13(1), 81\u201384.</p><p>Diener, E., &amp; Seligman, M. P. (2004). <em><a href=\"http://www.webcitation.org/66v41gxzT\">Beyond money</a></em>. Psychological science in the public interest, 5(1), 1\u201331.</p><p>Drnov\u0161ek, M., \u00d6rtqvist, D., &amp; Wincent, J. (2010). <em><a href=\"http://www.webcitation.org/66v473B0U\">The effectiveness of coping strategies used by entrepreneurs and their impact on personal well-being and venture performance</a></em>. Journal of Economics and Business, 28, 193\u2013220.</p><p>Dunn, E. W., Aknin, L. B., &amp; Norton, M. I. (2008). <em><a href=\"http://www.webcitation.org/66v4C5xDR\">Spending Money on Others Promotes Happiness</a></em>. Science, 319(5870), 1687\u20131688. doi:10.1126/science.1150952</p><p>Dunn, E. W., Ashton-James, C. E., Hanson, M. D., &amp; Aknin, L. B. (2010). <em><a href=\"http://www.webcitation.org/66v4Fy6CR\">On the Costs of Self-interested Economic Behavior: How Does Stinginess Get Under the Skin?</a></em> Journal of Health Psychology, 15(4), 627\u2013633. doi:10.1177/1359105309356366</p><p>Dunn, E. W., Gilbert, D. T., &amp; Wilson, T. D. (2011). <em><a href=\"http://www.webcitation.org/66v4VbCe0\">If money doesn\u201ct make you happy, then you probably aren't spending it right</a></em>. Journal of Consumer Psychology, 21(2), 115.</p><p>Fowler, J. H., &amp; Christakis, N. A. (2008). <em><a href=\"http://www.webcitation.org/66v4ntR9f\">The dynamic spread of happiness in a large social network</a></em>. BMJ: British medical journal, 337, a2338.</p><p>Frey, B. S., &amp; Stutzer, A. (2002). <em><a href=\"http://www.webcitation.org/66v4rgWaP\">The economics of happiness</a>.</em> World Economics, 3(1), 1\u201317.</p><p>Graham, C., Eggers, A., &amp; Sukhtankar, S. (2004). <em><a href=\"http://www.webcitation.org/66v4tajdV\">Does happiness pay?: An exploration based on panel data from Russia</a></em>. Journal of Economic Behavior &amp; Organization, 55(3), 319\u2013342. Elsevier.</p><p>Howell, R. T., Pchelin, P., &amp; Iyer, R. (2012). <em><a href=\"http://www.webcitation.org/66v4wEfar\">The preference for experiences over possessions: Measurement and construct validation of the Experiential Buying Tendency Scale</a></em>. The Journal of Positive Psychology, 7(1), 57\u201371.</p><p>Hudders, L., &amp; Pandelaere, M. (2011). <em><a href=\"http://www.webcitation.org/66vA8qaFI\">The Silver Lining of Materialism: The Impact of Luxury Consumption on Subjective Well-Being</a></em>. Journal of Happiness Studies. doi:10.1007/s10902-011-9271-9</p><p>Kahneman, D. (2006). <em><a href=\"http://www.webcitation.org/66v8HYlMx\">Would You Be Happier If You Were Richer? A Focusing Illusion</a></em>. Science, 312(5782), 1908\u20131910. doi:10.1126/science.1129688</p><p>Kahneman, D., &amp; Deaton, A. (2010). <em><a href=\"http://www.webcitation.org/66v8VGQZq\">High income improves evaluation of life but not emotional well-being</a></em>. Proceedings of the National Academy of Sciences, 107(38), 16489\u201316493. doi:10.1073/pnas.1011492107</p><p>Kasser, T., &amp; Ahuvia, A. (2002). <em><a href=\"http://www.webcitation.org/66v8ZfXqZ\">Materialistic values and well-being in business students</a></em>. European Journal of Social Psychology, 32(1), 137\u2013146. doi:10.1002/ejsp.85</p><p>Lyubomirsky, S., Dickerhoof, R., Boehm, J. K., &amp; Sheldon, K. M. (2011). <em><a href=\"http://www.webcitation.org/66v8qQWzE\">Becoming happier takes both a will and a proper way: An experimental longitudinal intervention to boost well-being</a></em>. Emotion, 11(2), 391.</p><p>Mogilner, C. (2010). <em><a href=\"http://www.webcitation.org/66v8uSg29\">The Pursuit of Happiness: Time, Money, and Social Connection</a></em>. Psychological Science, 21(9), 1348\u20131354. doi:10.1177/0956797610380696</p><p>Nickerson, C., Schwarz, N., Diener, E., &amp; Kahneman, D. (2003). <em><a href=\"http://www.webcitation.org/66vAQV7iZ\">Zeroing in on the Dark Side of the American Dream A Closer Look at the Negative Consequences of the Goal for Financial Success</a></em>. Psychological Science, 14(6), 531\u2013536.</p><p>Nicolao, L., Irwin, J., &amp; Goodman, J. (2009). <em><a href=\"http://www.webcitation.org/66vAnWBxM\">Happiness for Sale: Do Experiential Purchases Make Consumers Happier than Material Purchases?</a></em> Journal of Consumer Research, 36(2), 188\u2013198. doi:10.1086/597049</p><p>Otake, K., Shimai, S., Tanaka-Matsumi, J., Otsui, K., &amp; Fredrickson, B. L. (2006). <em><a href=\"http://www.webcitation.org/66vAqie8B\">Happy People Become Happier through Kindness: A Counting Kindnesses Intervention</a></em>. Journal of Happiness Studies, 7(3), 361\u2013375. doi:10.1007/s10902-005-3650-z</p><p>Philippe, F. L., Vallerand, R. J., &amp; Lavigne, G. L. (2009). <a href=\"http://www.webcitation.org/66vAtQEHN\"><em>Passion does make a difference in people's lives: A look at well-being in passionate and non-passionate individuals</em>.</a> Applied Psychology: Health and Well-Being, 1(1), 3\u201322.</p><p>Quoidbach, J., Dunn, E. W., Petrides, K. V, &amp; Mikolajczak, M. (2010). <em><a href=\"http://www.webcitation.org/66vAwIjQz\">Money Giveth, Money Taketh Away</a></em>. Psychological Science, 21(6), 759.</p><p>Ryan, R. M., &amp; Deci, E. L. (2001). <em><a href=\"http://www.webcitation.org/66vB2CG38\">On happiness and human potentials: A review of research on hedonic and eudaimonic well-being</a></em>. Annual Review of Psychology, 52(1), 141\u2013166.</p><p>Sheldon, K. M., Lyubomirsky, S. (2012).<em><a href=\"http://www.webcitation.org/66vB5ki7d\">The Challenge of Staying Happier: Testing the Hedonic Adaptation Prevention Model</a></em>. Personality and Social Psychology Bulletin. doi:10.1177/0146167212436400</p><p>Stutzer, A., &amp; Frey, B. S. (2010). <em><a href=\"http://www.webcitation.org/66vB7xoAB\">Recent advances in the economics of individual subjective well-being</a></em>. Social Research: An International Quarterly, 77(2), 679\u2013714.</p><p>Thomas, R. L. (2010). <em><a href=\"http://www.webcitation.org/66vBGi63B\">Mediating and moderating variables between discretionary purchases and happiness</a></em>. UNLV Theses/Dissertations/Professional Papers/Capstones. Paper 889.</p>", "sections": [{"title": "Money and Happiness", "anchor": "Money_and_Happiness", "level": 3}, {"title": "Work Satisfaction", "anchor": "Work_Satisfaction", "level": 3}, {"title": "Materialism and Purchasing", "anchor": "Materialism_and_Purchasing", "level": 3}, {"title": "Interpersonal", "anchor": "Interpersonal", "level": 3}, {"title": "Stretching Happiness (fighting hedonic adaptation)", "anchor": "Stretching_Happiness__fighting_hedonic_adaptation_", "level": 3}, {"title": "Appreciation", "anchor": "Appreciation", "level": 3}, {"title": "Optimal Happification", "anchor": "Optimal_Happification", "level": 3}, {"title": "Some Key Terms", "anchor": "Some_Key_Terms", "level": 3}, {"title": "Money and Happiness", "anchor": "Money_and_Happiness1", "level": 2}, {"title": "Spend on others, especially people you are close to. ", "anchor": "Spend_on_others__especially_people_you_are_close_to__", "level": 4}, {"title": "Don't be Stingy.", "anchor": "Don_t_be_Stingy_", "level": 2}, {"title": "Think about time, but don\u2019t think of time in terms of money (\u201cAn hour of my time is worth\u2026\u201d).", "anchor": "Think_about_time__but_don_t_think_of_time_in_terms_of_money___An_hour_of_my_time_is_worth____", "level": 4}, {"title": "Being richer will not necessarily make you happier.", "anchor": "Being_richer_will_not_necessarily_make_you_happier_", "level": 4}, {"title": "Pitfall of being wealthy: your ability to savor positive emotions and experiences will be impaired. Don\u2019t make money your priority. ", "anchor": "Pitfall_of_being_wealthy__your_ability_to_savor_positive_emotions_and_experiences_will_be_impaired__Don_t_make_money_your_priority__", "level": 4}, {"title": "Financial aspirations are bad for family life (and the quality of interpersonal relationships is a strong predictor of happiness).", "anchor": "Financial_aspirations_are_bad_for_family_life__and_the_quality_of_interpersonal_relationships_is_a_strong_predictor_of_happiness__", "level": 4}, {"title": "Don\u2019t live \u2018high\u2019.", "anchor": "Don_t_live__high__", "level": 4}, {"title": "Avoid conspicuous consumption.", "anchor": "Avoid_conspicuous_consumption_", "level": 4}, {"title": "More Recommendations", "anchor": "More_Recommendations", "level": 1}, {"title": "WORK SATISFACTION", "anchor": "WORK_SATISFACTION", "level": 2}, {"title": "Coping with Stress: React pragmatically rather than emotionally.", "anchor": "Coping_with_Stress__React_pragmatically_rather_than_emotionally_", "level": 4}, {"title": "Leaders and Entrepreneurs: Don\u2019t take on too many business partners. (See also AUTONOMY below)", "anchor": "Leaders_and_Entrepreneurs__Don_t_take_on_too_many_business_partners___See_also_AUTONOMY_below_", "level": 4}, {"title": "Prefer the \u2018approach\u2019 path instead of the \u2018avoid\u2019 path.", "anchor": "Prefer_the__approach__path_instead_of_the__avoid__path_", "level": 4}, {"title": "AUTONOMY: Make a point of prefering autonomous goals rather than heteronomous goals (goals imposed/expected by others).", "anchor": "AUTONOMY__Make_a_point_of_prefering_autonomous_goals_rather_than_heteronomous_goals__goals_imposed_expected_by_others__", "level": 4}, {"title": "AUTONOMY: Make sure you have spare discretionary time \u2014 even at financial cost.", "anchor": "AUTONOMY__Make_sure_you_have_spare_discretionary_time___even_at_financial_cost_", "level": 4}, {"title": "Be passionate, but don\u2019t obsess. \u201cPassion Does Make a Difference to People\u2019s Well-Being\u201d (Philippe, Vallerand, & Lavigne, 2009)", "anchor": "Be_passionate__but_don_t_obsess___Passion_Does_Make_a_Difference_to_People_s_Well_Being___Philippe__Vallerand____Lavigne__2009_", "level": 4}, {"title": "Do work that you enjoy doing. Flow.", "anchor": "Do_work_that_you_enjoy_doing__Flow_", "level": 4}, {"title": "Set goals that are reasonably challenging and reasonably achievable.", "anchor": "Set_goals_that_are_reasonably_challenging_and_reasonably_achievable_", "level": 4}, {"title": "Prefer intrinsic (vs. extrinsic) goals", "anchor": "Prefer_intrinsic__vs__extrinsic__goals", "level": 4}, {"title": "MATERIALISM AND PURCHASING", "anchor": "MATERIALISM_AND_PURCHASING", "level": 2}, {"title": "The Hedonic Treadmill: We adapt to life changes. Many things that give pleasure will soon cease to do so, thereby driving us to seek more, and more\u2026", "anchor": "The_Hedonic_Treadmill__We_adapt_to_life_changes__Many_things_that_give_pleasure_will_soon_cease_to_do_so__thereby_driving_us_to_seek_more__and_more_", "level": 4}, {"title": "Prefer experiential purchases; avoid materialistic goals. It is better to collect (positive) experiences than to collect things.", "anchor": "Prefer_experiential_purchases__avoid_materialistic_goals__It_is_better_to_collect__positive__experiences_than_to_collect_things_", "level": 4}, {"title": "Don\u2019t engage in \u2018comparison shopping.\u2019 And don\u2019t place much stock in the happiness potential of any one positive change.", "anchor": "Don_t_engage_in__comparison_shopping___And_don_t_place_much_stock_in_the_happiness_potential_of_any_one_positive_change_", "level": 4}, {"title": "'Follow the herd.' (Dunn et al., 2011)", "anchor": "_Follow_the_herd____Dunn_et_al___2011_", "level": 4}, {"title": "INTERPERSONAL", "anchor": "INTERPERSONAL", "level": 5}, {"title": "Socialize \u2014 with the right people.", "anchor": "Socialize___with_the_right_people_", "level": 4}, {"title": "Associate with happy people.", "anchor": "Associate_with_happy_people_", "level": 4}, {"title": "Give the people around you opportunities to be generous. Ask them for favors.", "anchor": "Give_the_people_around_you_opportunities_to_be_generous__Ask_them_for_favors_", "level": 4}, {"title": "Be actively kind (and occasionaly reminisce about your recent acts of kindness).", "anchor": "Be_actively_kind__and_occasionaly_reminisce_about_your_recent_acts_of_kindness__", "level": 4}, {"title": "STRETCHING HAPPINESS (fighting hedonic adaptation)", "anchor": "STRETCHING_HAPPINESS__fighting_hedonic_adaptation_", "level": 5}, {"title": "Choose smaller, more frequent successes rather than larger ones.", "anchor": "Choose_smaller__more_frequent_successes_rather_than_larger_ones_", "level": 4}, {"title": "Go for variety and surprise. Don\u2019t keep doing the same thing.", "anchor": "Go_for_variety_and_surprise__Don_t_keep_doing_the_same_thing_", "level": 4}, {"title": "Savor the anticipation. Delay consumption. Actively anticipate good experiences.", "anchor": "Savor_the_anticipation__Delay_consumption__Actively_anticipate_good_experiences_", "level": 4}, {"title": "Divide positive experiences into smaller pleasures, if possible.", "anchor": "Divide_positive_experiences_into_smaller_pleasures__if_possible_", "level": 4}, {"title": "Corollary: Conclude negative experiences as soon as possible.", "anchor": "Corollary__Conclude_negative_experiences_as_soon_as_possible_", "level": 4}, {"title": "Make a point of avoiding experiences that make you feel bad.", "anchor": "Make_a_point_of_avoiding_experiences_that_make_you_feel_bad_", "level": 4}, {"title": "APPRECIATION", "anchor": "APPRECIATION", "level": 2}, {"title": "Be grateful. Count your blessings (literally). Recycle happiness. Reminisce about good experiences.", "anchor": "Be_grateful__Count_your_blessings__literally___Recycle_happiness__Reminisce_about_good_experiences_", "level": 4}, {"title": "Think of counterfactuals. (\u201cIf I didn\u2019t have this, what do I lose?\u201d)", "anchor": "Think_of_counterfactuals____If_I_didn_t_have_this__what_do_I_lose___", "level": 4}, {"title": "Breathe deeply. Expand your time \u2014 by slowing down.", "anchor": "Breathe_deeply__Expand_your_time___by_slowing_down_", "level": 4}, {"title": "Stay in the present.", "anchor": "Stay_in_the_present_", "level": 4}, {"title": "OPTIMAL HAPPIFICATION", "anchor": "OPTIMAL_HAPPIFICATION", "level": 5}, {"title": "Actively want  to be happier. Motivation and investment matter.", "anchor": "Actively_want__to_be_happier__Motivation_and_investment_matter_", "level": 4}, {"title": "Learn about the science of happiness. Internalize the recommendations in this article and in here.", "anchor": "Learn_about_the_science_of_happiness__Internalize_the_recommendations_in_this_article_and_in_here_", "level": 4}, {"title": "EXTRA CONSIDERATIONS", "anchor": "EXTRA_CONSIDERATIONS", "level": 2}, {"title": "It will be easier to stay happy when you become happier", "anchor": "It_will_be_easier_to_stay_happy_when_you_become_happier", "level": 4}, {"title": "Happiness Interventions Work!", "anchor": "Happiness_Interventions_Work_", "level": 4}, {"title": "Extra extra: Cultural Differences", "anchor": "Extra_extra__Cultural_Differences", "level": 3}, {"title": "References", "anchor": "References", "level": 5}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "267 comments"}], "headingsCount": 60}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 278, "af": false, "version": "2.0.0", "pingbacks": {"Posts": ["ZbgCx2ntD5eu8Cno9", "vYsuM8cpuRgZS5rYB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T04:04:16.542Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Where Physics Meets Experience", "slug": "seq-rerun-where-physics-meets-experience", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:58.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bdddhfhx4P68EDBWq/seq-rerun-where-physics-meets-experience", "pageUrlRelative": "/posts/bdddhfhx4P68EDBWq/seq-rerun-where-physics-meets-experience", "linkUrl": "https://www.lesswrong.com/posts/bdddhfhx4P68EDBWq/seq-rerun-where-physics-meets-experience", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Where%20Physics%20Meets%20Experience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Where%20Physics%20Meets%20Experience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbdddhfhx4P68EDBWq%2Fseq-rerun-where-physics-meets-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Where%20Physics%20Meets%20Experience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbdddhfhx4P68EDBWq%2Fseq-rerun-where-physics-meets-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbdddhfhx4P68EDBWq%2Fseq-rerun-where-physics-meets-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>Today's post, <a href=\"/lw/ps/where_physics_meets_experience/\">Where Physics Meets Experience</a> was originally published on 25 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Meet the Ebborians, who reproduce by fission. The Ebborian brain is like a thick sheet of paper that splits down its thickness. They frequently experience dividing into two minds, and can talk to their other selves. It seems that their unified theory of physics is almost finished, and can answer every question, when one Ebborian asks: When exactly does one Ebborian become two people?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bqr/seq_rerun_which_basis_is_more_fundamental/\">Which Basis Is More Fundamental?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bdddhfhx4P68EDBWq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.849622827466172e-07, "legacy": true, "legacyId": "15242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WajiC3YWeJutyAXTn", "YJofGbvu78S8ceFoz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T04:31:05.018Z", "modifiedAt": null, "url": null, "title": "Open Thread, April 16 - 30, 2012", "slug": "open-thread-april-16-30-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "C3Sc9XJXd5AsAyksf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xizRCrxpddvLnN94x/open-thread-april-16-30-2012", "pageUrlRelative": "/posts/xizRCrxpddvLnN94x/open-thread-april-16-30-2012", "linkUrl": "https://www.lesswrong.com/posts/xizRCrxpddvLnN94x/open-thread-april-16-30-2012", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20April%2016%20-%2030%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20April%2016%20-%2030%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxizRCrxpddvLnN94x%2Fopen-thread-april-16-30-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20April%2016%20-%2030%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxizRCrxpddvLnN94x%2Fopen-thread-april-16-30-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxizRCrxpddvLnN94x%2Fopen-thread-april-16-30-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_bdb\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xizRCrxpddvLnN94x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.849736268919259e-07, "legacy": true, "legacyId": "15244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T06:11:24.129Z", "modifiedAt": null, "url": null, "title": "How was your meetup?", "slug": "how-was-your-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aMa2S7AbL3zBfyD44/how-was-your-meetup", "pageUrlRelative": "/posts/aMa2S7AbL3zBfyD44/how-was-your-meetup", "linkUrl": "https://www.lesswrong.com/posts/aMa2S7AbL3zBfyD44/how-was-your-meetup", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20was%20your%20meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20was%20your%20meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMa2S7AbL3zBfyD44%2Fhow-was-your-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20was%20your%20meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMa2S7AbL3zBfyD44%2Fhow-was-your-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMa2S7AbL3zBfyD44%2Fhow-was-your-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>Lots of meetups recently - great to see! &nbsp;We hear surprisingly little about them here on LW. &nbsp;Did you attend or host a meetup recently? How did it go?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aMa2S7AbL3zBfyD44", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 8.850160804283035e-07, "legacy": true, "legacyId": "15248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T08:37:09.827Z", "modifiedAt": null, "url": null, "title": "Meetup : Tel Aviv, Israel", "slug": "meetup-tel-aviv-israel", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:01.429Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4cFva3dK7sekZWxTY/meetup-tel-aviv-israel", "pageUrlRelative": "/posts/4cFva3dK7sekZWxTY/meetup-tel-aviv-israel", "linkUrl": "https://www.lesswrong.com/posts/4cFva3dK7sekZWxTY/meetup-tel-aviv-israel", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tel%20Aviv%2C%20Israel&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tel%20Aviv%2C%20Israel%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cFva3dK7sekZWxTY%2Fmeetup-tel-aviv-israel%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tel%20Aviv%2C%20Israel%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cFva3dK7sekZWxTY%2Fmeetup-tel-aviv-israel", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4cFva3dK7sekZWxTY%2Fmeetup-tel-aviv-israel", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/96\">Tel Aviv, Israel</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 April 2012 07:00:00PM (+0300)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Cafe Aroma, London Ministore, Sderot Sha'ul HaMelech, Tel Aviv, Israel</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Let's have a LW meetup in Tel Aviv. There are currently no ongoing meetings here - if this one goes well, we can repeat it.</p>\n<p>I'll be sitting at the Aroma cafe on the corner of Shaul HaMelech and Ibn Gvirol. I commit to being there this Saturday Apr 21 during 19:00-21:00 regardless of who else is coming. There'll be a LW sign on the table.</p>\n<p>We'll do introductions, find common topics to talk about, and maybe play a rationality game. If you've never done a LW meetup, don't let that hinder you (I haven't either).</p>\n<p>Any questions or issues are welcome here in comments, or send me a PM. If you plan to come, I'd appreciate a heads-up so I have some idea how many people plan to be there, but don't feel obliged to do that - feel free to just show up without telling anyone.</p>\n<p>Looking forward to meeting you!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/96\">Tel Aviv, Israel</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4cFva3dK7sekZWxTY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "15265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv__Israel\">Discussion article for the meetup : <a href=\"/meetups/96\">Tel Aviv, Israel</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 April 2012 07:00:00PM (+0300)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Cafe Aroma, London Ministore, Sderot Sha'ul HaMelech, Tel Aviv, Israel</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Let's have a LW meetup in Tel Aviv. There are currently no ongoing meetings here - if this one goes well, we can repeat it.</p>\n<p>I'll be sitting at the Aroma cafe on the corner of Shaul HaMelech and Ibn Gvirol. I commit to being there this Saturday Apr 21 during 19:00-21:00 regardless of who else is coming. There'll be a LW sign on the table.</p>\n<p>We'll do introductions, find common topics to talk about, and maybe play a rationality game. If you've never done a LW meetup, don't let that hinder you (I haven't either).</p>\n<p>Any questions or issues are welcome here in comments, or send me a PM. If you plan to come, I'd appreciate a heads-up so I have some idea how many people plan to be there, but don't feel obliged to do that - feel free to just show up without telling anyone.</p>\n<p>Looking forward to meeting you!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Tel_Aviv__Israel1\">Discussion article for the meetup : <a href=\"/meetups/96\">Tel Aviv, Israel</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tel Aviv, Israel", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv__Israel", "level": 1}, {"title": "Discussion article for the meetup : Tel Aviv, Israel", "anchor": "Discussion_article_for_the_meetup___Tel_Aviv__Israel1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T08:40:57.158Z", "modifiedAt": null, "url": null, "title": "Knowledge value = knowledge quality \u00d7 domain importance", "slug": "knowledge-value-knowledge-quality-domain-importance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:33.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E9Ch3ANCYwGKjbgTh/knowledge-value-knowledge-quality-domain-importance", "pageUrlRelative": "/posts/E9Ch3ANCYwGKjbgTh/knowledge-value-knowledge-quality-domain-importance", "linkUrl": "https://www.lesswrong.com/posts/E9Ch3ANCYwGKjbgTh/knowledge-value-knowledge-quality-domain-importance", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Knowledge%20value%20%3D%20knowledge%20quality%20%C3%97%20domain%20importance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKnowledge%20value%20%3D%20knowledge%20quality%20%C3%97%20domain%20importance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE9Ch3ANCYwGKjbgTh%2Fknowledge-value-knowledge-quality-domain-importance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Knowledge%20value%20%3D%20knowledge%20quality%20%C3%97%20domain%20importance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE9Ch3ANCYwGKjbgTh%2Fknowledge-value-knowledge-quality-domain-importance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE9Ch3ANCYwGKjbgTh%2Fknowledge-value-knowledge-quality-domain-importance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 445, "htmlBody": "<p>Months ago, my roommate and I were discussing someone who had tried to replicate Seth Roberts' <a href=\"http://blog.sethroberts.net/2010/08/13/arithmetic-and-butter/\">butter mind</a>&nbsp;self-experiment.&nbsp;My roommate seemed to be making almost no inference from the person's self-reports, because they weren't part of a scientific study.</p>\n<p>But knowledge does not come in two grades, \"scientific\" and \"useless\".&nbsp;Anecdotes <em>do</em> count as evidence, they are just weak evidence. And well designed scientific studies constitute stronger evidence then poorly designed studies. There's a continuum for knowledge quality.</p>\n<p>Knowing that humans are biased should make us take their stories and ad hoc inferences <em>less</em> seriously, but not discard them altogether.</p>\n<hr />\n<p>There exists some domains where <em>most</em> of our knowledge is fairly low-quality. But that doesn't mean they're not worth study, if the value of information in the domain is high.</p>\n<p>For example, a friend of mine read a bunch of books on negotiation and says <a href=\"http://www.amazon.com/Bargaining-Advantage-Negotiation-Strategies-Reasonable/dp/0143036971/ref=sr_1_1?ie=UTF8&amp;qid=1334551691&amp;sr=8-1\">this</a> is the best one.&nbsp;Flipping through my copy, it looks like the author is mostly just enumerating his own thoughts, stories, and theories. So one might be tempted to discard the book entirely because it isn't very scientific.</p>\n<p>But that would be a mistake. If a smart person thinks about something for a while and comes to a conclusion, that's decent-quality evidence that the conclusion is correct. (If you disagree with me on this point, why do you think about things?)</p>\n<p>And the value of information in the domain of negotiation can be very high: If you're a professional, being able to negotiate your salary better can net you hundreds of thousands over the course of a career. (Anchoring means your salary next year will probably just be an incremental raise from your salary last year, so starting salary is very important.)</p>\n<p>Similarly, <a href=\"http://www.amazon.com/Self-Discipline-10-days-Thinking-Doing/dp/1880115107/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1334559304&amp;sr=1-1\">this</a> self-help book is about as dopey and unscientific as they come. But doing one of the exercises from it years ago destroyed a large insecurity of mine that I was only peripherally aware of. So I probably got more out of it in instrumental terms than I would've gotten out of a chemistry textbook.</p>\n<p>In general, self-improvement seems like a domain of really high importance that's unfortunately flooded with low-quality knowledge. If you invest two hours implementing some self-improvement scheme and find yourself operating 10% more effectively, you'll double your investment in just a week, assuming a 40 hour work week. (<strong><a href=\"/lw/b1f/simple_but_important_ideas/\">ALERT</a></strong>: this seems like a really important point! I'd write an entire post about it, but I'm not sure what else there is to say.)</p>\n<p>Here are some free self-improvement resources where the knowledge quality seems at least middling: <a href=\"http://thinkingthingsdone.com/2010/02/the-lost-chapters.html\">For people who feel like failures</a>.&nbsp;<a href=\"http://calnewport.com/blog/\">For students</a>. <a href=\"http://terrytao.wordpress.com/career-advice/work-hard/\">For mathematicians</a>.&nbsp;<a href=\"http://www.aaronsw.com/weblog/productivity\">Productivity</a> <a href=\"https://chrome.google.com/webstore/detail/laankejkbhbdhmipfmgcngdelahlfoji\">and</a> <a href=\"http://www.sebastianmarshall.com/the-evolution-of-my-timehabitlife-tracking\">general</a>&nbsp;<a href=\"http://jessicamah.com/blog-18-1\">ass</a> <a href=\"http://lifehacker.com/281626/jerry-seinfelds-productivity-secret?tag=softwaremotivation\">kicking</a>&nbsp;(<a href=\"http://dontbreakthechain.com/\">web implementation</a> for that last idea). <a href=\"/lw/1sm/akrasia_tactics_review\">Even</a> <a href=\"http://www.correctcontrarian.com/summary-of-eat-that-frog\">more</a> <a href=\"http://www.markforster.net/\">ass</a>&nbsp;<a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">kicking</a>&nbsp;<a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">ideas</a>&nbsp;that you might have seen already.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E9Ch3ANCYwGKjbgTh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "15264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a3kv7AQuDhTqzhNrP", "rRmisKb45dN7DK4BW", "9o3QBg2xJXcRCxGjS", "Ty2tjPwv8uyPK9vrz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T11:01:00.544Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion vs. Co-operative Explosion", "slug": "intelligence-explosion-vs-co-operative-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:36.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rLzMBxew4S4TevtqB/intelligence-explosion-vs-co-operative-explosion", "pageUrlRelative": "/posts/rLzMBxew4S4TevtqB/intelligence-explosion-vs-co-operative-explosion", "linkUrl": "https://www.lesswrong.com/posts/rLzMBxew4S4TevtqB/intelligence-explosion-vs-co-operative-explosion", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20vs.%20Co-operative%20Explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20vs.%20Co-operative%20Explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLzMBxew4S4TevtqB%2Fintelligence-explosion-vs-co-operative-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20vs.%20Co-operative%20Explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLzMBxew4S4TevtqB%2Fintelligence-explosion-vs-co-operative-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLzMBxew4S4TevtqB%2Fintelligence-explosion-vs-co-operative-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4655, "htmlBody": "<p>Abstract: <em>In the FOOM debate, Eliezer emphasizes 'optimization power', something like intelligence, as the main thing that makes both evolution and humans so powerful. A different choice of <a href=\"http://www.overcomingbias.com/2008/11/abstraction-vs.html\">abstractions</a> says that the main thing that's been giving various organisms - from single-celled creatures to wasps to humans - an advantage is the capability to form superorganisms, thus reaping the gains of specialization and shifting evolutionary selection pressure to the level of the superorganism. There seem to be several ways by which a technological singularity could involve the creation of new kinds of superorganisms, which would then reap benefits above and beyond those that individual humans can achieve, and which would quite likely have quite different values. This strongly suggests that even if one is not worried about the intelligence explosion (because of e.g. finding a hard takeoff improbable), one should still be worried about the co-operative explosion.</em></p>\n<p>After watching <a href=\"http://en.wikipedia.org/wiki/Jonathan_Haidt\">Jonathan Haidt's</a> excellent <a href=\"http://www.ted.com/talks/lang/en/jonathan_haidt_humanity_s_stairway_to_self_transcendence.html\">new TEDTalk</a> yesterday, I bought his latest book, <em><a href=\"http://www.amazon.com/The-Righteous-Mind-Politics-Religion/dp/0307377903\">The Righteous Mind: Why Good People Are Divided by Politics and Religion</a>.</em> At one point, Haidt has a discussion of evolutionary superorganisms - cases where previously separate organisms have joined together into a single superorganism, shifting evolution's selection pressure to operate on the level of the superorganism and avoiding the <a href=\"http://wiki.lesswrong.com/wiki/Group_selection\">usual pitfalls that block group selection</a> (excerpts below). With an increased ability for the previously-separate organisms to co-operate, these new superorganisms can often out-compete simpler organisms.</p>\n<blockquote>\n<p>Suppose you entered a boat race. One hundred rowers, each in a separate rowboat, set out on a ten-mile race along a wide and slow-moving river. The first to cross the finish line will win $10,000. Halfway into the race, you&rsquo;re in the lead. But then, from out of nowhere, you&rsquo;re passed by a boat with two rowers, each pulling just one oar. No fair! Two rowers joined together into one boat! And then, stranger still, you watch as that rowboat is overtaken by a train of three such rowboats, all tied together to form a single long boat. The rowers are identical septuplets. Six of them row in perfect synchrony while the seventh is the coxswain, steering the boat and calling out the beat for the rowers. But those cheaters are deprived of victory just before they cross the finish line, for they in turn are passed by an enterprising group of twenty-four sisters who rented a motorboat. It turns out that there are no rules in this race about what kinds of vehicles are allowed.</p>\n<p>That was a metaphorical history of life on Earth. For the first billion years or so of life, the only organisms were prokaryotic cells (such as bacteria). Each was a solo operation, competing with others and reproducing copies of itself. But then, around 2 billion years ago, two bacteria somehow joined together inside a single membrane, which explains why mitochondria have their own DNA, unrelated to the DNA in the nucleus. These are the two-person rowboats in my example. Cells that had internal organelles could reap the benefits of cooperation and the division of labor (see Adam Smith). There was no longer any competition between these organelles, for they could reproduce only when the entire cell reproduced, so it was &ldquo;one for all, all for one.&rdquo; Life on Earth underwent what biologists call a &ldquo;major transition.&rdquo; Natural selection went on as it always had, but now there was a radically new kind of creature to be selected. There was a new kind of <em>vehicle</em> by which selfish genes could replicate themselves. Single-celled eukaryotes were wildly successful and spread throughout the oceans.</p>\n<p>A few hundred million years later, some of these eukaryotes developed a novel adaptation: they stayed together after cell division to form multicellular organisms in which every cell had exactly the same genes. These are the three-boat septuplets in my example. Once again, competition is suppressed (because each cell can only reproduce if the organism reproduces, via its sperm or egg cells). A group of cells becomes an individual, able to divide labor among the cells (which specialize into limbs and organs). A powerful new kind of vehicle appears, and in a short span of time the world is covered with plants, animals, and fungi. It&rsquo;s another major transition.</p>\n<p>Major transitions are rare. The biologists John Maynard Smith and E&ouml;rs Szathm&aacute;ry count just eight clear examples over the last 4 billion years (the last of which is human societies). But these transitions are among the most important events in biological history, and they are examples of multilevel selection at work. It&rsquo;s the same story over and over again: Whenever a way is found to suppress free riding so that individual units can cooperate, work as a team, and divide labor, selection at the lower level becomes less important, selection at the higher level becomes more powerful, and that higher-level selection favors the most cohesive superorganisms. (A superorganism is an organism made out of smaller organisms.) As these superorganisms proliferate, they begin to compete with each other, and to evolve for greater success in that competition. This competition among superorganisms is one form of group selection. There is variation among the groups, and the fittest groups pass on their traits to future generations of groups.</p>\n<p>Major transitions may be rare, but when they happen, the Earth often changes. Just look at what happened more than 100 million years ago when some wasps developed the trick of dividing labor between a queen (who lays all the eggs) and several kinds of workers who maintain the nest and bring back food to share. This trick was discovered by the early hymenoptera (members of the order that includes wasps, which gave rise to bees and ants) and it was discovered independently several dozen other times (by the ancestors of termites, naked mole rats, and some species of shrimp, aphids, beetles, and spiders). In each case, the free rider problem was surmounted and selfish genes began to craft relatively selfless group members who together constituted a supremely selfish group.</p>\n<p>These groups were a new kind of vehicle: a hive or colony of close genetic relatives, which functioned as a unit (e.g., in foraging and fighting) and reproduced as a unit. These are the motorboating sisters in my example, taking advantage of technological innovations and mechanical engineering that had never before existed. It was another transition. Another kind of group began to function as though it were a single organism, and the genes that got to ride around in colonies crushed the genes that couldn&rsquo;t &ldquo;get it together&rdquo; and rode around in the bodies of more selfish and solitary insects. The colonial insects represent just 2 percent of all insect species, but in a short period of time they claimed the best feeding and breeding sites for themselves, pushed their competitors to marginal grounds, and changed most of the Earth&rsquo;s terrestrial ecosystems (for<br />example, by enabling the evolution of flowering plants, which need pollinators). Now they&rsquo;re the majority, by weight, of all insects on Earth.</p>\n</blockquote>\n<p>Haidt's argument is that <a href=\"http://wiki.lesswrong.com/wiki/Color_politics\">color politics</a> and other political <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">mind-killingness</a> are due to a set of adaptations that temporarily lets people merge into a superorganism and set individual interest aside. To a lesser extent, so are moral intuitions about things such as fairness and proportionality. Yes, it's a <a href=\"http://wiki.lesswrong.com/wiki/Group_selection\">group selection</a> argument. Haidt acknowledges that group selection has been unpopular in biology for a while, but notes that it has also been making a comeback recently, and cites e.g. the work on <a href=\"http://en.wikipedia.org/wiki/Group_selection#Multilevel_selection_theory\">multi-level selection</a> as supporting his thesis. I mention some of his references (which I have not yet read) below.</p>\n<p>Anyway, the reason why I'm bringing this up is that I've been re-reading the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">FOOM debate</a> of late, and in <a href=\"/lw/w3/lifes_story_continues\">Life's Story Continues</a>, Eliezer references some of the same evolutionary milestones as Haidt does. And while Eliezer also mentions that the cells provided a major co-operative advantage that allowed for specialization, he views this merely through the lens of optimization power, and dismisses e.g. unicellular eukaryotes with the words \"meh, so what\".</p>\n<blockquote>\n<p>Cells: Force a set of genes, RNA strands, or catalytic chemicals to share a common reproductive fate.&nbsp; (This is the real point of the cell boundary, not \"protection from the environment\" - it keeps the fruits of chemical labor inside a spatial boundary.)&nbsp; But, as we've defined our abstractions, this is mostly a matter of optimization slope - the quality of the search neighborhood.&nbsp; The advent of cells opens up a tremendously rich new neighborhood defined by <em>specialization</em> and division of labor.&nbsp; It also increases the slope by ensuring that chemicals get to keep the fruits of their own labor in a spatial boundary, so that fitness advantages increase.&nbsp; But does it hit back to the meta-level?&nbsp; How you define that seems to me like a matter of taste.&nbsp; Cells don't quite change the mutate-reproduce-select cycle.&nbsp; But if we're going to define sexual recombination as a meta-level innovation, then we should also define cellular isolation as a meta-level innovation. (<a href=\"/lw/w3/lifes_story_continues/\">Life's Story Continues</a>)</p>\n</blockquote>\n<p>The interesting thing about the FOOM debate is that both Eliezer and Robin seem to talk a lot about the significance of co-operation, but they never quite take it up explicitly. Robin talks about the way that isolated groups typically aren't able to take over the world, because it's much more effective to co-operate with others than try to do everything yourself, or because information within the group tends to leak out to other parties. Eliezer talks about the way that cells allowed the ability for specialization, and how writing allowed human culture to accumulate and people to build on each other's inventions.</p>\n<p>Even as Eliezer talks about intelligence, <a href=\"/lw/w5/cascades_cycles_insight/\">insight</a>, and <a href=\"/lw/w6/recursion_magic/\">recursion</a>, one could view this too as discussion about the power of specialization, co-operation and superorganisms - for intelligence <a href=\"/lw/6z3/modularity_and_buzzy/\">seems to consist</a> of a large number of specialized modules, all somehow merged to work in the same organism. And Robin seems to take the view of large groups of people acting as some kind of a loose superorganism, thus beating smaller groups that try to do things alone:</p>\n<blockquote>\n<p>Independent competitors can more easily displace each another than interdependent ones.&nbsp; For example, since the unit of the industrial revolution <a href=\"http://www.overcomingbias.com/2008/06/britain-was-too.html\">seems to have been</a> Western Europe, Britain who started it did not gain much relative to the rest of Western Europe, but Western Europe gained more substantially relative to outsiders.&nbsp; So as the world becomes <a href=\"http://hanson.gmu.edu/dreamautarky.html\">interdependent</a> on larger scales, smaller groups find it harder to displace others.&nbsp;(<a href=\"http://www.overcomingbias.com/2008/06/singularity-out.html\">Outside View of Singularity</a>)</p>\n</blockquote>\n<blockquote>\n<p>[Today] innovations and advances in each part of the world depending on advances made in all other parts of the world. &hellip; Visions of a local singularity, in contrast, imagine that sudden technological advances in one small group essentially allow that group to suddenly grow big enough to take over everything. &hellip; The key common assumption is that of a very powerful but autonomous area of technology.&nbsp; Overall progress in that area must depend only on advances in this area, advances that a small group of researchers can continue to produce at will. And great progress in this area alone must be sufficient to let a small group essentially take over the world. &hellip;</p>\n<p>[Consider also] complaints about the great specialization in modern academic and intellectual life.&nbsp; People complain that ordinary folks should know more science, so they can judge simple science arguments for themselves. &hellip; Many want policy debates to focus on intrinsic merits, rather than on appeals to authority.&nbsp; Many people wish students would study a wider range of subjects, and so be better able to see the big picture.&nbsp; And they wish researchers weren&rsquo;t so penalized for working between disciplines, or for failing to cite every last paper someone might think is related somehow.</p>\n<p>It seems to me plausible to attribute all of these dreams of autarky to people not yet coming fully to terms with our newly heightened interdependence. &hellip; We picture our ideal political unit and future home to be the largely self-sufficient small tribe of our evolutionary heritage. &hellip; I suspect that future software, manufacturing plants, and colonies will typically be much more dependent on everyone else than dreams of autonomy imagine. Yes, small isolated entities are getting more capable, but so are small non-isolated entities, and the later remain far more capable than the former. The riches that come from a worldwide division of labor have rightly seduced us away from many of our dreams of autarky. We may fantasize about dropping out of the rat race and living a life of ease on some tropical island. But very few of us ever do. (<a href=\"http://www.overcomingbias.com/2008/11/dreams-of-autar.html\">Dreams of Autarky</a>)</p>\n</blockquote>\n<p>Robin has also explicitly made the point that it is the difficulty of co-operation which suggests that we can keep ourselves safe from uploads or AIs with hostile intentions:</p>\n<blockquote>\n<p>What if uploads decide to take over by force, refusing to pay back their loans and grabbing other forms of capital? Well for comparison, consider the question: What if our children take over, refusing to pay back their student loans or to pay for Social Security? Or consider: What if short people revolt tonight, and kill all the tall people?</p>\n<p>In general, most societies have many potential subgroups who could plausibly take over by force, if they could coordinate among themselves. But such revolt is rare in practice; short people know that if they kill all the tall folks tonight, all the blond people might go next week, and who knows where it would all end? And short people are highly integrated into society; some of their best friends are tall people.</p>\n<p>In contrast, violence is more common between geographic and culturally separated subgroups. Neighboring nations have gone to war, ethnic minorities have revolted against governments run by other ethnicities, and slaves and other sharply segregated economic classes have rebelled.</p>\n<p>Thus the best way to keep the peace with uploads would be to allow them as full as possible integration in with the rest of society. Let them live and work with ordinary people, and let them loan and sell to each other through the same institutions they use to deal with ordinary humans. Banning uploads into space, the seas, or the attic so as not to shock other folks might be ill-advised. Imposing especially heavy upload taxes, or treating uploads as property, as just software someone owns or as non-human slaves like dogs, might be especially unwise. (<a href=\"/hanson.gmu.edu/uploads.html\">If Uploads Come First</a>)</p>\n</blockquote>\n<p>Situations like war or violent rebellions are, arguably, cases where the \"human superorganism adaptations\" kick in the strongest - where people have the strongest propensity to view themselves primarily as a part of a group, and where they are the most ready to sacrifice themselves for the interest of the group. Indeed, Haidt quotes (both in the book and <a href=\"http://www.ted.com/talks/lang/en/jonathan_haidt_humanity_s_stairway_to_self_transcendence.html\">the TEDTalk</a>) former soldiers who say that there's something very unique in the states of consciousness that war can produce:</p>\n<blockquote>\n<p><span class=\"transcriptLink\">So many books about war say the same thing,</span> <span class=\"transcriptLink\">that nothing brings people together</span> <span class=\"transcriptLink\">like war.</span> <span class=\"transcriptLink\">And that bringing them together opens up the possibility</span> <span class=\"transcriptLink\">of extraordinary self-transcendent experiences.</span> <span class=\"transcriptLink\">I'm going to play for you an excerpt</span> <span class=\"transcriptLink\">from this book by Glenn Gray.</span> <span class=\"transcriptLink\">Gray was a soldier in the American army in World War II.</span> <span class=\"transcriptLink\">And after the war he interviewed a lot of other soldiers</span> <span class=\"transcriptLink\">and wrote about the experience of men in battle.</span> <span class=\"transcriptLink\">Here's a key passage</span> <span class=\"transcriptLink\">where he basically describes the staircase.</span></p>\n<p><span class=\"transcriptLink\">Glenn Gray: Many veterans will admit</span> <span class=\"transcriptLink\">that the experience of communal effort in battle</span> <span class=\"transcriptLink\">has been the high point of their lives.</span> <span class=\"transcriptLink\">\"I\" passes insensibly into a \"we,\"</span> <span class=\"transcriptLink\">\"my\" becomes \"our\"</span> <span class=\"transcriptLink\">and individual faith</span> <span class=\"transcriptLink\">loses its central importance.</span> <span class=\"transcriptLink\">I believe that it is nothing less</span> <span class=\"transcriptLink\">than the assurance of immortality</span> <span class=\"transcriptLink\">that makes self-sacrifice at these moments</span> <span class=\"transcriptLink\">so relatively easy.</span> <span class=\"transcriptLink\">I may fall, but I do not die,</span> <span class=\"transcriptLink\">for that which is real in me goes forward</span> <span class=\"transcriptLink\">and lives on in the comrades</span> <span class=\"transcriptLink\">for whom I gave up my life.</span></p>\n</blockquote>\n<p>So Robin, in If Uploads Come First, seems to basically be saying that uploads are dangerous <em>if we let them become superorganisms. </em>Usually, individuals have a large number of their own worries and priorities, and even if they did have much to gain by co-operating, they can't trust each other enough nor avoid the temptation to free-ride enough to really work together well enough to become dangerous.</p>\n<p>Incidentally, this provides an easy rebuttal to the \"corporations are already superintelligent\" claim - while corporations have a variety of mechanisms for trying to provide their employees with the proper incentives, anyone who's worked for a big company knows that they employees tend to follow their own interests, even when they conflict with those of the company. It's certainly nothing like the situation with a cell, where the survival of each cell organ depends on the survival of the whole cell. If the cell dies, the cell organs die; if the company fails, the employees can just get a new job.</p>\n<p>It would seem to me that, whatever your take on the intelligence explosion is, the current evolutionary history would <em>strongly suggest</em> that new kinds of superorganisms - larger, more cohesive than human groups, and less dependent on crippling their own rationality in order to maintain group cohesion - would be a major risk for humanity. This is not to say that an intelligence explosion wouldn't be dangerous as well - I have no idea what a mind that could think 1,000 times faster than me could do - but a co-operative explosion should be considered dangerous even if you thought a hard takeoff via recursive self-improvement (say) was impossible. And many of the ways for creating a superorganism (see below) seem to involve processes that could conceivably lead to the superorganisms having quite different values from humans. Even if no single superorganism could take over, that's not much of a comfort for the ordinary humans who are caught in a crossfire.</p>\n<p>How might a co-operative explosion happen? I see at least three possibilities:</p>\n<ul>\n<li><strong>Self-copying artificial intelligences.</strong> An AI doesn't need to have the evolved idea of a \"self\" whose interests need to be protected, above those of identical copies of the AI. An AI could be programmed to only care about the completion of a single goal (e.g. paperclips), and it could then copy itself freely, knowing that all of those copies will be working towards the same goal.</li>\n<li><strong>Upload copy clans. </strong>Carl Shulman discusses this possibility in <a href=\"http://intelligence.org/upload/WBE-superorganisms.pdf\">Whole Brain Emulation and the Evolution of Superorganisms</a>. Some people might have a view about personal identity which accepts the possibility of somebody deleting you, if there exist close-enough copies of you. In a world where uploading is possible, there could be people who could copy themselves and then have those copies work together in order to further the goals of the joint organism. If the copies were willing to have themselves deleted or be experimented on, they could come up with ways of brain modification that further increased the devotion to the superorganism. Furthermore, each copy could consent to being deleted if it seemed like its interests were drifting apart from those of the organism as a whole.</li>\n<li><strong>Mind coalescences. </strong>In <a href=\"http://www.xuenay.net/Papers/CoalescingMinds.pdf\">Coalescing Minds: Mind Uploading-Related Group Mind Scenarios</a>, I and Harri Valpola discuss the notion of coalesced minds, hypothetical minds created by merging together two brains through a sufficient number of high-bandwidth neural connections. In a world where uploading was possible, the creation of mind coalescences could be relatively straightforward. Then, several independent organisms could <em>literally</em> join together to become a single entity.</li>\n</ul>\n<p>Below are some more excerpts from Haidt's book:</p>\n<blockquote>Many animals are social: they live in groups, flocks, or herds. But only a few animals have crossed the threshold and become ultrasocial, which means that they live in very large groups that have some internal structure, enabling them to reap the benefits of the division of labor. Beehives and ant nests, with their separate castes of soldiers, scouts, and nursery attendants, are examples of ultrasociality, and so are human societies.\n<p>One of the key features that has helped all the nonhuman ultra-socials to cross over appears to be the need to defend a shared nest. [...] H&ouml;lldobler and Wilson give supporting roles to two other factors: the need to feed offspring over an extended period (which gives an advantage to species that can recruit siblings or males to help out Mom) and intergroup conflict. All three of these factors applied to those first early wasps camped out together in defensible naturally occurring nests (such as holes in trees). From that point on, the most cooperative groups got to keep the best nesting sites, which they then modified in increasingly elaborate ways to make themselves even more productive and more protected. Their descendants include the honeybees we know today, whose hives have been described as &ldquo;a factory inside a fortress.&rdquo;</p>\n<p>Those same three factors applied to human beings. Like bees, our ancestors were (1) territorial creatures with a fondness for defensible nests (such as caves) who (2) gave birth to needy offspring that required enormous amounts of care, which had to be given while (3) the group was under threat from neighboring groups. For hundreds of thousands of years, therefore, conditions were in place that pulled for the evolution of ultrasociality, and as a result, we are the only ultrasocial primate. The human lineage may have started off acting very much like chimps,48 but by the time our ancestors started walking out of Africa, they had become at least a little bit like bees.</p>\n<p>And much later, when some groups began planting crops and orchards, and then building granaries, storage sheds, fenced pastures, and permanent homes, they had an even steadier food supply that had to be defended even more vigorously. Like bees, humans began building ever more elaborate nests, and in just a few thousand years, a new kind of vehicle appeared on Earth&mdash;the city-state, able to raise walls and armies. City-states and, later, empires spread rapidly across Eurasia, North Africa, and Mesoamerica, changing many of the Earth&rsquo;s ecosystems and allowing the total tonnage of human beings to shoot up from insignificance at the start of the Holocene (around twelve thousand years ago) to world domination today.</p>\n<p>As the colonial insects did to the other insects, we have pushed all other mammals to the margins, to extinction, or to servitude. The analogy to bees is not shallow or loose. Despite their many differences, human civilizations and beehives are both products of major transitions in evolutionary history. They are motorboats.</p>\n<p>The discovery of major transitions is Exhibit A in the retrial of group selection. Group selection may or may not be common among other animals, but it happens whenever individuals find ways to suppress selfishness and work as a<br />team, in competition with other teams. Group selection creates group-related adaptations. It is not far-fetched, and it should not be a heresy to suggest that this is how we got the groupish overlay that makes up a crucial part of our righteous minds. [...]</p>\n<p>According to Tomasello, human cognition veered away from that of other primates when our ancestors developed shared intentionality. At some point in the last million years, a small group of our ancestors developed the ability to share mental representations of tasks that two or more of them were pursuing together. For example, while foraging, one person pulls down a branch while the other plucks the fruit, and they both share the meal. Chimps never do this. Or while hunting, the pair splits up to approach an animal from both sides. Chimps sometimes appear to do this, as in the widely reported cases of chimps hunting colobus monkeys, but Tomasello argues that the chimps are not really working together. Rather, each chimp is surveying the scene and then taking the action that seems best to him at that moment. Tomasello notes that these monkey hunts are the only time that chimps seem to be working together, yet even in these rare cases they fail to show the signs of real cooperation. They make no effort to communicate with each other, for example, and they are terrible at sharing the spoils among the hunters, each of whom must use force to obtain a share of meat at the end. They all chase the monkey at the same time, yet they don&rsquo;t all seem to be on the same page about the hunt.</p>\n<p>In contrast, when early humans began to share intentions, their ability to hunt, gather, raise children, and raid their neighbors increased exponentially. Everyone on the team now had a mental representation of the task, knew that his or her partners shared the same representation, knew when a partner had acted in a way that impeded success or that hogged the spoils, and reacted negatively to such violations. When everyone in a group began to share a common understanding of how things were supposed to be done, and then felt a flash of negativity when any individual violated those expectations, the first moral matrix was born. (Remember that a matrix is a consensual hallucination.) That, I believe, was our Rubicon crossing.</p>\n<p>Tomasello believes that human ultrasociality arose in two steps. The first was the ability to share intentions in groups of two or three people who were actively hunting or foraging together. (That was the Rubicon.) Then, after several hundred thousand years of evolution for better sharing and collaboration as nomadic hunter-gatherers, more collaborative groups began to get larger, perhaps in response to the threat of other groups. Victory went to the most cohesive groups&mdash;the ones that could scale up their ability to share intentions from three people to three hundred or three thousand people. This was the second step: Natural selection favored increasing levels of what Tomasello calls &ldquo;group-mindedness&rdquo;&mdash;the ability to learn and conform to social norms, feel and share group-related emotions, and, ultimately, to create and obey social institutions, including religion. A new set of selection pressures operated within groups (e.g., nonconformists were punished, or at very least were less likely to be chosen as partners for joint ventures) as well as between groups (cohesive groups took territory and other resources from less cohesive groups).</p>\n<p>Shared intentionality is Exhibit B in the retrial of group selection. Once you grasp Tomasello&rsquo;s deep insight, you begin to see the vast webs of shared intentionality out of which human groups are constructed. Many people assume that language was our Rubicon, but language became possible only after our ancestors got shared intentionality. Tomasello notes that a word is not a relationship between a sound and an object. It is an agreement among people who share a joint representation of the things in their world, and who share a set of conventions for communicating with each other about those things. If the key to group selection is a shared defensible nest, then shared intentionality allowed humans to construct nests that were vast and ornate yet weightless and portable. Bees construct hives out of wax and wood fibers, which they then fight, kill, and die to defend. Humans construct moral communities out of shared norms, institutions, and gods that, even in the twenty-first century, they fight, kill, and die to defend.</p>\n</blockquote>\n<p>Haidt's references on this include, though are not limited to, the following:</p>\n<p>Okasha, S. (2006) <em>Evolution and the Levels of Selection.</em> Oxford: Oxford University Press.</p>\n<p>H&ouml;lldobler, B., and E. O. Wilson. (2009) <em>The Superorganism: The Beauty, Elegance, and Strangeness of Insect Societies.</em> New York: Norton.</p>\n<p>Bourke, A. F. G. (2011) <em>Principles of Social Evolution.</em> New York: Oxford University Press.</p>\n<p>Wilson, E. O., and B. H&ouml;lldobler. (2005) &ldquo;Eusociality: Origin and Consequences.&rdquo; <em>Proceedings of the National Academy of Sciences of the United States of America</em> 102:13367&ndash;71.</p>\n<p>Tomasello, M., A. Melis, C. Tennie, E. Wyman, E. Herrmann, and A. Schneider. (Forthcoming) &ldquo;<em>Two Key Steps in the Evolution of Human Cooperation: The Mutualism Hypothesis.</em>&rdquo; Current Anthropology.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rLzMBxew4S4TevtqB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 34, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "15267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RXLoo3pXgY2hjdXrg", "dq3KsCsqNotWc8nAK", "rJLviHqJMTy8WQkow", "jgkWqbNph57rAfPsi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T11:05:22.766Z", "modifiedAt": null, "url": null, "title": "Server Sky: lots of very thin computer satellites", "slug": "server-sky-lots-of-very-thin-computer-satellites", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:05.751Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EXMhqsKKjWEPDFe5e/server-sky-lots-of-very-thin-computer-satellites", "pageUrlRelative": "/posts/EXMhqsKKjWEPDFe5e/server-sky-lots-of-very-thin-computer-satellites", "linkUrl": "https://www.lesswrong.com/posts/EXMhqsKKjWEPDFe5e/server-sky-lots-of-very-thin-computer-satellites", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Server%20Sky%3A%20lots%20of%20very%20thin%20computer%20satellites&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AServer%20Sky%3A%20lots%20of%20very%20thin%20computer%20satellites%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEXMhqsKKjWEPDFe5e%2Fserver-sky-lots-of-very-thin-computer-satellites%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Server%20Sky%3A%20lots%20of%20very%20thin%20computer%20satellites%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEXMhqsKKjWEPDFe5e%2Fserver-sky-lots-of-very-thin-computer-satellites", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEXMhqsKKjWEPDFe5e%2Fserver-sky-lots-of-very-thin-computer-satellites", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1337, "htmlBody": "<p><em>The following is intended as 1) request for specific criticisms regarding the value of time investment on this project, and 2) pending favorable answer to this, a request for further involvement from qualified individuals. It is <strong>not</strong> intended as a random piece of interesting <a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6csb\">pop-sci</a>, despite the subject matter, but as a volunteer opportunity.<br /></em></p>\n<p><a href=\"http://server-sky.com/ServerSky\">Server Sky</a> is a an engineering proposal to place thousands (eventually millions) of micron-thin satellites into medium orbit around the earth in the near term. It is being put forth by <a href=\"http://www.keithl.com/\">Keith Lofstrom</a>, the inventor of the <a href=\"http://launchloop.com/LaunchLoop\">Launch Loop</a>.</p>\n<p>Abstract from the <a href=\"http://server-sky.com/slides/serversky24p.pdf\">2009 paper</a>:</p>\n<blockquote>It is easier to move bits than atoms or energy.&nbsp; Server&shy;-sats are ultralight disks of silicon that convert sunlight into computation and communications.&nbsp; Powered by a large solar cell, propelled and steered by light pressure, networked and located by microwaves, and cooled by black&shy;-body radiation. Arrays of thousands of server&shy;-sats form highly redundant computation and database servers, as well as phased array antennas to reach thousands of transceivers on the ground.<br /><br />First generation server&shy;-sats are 20 centimeters across ( about 8 inches ), 0.1 millimeters (100 microns) thick, and weigh 7 grams. They can be mass produced with off&shy;-the&shy;-shelf semiconductor technologies. Gallium arsenide radio chips provide intra&shy;-array, inter&shy;-array, and ground communication, as well as precise location information. Server&shy;-sats are launched stacked by the thousands in solid cylinders, shrouded and vibration-&shy;isolated inside a traditional satellite bus.</blockquote>\n<p><br />Links:</p>\n<p><a href=\"http://server-sky.com/AMSAT\">Papers and Presentations</a></p>\n<p><a href=\"http://server-sky.com/slides/SEsky/\">Slide Show</a></p>\n<p><a href=\"http://server-sky.com/ServerSky\">Wiki Main Page</a></p>\n<p><a href=\"http://server-sky.com/HelpWanted\">Help Wanted </a></p>\n<p><a href=\"http://lists.server-sky.com/mailman/listinfo/server-sky\">Mailing List</a></p>\n<p>Some mildly negative evidence to start with: I have already had a satellite scientist <a href=\"https://plus.google.com/116665417191671711571/posts/HKpN1v4y3Ai\">tell me</a> that this seems unlikely to work. Avoiding space debris and <a href=\"http://en.wikipedia.org/wiki/Kessler_syndrome\">Kessler Syndrome</a>, radio communications difficulties (especially uplink), and the need for precise synchronization are the obstacles he stressed as significant. He did not seem to have studied the proposal closely, but this at least tells us to be careful where to set our priors.</p>\n<p>On the other hand, it appears Keith has given these problems a lot of thought already, and solutions can probably be worked out. The <a href=\"http://server-sky.com/ThinsatV3\">thinsats</a> would have optical thrusters (small solar sails) and would thus be able to move themselves and each other around; defective ones could be collected for disposal without mounting an expensive retrieval mission, and the thrusters would also help avoid things in the first place. Furthermore the zone chosen (the <a href=\"http://server-sky.com/OrbitsV01\">m288 orbit</a>) is relatively unused, so <a href=\"http://server-sky.com/ProblemCollisions\">collisions</a> with other satellites are unlikely. Also the satellites have powerful radar capabilities, which should lead to more easily detecting and eliminating space junk.</p>\n<p>For the communications problem, the idea is to use three dimensional <a href=\"http://server-sky.com/RadioV01\">phased arrays</a> of thinsats -- basically a bunch of satellites in a large block working in unison to generate a specific signal, behaving as if they were a much larger antenna. This is tricky and requires precision timing and exact distance information. The array's physical configuration will need to be randomized (or perhaps arranged according to an optimized pattern) in order to prevent <a href=\"http://server-sky.com/OneDimensionalGratingLobes\">grating lobes</a>, a problem with interference patterns that is common with phased arrays. They would link with GPS and each other by radio on multiple bands to achieve \"micron-precision thinsat location and orientation within the array\".</p>\n<p>According to the wiki, the most likely technical show-stopper (which makes sense given the fact that m288 is outside of the inner Van Allen belt) is <a href=\"http://server-sky.com/RadiationDamage\">radiation damage</a>. Proposed fixes include periodic annealing (heating the circuit with a heating element) to repair the damage, and the use of radiation-resistant materials for circuitry.</p>\n<p>Has anyone else here researched this idea, or have relevant knowledge? It seems like a great potential source of computing power for AI research, mind uploads, and so forth, but also for all those mundane, highly lucrative near term demands like web hosting and distributed business infrastructures.</p>\n<p>From an altruistic standpoint, this kind of system could reduce poverty and increase equitable distribution of computing resources. It could also make solving hard scientific problems like aging and cryopreservation easier, and pave the road to solar power satellites. As it scales, it should also create demand (as well as available funding and processing power) for Launch Loop construction, or some other similarly low-cost form of space travel.</p>\n<p><strong>Value of information</strong> as to whether it can work or not therefore <em>appears</em> to be extremely high, something I think is crucial for a rationalist project. If it <em>can</em> work, the value of taking productive action (leadership, getting it funded, working out the problems, etc.) should be correspondingly high as well.</p>\n<hr />\n<p>Update: Keith Lofstrom has <a href=\"http://server-sky.com/Brin2012Mar17\">responded</a> on the wiki to the questions raised by the satellite scientist.</p>\n<p><em>Note: Not all aspects of the project have complete descriptions yet, but there are answers to a lot of questions in the wiki.</em></p>\n<p>Here is a summary list of questions raised and answers so far:<em><br /></em></p>\n<ul>\n<li>How does this account for Moore's Law? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6co4\">kilobug</a>)<br />In his <a rel=\"nofollow\" href=\"http://server-sky.com/Brin2012Mar17\">reply</a> to the comments on Brin's post, Keith Lofstrom mentions using obsolete sats as ballast for much thinner sats that would be added to the arrays as the manufacturing process improves. Obsolete sats would not stay in use for long.</li>\n<li>What about ping time limits? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6co4\">kilobug</a>)<br /><a rel=\"nofollow\" href=\"http://server-sky.com/OrbitsV01\">Ping times</a> are going to be limited (70ms or so), and worse than you can theoretically get with a fat pipe (42ms), but it is still much better than you get with GEO (250+ ms). This is bad for high frequency trading, but fine for (parallelizable) number crunching and most other practical purposes.</li>\n<li>What kind of power consumption? Doesn't it cost more to launch than you save? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ckp\">Vanvier</a>)<br /> It takes <a rel=\"nofollow\" href=\"http://server-sky.com/LaunchEnergy\">roughly</a>&nbsp; 2 months for a 3 gram thinsat to pay for the launch energy if it gets 4 watts, assuming 32% fuel manufacturing efficiency. Blackbody cooling is another benefit.</li>\n<li>Bits being flipped by cosmic radiation is a problem on earth, how can it be solved in space? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ckp\">Vanvier</a>)<br />Flash memory is <a rel=\"nofollow\" href=\"http://server-sky.com/Manufacturing\">acknowledged</a> to be the most radiation sensitive component of the satellite. The solution would involve extensive error correction software and caching on multiple satellites. </li>\n<li>Periodic annealing tends to short circuits. Wouldn't this result in very short lifetimes? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ckp\">Vanvier</a>)<br />Circuits will be <a rel=\"nofollow\" href=\"http://server-sky.com/Manufacturing\">manufactured</a> as two dimensional planes, which don't short as easily. Another significant engineering challenge: Thermal properties in the glass will need to be matched with the silicon and wires (for example, slotted wiring with silicon dioxide between the gaps) to prevent circuit damage. Per Vanvier, it <a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ctq\">may</a> be less expensive to replace silicon with other materials for this purpose.</li>\n<li>What are the specific of putting servers in space? (<a href=\"/r/discussion/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6dbd\">ZankerH</a>)<br />Efficient power/cooling, increased communications, overall scalability, relative lack of environmental impact.</li>\n</ul>\n<p>Yet to be answered:</p>\n<ul>\n<li>Is the amount of speculative tech too high? E.g. if future kinds of RAM are needed, costs may be higher. (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ctq\">Vanvier</a>)</li>\n<li>Is it easier to replace silicon with something else than find ways to make the rest of the sat match thermal expansion of silicon? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ctq\">Vanvier</a>)</li>\n<li>Can we get more data on economics/business plan? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ctq\">Vanvier</a>)</li>\n<li>Solar sails have been known to stick together. Is this a problem for thinsats, which are shipped stuck together? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6ctq\">Vanvier</a>)</li>\n<li>Do most interesting processes bottleneck on communication efficiency? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6d0i\">skelterpot</a>)</li>\n<li>What decreases in cost might we see with increased manufacturing yield? (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6d07\">skelterpot</a>)</li>\n</ul>\n<p>Insightful comments:</p>\n<ul>\n<li>Launch energy vs energy collection (answer above is more specific, but this was a commendable quick-check).&nbsp; (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6cm5\">tgb</a>)</li>\n<li>ECC RAM is standard technology used in server computers. (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6cp2\">JoachimShipper</a>)</li>\n<li>Fixing bit errors outside the memory (e.g. in CPU) is harder, something like <a href=\"http://en.wikipedia.org/wiki/Tandem_Computers\">Tandem Computers</a> could be used, with added expense. (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6cp2\">JoachimShipper</a>)</li>\n<li>Some processor-heavy computing tasks, like calculating <a rel=\"nofollow\" href=\"http://www.tarsnap.com/scrypt.html\">scrypt</a> hashes, are not very <a href=\"http://en.wikipedia.org/wiki/Embarrassingly_parallel\">parallelizable</a>. (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6d0i\">skelterpot</a>)</li>\n<li>Other approaches like redundant hardware and error-checking within the CPU are possible, but they drive up the die area used. (<a href=\"/lw/bnu/server_sky_lots_of_very_thin_computer_satellites/6d07\">skelterpot</a>)</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EXMhqsKKjWEPDFe5e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 7, "extendedScore": null, "score": 8.851405085086853e-07, "legacy": true, "legacyId": "15114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T15:34:53.811Z", "modifiedAt": null, "url": null, "title": "Interesting rationalist exercise about French election", "slug": "interesting-rationalist-exercise-about-french-election", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:09.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kilobug", "createdAt": "2011-09-02T14:37:51.213Z", "isAdmin": false, "displayName": "kilobug"}, "userId": "7BQMuDSmLE2XRq2ph", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nrcxWSfh3Zohx84kp/interesting-rationalist-exercise-about-french-election", "pageUrlRelative": "/posts/nrcxWSfh3Zohx84kp/interesting-rationalist-exercise-about-french-election", "linkUrl": "https://www.lesswrong.com/posts/nrcxWSfh3Zohx84kp/interesting-rationalist-exercise-about-french-election", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interesting%20rationalist%20exercise%20about%20French%20election&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteresting%20rationalist%20exercise%20about%20French%20election%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrcxWSfh3Zohx84kp%2Finteresting-rationalist-exercise-about-french-election%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interesting%20rationalist%20exercise%20about%20French%20election%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrcxWSfh3Zohx84kp%2Finteresting-rationalist-exercise-about-french-election", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrcxWSfh3Zohx84kp%2Finteresting-rationalist-exercise-about-french-election", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 311, "htmlBody": "<p>The newspaper \"<a href=\"http://www.lemonde.fr\">Le Monde</a>\" made an interesting exercise for rationalists in the context of the French election.</p>\n<p>They first made a classical \"which is your best candidate\" poll, in which they ask multiple choice questions about various topics (for each question, you must select one answer, and how important the issue is for you), and at the end, they select the candidate that (according to them) is closest to your answers. Nothing new in that.</p>\n<p>But then, they made a much more interesting (at least from a rationality training point of view) exercise : they asked the same questions, but not asking \"what is your opinion on the topic ?\" and \"how is this question important for you ?\" but they asked \"what do you think the majority of our readers answered ?\" and \"how do they think they rated the importance of this issue ?\". And then they give you a score from 0 to 1000 on how good your \"predictions\" were.</p>\n<p>It's in French, so it'll be hard for most of you to try it, but if you want it is <a href=\"http://www.lemonde.fr/election-presidentielle-2012/quiz-programmes.html\">available online here</a>.</p>\n<p>I found this kind of exercise (trying to guess what other people will have answered) to be interesting from a LW point of view, because it somehow makes your beliefs (about the opinions, priority and mentalities of French people) <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">pay rent</a>.</p>\n<p>So I wanted to share it with the LW community, and ask if you know about similar exercises elsewhere, that gives you a way to check how accurate your belief network is in complicated issues, if you find them interesting too, and how they could be improved.</p>\n<p>As an idea of improvement, I would like adding a confidence rating to each question, the more confident you feel in your answer, the more points you get if you get it right, but the more you lose if you get it wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nrcxWSfh3Zohx84kp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 8.852546112678483e-07, "legacy": true, "legacyId": "15268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T15:53:37.975Z", "modifiedAt": null, "url": null, "title": "Meetup : First Copenhagen meetup", "slug": "meetup-first-copenhagen-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JoEEjJ3YokMn7uuQp/meetup-first-copenhagen-meetup", "pageUrlRelative": "/posts/JoEEjJ3YokMn7uuQp/meetup-first-copenhagen-meetup", "linkUrl": "https://www.lesswrong.com/posts/JoEEjJ3YokMn7uuQp/meetup-first-copenhagen-meetup", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Copenhagen%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Copenhagen%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJoEEjJ3YokMn7uuQp%2Fmeetup-first-copenhagen-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Copenhagen%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJoEEjJ3YokMn7uuQp%2Fmeetup-first-copenhagen-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJoEEjJ3YokMn7uuQp%2Fmeetup-first-copenhagen-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/97'>First Copenhagen meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 April 2012 05:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frederiksborggade 7, 1360 K\u00f8benhavn, Denmark</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's see how many Less Wrong readers there are in and around Copenhagen! The location for this meetup is the Barresso Coffee at Frederiksborggade 7, 1360 K\u00f8benhavn. That's near the N\u00f8rreport metro station. Let's meet at 17:00. (The coffeehouse closes at 19:00.)</p>\n\n<p>I will be there with a sign that says \"Less Wrong\". In addition to getting to know each other and talking about interesting things, we can have a discussion centered around the prompt \"Describe a time when you felt belonging in a community.\" I look forward to meeting you!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/97'>First Copenhagen meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JoEEjJ3YokMn7uuQp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.85262544314465e-07, "legacy": true, "legacyId": "15269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Copenhagen_meetup\">Discussion article for the meetup : <a href=\"/meetups/97\">First Copenhagen meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 April 2012 05:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frederiksborggade 7, 1360 K\u00f8benhavn, Denmark</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's see how many Less Wrong readers there are in and around Copenhagen! The location for this meetup is the Barresso Coffee at Frederiksborggade 7, 1360 K\u00f8benhavn. That's near the N\u00f8rreport metro station. Let's meet at 17:00. (The coffeehouse closes at 19:00.)</p>\n\n<p>I will be there with a sign that says \"Less Wrong\". In addition to getting to know each other and talking about interesting things, we can have a discussion centered around the prompt \"Describe a time when you felt belonging in a community.\" I look forward to meeting you!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Copenhagen_meetup1\">Discussion article for the meetup : <a href=\"/meetups/97\">First Copenhagen meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Copenhagen meetup", "anchor": "Discussion_article_for_the_meetup___First_Copenhagen_meetup", "level": 1}, {"title": "Discussion article for the meetup : First Copenhagen meetup", "anchor": "Discussion_article_for_the_meetup___First_Copenhagen_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T18:10:35.486Z", "modifiedAt": null, "url": null, "title": "Using People's Irrationality To Do Good by Leslie John", "slug": "using-people-s-irrationality-to-do-good-by-leslie-john", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:58.625Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Utopiah", "createdAt": "2012-03-10T14:57:03.414Z", "isAdmin": false, "displayName": "Utopiah"}, "userId": "gBoAdZcTffQnbsMAb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GN2ALLE4RD7NAbFry/using-people-s-irrationality-to-do-good-by-leslie-john", "pageUrlRelative": "/posts/GN2ALLE4RD7NAbFry/using-people-s-irrationality-to-do-good-by-leslie-john", "linkUrl": "https://www.lesswrong.com/posts/GN2ALLE4RD7NAbFry/using-people-s-irrationality-to-do-good-by-leslie-john", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Using%20People's%20Irrationality%20To%20Do%20Good%20by%20Leslie%20John&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsing%20People's%20Irrationality%20To%20Do%20Good%20by%20Leslie%20John%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGN2ALLE4RD7NAbFry%2Fusing-people-s-irrationality-to-do-good-by-leslie-john%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Using%20People's%20Irrationality%20To%20Do%20Good%20by%20Leslie%20John%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGN2ALLE4RD7NAbFry%2Fusing-people-s-irrationality-to-do-good-by-leslie-john", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGN2ALLE4RD7NAbFry%2Fusing-people-s-irrationality-to-do-good-by-leslie-john", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>http://www.youtube.com/watch?v=MyRPL-QoZG8</p>\n<p>Official description:</p>\n<blockquote>\n<p>Identifying effective obesity treatment is both a clinical challenge and a public health priority. Can monetary incentives stimulate weight loss? Leslie John presents a study that examines different economic incentives for weight loss during a 16 week intervention.<br /><br />Leslie John presented at the \"The Science of Getting People to Do Good\" research briefing at the Stanford Graduate School of Business, co-sponsored by the Center for Social Innovation.<br /><br />Related Links:<br /><a class=\"yt-uix-redirect-link\" title=\"http://csi.gsb.stanford.edu/special-event-science-getting-people-do-good\" dir=\"ltr\" rel=\"nofollow\" href=\"http://csi.gsb.stanford.edu/special-event-science-getting-people-do-good\" target=\"_blank\">http://csi.gsb.stanford.edu/special-event-science-getting-people-do-good</a><br /><br /><a class=\"yt-uix-redirect-link\" title=\"http://drfd.hbs.edu/fit/public/facultyInfo.do?facInfo=ovr&amp;facId=589473\" dir=\"ltr\" rel=\"nofollow\" href=\"http://drfd.hbs.edu/fit/public/facultyInfo.do?facInfo=ovr&amp;facId=589473\" target=\"_blank\">http://drfd.hbs.edu/fit/public/facultyInfo.do?facInfo=ovr&amp;facId=589473</a></p>\n</blockquote>\n<p>Simply sharing this resource here as it could start interesting discussions on moral and rationality.</p>\n<blockquote>\n<p>&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GN2ALLE4RD7NAbFry", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 8.853205378853826e-07, "legacy": true, "legacyId": "15270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-16T18:15:53.428Z", "modifiedAt": null, "url": null, "title": "The Blue School", "slug": "the-blue-school", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y4H4kdZ3RvxfkpSd4/the-blue-school", "pageUrlRelative": "/posts/Y4H4kdZ3RvxfkpSd4/the-blue-school", "linkUrl": "https://www.lesswrong.com/posts/Y4H4kdZ3RvxfkpSd4/the-blue-school", "postedAtFormatted": "Monday, April 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Blue%20School&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Blue%20School%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4H4kdZ3RvxfkpSd4%2Fthe-blue-school%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Blue%20School%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4H4kdZ3RvxfkpSd4%2Fthe-blue-school", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4H4kdZ3RvxfkpSd4%2Fthe-blue-school", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p><a href=\"http://www.nytimes.com/2012/04/15/nyregion/at-the-blue-school-kindergarten-curriculum-includes-neurology.html\">The Blue School</a> appears to be a neuroscience driven playgroup for children to learn about their own neuroanatomy and emotional self-regulation and are given language to describe their feelings...</p>\n<p>\"So young children at the Blue School learn about what has been called  &ldquo;the amygdala hijack&rdquo; &mdash; what happens to their brains when they flip out.  Teachers try to get children into a &ldquo;toward state,&rdquo; in which they are  open to new ideas. Periods of reflection are built into the day for  students and teachers alike, because reflection helps executive  function&nbsp;&mdash; the ability to process information in an orderly way, focus  on tasks and exhibit self-control. Last year, the curriculum guide was  amended to include the term &ldquo;meta-cognition&rdquo;: the ability to think about  thinking.</p>\n<p>&ldquo;Having language for these mental experiences gives children more  chances to regulate their emotions,&rdquo; said David Rock, who is a member of  the Blue School&rsquo;s board and a founder of <a href=\"http://www.neuroleadership.org/\">NeuroLeadership Institute</a>, a global research group dedicated to understanding the brain science of leadership.</p>\n<p>That language is then filtered through a 6-year-old&rsquo;s brain.</p>\n<p>Miles, one of the kindergartners drawing their emotions, showed off his  picture and described the battle it depicted between happiness and anger  this way: &ldquo;The happy fights angry, but angry gets blocked by the force  field and can&rsquo;t get out.&rdquo; Happiness could escape through his mouth,  Miles explained. But anger got trapped, turning into sadness.</p>\n<p>With ample research showing that negative emotions impede learning while  positive emotions broaden children&rsquo;s attention and their ability to  acquire and retain information, strategies for regulating emotions are  getting more emphasis in progressive schools across the country.\"</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1, "Q55STnFh6gbSezRuR": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y4H4kdZ3RvxfkpSd4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 31, "extendedScore": null, "score": 8.853227818313335e-07, "legacy": true, "legacyId": "15271", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T03:42:03.342Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Where Experience Confuses Physicists", "slug": "seq-rerun-where-experience-confuses-physicists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:05.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xij6ij2FXB8HsThYr/seq-rerun-where-experience-confuses-physicists", "pageUrlRelative": "/posts/xij6ij2FXB8HsThYr/seq-rerun-where-experience-confuses-physicists", "linkUrl": "https://www.lesswrong.com/posts/xij6ij2FXB8HsThYr/seq-rerun-where-experience-confuses-physicists", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Where%20Experience%20Confuses%20Physicists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Where%20Experience%20Confuses%20Physicists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxij6ij2FXB8HsThYr%2Fseq-rerun-where-experience-confuses-physicists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Where%20Experience%20Confuses%20Physicists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxij6ij2FXB8HsThYr%2Fseq-rerun-where-experience-confuses-physicists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxij6ij2FXB8HsThYr%2Fseq-rerun-where-experience-confuses-physicists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>Today's post, <a href=\"/lw/pt/where_experience_confuses_physicists/\">Where Experience Confuses Physicists</a> was originally published on 26 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It then turns out that the entire planet of Ebbore is splitting along a fourth-dimensional thickness, duplicating all the people within it. But why does the apparent chance of \"ending up\" in one of those worlds, equal the square of the fourth-dimensional thickness? Many mysterious answers are proposed to this question, and one non-mysterious one.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bre/seq_rerun_where_physics_meets_experience/\">Where Physics Meets Experience</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xij6ij2FXB8HsThYr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.855625904591023e-07, "legacy": true, "legacyId": "15290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vGbHKfgFNDeJohfeN", "bdddhfhx4P68EDBWq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T06:54:18.488Z", "modifiedAt": null, "url": null, "title": "How accurate is the quantum physics sequence?", "slug": "how-accurate-is-the-quantum-physics-sequence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:34.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x3Ckt4T2z4abt7ZKs/how-accurate-is-the-quantum-physics-sequence", "pageUrlRelative": "/posts/x3Ckt4T2z4abt7ZKs/how-accurate-is-the-quantum-physics-sequence", "linkUrl": "https://www.lesswrong.com/posts/x3Ckt4T2z4abt7ZKs/how-accurate-is-the-quantum-physics-sequence", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20accurate%20is%20the%20quantum%20physics%20sequence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20accurate%20is%20the%20quantum%20physics%20sequence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3Ckt4T2z4abt7ZKs%2Fhow-accurate-is-the-quantum-physics-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20accurate%20is%20the%20quantum%20physics%20sequence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3Ckt4T2z4abt7ZKs%2Fhow-accurate-is-the-quantum-physics-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx3Ckt4T2z4abt7ZKs%2Fhow-accurate-is-the-quantum-physics-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p><a href=\"/lw/bql/our_phyg_is_not_exclusive_enough/6cpu?context=5#6cpu\">Prompted by Mitchell Porter</a>, I asked on Physics StackExchange about the accuracy of the physics in the Quantum Physics sequence:</p>\n<p style=\"padding-left: 30px;\">What errors would one learn from Eliezer Yudkowsky's introduction to quantum physics?</p>\n<p style=\"padding-left: 30px;\">Eliezer Yudkowsky wrote an introduction to quantum physics from a strictly realist standpoint. However, he has no qualifications in the subject and it is not his specialty. Does it paint an accurate picture overall? What mistaken ideas about QM might someone who read only this introduction come away with?</p>\n<p>I've had some interesting answers so far, including one from a friend that seems to point up a definite error, though AFAICT not a very consequential one: in <a href=\"/lw/pd/configurations_and_amplitude/\">Configurations and Amplitude</a>, a multiplication factor of <em>i</em> is used for the mirrors where -1 is correct.</p>\n<p>Physics StackExchange:&nbsp;<a href=\"http://physics.stackexchange.com/questions/23785/what-errors-would-one-learn-from-eliezer-yudkowskys-introduction-to-quantum-phy\">What errors would one learn from Eliezer Yudkowsky's introduction to quantum physics?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "csMv9MvvjYJyeHqoo": 2, "JMD7LTXTisBzGAfhX": 5, "zPwTiHduqxnMHCMSu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x3Ckt4T2z4abt7ZKs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 72, "extendedScore": null, "score": 0.000174, "legacy": true, "legacyId": "15303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5vZD32EynD9n94dhr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T11:57:22.405Z", "modifiedAt": null, "url": null, "title": "Question about brains and big numbers", "slug": "question-about-brains-and-big-numbers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:05.373Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tMi8SZEJjkYts2mcx/question-about-brains-and-big-numbers", "pageUrlRelative": "/posts/tMi8SZEJjkYts2mcx/question-about-brains-and-big-numbers", "linkUrl": "https://www.lesswrong.com/posts/tMi8SZEJjkYts2mcx/question-about-brains-and-big-numbers", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20brains%20and%20big%20numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20brains%20and%20big%20numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtMi8SZEJjkYts2mcx%2Fquestion-about-brains-and-big-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20brains%20and%20big%20numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtMi8SZEJjkYts2mcx%2Fquestion-about-brains-and-big-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtMi8SZEJjkYts2mcx%2Fquestion-about-brains-and-big-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p>From time to time I encounter people who claim that our brains are really slow compared to even an average laptop computer and can't process big numbers.</p>\n<p>At the risk of revealing my complete lack of knowledge of neural networks and how the brain works, I want to ask if this is actually true?</p>\n<p>It took massive amounts of number crunching to create movies like James Cameron's <a href=\"http://en.wikipedia.org/wiki/Avatar_%282009_film%29\">Avatar</a>. Yet I am able to create more realistic and genuine worlds in front of my minds eye, on the fly. I can even simulate other agents. For example, I can easily simulate sexual intercourse between me and another human. Which includes tactile and olfactory information.</p>\n<p>I am further able to run real-time egocentric world-simulations to extrapolate and predict the behavior of physical systems and other agents. You can do that too. Having a discussion or playing football are two examples.</p>\n<p>Yet any computer can outperform me at simple calculations.</p>\n<p>But it seems to me, maybe naively so, that most of my human abilities involve massive amounts of number crunching that no desktop computer could do.</p>\n<p>So what's the difference? Can someone point me to some digestible material that I can read up on to dissolve possible confusions I have with respect to my question?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tMi8SZEJjkYts2mcx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 8.85772484748807e-07, "legacy": true, "legacyId": "15306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T12:56:40.358Z", "modifiedAt": null, "url": null, "title": "Who's afraid of impossible worlds?", "slug": "who-s-afraid-of-impossible-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8uBdbEoYuvY3ghgBq/who-s-afraid-of-impossible-worlds", "pageUrlRelative": "/posts/8uBdbEoYuvY3ghgBq/who-s-afraid-of-impossible-worlds", "linkUrl": "https://www.lesswrong.com/posts/8uBdbEoYuvY3ghgBq/who-s-afraid-of-impossible-worlds", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Who's%20afraid%20of%20impossible%20worlds%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWho's%20afraid%20of%20impossible%20worlds%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8uBdbEoYuvY3ghgBq%2Fwho-s-afraid-of-impossible-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Who's%20afraid%20of%20impossible%20worlds%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8uBdbEoYuvY3ghgBq%2Fwho-s-afraid-of-impossible-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8uBdbEoYuvY3ghgBq%2Fwho-s-afraid-of-impossible-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1687, "htmlBody": "<p>In order to clarify the semantics of <a href=\"/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">paraconsistent</a> and relevance logics, we need to make a detour into impossible worlds - a fruitful detour opening up&nbsp;fun new vistas. Note that this is an intuitive introduction to the subject, and logic is probably the area of mathematics where it is the most dangerous to rely on your&nbsp;intuition; this is no substitute for rigorously going through the formal definitions. With that proviso in mind, let's get cracking.</p>\n<p>&nbsp;</p>\n<h2>Possible worlds: the meaning of necessity</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Modal_logic\">Modal logics</a> were developed around the concept of necessity and possibility. They do this with two extra operators: the necessity operator&nbsp;\u25a1, and possibility operator &loz;. The sentence&nbsp;\u25a1A is taken to mean \"it is necessary that A\" and&nbsp;&loz;A means \"it is possible that A\". The two operators are dual to each other &ndash; thus \"it is necessary that A\" is the same as \"it not possible that not-A\" (in symbols:&nbsp;\u25a1A&nbsp;&harr; &not;&loz;&not;A). A few intuitive axioms then lead to an elegant theory.</p>\n<p>There was just one problem: early modal logicians&nbsp;didn't&nbsp;have a clue what they were talking about. They had the <a href=\"http://en.wikipedia.org/wiki/Syntax_(logic)\">syntax</a>, the symbols, the formal rules, but they&nbsp;didn't&nbsp;have the semantics, the models, the <a href=\"http://en.wikipedia.org/wiki/Interpretation_(logic)\">meanings</a>&nbsp;of their symbols.</p>\n<p>To see the trouble they had, imagine someone tossing a coin and covering it with their hand. Call w<sub>H</sub> the world in which it comes out heads and w<sub>T</sub> the world in which it comes out, you guessed it, tails. Now, is the coin necessarily heads? Is it possibly heads?</p>\n<p>Intuitively, the answers should be no and yes. But this causes a problem. We may be in the world w<sub>H</sub>. So if we agree that the coin is not necessarily heads, then it is not necessarily heads even though it is actually heads&nbsp;(forget your Bayescraft here and start thinking like a logician). Similarly, in w<sub>T</sub>, the coin is in&nbsp;actuality&nbsp;tails yet it is possibly heads.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Saul_Kripke\">Saul Kripke</a> found the breakthrough: necessity and possibility are not about individual worlds, but about collections of possible worlds, and relationships between them. In this case, there is a&nbsp;indistinguishability&nbsp;relationship between w<sub>T</sub> and w<sub>H</sub>, because we can't (currently) tell them apart.</p>\n<p>Because of this relationship, the statement A:\"the coin is heads\" is possible in both w<sub>T</sub> and w<sub>H</sub>.&nbsp;The rule is that a statement is possible in world w if it is true in at least one world that is related to w. For w=w<sub>H</sub>,&nbsp;&loz;A&nbsp;is true because A is true in w<sub>H</sub> and w<sub>H</sub> is related to itself. Similarly, for w=w<sub>T</sub>,&nbsp;&loz;A is true because A is true in w<sub>H</sub> and w<sub>H</sub> is related to w<sub>T</sub>.</p>\n<p>Conversely B:\"the coin is heads or the coin is tails\" is necessary in both w<sub>T</sub> and w<sub>H</sub>: here the rule is that a statement is necessary in world w if it is true in all worlds related to w. w<sub>T</sub>&nbsp;and w<sub>H&nbsp;</sub>are related only to each other through the&nbsp;indistinguishability&nbsp;relationship, and B is true in both of them, so&nbsp;\u25a1B is also true in both of them. However&nbsp;\u25a1A is not true in either&nbsp;w<sub>T</sub>&nbsp;or&nbsp;w<sub>H</sub>, because both those worlds are related to w<sub>T</sub> and A is false in w<sub>T</sub>.<a id=\"more\"></a></p>\n<p>This idea was formalised as a <a href=\"http://en.wikipedia.org/wiki/Kripke_semantics\">Kripke structure</a>: a set W of worlds, a binary relationship R between some of the elements of W, and a rule that assigned the truth and falsity of statements in any particular world w. R is called the accessibility relationship (it need not be indistinguishability): wRv means that world v is 'accessible' from world w, and necessity and possibility are define, via R, as above.</p>\n<p>As an extra treat for the less wrong crowd, it should be noted that Kripke structures are used in the semantics of <a href=\"http://en.wikipedia.org/wiki/Common_knowledge_(logic)\">common knowledge</a>, key to the all-important <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann agreement theorem</a>.</p>\n<p>&nbsp;</p>\n<h2>Impossible worlds: when necessary truth is not true, necessarily</h2>\n<p>It should be noted that all the worlds in W are possible worlds: the (classical) laws of logic apply. Hence every tautology is true in every world in W, and every contradiction is false. In standard ('normal') modal logic, there is a necessitation requirement: if A is a tautology, then&nbsp;\u25a1A (\"it is necessary that A\") is true in every world.</p>\n<p>However, what if this were not the case? What if we added the requirement that&nbsp;\u25a1A&nbsp;not be true? Formally and symbolically, this seems a perfectly respectable thing to ask for. But can we come up with a model for this logic?</p>\n<p>Not by using possible worlds, of course: since A is always true, we won't get very far. But we can enlarge our set W to also include <a href=\"http://en.wikipedia.org/wiki/Impossible_worlds\">impossible worlds</a>: worlds where the laws of logic don't apply. Impossible worlds have truth values assigned to statements, but they don't need to be consistent or coherent (\"Socrates is a man\", \"all men are mortal\" and \"Socrates is immortal\" could all be true).</p>\n<p>Now&nbsp;assume there is a world w', where A is not labelled true (since A is a tautology, w' must be an impossible world). Also assume&nbsp;that w' is accessible from possible world w (hence wRw'). Then w is a perfectly respectable possible world, with A true in it; however,&nbsp;\u25a1A is not true in w, because it has an accessible world (w') where A isn't true. Voila!</p>\n<p>This seems like a cheap trick &ndash; a hack to build a model for a quirky odd logic that doesn't feel particularly intuitive anyway. But there are more things to be done with impossible worlds...</p>\n<p>&nbsp;</p>\n<h2>Some possibilities are more possible than others</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/David_Kellogg_Lewis\">Lewis</a> built on the possible world idea to construct a <a href=\"http://en.wikipedia.org/wiki/Counterfactual_conditional\">theory of counterfactuals</a>. Assume that the world is as we know it, and consider the two statements:</p>\n<ul>\n<li>A: If the Titanic didn't hit an iceberg, then it didn't sink on its maiden voyage.</li>\n<li>B: If the Titanic didn't hit an iceberg, then it was elected Pope in the controversial Martian election of 58 BC.</li>\n</ul>\n<p>Both of these statements are true in our world: we can show that the antecedent is contradicted (because the Titanic actually did hit an iceberg), and then imply anything. Nevertheless, there is a strong intuitive sense in which A is 'more true' than B. The consequent of A is 'less extraordinary' than that of B, and seems to derive more directly from the antecedent. A seems at least <a href=\"http://en.wikipedia.org/wiki/Counterfactual_conditional\">counterfactually</a> truer than B.</p>\n<p>We can't use the Kripke accessibility relationship to encode this: R is a simple yes or no binary, with no gradation. Lewis suggested that we instead use a measure that encodes relative distance between possible worlds, some way of saying that world v is closer to w than u is. Some way of saying that a world where the English invented chocolate is less distant from us than a world in which all the English are made of chocolate.</p>\n<p>With this distance, we can&nbsp;rigorously state whether A and B are true as counterfactual statements. Define C:\"the Titanic didn't hit an iceberg\", D:\"the Titanic didn't sink on its maiden voyage\" and E:\"the Titanic was elected Pope in the controversial Martian election of 58 BC.\"</p>\n<p>So, how shall we encode \"C counterfactually implies D (in our world)\"? Well, this could be true if C were false in every possible world; but no such luck. Otherwise, we could look for worlds where C is true and D is also true; yes, but there are also possible worlds where C is true but D is false (such as the world where the Titanic swerved to avoid the iceberg only to get sunk by the <a href=\"http://en.wikipedia.org/wiki/Nadia:_The_Secret_of_Blue_Water\">Nautilus</a>). This is where the distance between worlds comes in: we say that \"C counterfactually implies D (in our world)\" if D is true in w, where w is the closest world to our own where C is true. For any intuitive distance measure, it does seem quite likely that C does counterfactually imply D (in our world): most ships cross the Atlantic safely, so if the Titanic hadn't hit an iceberg, it should have done so as well.</p>\n<p>Basically a counterfactual is true if the consequent is true in the <em>least weird</em> world where the antecedent is true.&nbsp;A similar approach shows that \"C counterfactually implies E (in our world)\" is false. Unless we have made some very peculiar choices for our distance measure, the closest world to us in which the Titanic didn't hit an iceberg, is not one in which a large metal contraption was elected Pope on an uninhabited planet thousands of years before it was even built.</p>\n<p>So far, this seems pretty intuitive; but what about this \"distance measure\" between possible worlds, which seems to be doing all the work? Where does that come from? Well, that, of course, is left as an exercise to the interested reader. By which I mean the problem is unsolved. But there is progress: we started with an unsolved informal problem (what is a counterfactual) and replaced it with an unsolved formal problem (how to specify the relative distance between possible worlds). Things are looking up!</p>\n<p>&nbsp;</p>\n<h2>Some impossibilities are less impossible than others</h2>\n<p>It should be noted that in the previous section, the fact that the worlds were logically consistent was never used. This immediately suggests we can extend the setup to include impossible worlds as well. This was Edwin Mares's <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.ndjfl/1039540767\">idea</a>, and it allowed him to construct models for the <a href=\"/lw/bgx/logic_of_paradox_a_too_simple_paraconsistent_logic/\">logic of paradox</a>.</p>\n<p>All that we need is our magical \"relative distance between worlds\" measure, as before, except we now measure&nbsp;relative&nbsp;distance to impossible worlds as well. With that, we can start talking about the truths of counterpossible statements - conditional statements where the antecedent is contradictory. Assume for instance that both Euclidean geometry and Peano arithmetic are true, and consider:</p>\n<ul>\n<li>A: the existence of an equilateral triangle with a right angle counterpossibly implies the existence of a triangle whose angles sum up to 270 degrees.</li>\n<li>B: the existence of an equilateral triangle with a right angle counterpossibly implies the existence of many even primes.</li>\n</ul>\n<p>For many intuitive distance-between-worlds measures, A is true in our world and B is false. This is, for instance, the case for some distance measures that penalise doubly impossible worlds (worlds where Euclidean geometry and Peano arithmetic both have contradictions) more than singly impossible worlds (where only one of the theories have contradictions). Again, the informal mystery about counterpossibles is reduced to the formal problem of specifying this distance measure between worlds.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8uBdbEoYuvY3ghgBq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "15266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In order to clarify the semantics of <a href=\"/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">paraconsistent</a> and relevance logics, we need to make a detour into impossible worlds - a fruitful detour opening up&nbsp;fun new vistas. Note that this is an intuitive introduction to the subject, and logic is probably the area of mathematics where it is the most dangerous to rely on your&nbsp;intuition; this is no substitute for rigorously going through the formal definitions. With that proviso in mind, let's get cracking.</p>\n<p>&nbsp;</p>\n<h2 id=\"Possible_worlds__the_meaning_of_necessity\">Possible worlds: the meaning of necessity</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Modal_logic\">Modal logics</a> were developed around the concept of necessity and possibility. They do this with two extra operators: the necessity operator&nbsp;\u25a1, and possibility operator \u25ca. The sentence&nbsp;\u25a1A is taken to mean \"it is necessary that A\" and&nbsp;\u25caA means \"it is possible that A\". The two operators are dual to each other \u2013 thus \"it is necessary that A\" is the same as \"it not possible that not-A\" (in symbols:&nbsp;\u25a1A&nbsp;\u2194 \u00ac\u25ca\u00acA). A few intuitive axioms then lead to an elegant theory.</p>\n<p>There was just one problem: early modal logicians&nbsp;didn't&nbsp;have a clue what they were talking about. They had the <a href=\"http://en.wikipedia.org/wiki/Syntax_(logic)\">syntax</a>, the symbols, the formal rules, but they&nbsp;didn't&nbsp;have the semantics, the models, the <a href=\"http://en.wikipedia.org/wiki/Interpretation_(logic)\">meanings</a>&nbsp;of their symbols.</p>\n<p>To see the trouble they had, imagine someone tossing a coin and covering it with their hand. Call w<sub>H</sub> the world in which it comes out heads and w<sub>T</sub> the world in which it comes out, you guessed it, tails. Now, is the coin necessarily heads? Is it possibly heads?</p>\n<p>Intuitively, the answers should be no and yes. But this causes a problem. We may be in the world w<sub>H</sub>. So if we agree that the coin is not necessarily heads, then it is not necessarily heads even though it is actually heads&nbsp;(forget your Bayescraft here and start thinking like a logician). Similarly, in w<sub>T</sub>, the coin is in&nbsp;actuality&nbsp;tails yet it is possibly heads.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Saul_Kripke\">Saul Kripke</a> found the breakthrough: necessity and possibility are not about individual worlds, but about collections of possible worlds, and relationships between them. In this case, there is a&nbsp;indistinguishability&nbsp;relationship between w<sub>T</sub> and w<sub>H</sub>, because we can't (currently) tell them apart.</p>\n<p>Because of this relationship, the statement A:\"the coin is heads\" is possible in both w<sub>T</sub> and w<sub>H</sub>.&nbsp;The rule is that a statement is possible in world w if it is true in at least one world that is related to w. For w=w<sub>H</sub>,&nbsp;\u25caA&nbsp;is true because A is true in w<sub>H</sub> and w<sub>H</sub> is related to itself. Similarly, for w=w<sub>T</sub>,&nbsp;\u25caA is true because A is true in w<sub>H</sub> and w<sub>H</sub> is related to w<sub>T</sub>.</p>\n<p>Conversely B:\"the coin is heads or the coin is tails\" is necessary in both w<sub>T</sub> and w<sub>H</sub>: here the rule is that a statement is necessary in world w if it is true in all worlds related to w. w<sub>T</sub>&nbsp;and w<sub>H&nbsp;</sub>are related only to each other through the&nbsp;indistinguishability&nbsp;relationship, and B is true in both of them, so&nbsp;\u25a1B is also true in both of them. However&nbsp;\u25a1A is not true in either&nbsp;w<sub>T</sub>&nbsp;or&nbsp;w<sub>H</sub>, because both those worlds are related to w<sub>T</sub> and A is false in w<sub>T</sub>.<a id=\"more\"></a></p>\n<p>This idea was formalised as a <a href=\"http://en.wikipedia.org/wiki/Kripke_semantics\">Kripke structure</a>: a set W of worlds, a binary relationship R between some of the elements of W, and a rule that assigned the truth and falsity of statements in any particular world w. R is called the accessibility relationship (it need not be indistinguishability): wRv means that world v is 'accessible' from world w, and necessity and possibility are define, via R, as above.</p>\n<p>As an extra treat for the less wrong crowd, it should be noted that Kripke structures are used in the semantics of <a href=\"http://en.wikipedia.org/wiki/Common_knowledge_(logic)\">common knowledge</a>, key to the all-important <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann agreement theorem</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"Impossible_worlds__when_necessary_truth_is_not_true__necessarily\">Impossible worlds: when necessary truth is not true, necessarily</h2>\n<p>It should be noted that all the worlds in W are possible worlds: the (classical) laws of logic apply. Hence every tautology is true in every world in W, and every contradiction is false. In standard ('normal') modal logic, there is a necessitation requirement: if A is a tautology, then&nbsp;\u25a1A (\"it is necessary that A\") is true in every world.</p>\n<p>However, what if this were not the case? What if we added the requirement that&nbsp;\u25a1A&nbsp;not be true? Formally and symbolically, this seems a perfectly respectable thing to ask for. But can we come up with a model for this logic?</p>\n<p>Not by using possible worlds, of course: since A is always true, we won't get very far. But we can enlarge our set W to also include <a href=\"http://en.wikipedia.org/wiki/Impossible_worlds\">impossible worlds</a>: worlds where the laws of logic don't apply. Impossible worlds have truth values assigned to statements, but they don't need to be consistent or coherent (\"Socrates is a man\", \"all men are mortal\" and \"Socrates is immortal\" could all be true).</p>\n<p>Now&nbsp;assume there is a world w', where A is not labelled true (since A is a tautology, w' must be an impossible world). Also assume&nbsp;that w' is accessible from possible world w (hence wRw'). Then w is a perfectly respectable possible world, with A true in it; however,&nbsp;\u25a1A is not true in w, because it has an accessible world (w') where A isn't true. Voila!</p>\n<p>This seems like a cheap trick \u2013 a hack to build a model for a quirky odd logic that doesn't feel particularly intuitive anyway. But there are more things to be done with impossible worlds...</p>\n<p>&nbsp;</p>\n<h2 id=\"Some_possibilities_are_more_possible_than_others\">Some possibilities are more possible than others</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/David_Kellogg_Lewis\">Lewis</a> built on the possible world idea to construct a <a href=\"http://en.wikipedia.org/wiki/Counterfactual_conditional\">theory of counterfactuals</a>. Assume that the world is as we know it, and consider the two statements:</p>\n<ul>\n<li>A: If the Titanic didn't hit an iceberg, then it didn't sink on its maiden voyage.</li>\n<li>B: If the Titanic didn't hit an iceberg, then it was elected Pope in the controversial Martian election of 58 BC.</li>\n</ul>\n<p>Both of these statements are true in our world: we can show that the antecedent is contradicted (because the Titanic actually did hit an iceberg), and then imply anything. Nevertheless, there is a strong intuitive sense in which A is 'more true' than B. The consequent of A is 'less extraordinary' than that of B, and seems to derive more directly from the antecedent. A seems at least <a href=\"http://en.wikipedia.org/wiki/Counterfactual_conditional\">counterfactually</a> truer than B.</p>\n<p>We can't use the Kripke accessibility relationship to encode this: R is a simple yes or no binary, with no gradation. Lewis suggested that we instead use a measure that encodes relative distance between possible worlds, some way of saying that world v is closer to w than u is. Some way of saying that a world where the English invented chocolate is less distant from us than a world in which all the English are made of chocolate.</p>\n<p>With this distance, we can&nbsp;rigorously state whether A and B are true as counterfactual statements. Define C:\"the Titanic didn't hit an iceberg\", D:\"the Titanic didn't sink on its maiden voyage\" and E:\"the Titanic was elected Pope in the controversial Martian election of 58 BC.\"</p>\n<p>So, how shall we encode \"C counterfactually implies D (in our world)\"? Well, this could be true if C were false in every possible world; but no such luck. Otherwise, we could look for worlds where C is true and D is also true; yes, but there are also possible worlds where C is true but D is false (such as the world where the Titanic swerved to avoid the iceberg only to get sunk by the <a href=\"http://en.wikipedia.org/wiki/Nadia:_The_Secret_of_Blue_Water\">Nautilus</a>). This is where the distance between worlds comes in: we say that \"C counterfactually implies D (in our world)\" if D is true in w, where w is the closest world to our own where C is true. For any intuitive distance measure, it does seem quite likely that C does counterfactually imply D (in our world): most ships cross the Atlantic safely, so if the Titanic hadn't hit an iceberg, it should have done so as well.</p>\n<p>Basically a counterfactual is true if the consequent is true in the <em>least weird</em> world where the antecedent is true.&nbsp;A similar approach shows that \"C counterfactually implies E (in our world)\" is false. Unless we have made some very peculiar choices for our distance measure, the closest world to us in which the Titanic didn't hit an iceberg, is not one in which a large metal contraption was elected Pope on an uninhabited planet thousands of years before it was even built.</p>\n<p>So far, this seems pretty intuitive; but what about this \"distance measure\" between possible worlds, which seems to be doing all the work? Where does that come from? Well, that, of course, is left as an exercise to the interested reader. By which I mean the problem is unsolved. But there is progress: we started with an unsolved informal problem (what is a counterfactual) and replaced it with an unsolved formal problem (how to specify the relative distance between possible worlds). Things are looking up!</p>\n<p>&nbsp;</p>\n<h2 id=\"Some_impossibilities_are_less_impossible_than_others\">Some impossibilities are less impossible than others</h2>\n<p>It should be noted that in the previous section, the fact that the worlds were logically consistent was never used. This immediately suggests we can extend the setup to include impossible worlds as well. This was Edwin Mares's <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.ndjfl/1039540767\">idea</a>, and it allowed him to construct models for the <a href=\"/lw/bgx/logic_of_paradox_a_too_simple_paraconsistent_logic/\">logic of paradox</a>.</p>\n<p>All that we need is our magical \"relative distance between worlds\" measure, as before, except we now measure&nbsp;relative&nbsp;distance to impossible worlds as well. With that, we can start talking about the truths of counterpossible statements - conditional statements where the antecedent is contradictory. Assume for instance that both Euclidean geometry and Peano arithmetic are true, and consider:</p>\n<ul>\n<li>A: the existence of an equilateral triangle with a right angle counterpossibly implies the existence of a triangle whose angles sum up to 270 degrees.</li>\n<li>B: the existence of an equilateral triangle with a right angle counterpossibly implies the existence of many even primes.</li>\n</ul>\n<p>For many intuitive distance-between-worlds measures, A is true in our world and B is false. This is, for instance, the case for some distance measures that penalise doubly impossible worlds (worlds where Euclidean geometry and Peano arithmetic both have contradictions) more than singly impossible worlds (where only one of the theories have contradictions). Again, the informal mystery about counterpossibles is reduced to the formal problem of specifying this distance measure between worlds.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Possible worlds: the meaning of necessity", "anchor": "Possible_worlds__the_meaning_of_necessity", "level": 1}, {"title": "Impossible worlds: when necessary truth is not true, necessarily", "anchor": "Impossible_worlds__when_necessary_truth_is_not_true__necessarily", "level": 1}, {"title": "Some possibilities are more possible than others", "anchor": "Some_possibilities_are_more_possible_than_others", "level": 1}, {"title": "Some impossibilities are less impossible than others", "anchor": "Some_impossibilities_are_less_impossible_than_others", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oeMYMKnALbniFaoQL", "rwysnoyCxMLqQZnCY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T13:56:30.274Z", "modifiedAt": null, "url": null, "title": "Question: Being uncertain without worrying?", "slug": "question-being-uncertain-without-worrying", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.518Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/irrmHqXxM2LgXhzKo/question-being-uncertain-without-worrying", "pageUrlRelative": "/posts/irrmHqXxM2LgXhzKo/question-being-uncertain-without-worrying", "linkUrl": "https://www.lesswrong.com/posts/irrmHqXxM2LgXhzKo/question-being-uncertain-without-worrying", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%3A%20Being%20uncertain%20without%20worrying%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%3A%20Being%20uncertain%20without%20worrying%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirrmHqXxM2LgXhzKo%2Fquestion-being-uncertain-without-worrying%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%3A%20Being%20uncertain%20without%20worrying%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirrmHqXxM2LgXhzKo%2Fquestion-being-uncertain-without-worrying", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirrmHqXxM2LgXhzKo%2Fquestion-being-uncertain-without-worrying", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>I currently face a pretty major life decision. After some careful analysis, I've concluded that my final decision depends on the answers from some queries that I have made, but whose answers I won't receive for days or perhaps weeks.</p>\n<p>In the meantime, I've had great difficulty not obsessing over the pending decision. It warps my priorities and kills my motivation; I'm doing less, with less vigor, and enjoying it less. I've noticed, in the past, that compulsion to worry correlates tightly with depressed mood; given what I know about the mind, I assume that each can cause the other.</p>\n<p>In general, this connection seems to make changing one's mind <em>painful</em>, and probably conditions people to hold their ideas with certainty, rather than uncertainty. As such, ways to stave it off should be of major use to this community...</p>\n<p>I know some things to do to stave off a depressed mood (e.g. get exercise, eat well, talk to friends, achieve small-but-satisfying goals). I don't know any ways to avoid the compulsion to worry about an uncertain future decision, except, possibly, to notice the worrying and tell myself, verbally, that uncertainty is ok. Which brings me to my</p>\n<p><strong>Question:&nbsp;</strong>Does anyone know any methods for avoiding fruitless worrying over properly-uncertain facts or actions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "irrmHqXxM2LgXhzKo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.858229805571474e-07, "legacy": true, "legacyId": "15307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T19:17:11.527Z", "modifiedAt": null, "url": null, "title": "Meetup : Big Berkeley Meetup", "slug": "meetup-big-berkeley-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thausler", "createdAt": "2010-05-24T04:40:49.214Z", "isAdmin": false, "displayName": "Thausler"}, "userId": "oPa5EPBs6ow6MY2Ao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Me8bZ2C39rtJrS5jA/meetup-big-berkeley-meetup-0", "pageUrlRelative": "/posts/Me8bZ2C39rtJrS5jA/meetup-big-berkeley-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/Me8bZ2C39rtJrS5jA/meetup-big-berkeley-meetup-0", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Big%20Berkeley%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Big%20Berkeley%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMe8bZ2C39rtJrS5jA%2Fmeetup-big-berkeley-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Big%20Berkeley%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMe8bZ2C39rtJrS5jA%2Fmeetup-big-berkeley-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMe8bZ2C39rtJrS5jA%2Fmeetup-big-berkeley-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/98'>Big Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 April 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2011 Shattuck Ave, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 18 April 2012 06:30:00PM  WHERE: 2011 Shattuck Ave, Berkeley, CA This will be a joint dinner with the MPHD seminar folks. We'll meet at Biryani House on Shattuck Ave.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/98'>Big Berkeley Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Me8bZ2C39rtJrS5jA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.859589349020746e-07, "legacy": true, "legacyId": "15309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_Meetup\">Discussion article for the meetup : <a href=\"/meetups/98\">Big Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 April 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2011 Shattuck Ave, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 18 April 2012 06:30:00PM  WHERE: 2011 Shattuck Ave, Berkeley, CA This will be a joint dinner with the MPHD seminar folks. We'll meet at Biryani House on Shattuck Ave.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/98\">Big Berkeley Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Big Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Big Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-17T20:01:45.447Z", "modifiedAt": null, "url": null, "title": "[META] Alternatives to rot13 and karma sinks", "slug": "meta-alternatives-to-rot13-and-karma-sinks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n2ehsEr2BwsFax2yJ/meta-alternatives-to-rot13-and-karma-sinks", "pageUrlRelative": "/posts/n2ehsEr2BwsFax2yJ/meta-alternatives-to-rot13-and-karma-sinks", "linkUrl": "https://www.lesswrong.com/posts/n2ehsEr2BwsFax2yJ/meta-alternatives-to-rot13-and-karma-sinks", "postedAtFormatted": "Tuesday, April 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Alternatives%20to%20rot13%20and%20karma%20sinks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Alternatives%20to%20rot13%20and%20karma%20sinks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn2ehsEr2BwsFax2yJ%2Fmeta-alternatives-to-rot13-and-karma-sinks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Alternatives%20to%20rot13%20and%20karma%20sinks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn2ehsEr2BwsFax2yJ%2Fmeta-alternatives-to-rot13-and-karma-sinks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn2ehsEr2BwsFax2yJ%2Fmeta-alternatives-to-rot13-and-karma-sinks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 625, "htmlBody": "<p><strong>rot13 is...</strong> (<a href=\"http://www.makefoil.com/poll.php?poll=33\">Results</a>)<br />A quaint internet tradition that I quite enjoy<a href=\"http://www.makefoil.com/poll.php?button=60\"><img src=\"http://www.makefoil.com/pollButton.png?button=60\" alt=\"\" /></a><br />So annoying<a href=\"http://www.makefoil.com/poll.php?button=61\"><img src=\"http://www.makefoil.com/pollButton.png?button=61\" alt=\"\" /></a><br />Easy to use because I have Leet Key<a href=\"http://www.makefoil.com/poll.php?button=62\"><img src=\"http://www.makefoil.com/pollButton.png?button=62\" alt=\"\" /></a><br />Vapbzcerurafvoyr<a href=\"http://www.makefoil.com/poll.php?button=63\"><img src=\"http://www.makefoil.com/pollButton.png?button=63\" alt=\"\" /></a></p>\n<p><a id=\"more\"></a></p>\n<p><a href=\"/lw/8bn/reforming_rot13/\">In November</a>, DanielVarga complained about rot13.com as a way of hiding spoilers, and suggested some specifications for an alternative web service, with an algorithm that can operate on any character.&nbsp; I whipped up two, <a href=\"http://www.makefoil.com/markdownSpoiler.html\">one using a bitwise not</a> and <a href=\"http://www.makefoil.com/passwordXor.html\">the other using a password</a>.&nbsp; These have a usability advantage over rot13.com: they automatically generate a decode link, so that people who don't have an add-on such as Leet Key (<a href=\"https://addons.mozilla.org/en-US/firefox/addon/leet-key/\">get Leet Key!</a> <a href=\"https://chrome.google.com/webstore/detail/gncnbkghencmkfgeepfaonmegemakcol\">or d3coder if you're on Chrome!</a>) on whatever device they're on can still use them.&nbsp; The advantage over using a site like pastebin is that all of the text is encoded in the url, so if makefoil.com is down or you're offline you can still extract it with a bit of work.&nbsp; Both algorithms are in unobfuscated javascript; feel free to host your own version.</p>\n<p>Recently, I had the thought of doing a similar thing with polls.&nbsp; This site doesn't have built-in polling functionality, and the project to create it seems to have stalled.&nbsp; People generally solve this by creating a comment for each option, asking people to upvote the comment of their choice, and asking people to downvote another comment to balance the karma.&nbsp; It's elegant as these kludges go, but it's still visually confusing in a comment thread, clogs the \"Top Comments\" lists, and somehow rarely ends up balancing correctly.</p>\n<p><a href=\"http://www.makefoil.com/poll.php\">Here's the site I used to create the above poll.</a>&nbsp; Those vote buttons will turn into vote counts once you vote (you may need to refresh).&nbsp; The site outputs poll syntax in two formats: html, which you can use by creating an article and clicking the html button on the top tool bar, or markdown, which is for comments.&nbsp; The markdown display is a little different because the Less Wrong functionality for having an image be a link seems to be broken; the image gets double-encoded or something.</p>\n<p>I hope you'll consider using <a href=\"http://www.makefoil.com/markdownSpoiler.html\">markdownSpoiler</a> or <a href=\"http://www.makefoil.com/passwordXor.html\">passwordXor</a> when sharing secret info on the next <em>Harry Potter and the Methods of Rationality</em> chapter, and getting input with a <a href=\"http://www.makefoil.com/poll.php\">makefoil poll</a>.</p>\n<p>Notes:</p>\n<ul>\n<li>If you use the password version, the spoilered output includes some ascii special characters that LessWrong will automatically wrap in multiple &lt;code&gt; tags.&nbsp; It works, but it's not as pretty as the other options.</li>\n<li>The services are on very bare pages right now, like rot13.com. That won't change much, but at some point I'll be turning makefoil.com into a navigable personal site so there'll probably be a navigation bar.&nbsp; If traffic starts becoming an issue, I may even add some third-party text ads.</li>\n<li>The polls are \"checkbox\" style: you can vote for as many options as you want, but you can't vote for the same option more than once.</li>\n<li>The polls are extremely easy to abuse, and I don't anticipate putting much effort into changing that.&nbsp; If you have a serious concern about ballot-box stuffing, probably best to stick to the old system, where presumably there are safeguards against massive sock-puppet creation.</li>\n<li>If you host the spoiler pages on your own site, bear in mind that special characters in GET parameters are often blocked or escaped by default by security modules on the web server.</li>\n<li>Yes, I know the <em>right </em>way to do this is to mod Less Wrong itself to create spoiler tags and poll embedding.&nbsp; But that requires getting multiple organizations to work with me, which is a higher order of problem than creating embeddable kludges.&nbsp; And this way, you can use these services on any forum, comment thread, or whatever that allows images and links.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n2ehsEr2BwsFax2yJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 31, "extendedScore": null, "score": 8.859776365148727e-07, "legacy": true, "legacyId": "15311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gDe37MvsqTE35uNzt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T00:55:31.081Z", "modifiedAt": null, "url": null, "title": "Visual maps of the historical arguments in the topic, \"Can computers think?\"", "slug": "visual-maps-of-the-historical-arguments-in-the-topic-can", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.715Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Logos01", "createdAt": "2011-07-21T18:59:16.270Z", "isAdmin": false, "displayName": "Logos01"}, "userId": "WZxoXCWQviJp9dNc5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b2fnhQiNPrCduRZmp/visual-maps-of-the-historical-arguments-in-the-topic-can", "pageUrlRelative": "/posts/b2fnhQiNPrCduRZmp/visual-maps-of-the-historical-arguments-in-the-topic-can", "linkUrl": "https://www.lesswrong.com/posts/b2fnhQiNPrCduRZmp/visual-maps-of-the-historical-arguments-in-the-topic-can", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Visual%20maps%20of%20the%20historical%20arguments%20in%20the%20topic%2C%20%22Can%20computers%20think%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVisual%20maps%20of%20the%20historical%20arguments%20in%20the%20topic%2C%20%22Can%20computers%20think%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb2fnhQiNPrCduRZmp%2Fvisual-maps-of-the-historical-arguments-in-the-topic-can%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Visual%20maps%20of%20the%20historical%20arguments%20in%20the%20topic%2C%20%22Can%20computers%20think%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb2fnhQiNPrCduRZmp%2Fvisual-maps-of-the-historical-arguments-in-the-topic-can", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb2fnhQiNPrCduRZmp%2Fvisual-maps-of-the-historical-arguments-in-the-topic-can", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>Located here: http://www.macrovu.com/CCTGeneralInfo.html</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.macrovu.com/CCTWeb/CCT1/CCTMap1.html\">Map 1: Can computers think?</a><br /> <a href=\"http://www.macrovu.com/CCTWeb/CCT2/CCTMap2.html\">Map 2: Can the Turing test determine whether computers can think?</a><br /> <a href=\"http://www.macrovu.com/CCTWeb/CCT3/CCTMap3.html\">Map 3: Can physical symbol systems think?</a><br /> <a href=\"http://www.macrovu.com/CCTMap4.html\">Map 4: Can Chinese Rooms think?</a><br /> <a href=\"http://www.macrovu.com/CCTMap5.html\">Map 5, Part 1: Can connectionist networks think?</a><br /> <a href=\"http://www.macrovu.com/CCTMap5.html\">Map 5, Part 2: Can computers think in images?</a><br /> <a href=\"http://www.macrovu.com/CCTWeb/CCT6/CCTMap6.html\">Map 6: Do computers have to be conscious to think?</a><br /> <a href=\"http://www.macrovu.com/CCTMap7.html\">Map 7: Are thinking computers mathematically possible?</a></p>\n<p>These are available, apparently, for purchase in their full (wall-poster) size.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b2fnhQiNPrCduRZmp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 8.861024068595676e-07, "legacy": true, "legacyId": "15320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T02:30:26.958Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 16, chapter 85", "slug": "harry-potter-and-the-methods-of-rationality-discussion-6", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:08.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FAWS", "createdAt": "2010-01-09T18:58:38.832Z", "isAdmin": false, "displayName": "FAWS"}, "userId": "a7Neq3q2DbWrbo6B6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QkhX5YeuYHzPW7Waz/harry-potter-and-the-methods-of-rationality-discussion-6", "pageUrlRelative": "/posts/QkhX5YeuYHzPW7Waz/harry-potter-and-the-methods-of-rationality-discussion-6", "linkUrl": "https://www.lesswrong.com/posts/QkhX5YeuYHzPW7Waz/harry-potter-and-the-methods-of-rationality-discussion-6", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2016%2C%20chapter%2085&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2016%2C%20chapter%2085%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkhX5YeuYHzPW7Waz%2Fharry-potter-and-the-methods-of-rationality-discussion-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2016%2C%20chapter%2085%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkhX5YeuYHzPW7Waz%2Fharry-potter-and-the-methods-of-rationality-discussion-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkhX5YeuYHzPW7Waz%2Fharry-potter-and-the-methods-of-rationality-discussion-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>The next discussion thread is <a href=\"/r/discussion/lw/fyv/harry_potter_and_the_methods_of_rationality/\">here</a>.</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing <a href=\"http://www.fanfiction.net/s/5782108/85/\">chapter</a> <a href=\"http://hpmor.com/chapter/85\">85</a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">The previous thread</a>&nbsp; has long passed 500 comments. C<strong>omment in&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">the 15th thread</a>&nbsp;until you read chapter 85.</strong>&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.&nbsp; Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>, <a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>, <a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">As a reminder, it&rsquo;s often useful to start your comment by indicating which chapter you are commenting on.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QkhX5YeuYHzPW7Waz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 8.861424765413352e-07, "legacy": true, "legacyId": "15324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1114, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4sY9rqAqty8rHWGSW", "XN4WDRSPFo9iGEuk3", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T05:06:36.445Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] On Being Decoherent", "slug": "seq-rerun-on-being-decoherent", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:16.045Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YTgkFHZoq9veRiK96/seq-rerun-on-being-decoherent", "pageUrlRelative": "/posts/YTgkFHZoq9veRiK96/seq-rerun-on-being-decoherent", "linkUrl": "https://www.lesswrong.com/posts/YTgkFHZoq9veRiK96/seq-rerun-on-being-decoherent", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20On%20Being%20Decoherent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20On%20Being%20Decoherent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTgkFHZoq9veRiK96%2Fseq-rerun-on-being-decoherent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20On%20Being%20Decoherent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTgkFHZoq9veRiK96%2Fseq-rerun-on-being-decoherent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTgkFHZoq9veRiK96%2Fseq-rerun-on-being-decoherent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>Today's post, <a href=\"/lw/pu/on_being_decoherent/\">On Being Decoherent</a> was originally published on 27 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When a sensor measures a particle whose amplitude distribution stretches over space - perhaps seeing if the particle is to the left or right of some dividing line - then the standard laws of quantum mechanics call for the sensor+particle system to evolve into a state of (particle left, sensor measures LEFT) + (particle right, sensor measures RIGHT). But when we humans look at the sensor, it only seems to say \"LEFT\" or \"RIGHT\", never a mixture like \"LIGFT\". This, of course, is because we ourselves are made of particles, and subject to the standard quantum laws that imply decoherence. Under standard quantum laws, the final state is (particle left, sensor measures LEFT, human sees \"LEFT\") + (particle right, sensor measures RIGHT, human sees \"RIGHT\").</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bsq/seq_rerun_where_experience_confuses_physicists/\">Where Experience Confuses Physicists</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YTgkFHZoq9veRiK96", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.86208911444735e-07, "legacy": true, "legacyId": "15331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pRrksC5Y6TbyvKDJE", "xij6ij2FXB8HsThYr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T07:51:36.365Z", "modifiedAt": null, "url": null, "title": "[META] Karma counting problem", "slug": "meta-karma-counting-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.477Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EiW2Xg3S8d6sgDd9a/meta-karma-counting-problem", "pageUrlRelative": "/posts/EiW2Xg3S8d6sgDd9a/meta-karma-counting-problem", "linkUrl": "https://www.lesswrong.com/posts/EiW2Xg3S8d6sgDd9a/meta-karma-counting-problem", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Karma%20counting%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Karma%20counting%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEiW2Xg3S8d6sgDd9a%2Fmeta-karma-counting-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Karma%20counting%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEiW2Xg3S8d6sgDd9a%2Fmeta-karma-counting-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEiW2Xg3S8d6sgDd9a%2Fmeta-karma-counting-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>At the moment my karma is 450 and -50 for the last 30 days. I had never 500. Also not a month ago.</p>\n<p>Must be a bug in the karma counting system.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EiW2Xg3S8d6sgDd9a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 8.862789111594998e-07, "legacy": true, "legacyId": "15342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T10:17:32.110Z", "modifiedAt": null, "url": null, "title": "If you can't be right, at least be relevant", "slug": "if-you-can-t-be-right-at-least-be-relevant", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:03.674Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oLQamyNmzsgPs8nE6/if-you-can-t-be-right-at-least-be-relevant", "pageUrlRelative": "/posts/oLQamyNmzsgPs8nE6/if-you-can-t-be-right-at-least-be-relevant", "linkUrl": "https://www.lesswrong.com/posts/oLQamyNmzsgPs8nE6/if-you-can-t-be-right-at-least-be-relevant", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20you%20can't%20be%20right%2C%20at%20least%20be%20relevant&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20you%20can't%20be%20right%2C%20at%20least%20be%20relevant%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoLQamyNmzsgPs8nE6%2Fif-you-can-t-be-right-at-least-be-relevant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20you%20can't%20be%20right%2C%20at%20least%20be%20relevant%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoLQamyNmzsgPs8nE6%2Fif-you-can-t-be-right-at-least-be-relevant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoLQamyNmzsgPs8nE6%2Fif-you-can-t-be-right-at-least-be-relevant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1160, "htmlBody": "<p>We've done the <a href=\"/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">motivation</a>, and the <a href=\"/lw/bgx/logic_of_paradox_a_too_simple_paraconsistent_logic/\">warm</a>-<a href=\"/lw/bs2/whos_afraid_of_impossible_worlds/\">ups</a>, and it's now time to get to the meat of this arc: <a href=\"http://plato.stanford.edu/entries/logic-relevance/\">relevance logics</a>. These try to recapture a more intuitive meaning to the material implication, \"if... then...\" and \"&rarr;\". Consider for instance the following:</p>\n<ul>\n<li>\"It rains in Spain\"&nbsp;&rarr; (\"Darwin died for our sins\"&nbsp;&rarr; \"It rains in Spain\")</li>\n<li>(\"It rains in Spain\" and \"It doesn't rain in Spain\")&nbsp;&rarr; \"Ravens are the stupidest of birds\"</li>\n</ul>\n<p>Both of these are perfectly acceptable classical logical theorems. The problem is that there doesn't seem to be any connection between the different components: no logical leap between rain, Darwin or ravens. These are all tautologies, so we could substitute any other propositions in here and it would still be true. In common speech, \"if A then B\" implies some sort of relevant connection between A and B. Relevance logics attempt to do the same for the formal \"&rarr;\".</p>\n<p>Unfortunately, it is easier to motivate relevance logics, and write formal axiomatic systems for them, than it is to pin down exactly what \"relevance\" means - see the later section on relevance semantics. One oft-mentioned requirement is that the expressions on both sides of \"&rarr;\" share propositional variables. A propositional variable is a basic sentence such as A:\"It rains in Spain\". Sharing A would mean that the whole expression would be something like:</p>\n<p style=\"padding-left: 30px;\">(blah&nbsp;&and;&nbsp;blah &or;&nbsp;<strong>A</strong> &and; blah) &nbsp;&rarr; &nbsp;(blah&nbsp;&and;&nbsp;&not;<strong>A</strong> &or; blah&nbsp;&and; blah)</p>\n<p>Unfortunately, though this is a necessary condition for relevance, it is not sufficient. Even the hated <a href=\"http://en.wikipedia.org/wiki/Disjunctive_syllogism\">disjunctive syllogism</a> (the very thing we are <a href=\"/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">trying to avoid</a>) shares variables. In theorem form, it is:</p>\n<p style=\"padding-left: 30px;\">(<strong>A</strong>&nbsp;&or; B)&nbsp;&and;&nbsp;&not;B &nbsp;&rarr; &nbsp;<strong>A</strong></p>\n<p>So let's defer worrying about what relevance means, and look at what relevance is.<a id=\"more\"></a></p>\n<h2>Axioms of relevance</h2>\n<p>When we rejected the disjunctive syllogism, we lost a lot of useful stuff. And not all of that loss was necessary: we can add a bit back without returning to the <a href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">principle of explosion</a>. Hence the axiom schemas of relevance logics tend to be longer than that of classical logic, as we put in by hand all the useful results we want to keep. \"<a href=\"http://www.springerlink.com/content/wgv5687205364m55/\">Dialectical</a>&nbsp;<a href=\"http://www.amazon.com/Introduction-Paraconsistent-Logics-Bremer-Manuel/dp/3631534132\">logic</a>\" is one example of relevance logics; it uses thirteen axiom schemas:</p>\n<ol>\n<li>A&nbsp;&rarr; A</li>\n<li>(A &rarr;&nbsp;B)&nbsp;&and; (B&nbsp;&rarr; C) &rarr; (A &rarr; C)</li>\n<li>A&nbsp;&and; B &rarr; A</li>\n<li>A&nbsp;&and; B&nbsp;&rarr; B</li>\n<li>(A &rarr; B)&nbsp;&and; (A &rarr;&nbsp;C) &rarr;&nbsp;(A &rarr; (B &and; C))</li>\n<li>A &and; (B &or; C)&nbsp; &rarr; (A &and;&nbsp;B) &or;&nbsp;(A &and;&nbsp;C)</li>\n<li>&not;&not;A &rarr; A</li>\n<li>(A &rarr; &not;B) &rarr; (B &rarr; &not;A)</li>\n<li>A &rarr; A &or;&nbsp;B</li>\n<li>B&nbsp;&rarr; A &or;&nbsp;B</li>\n<li>(A &rarr; C) &and;&nbsp;(B &rarr; C) &rarr; ((A &or;&nbsp;B) &rarr; C)</li>\n<li>&not;A &and;&nbsp;&not;B &rarr;&nbsp;&not;(A &or; B)</li>\n<li>&not;(A &and;&nbsp;B) &rarr;&nbsp;&not;A &or; &not;B</li>\n</ol>\n<p>Beyond this, it has two rules of inference:</p>\n<ol>\n<li>A, A &rarr; B&nbsp;<strong>&rArr;</strong> B</li>\n<li>A &rarr;&nbsp;B <strong>&rArr;</strong> &not;B &rarr;&nbsp;&not;A</li>\n</ol>\n<p>We have modus ponens! We have modus ponens! We finally have a paraconsistent logic that actually has the most used rule of inference. Notice a few things: distribution needs a special axiom (6.) and there are two axioms (12. and 13.) to establish the <a href=\"http://en.wikipedia.org/wiki/De_Morgan%27s_laws\">DeMorgan dualities</a>. Also, contraposition needs a separate rule of inference in its own right (I believe - but haven't checked - that this is because adding (A &rarr;&nbsp;B)&nbsp;&rarr; (&not;B &rarr;&nbsp;&not;A) as an axiom would collapse the paraconsistency). Axiom 7. demonstrates that this is not an&nbsp;<a href=\"http://en.wikipedia.org/wiki/Intuitionistic_logic\">intuitionist&nbsp;logic</a>: from a double negative, we <em>can</em> prove a positive (though not vice versa). It is also strictly weaker than classical logic: if dialectic logic can prove something, then the proof works in classical logic as well.</p>\n<p>Another thing to note about dialectical logic (and about most relevance logics) is that the <a href=\"http://en.wikipedia.org/wiki/Deduction_theorem\">deduction theorem</a> fails. The deduction theorem says that if A\u22a2B (if, starting from A, we can prove B), then also&nbsp;\u22a2(A&rarr;B) (starting from nothing, we can prove that A implies B). It is easy to see why this would fail in relevance logics. If for instance B were a tautology, then we could certainly get&nbsp;A\u22a2B; but we could not get&nbsp;\u22a2(A&rarr;B) unless there were some relevant connection between A and B.</p>\n<h2>The semantics, and the semantics of the semantics</h2>\n<p>So, that's the definition of a relevance logic; but what does it all mean? There is no getting around the fact that the semantics of relevance logics are bizarre. Bizarre in the \"what's going on? we're probably missing a big part of the picture\" kind of way.</p>\n<p>We discussed impossible worlds and (binary) accessibility relations in the <a href=\"/r/discussion/lw/bs2/whos_afraid_of_impossible_worlds/\">previous post</a>. They are not however enough to get models/semantics/meanings for relevance logics. Instead we have to use a strange accessibility relation <strong>R</strong> than isn't binary but ternary (taking three inputs). Writing <strong>R</strong><strong>abc</strong> means that worlds <strong>a</strong>, <strong>b</strong> and <strong>c</strong> are related in this formal way. Then the definition of&nbsp;&rarr;&nbsp;in the model becomes:</p>\n<p style=\"padding-left: 30px;\">A &rarr; B is true at a world <strong>a</strong> if and only if for all worlds <strong>b</strong> and <strong>c</strong> such that <strong>R</strong><strong>abc</strong>, either A is false at <strong>b</strong> or B is true at <strong>c</strong>.</p>\n<p>So these semantics define the value of&nbsp;A &rarr; B at <strong>a</strong> by looking at some kind of implication result across two completely separate worlds. Not only that, these worlds will have to include impossible worlds as well (or else the semantics would collapse to classical semantics).</p>\n<p>Generally, when discussing a theory, to explain its meaning one gives a model and leaves it at that. But this model is so counterintutive, that it seems fair to demand further explanation: a meaning for the meaning, semantics for the semantics. <a href=\"http://plato.stanford.edu/entries/logic-relevance/\">One</a> possible interesting interpretation is that instead of talking about worlds, we interpret <strong>a</strong>, <strong>b</strong> and <strong>c</strong> as collections of information. And then <strong>Rabc</strong> is taken to mean \"the combination of the information states of <strong>a</strong> and <strong>b</strong> is contained in the information state of <strong>c</strong>\" (Dunn, J.M., 1986, \"Relevance Logic and Entailment\").</p>\n<p>Yet about approach (Jon Barwise (1993) and developed in Restall (1996))&nbsp;models this as information flow in distributed systems. Here <strong>a</strong>, <strong>b</strong> and <strong>c</strong> are not worlds, but sites of information, or channels between sites along which information can flow (the fact that some \"worlds\" are sites while others are channels is not a problem - after all, some worlds are possible and others impossible, so&nbsp;distinguishing&nbsp;between different types of worlds is not an unusual requirement). Then <strong>R</strong><strong>abc</strong> is taken to mean \"<strong>a</strong> is a channel connecting the sites <strong>b</strong> and <strong>c</strong>\". Then the definition of&nbsp;&rarr;&nbsp;in the model becomes:</p>\n<p style=\"padding-left: 30px;\">A &rarr; B is true on the channel <strong>a</strong> if and only if for all sites <strong>b</strong> and <strong>c</strong> connected by <strong>a</strong>, if information A is available at one end (<strong>b</strong>) of the channel, then information B is available at the other end (<strong>c</strong>).</p>\n<p>More can, and has, been said about this. But that's enough for a short introduction to the subject, and will hopefully given a start to anyone wanting to mesh relevance logics with UDT. My next and last post will briefly touch on attempts to use as much of classical logic as we can get away with, while still staying paraconsistent when we have to.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oLQamyNmzsgPs8nE6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 19, "extendedScore": null, "score": 8.863408289218752e-07, "legacy": true, "legacyId": "15308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>We've done the <a href=\"/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">motivation</a>, and the <a href=\"/lw/bgx/logic_of_paradox_a_too_simple_paraconsistent_logic/\">warm</a>-<a href=\"/lw/bs2/whos_afraid_of_impossible_worlds/\">ups</a>, and it's now time to get to the meat of this arc: <a href=\"http://plato.stanford.edu/entries/logic-relevance/\">relevance logics</a>. These try to recapture a more intuitive meaning to the material implication, \"if... then...\" and \"\u2192\". Consider for instance the following:</p>\n<ul>\n<li>\"It rains in Spain\"&nbsp;\u2192 (\"Darwin died for our sins\"&nbsp;\u2192 \"It rains in Spain\")</li>\n<li>(\"It rains in Spain\" and \"It doesn't rain in Spain\")&nbsp;\u2192 \"Ravens are the stupidest of birds\"</li>\n</ul>\n<p>Both of these are perfectly acceptable classical logical theorems. The problem is that there doesn't seem to be any connection between the different components: no logical leap between rain, Darwin or ravens. These are all tautologies, so we could substitute any other propositions in here and it would still be true. In common speech, \"if A then B\" implies some sort of relevant connection between A and B. Relevance logics attempt to do the same for the formal \"\u2192\".</p>\n<p>Unfortunately, it is easier to motivate relevance logics, and write formal axiomatic systems for them, than it is to pin down exactly what \"relevance\" means - see the later section on relevance semantics. One oft-mentioned requirement is that the expressions on both sides of \"\u2192\" share propositional variables. A propositional variable is a basic sentence such as A:\"It rains in Spain\". Sharing A would mean that the whole expression would be something like:</p>\n<p style=\"padding-left: 30px;\">(blah&nbsp;\u2227&nbsp;blah \u2228&nbsp;<strong>A</strong> \u2227 blah) &nbsp;\u2192 &nbsp;(blah&nbsp;\u2227&nbsp;\u00ac<strong>A</strong> \u2228 blah&nbsp;\u2227 blah)</p>\n<p>Unfortunately, though this is a necessary condition for relevance, it is not sufficient. Even the hated <a href=\"http://en.wikipedia.org/wiki/Disjunctive_syllogism\">disjunctive syllogism</a> (the very thing we are <a href=\"/lw/bfs/paraconsistency_and_relevance_avoid_logical/\">trying to avoid</a>) shares variables. In theorem form, it is:</p>\n<p style=\"padding-left: 30px;\">(<strong>A</strong>&nbsp;\u2228 B)&nbsp;\u2227&nbsp;\u00acB &nbsp;\u2192 &nbsp;<strong>A</strong></p>\n<p>So let's defer worrying about what relevance means, and look at what relevance is.<a id=\"more\"></a></p>\n<h2 id=\"Axioms_of_relevance\">Axioms of relevance</h2>\n<p>When we rejected the disjunctive syllogism, we lost a lot of useful stuff. And not all of that loss was necessary: we can add a bit back without returning to the <a href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">principle of explosion</a>. Hence the axiom schemas of relevance logics tend to be longer than that of classical logic, as we put in by hand all the useful results we want to keep. \"<a href=\"http://www.springerlink.com/content/wgv5687205364m55/\">Dialectical</a>&nbsp;<a href=\"http://www.amazon.com/Introduction-Paraconsistent-Logics-Bremer-Manuel/dp/3631534132\">logic</a>\" is one example of relevance logics; it uses thirteen axiom schemas:</p>\n<ol>\n<li>A&nbsp;\u2192 A</li>\n<li>(A \u2192&nbsp;B)&nbsp;\u2227 (B&nbsp;\u2192 C) \u2192 (A \u2192 C)</li>\n<li>A&nbsp;\u2227 B \u2192 A</li>\n<li>A&nbsp;\u2227 B&nbsp;\u2192 B</li>\n<li>(A \u2192 B)&nbsp;\u2227 (A \u2192&nbsp;C) \u2192&nbsp;(A \u2192 (B \u2227 C))</li>\n<li>A \u2227 (B \u2228 C)&nbsp; \u2192 (A \u2227&nbsp;B) \u2228&nbsp;(A \u2227&nbsp;C)</li>\n<li>\u00ac\u00acA \u2192 A</li>\n<li>(A \u2192 \u00acB) \u2192 (B \u2192 \u00acA)</li>\n<li>A \u2192 A \u2228&nbsp;B</li>\n<li>B&nbsp;\u2192 A \u2228&nbsp;B</li>\n<li>(A \u2192 C) \u2227&nbsp;(B \u2192 C) \u2192 ((A \u2228&nbsp;B) \u2192 C)</li>\n<li>\u00acA \u2227&nbsp;\u00acB \u2192&nbsp;\u00ac(A \u2228 B)</li>\n<li>\u00ac(A \u2227&nbsp;B) \u2192&nbsp;\u00acA \u2228 \u00acB</li>\n</ol>\n<p>Beyond this, it has two rules of inference:</p>\n<ol>\n<li>A, A \u2192 B&nbsp;<strong>\u21d2</strong> B</li>\n<li>A \u2192&nbsp;B <strong>\u21d2</strong> \u00acB \u2192&nbsp;\u00acA</li>\n</ol>\n<p>We have modus ponens! We have modus ponens! We finally have a paraconsistent logic that actually has the most used rule of inference. Notice a few things: distribution needs a special axiom (6.) and there are two axioms (12. and 13.) to establish the <a href=\"http://en.wikipedia.org/wiki/De_Morgan%27s_laws\">DeMorgan dualities</a>. Also, contraposition needs a separate rule of inference in its own right (I believe - but haven't checked - that this is because adding (A \u2192&nbsp;B)&nbsp;\u2192 (\u00acB \u2192&nbsp;\u00acA) as an axiom would collapse the paraconsistency). Axiom 7. demonstrates that this is not an&nbsp;<a href=\"http://en.wikipedia.org/wiki/Intuitionistic_logic\">intuitionist&nbsp;logic</a>: from a double negative, we <em>can</em> prove a positive (though not vice versa). It is also strictly weaker than classical logic: if dialectic logic can prove something, then the proof works in classical logic as well.</p>\n<p>Another thing to note about dialectical logic (and about most relevance logics) is that the <a href=\"http://en.wikipedia.org/wiki/Deduction_theorem\">deduction theorem</a> fails. The deduction theorem says that if A\u22a2B (if, starting from A, we can prove B), then also&nbsp;\u22a2(A\u2192B) (starting from nothing, we can prove that A implies B). It is easy to see why this would fail in relevance logics. If for instance B were a tautology, then we could certainly get&nbsp;A\u22a2B; but we could not get&nbsp;\u22a2(A\u2192B) unless there were some relevant connection between A and B.</p>\n<h2 id=\"The_semantics__and_the_semantics_of_the_semantics\">The semantics, and the semantics of the semantics</h2>\n<p>So, that's the definition of a relevance logic; but what does it all mean? There is no getting around the fact that the semantics of relevance logics are bizarre. Bizarre in the \"what's going on? we're probably missing a big part of the picture\" kind of way.</p>\n<p>We discussed impossible worlds and (binary) accessibility relations in the <a href=\"/r/discussion/lw/bs2/whos_afraid_of_impossible_worlds/\">previous post</a>. They are not however enough to get models/semantics/meanings for relevance logics. Instead we have to use a strange accessibility relation <strong>R</strong> than isn't binary but ternary (taking three inputs). Writing <strong>R</strong><strong>abc</strong> means that worlds <strong>a</strong>, <strong>b</strong> and <strong>c</strong> are related in this formal way. Then the definition of&nbsp;\u2192&nbsp;in the model becomes:</p>\n<p style=\"padding-left: 30px;\">A \u2192 B is true at a world <strong>a</strong> if and only if for all worlds <strong>b</strong> and <strong>c</strong> such that <strong>R</strong><strong>abc</strong>, either A is false at <strong>b</strong> or B is true at <strong>c</strong>.</p>\n<p>So these semantics define the value of&nbsp;A \u2192 B at <strong>a</strong> by looking at some kind of implication result across two completely separate worlds. Not only that, these worlds will have to include impossible worlds as well (or else the semantics would collapse to classical semantics).</p>\n<p>Generally, when discussing a theory, to explain its meaning one gives a model and leaves it at that. But this model is so counterintutive, that it seems fair to demand further explanation: a meaning for the meaning, semantics for the semantics. <a href=\"http://plato.stanford.edu/entries/logic-relevance/\">One</a> possible interesting interpretation is that instead of talking about worlds, we interpret <strong>a</strong>, <strong>b</strong> and <strong>c</strong> as collections of information. And then <strong>Rabc</strong> is taken to mean \"the combination of the information states of <strong>a</strong> and <strong>b</strong> is contained in the information state of <strong>c</strong>\" (Dunn, J.M., 1986, \"Relevance Logic and Entailment\").</p>\n<p>Yet about approach (Jon Barwise (1993) and developed in Restall (1996))&nbsp;models this as information flow in distributed systems. Here <strong>a</strong>, <strong>b</strong> and <strong>c</strong> are not worlds, but sites of information, or channels between sites along which information can flow (the fact that some \"worlds\" are sites while others are channels is not a problem - after all, some worlds are possible and others impossible, so&nbsp;distinguishing&nbsp;between different types of worlds is not an unusual requirement). Then <strong>R</strong><strong>abc</strong> is taken to mean \"<strong>a</strong> is a channel connecting the sites <strong>b</strong> and <strong>c</strong>\". Then the definition of&nbsp;\u2192&nbsp;in the model becomes:</p>\n<p style=\"padding-left: 30px;\">A \u2192 B is true on the channel <strong>a</strong> if and only if for all sites <strong>b</strong> and <strong>c</strong> connected by <strong>a</strong>, if information A is available at one end (<strong>b</strong>) of the channel, then information B is available at the other end (<strong>c</strong>).</p>\n<p>More can, and has, been said about this. But that's enough for a short introduction to the subject, and will hopefully given a start to anyone wanting to mesh relevance logics with UDT. My next and last post will briefly touch on attempts to use as much of classical logic as we can get away with, while still staying paraconsistent when we have to.</p>", "sections": [{"title": "Axioms of relevance", "anchor": "Axioms_of_relevance", "level": 1}, {"title": "The semantics, and the semantics of the semantics", "anchor": "The_semantics__and_the_semantics_of_the_semantics", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oeMYMKnALbniFaoQL", "rwysnoyCxMLqQZnCY", "8uBdbEoYuvY3ghgBq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T11:40:26.079Z", "modifiedAt": null, "url": null, "title": "[link] Why We Reason (psychology blog)", "slug": "link-why-we-reason-psychology-blog", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:07.404Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oFp6JLn8z9uxgdPp8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KxRFbQ54HuYv9ACnR/link-why-we-reason-psychology-blog", "pageUrlRelative": "/posts/KxRFbQ54HuYv9ACnR/link-why-we-reason-psychology-blog", "linkUrl": "https://www.lesswrong.com/posts/KxRFbQ54HuYv9ACnR/link-why-we-reason-psychology-blog", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Why%20We%20Reason%20(psychology%20blog)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Why%20We%20Reason%20(psychology%20blog)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxRFbQ54HuYv9ACnR%2Flink-why-we-reason-psychology-blog%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Why%20We%20Reason%20(psychology%20blog)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxRFbQ54HuYv9ACnR%2Flink-why-we-reason-psychology-blog", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKxRFbQ54HuYv9ACnR%2Flink-why-we-reason-psychology-blog", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p><a href=\"http://whywereason.wordpress.com\">Why We Reason</a> is an excellent psychology blog that has a great deal of subject matter in common with Less Wrong. Some of the topics discussed on the blog include social psychology, judgement and decision making, neuroscience, cognitive biases, and creativity. And there's even a hint of the kind of \"<a href=\"/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/\">cognitive philosophy</a>\" practiced on Less Wrong.</p>\n<p>The author, <a href=\"http://whywereason.wordpress.com/about-me/\">Sam McNerney</a>, is blessed with the rare gift of being able to distill psychology topics for a lay audience, and his posts are very lucid.</p>\n<p>There's also a handy <a href=\"http://whywereason.wordpress.com/all-posts-2/\">archive</a> of every post on the site.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KxRFbQ54HuYv9ACnR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 8.863760066623052e-07, "legacy": true, "legacyId": "15349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oTX2LXHqXqYg2u4g6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T16:13:27.029Z", "modifiedAt": null, "url": null, "title": "[link]Mass replication of Psychology articles planed.", "slug": "link-mass-replication-of-psychology-articles-planed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.541Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beoShaffer", "createdAt": "2011-05-29T15:52:29.240Z", "isAdmin": false, "displayName": "beoShaffer"}, "userId": "589WwYp3jytZqATFL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xi23Zkptm5XNuCnKN/link-mass-replication-of-psychology-articles-planed", "pageUrlRelative": "/posts/xi23Zkptm5XNuCnKN/link-mass-replication-of-psychology-articles-planed", "linkUrl": "https://www.lesswrong.com/posts/xi23Zkptm5XNuCnKN/link-mass-replication-of-psychology-articles-planed", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5DMass%20replication%20of%20Psychology%20articles%20planed.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5DMass%20replication%20of%20Psychology%20articles%20planed.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxi23Zkptm5XNuCnKN%2Flink-mass-replication-of-psychology-articles-planed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5DMass%20replication%20of%20Psychology%20articles%20planed.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxi23Zkptm5XNuCnKN%2Flink-mass-replication-of-psychology-articles-planed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxi23Zkptm5XNuCnKN%2Flink-mass-replication-of-psychology-articles-planed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>http://chronicle.com/blogs/percolator/is-psychology-about-to-come-undone/29045</p>\n<p>The plan is to replicate or fail to replicate all 2008 articles from three major Psychology journals.</p>\n<p>ETA:&nbsp;http://openscienceframework.org/ is the homepage of the group behind this. &nbsp;It's still in Beta, but will eventually include some nifty looking science toolkits in addition to the reproducibility project.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "tk4R4LrX88gmFeMmY": 1, "vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xi23Zkptm5XNuCnKN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 41, "extendedScore": null, "score": 8.864918762512271e-07, "legacy": true, "legacyId": "15351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T18:00:46.067Z", "modifiedAt": null, "url": null, "title": "The Quick Bayes Table", "slug": "the-quick-bayes-table", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:55.825Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "farsan", "createdAt": "2011-06-07T07:19:00.474Z", "isAdmin": false, "displayName": "farsan"}, "userId": "6FzkDcW2PvTzJj3x6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GDLP8MjvyhK3wx6hc/the-quick-bayes-table", "pageUrlRelative": "/posts/GDLP8MjvyhK3wx6hc/the-quick-bayes-table", "linkUrl": "https://www.lesswrong.com/posts/GDLP8MjvyhK3wx6hc/the-quick-bayes-table", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Quick%20Bayes%20Table&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Quick%20Bayes%20Table%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDLP8MjvyhK3wx6hc%2Fthe-quick-bayes-table%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Quick%20Bayes%20Table%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDLP8MjvyhK3wx6hc%2Fthe-quick-bayes-table", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDLP8MjvyhK3wx6hc%2Fthe-quick-bayes-table", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 743, "htmlBody": "<p>This is an effort to make Bayes' Theorem available to people without heavy math skills. It is possible that this has already been invented, because it is just a direct result of expanding something I read at Yudkowsky&rsquo;s <a href=\"http://yudkowsky.net/rational/bayes\">Intuitive Explanation of Bayes Theorem</a>. In that case, excuse me for reinventing the wheel. Also, English is my second language.</p>\n<p>When I read Yudkowsky&rsquo;s Intuitive Explanation of Bayes Theorem, the notion of using decibels to measure the likelihood ratio of additional evidence struck me as extremely intuitive. But in the article, the notion was just a little footnote, and I wanted to check if this could be used to simplify the theorem.<br /><br />It is harder to use logarithms than just using the Bayes Theorem the normal way, but I remembered that before modern calculators were made, mathematics carried around small tables of base 10 logarithms that saved them work in laborious multiplications and divisions, and I wondered if we could use the same in order to get quick approximations to Bayes' Theorem.<br /><br />I calculated some numbers and produced this table in order to test my idea:</p>\n<p>&nbsp;</p>\n<div dir=\"ltr\">\n<table style=\"border:none;border-collapse:collapse\" border=\"0\">\n<colgroup><col width=\"86\"></col><col width=\"86\"></col><col width=\"86\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;background-color:#c0c0c0;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Decibels</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;background-color:#c0c0c0;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Probability</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;background-color:#c0c0c0;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">Odds</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-30</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">0.1%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:1000</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-24</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">0.4%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:251</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-20</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:100</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-18</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1,5%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:63</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-15</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">3%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:32</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-12</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">6%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:16</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-11</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">7%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:12.6</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-10</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">9%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:10</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-9</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">11%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:7.9</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-8</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">14%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:6.3</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-7</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">17%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:5</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-6</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">20%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:4</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-5</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">24%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:3.2</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-4</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">28%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:2.5</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-3</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">33%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:2</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-2</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">38%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:1.6</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">-1</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">44%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:1.3</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">0</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">50%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+1</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">56%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1.3:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+2</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">62%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1.6:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+3</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">67%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">2:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+4</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">72%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">2.5:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+5</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">76%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">3.2:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+6</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">80%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">4:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+7</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">83%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">5:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+8</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">86%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">6.3:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+9</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">89%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">7.9:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+10</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">91%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">10:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+11</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">93%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">12.6:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+12</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">94%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">16:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+15</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">97%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">32:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+18</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">98.5%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">63:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+20</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">99%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">100:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+24</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">99.6%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">251:1</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">+30</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">99.9%</span></p>\n</td>\n<td style=\"border:1px dotted #aaa;vertical-align:middle;padding:1px 1px 1px 1px\">\n<p style=\"text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size:13px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;\">1000:1</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>This table's values are approximate for easier use. The odds approximately double every 3 dB (The real odds are&nbsp;1.995:1 in 3 dB) and are multiplied by 10 every 10 dB exactly.</p>\n<p>In order to use this table, you must add the decibels results from the prior probability (Using the probability column) and the likelihood ratio (Using the ratio column) in order to get the approximated answer (Probability column of the decibel result). In case of doubt between two rows, choose the closest to 0.<br /><br />For example, let's try to solve the problem in Yudkowsky&rsquo;s article:<br /><br />1% of women at age forty who participate in routine screening have breast cancer.&nbsp; 80% of women with breast cancer will get positive mammographies.&nbsp; 9.6% of women without breast cancer will also get positive mammographies.&nbsp; A woman in this age group had a positive mammography in a routine screening.&nbsp; What is the probability that she actually has breast cancer?<br /><br />1% prior gets us -20 dB in the table. For the likelihood ratio, 80% true positive versus 9.6% false positive is about a 8:1 ratio, +9 dB in the table.&nbsp; Adding both results, -20 dB + 9 dB = -11dB, and that translates into a 7% as the answer. The true answer is 7.9%, so this method managed to get close to the real answer with just a simple addition.<br /><br />--<br /><br />Yudkowsky says that the likelihood ratio doesn't tell the whole story about the possible results of a test, but I think we can use this method to get the rest of the story.<br /><br />If you can get the positive likelihood ratio as the meaning of a positive result, then you can use the negative likelihood ratio as the meaning of the negative result just reworking the problem.<br /><br />I'll use Yudkowsky's problem in order to explain myself. If 80% of women with breast cancer get positive mammographies, then 20% of them will get negative mammographies, and they will be false negatives. If 9.6% of women without breast cancer get positive mammographies, then 90.4% of them will get negative mammographies, true negatives.<br /><br />The ratio between those two values will get us the meaning of a negative result: 20% false negative versus 90.4% true negative is between 1:4 and 1:5 ratio. We get the decibel value closest to 0, -6 dB. -20 dB - 6 dB = -26 dB. This value is between -24 dB and -30 dB, so the answer will be between 0.1% and 0.4%. The true answer is 0.2%, so it also works this way.<br /><br />--<br /><br />The positive likelihood ratio and the negative likelihood ratio are a good way of describing how a certain test adds additional data. We could describe the mammography test as a +9dB/-6dB test, and with only this information we know everything we need to know about the test. If the result is positive, it adds 9dB to the evidence, and if it is negative, it subtracts 6dB to it.<br /><br />Simple and intuitive.<br /><br />By the way, as decibels are used to measure physical quantities, not probabilities, I believe that renaming the unit would be appropriate in this case. What about DeciBayes?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GDLP8MjvyhK3wx6hc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 57, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "15353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-18T22:01:12.772Z", "modifiedAt": null, "url": null, "title": "How can we get more and better LW contrarians?", "slug": "how-can-we-get-more-and-better-lw-contrarians", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:26.046Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HBupaMix2NZLngQuJ/how-can-we-get-more-and-better-lw-contrarians", "pageUrlRelative": "/posts/HBupaMix2NZLngQuJ/how-can-we-get-more-and-better-lw-contrarians", "linkUrl": "https://www.lesswrong.com/posts/HBupaMix2NZLngQuJ/how-can-we-get-more-and-better-lw-contrarians", "postedAtFormatted": "Wednesday, April 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20can%20we%20get%20more%20and%20better%20LW%20contrarians%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20can%20we%20get%20more%20and%20better%20LW%20contrarians%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBupaMix2NZLngQuJ%2Fhow-can-we-get-more-and-better-lw-contrarians%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20can%20we%20get%20more%20and%20better%20LW%20contrarians%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBupaMix2NZLngQuJ%2Fhow-can-we-get-more-and-better-lw-contrarians", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBupaMix2NZLngQuJ%2Fhow-can-we-get-more-and-better-lw-contrarians", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 630, "htmlBody": "<p>I'm worried that LW doesn't have enough good contrarians and skeptics, people who disagree with us or like to find fault in every idea they see, but do so in a way that is often right and can change our minds when they are. I fear that when contrarians/skeptics join us but aren't \"good enough\", we tend to drive them away instead of improving them.</p>\n<p>For example, I know a couple of people who occasionally had interesting ideas that were contrary to the local LW consensus, but were (or appeared to be) too confident in their ideas, both good and bad. Both people ended up being repeatedly downvoted and left our community a few months after they arrived. This must have happened more often than I have noticed (partly evidenced by the large number of comments/posts now marked as written by&nbsp;<strong>[deleted]</strong>, sometimes with whole threads written&nbsp;entirely&nbsp;by deleted accounts). I feel that this is a waste that we should try to prevent (or at least think about how we might). So here are some ideas:</p>\n<ul>\n<li>Try to \"fix\" them by telling them that they are overconfident and give them hints about how to get LW to take their ideas seriously.&nbsp;Unfortunately, from their perspective such advice must appear to come from someone who is themselves overconfident and wrong, so they're not likely to be very inclined to accept the advice.</li>\n<li>Create a&nbsp;separate section with different social norms, where people are not expected to maintain the \"proper\" level of confidence and niceness (on pain of being downvoted), and direct overconfident newcomers to it. Perhaps through no-holds-barred debate we can convince them that we're not as crazy and wrong as they thought, and<em>&nbsp;then</em>&nbsp;give them the above-mentioned advice and move them to the main sections.</li>\n<li>Give newcomers some sort of honeymoon period (marked by color-coding of their usernames or something like that), where we ignore their overconfidence and associated social transgressions (or just be extra nice and tolerant towards them), and take their ideas on their own merits. Maybe if they see us take their ideas seriously, that will cause them to reciprocate and take us more seriously when we point out that they may be wrong or overconfident.</li>\n</ul>\n<div>I guess these ideas sounded better in my head than written down, but maybe they'll inspire other people to think of better ones. And it might help a bit just to keep this issue in the back of one's mind and occasionally think strategically about how to improve the person you're arguing against, instead of only trying to win the particular argument at hand or downvoting them into leaving.</div>\n<div>P.S., after writing most of the above, I saw &nbsp;<a href=\"http://wallowinmaya.wordpress.com/2012/04/17/youre-calling-who-a-cult-leader/\">this post</a>:</div>\n<blockquote>\n<div>OTOH, I don&rsquo;t think group think is a big problem. Criticism by folks like Will Newsome, Vladimir Slepnev and especially Wei Dai is often upvoted. (I upvote almost every comment of Dai or Newsome if I don&rsquo;t forget it. Dai makes always very good points and Newsome is often wrong but also hilariously funny or just brilliant and right.) Of course, folks like this Dymytry guy are often downvoted, but IMO with good reason.</div>\n</blockquote>\n<div>To be clear, I don't think \"group think\" is the problem. In other words, it's not that we're refusing to accept valid criticisms, but more like our group dynamics (and other factors) cause there to be fewer good contrarians in our community than is optimal. Of course&nbsp;what is optimal&nbsp;might be open to debate, but from my perspective, it can't be right that my own criticisms are valued so highly (especially since I've been moving closer to the SingInst \"inner circle\" and my critical tendencies have been decreasing). In the spirit of making oneself redundant, I'd feel much better if my occasional voice of dissent is just considered one amongst many.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HBupaMix2NZLngQuJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 82, "extendedScore": null, "score": 0.000186, "legacy": true, "legacyId": "15312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 82, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 335, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-19T03:43:58.810Z", "modifiedAt": null, "url": null, "title": "Partial Transcript of the Hanson-Yudkowsky June 2011 Debate", "slug": "partial-transcript-of-the-hanson-yudkowsky-june-2011-debate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.295Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XvmrwJsRMogXrYG3h/partial-transcript-of-the-hanson-yudkowsky-june-2011-debate", "pageUrlRelative": "/posts/XvmrwJsRMogXrYG3h/partial-transcript-of-the-hanson-yudkowsky-june-2011-debate", "linkUrl": "https://www.lesswrong.com/posts/XvmrwJsRMogXrYG3h/partial-transcript-of-the-hanson-yudkowsky-june-2011-debate", "postedAtFormatted": "Thursday, April 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Partial%20Transcript%20of%20the%20Hanson-Yudkowsky%20June%202011%20Debate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APartial%20Transcript%20of%20the%20Hanson-Yudkowsky%20June%202011%20Debate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvmrwJsRMogXrYG3h%2Fpartial-transcript-of-the-hanson-yudkowsky-june-2011-debate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Partial%20Transcript%20of%20the%20Hanson-Yudkowsky%20June%202011%20Debate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvmrwJsRMogXrYG3h%2Fpartial-transcript-of-the-hanson-yudkowsky-june-2011-debate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvmrwJsRMogXrYG3h%2Fpartial-transcript-of-the-hanson-yudkowsky-june-2011-debate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2642, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">So I'm currently trying to write an article on the nature of intelligence and the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI FOOM</a> issue. I listened to the <a href=\"/lw/6h8/hanson_debating_yudkowsky_jun_2011/\">live debate</a> between Eliezer Yudkowsky and Robin Hanson they did in June 2011, and thought taken together Eliezer's comments in that debate make for a really good concise statement of his position. However, videos and audios have the disadvantage that when you're trying to make sure you remember or understand something correctly, it's can be inconvenient to find the part and re-listen to it (because of inability to skim, and to an extent because I read faster than most people talk). I finally decided to transcribe substantial chunks of Eliezer's comments for my own use, and I think my transcript would be useful to the Less Wrong community in general.</p>\n<p class=\"MsoNormal\">This is not a word for word transcript; I've eliminated verbal filler and false starts (where Eliezer stopped mid-sentence and decided to say something a different way.) If people want to transcribe parts I left out, including Robin's remarks, they can put them in the comments and I'll edit them in to the main post along with attribution. Corrections and suggested tweaks to what I've transcribed so far are also welcome.</p>\n<p class=\"MsoNormal\">EDIT: Partial credit for this post goes to kalla724, not because she contributed directly, but because I'm not sure I would have been able to do this without having read <a href=\"/lw/blr/attention_control_is_critical_for/\">this post.</a> It instantly increased my ability to avoid distractions while getting work done, sending my productivity through the roof. I hope the effects are permanent.</p>\n<p class=\"MsoNormal\">[7:30] When we try to visualize how all this is likely to go down, we tend to visualize a scenario that someone else once termed a &ldquo;brain in a box in a basement.&rdquo; And I love that phrase, so I stole it. In other words, we tend to visualize that there&rsquo;s this AI programming team a lot like the wannabe AI programming teams you see nowadays trying to create artificial general intelligence like the artificial general intelligence projects you see nowadays and they manage to acquire some new deep insights which combined with published insights in the general scientific community let them go down to their basement and work on it for a while and create an AI which is smart enough to reprogram itself and then you get an intelligence explosion.</p>\n<p class=\"MsoNormal\">[21:24] If you actually look at the genome, we&rsquo;ve got about 30,000 genes in here. Most of our 750 megabytes of DNA is repetitive and almost certainly junk, as best we understand it. And the brain is simply not a very complicated artifact by comparison to, say, Windows Vista. Now the complexity that it does have it uses a lot more effectively than Windows Vista does. It probably contains a number of design principles which Microsoft knows not. (And I&rsquo;m not saying it&rsquo;s that small because it&rsquo;s 750 megabytes, I&rsquo;m saying it&rsquo;s gotta be that small because at least 90% of the 750 megabytes is junk and there&rsquo;s only 30,000 genes for the whole body, never mind the brain.)</p>\n<p class=\"MsoNormal\">That something that simple can be this powerful, and this hard to understand, is a shock. But if you look at the brain design, it&rsquo;s got 52 major areas on each side of the cerebral cortex, distinguishable by the local pattern, the tiles and so on, it just doesn&rsquo;t really look all that complicated. It&rsquo;s very powerful. It&rsquo;s very mysterious. What can say about it is that it probably involves 1,000 different deep, major, mathematical insights into the nature of intelligence that we need to comprehend before we can build it.</p>\n<p class=\"MsoNormal\">This is probably one of the more intuitive, less easily quantified and argued by reference to large bodies of experimental evidence type thing. It&rsquo;s more a sense of, you read through <em>The MIT Encyclopedia of Cognitive Sciences</em> and you read Judea Pearl&rsquo;s <em>Probabilistic Reasoning in Intelligent Systems, </em>so here&rsquo;s an insight, it&rsquo;s an insight into the nature of causality, how many more insights of this size do we need given that this is what <em>The MIT Encyclopedia of Cognitive Sciences</em> seems to indicate we already understand and what it doesn&rsquo;t? And you sort of take a gander on it and you say it&rsquo;s probably about ten more insights, definitely not one, not a thousand, probably not a hundred either.</p>\n<p class=\"MsoNormal\">[27:34] Our nearest neighbors, the chimpanzees, have 95% shared DNA with us. Now in one sense that may be a little misleading because what they don&rsquo;t share is probably more heavily focused on brain than body type stuff. But on the other hand you can look at those brains. You can put the brains through an MRI. They have almost exactly the same brain areas as us. We just have larger versions of some brain areas. And I think there&rsquo;s one sort of neuron we have that they don&rsquo;t, or possibly even they had it but only in very tiny quantities.</p>\n<p class=\"MsoNormal\">This is because there have been only 5 million years since we split off from the chimpanzees. There simply has not been time to do any major changes to brain architecture in 5 million years. It&rsquo;s just not enough to do really significant complex machinery. The intelligence we have is the last layer of icing on the cake, and yet if you look at the curve of evolutionary optimization into the hominid line versus how much optimization power put out, how much horse power was the intelligence, it goes like this:</p>\n<p class=\"MsoNormal\">[Gestures to indicate something like a hyperbola or maybe a step function&mdash;a curve that is almost horizontal for a while, then becomes almost verticle.]</p>\n<p class=\"MsoNormal\">If we look at the world today, we find that taking a little bit out of the architecture produces something that is just not in the running as an ally or a competitor when it comes to doing cognitive labor. Chimpanzees don&rsquo;t really participate in the economy at all, in fact. But the key point from our perspective is that although they are in a different environment, they grow up different things, there are genuinely skills that chimpanzees have that we don&rsquo;t, such as being able to poke a branch into an anthill and pull it out in such a way as to have it covered with lots of tasty ants, nonetheless there are no branches of science where the chimps do better because they have mostly the same architecture and more relevant content.</p>\n<p class=\"MsoNormal\">So it seems to me at least that if we look at the present cognitive landscape we are getting really strong information that&mdash;pardon me, we&rsquo;re trying to reason from one sample, but pretty much all of this is trying to reason in one way or another&mdash;we&rsquo;re seeing that in this particular case at least, humans can develop all sorts of content that lets them totally outcompete other animal species who have been doing things for millions of things longer than we have, by virtue of architecture, and anyone who doesn&rsquo;t have the architecture isn&rsquo;t really in the running for it.</p>\n<p class=\"MsoNormal\">[33:20] This thing [picks up laptop] does run at around two billion Hz, and this thing [points to head] runs at about two hundred Hz. So if you can have architectural innovations which merely allow this thing [picks up laptop again] to do the sort of thing that this thing [points to head again] is doing, only a million times faster, then that million times faster means that 31 seconds works out to about a subjective year and all the time between ourselves and Socrates works out to about 8 hours.</p>\n<p class=\"MsoNormal\">[40:00] People have tried raising chimps in human surroundings, and they absorb this mysterious capacity for abstraction that sets them apart from other chimps. There&rsquo;s this wonderful book about one of these chimps, Kanzi was his name, very famous chimpanzee, probably the world&rsquo;s most famous chimpanzee, and probably the world&rsquo;s smartest chimpanzee as well. They were trying to teach his mother to do these human things and he was just a little baby chimp and he was watching and he picked stuff up. It&rsquo;s amazing, but nonetheless he did not go on to become the world&rsquo;s leading chimpanzee scientist using his own chimpanzee abilities separately.</p>\n<p class=\"MsoNormal\">If you look at human beings we have this enormous processing object containing billions upon billions of neurons and people still fail the Wason selection task. They cannot figure out which playing card they need to turn over to verify the rule &ldquo;if a card has an even number on one side it has a vowel on the other.&rdquo; They can&rsquo;t figure out which cards they need to turn over to verify whether this rule is true or false.</p>\n<p class=\"MsoNormal\">[47:55] The reason why I expect localish sort of things is that I expect one project to go over the threshold for intelligence in much the same way that chimps went over the threshold of intelligence and became humans (yes I know that&rsquo;s not evolutionarily accurate) and then even though they now have this functioning mind to which they can make all sorts of interesting improvements and have it run even better and better, whereas meanwhile all the other cognitive work on the planet is being done by these non-enduser-modifiable human intelligences.</p>\n<p class=\"MsoNormal\">[55:25] As far as I can tell what happens when the government tries to develop AI is nothing. But that could just be an artifact of our local technological level and it might change over the next few decades. To me it seems like a deeply confusing issue whose answer is probably not very complicated in an absolute sense. Like we know why it&rsquo;s difficult to build a star. You&rsquo;ve got to gather a very large amount of interstellar hydrogen in one place. So we understand what sort of labor goes into a star and we know why a star is difficult to build. When it comes to building a mind, we don&rsquo;t know how to do it so it seems very hard. We like query our brains to say &ldquo;map us a strategy to build this thing&rdquo; and it returns null so it feels like it&rsquo;s a very difficult problem. But in point of fact we don&rsquo;t actually know that the problem is difficult apart from being confusing. We understand the star-building problem so we know it&rsquo;s difficult. This one we don&rsquo;t know how difficult it&rsquo;s going to be after it&rsquo;s no longer confusing.</p>\n<p class=\"MsoNormal\">So to me the AI problem looks like a&mdash;it looks to me more like the sort of thing that the problem is finding bright enough researchers, bringing them together, letting them work on that problem instead of demanding that they work on something where they&rsquo;re going to produce a progress report in two years which will validate the person who approved the grant and advance their career. And so the government has historically been tremendously bad at producing basic research progress in AI, in part because the most senior people in AI are often people who got to be very senior by having failed to build it for the longest period of time. (This is not a universal statement. I&rsquo;ve met smart senior people in AI.)</p>\n<p class=\"MsoNormal\">But nonetheless, basically I&rsquo;m not very afraid of the government because I don&rsquo;t think it&rsquo;s a throw warm bodies at the problem and I don&rsquo;t think it&rsquo;s a throw warm computers at the problem. I think it&rsquo;s a good methodology, good people selection, letting them do sufficiently blue sky stuff, and so far historically the government has been tremendously bad at producing that kind of progress. (When they have a great big project to try to build something it doesn&rsquo;t work. When they fund long-term research it works.)</p>\n<p class=\"MsoNormal\">[1:01:11] Here at the Singularity Institute we plan to keep all of our most important insights private and hope that everyone else releases their results.</p>\n<p class=\"MsoNormal\">[1:02:59] The human brain is a completely crap design, which is why it can&rsquo;t solve the Wason selection task. You think up any bit of the heuristics and biases literature and there&rsquo;s 100 different ways this thing reliably, experimentally malfunctions when you give it some simple-seeming problem.</p>\n<p class=\"MsoNormal\">[1:04:26] I would hope to build an AI that was sufficiently unlike human, because it worked better, that there would be no direct concept of how fast does this run relative to you. It would be able to solve some problems very quickly and if it can solve all problems much faster than you you&rsquo;re already getting into the superintelligence range. But at the beginning you would already expect it to be able to do arithmetic immensely faster than you and at the same time it might be doing basic scientific research a bit slower. Then eventually it&rsquo;s faster than you at everything, but possibly not the first time you boot up the code.</p>\n<p class=\"MsoNormal\">[1:17:49] It seems like human brains are just not all that impressive. We don&rsquo;t add that well. We can&rsquo;t communicate with other people. One billion squirrels could not compete with a human brain. Our brains is about four times as large as a chimp, but four chimps cannot compete with one human. Making a brain twice as large, and actually incorporating that into the architecture, seems to produce a scaling of output of intelligence that is not even remotely comparable to the effect taking two brains of fixed size and letting them talk to each other using words. So an artificial intelligence can do all this neat stuff internally and possibly scale its processing power by orders of magnitude that itself has a completely different output function than human brains trying to talk to each other.</p>\n<p class=\"MsoNormal\">[1:34:12] So it seems to me that this [Hanson&rsquo;s view] is all strongly dependent first on the belief that the causes of intelligence get divided up very finely into lots of little pieces that get developed in a wide variety of different places, so that nobody gets an advantage. And second that if you do get a small advantage you&rsquo;re only doing a small fraction of the total intellectual labor going into the problem so you don&rsquo;t have a \"nuclear pile gone critical effect\" because any given pile is still a very small fraction of all the thinking that&rsquo;s going into AI everywhere.</p>\n<p class=\"MsoNormal\">I&rsquo;m not quite sure what to say besides when I look at the world it doesn&rsquo;t actually look like the world looks like that. There aren&rsquo;t twenty different species all of whom are good at different aspects of intelligence and have different advantages. G factor is pretty weak evidence, but it exists. The people talking about g factor do seem to be winning on the experimental predictions test versus the people who previously went around talking about multiple intelligences.</p>\n<p class=\"MsoNormal\">It&rsquo;s not a very transferable argument, but to extent that I actually have a grasp of cognitive science it does not look like it&rsquo;s sliced into lots of little pieces. It looks like there&rsquo;s a bunch of major systems doing particular tasks and they&rsquo;re all cooperating with each other. It&rsquo;s sort of like we have <em>a</em> heart, and not one hundred little mini hearts distributed around the body. It might have been a better system, but nonetheless we just have one big heart over there.</p>\n<p class=\"MsoNormal\">It looks to me like there&rsquo;s really obvious, hugely important things you could do with the first prototype intelligence that actually worked. And so I expect that the critical thing is going to be the first prototype intelligence that actually works and runs on a two gigahertz processor and can do little experiments to find out which of its own mental processes work better and things like that. And that the first AI that really works is already going to have a pretty large advantage relative to the biological system so that the key driver of change looks more like somebody builds a prototype and not like this large existing industry reaches a certain quality level at the point where it is mainly being driven by incremental improvements leaking out of particular organizations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XvmrwJsRMogXrYG3h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "15352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8vbdEWLHnjB7EwdbA", "rD57ysqawarsbry6v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-19T04:26:37.310Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Conscious Sorites Paradox", "slug": "seq-rerun-the-conscious-sorites-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:09.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8k4rHLgw8EtJec4NK/seq-rerun-the-conscious-sorites-paradox", "pageUrlRelative": "/posts/8k4rHLgw8EtJec4NK/seq-rerun-the-conscious-sorites-paradox", "linkUrl": "https://www.lesswrong.com/posts/8k4rHLgw8EtJec4NK/seq-rerun-the-conscious-sorites-paradox", "postedAtFormatted": "Thursday, April 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Conscious%20Sorites%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Conscious%20Sorites%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8k4rHLgw8EtJec4NK%2Fseq-rerun-the-conscious-sorites-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Conscious%20Sorites%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8k4rHLgw8EtJec4NK%2Fseq-rerun-the-conscious-sorites-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8k4rHLgw8EtJec4NK%2Fseq-rerun-the-conscious-sorites-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>Today's post, <a href=\"/lw/pv/the_conscious_sorites_paradox/\">The Conscious Sorites Paradox</a> was originally published on 28 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Decoherence is implicit in quantum physics, not an extra law on top of it. Asking exactly when \"one world\" splits into \"two worlds\" may be like asking when, if you keep removing grains of sand from a pile, it stops being a \"heap\". Even if you're inside the world, there may not be a definite answer. This puzzle does not arise only in quantum physics; the Ebborians could face it in a classical universe, or we could build sentient flat computers that split down their thickness. Is this really a physicist's problem?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/btv/seq_rerun_on_being_decoherent/\">On Being Decoherent</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8k4rHLgw8EtJec4NK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.868031711034526e-07, "legacy": true, "legacyId": "15370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nso8WXdjHLLHkJKhr", "YTgkFHZoq9veRiK96", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-19T04:35:23.519Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia Meetup: Against Rationalization", "slug": "meetup-philadelphia-meetup-against-rationalization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:07.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DasAllFolks", "createdAt": "2011-12-23T17:34:00.896Z", "isAdmin": false, "displayName": "DasAllFolks"}, "userId": "s7hnECaJ7cv5oJApQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pthha4kQM3LrDK4c7/meetup-philadelphia-meetup-against-rationalization", "pageUrlRelative": "/posts/pthha4kQM3LrDK4c7/meetup-philadelphia-meetup-against-rationalization", "linkUrl": "https://www.lesswrong.com/posts/pthha4kQM3LrDK4c7/meetup-philadelphia-meetup-against-rationalization", "postedAtFormatted": "Thursday, April 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia%20Meetup%3A%20Against%20Rationalization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia%20Meetup%3A%20Against%20Rationalization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpthha4kQM3LrDK4c7%2Fmeetup-philadelphia-meetup-against-rationalization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia%20Meetup%3A%20Against%20Rationalization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpthha4kQM3LrDK4c7%2Fmeetup-philadelphia-meetup-against-rationalization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpthha4kQM3LrDK4c7%2Fmeetup-philadelphia-meetup-against-rationalization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/99\">Philadelphia Meetup: Against Rationalization</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">26 April 2012 07:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">1301 Chestnut Street Philadelphia, PA 191073521 </span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be having our next meetup on Thursday, April 26th at 7 P.M. at the Starbucks in Macy's Center City (same location as for our immediate past meetup on macroeconomics).</p>\n<p>Topic of discussion will be the <a title=\"Against Rationalization\" href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\" target=\"_blank\">Against Rationalization</a> subsequence of the critical <a title=\"How to Actually Change Your Mind\" href=\"http://wiki.lesswrong.com/wiki/Sequences#How_To_Actually_Change_Your_Mind\" target=\"_blank\">How to Actually Change Your Mind</a> sequence of Less Wrong.</p>\n<p>Those interested in attending the meetup are kindly requested to please complete the following three tasks in advance of the 26th:</p>\n<ol>\n<li>Read the articles (if you haven't already)!</li>\n<li>Bring at least one question concerning the articles to discuss with the group (i.e maybe something which confused you, or which you think would make for thought-provoking discussion).</li>\n<li>Bring in a real-world example/case study which illustrates a concept from the articles, or else why you find said concepts important (or not important!). Your example/case study can be from your own life, someone else's life, world news, or pretty much whatever other field suits your needs best.</li>\n</ol>\n<p>The Starbucks closes at 9 P.M. that night, so we should have a good 2 hours to get through all the material the above three tasks should (hopefully) fuel in the conversation.</p>\n<p>Looking forward to seeing many of you again soon!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/99\">Philadelphia Meetup: Against Rationalization</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pthha4kQM3LrDK4c7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 8.868068959607518e-07, "legacy": true, "legacyId": "15371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Philadelphia_Meetup__Against_Rationalization\">Discussion article for the meetup : <a href=\"/meetups/99\">Philadelphia Meetup: Against Rationalization</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">26 April 2012 07:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">1301 Chestnut Street Philadelphia, PA 191073521 </span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be having our next meetup on Thursday, April 26th at 7 P.M. at the Starbucks in Macy's Center City (same location as for our immediate past meetup on macroeconomics).</p>\n<p>Topic of discussion will be the <a title=\"Against Rationalization\" href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\" target=\"_blank\">Against Rationalization</a> subsequence of the critical <a title=\"How to Actually Change Your Mind\" href=\"http://wiki.lesswrong.com/wiki/Sequences#How_To_Actually_Change_Your_Mind\" target=\"_blank\">How to Actually Change Your Mind</a> sequence of Less Wrong.</p>\n<p>Those interested in attending the meetup are kindly requested to please complete the following three tasks in advance of the 26th:</p>\n<ol>\n<li>Read the articles (if you haven't already)!</li>\n<li>Bring at least one question concerning the articles to discuss with the group (i.e maybe something which confused you, or which you think would make for thought-provoking discussion).</li>\n<li>Bring in a real-world example/case study which illustrates a concept from the articles, or else why you find said concepts important (or not important!). Your example/case study can be from your own life, someone else's life, world news, or pretty much whatever other field suits your needs best.</li>\n</ol>\n<p>The Starbucks closes at 9 P.M. that night, so we should have a good 2 hours to get through all the material the above three tasks should (hopefully) fuel in the conversation.</p>\n<p>Looking forward to seeing many of you again soon!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Philadelphia_Meetup__Against_Rationalization1\">Discussion article for the meetup : <a href=\"/meetups/99\">Philadelphia Meetup: Against Rationalization</a></h2>", "sections": [{"title": "Discussion article for the meetup : Philadelphia Meetup: Against Rationalization", "anchor": "Discussion_article_for_the_meetup___Philadelphia_Meetup__Against_Rationalization", "level": 1}, {"title": "Discussion article for the meetup : Philadelphia Meetup: Against Rationalization", "anchor": "Discussion_article_for_the_meetup___Philadelphia_Meetup__Against_Rationalization1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-19T09:39:37.656Z", "modifiedAt": null, "url": null, "title": "Adaptive Logics: the best of both worlds?", "slug": "adaptive-logics-the-best-of-both-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ubzmSsBaKEDszeWaD/adaptive-logics-the-best-of-both-worlds", "pageUrlRelative": "/posts/ubzmSsBaKEDszeWaD/adaptive-logics-the-best-of-both-worlds", "linkUrl": "https://www.lesswrong.com/posts/ubzmSsBaKEDszeWaD/adaptive-logics-the-best-of-both-worlds", "postedAtFormatted": "Thursday, April 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Adaptive%20Logics%3A%20the%20best%20of%20both%20worlds%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdaptive%20Logics%3A%20the%20best%20of%20both%20worlds%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubzmSsBaKEDszeWaD%2Fadaptive-logics-the-best-of-both-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Adaptive%20Logics%3A%20the%20best%20of%20both%20worlds%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubzmSsBaKEDszeWaD%2Fadaptive-logics-the-best-of-both-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubzmSsBaKEDszeWaD%2Fadaptive-logics-the-best-of-both-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 847, "htmlBody": "<p>Consider the following premises from which we would like to reason:</p>\n<ol>\n<li>Abraham Lincoln was born in Michigan.</li>\n<li>Abraham Lincoln was born in Kentucky.</li>\n<li>Eliezer Yudkowsky has a beard or he has blond hair.</li>\n<li>Eliezer Yudkowsky <a href=\"http://en.wikipedia.org/wiki/File:Eliezer_Yudkowsky,_Stanford_2006_(square_crop).jpg\">doesn't</a> have blond hair.</li>\n</ol>\n<p>We'd like to be able to conclude that Eliezer Yudkowsky has a beard, but we have a problem doing that. Classical logic can certainly derive that result, but because (1) and (2) contradict each other, by the <a href=\"/r/discussion/lw/bfs/paraconsistency_avoid_logical_explosions/\">principle</a> of <a href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">explosion</a>, classical logic can derive any result - including the fact that Eliezer doesn't have a beard. If we use <a href=\"http://en.wikipedia.org/wiki/Paraconsistent_logic\">paraconsistent logics</a> then we're safe from explosion. But because we've been rejecting the <a href=\"http://en.wikipedia.org/wiki/Disjunctive_syllogism\">disjunctive syllogism</a> in order to avoid explosion, we can't actually put (3) and (4) together in order to get the result we want.</p>\n<p>Informally, what we'd want to do is something like \"use classical logic wherever there's no contradictions, but restrict to paraconsistent logic whenever there are\". <a href=\"http://www.amazon.com/Introduction-Paraconsistent-Logics-Bremer-Manuel/dp/3631534132\">Adaptive logics</a> attempt to implement this idea. The presentation in this post will be even more informal than usual, as the ideas of adaptive logics are more useful than the specific implementation I've seen.</p>\n<h2>Between sky and earth, between classical and relevance</h2>\n<p>Adaptive logics make use of two logical systems: an upper limit logic ULL (generally take to be classical logic) and a weaker lower limit logic LLL (we will be using a <a href=\"/r/discussion/lw/bt8/if_you_cant_be_right_at_least_be_relevant/\">relevance logic</a> for this). Adaptive logics will then start with a collection of premises (that may include contradictions), and reason from there.</p>\n<p>The basic rules are that proofs using the LLL are always valid (since it is a weaker logic, they are also valid for the ULL). Proofs using the ULL are valid if they did not use a contradiction in their reasoning. In the example above, we would freely apply classical logic when using premises (3) and (4), but would restrict to relevance logic when using premises (1) and (2).</p>\n<p><strong>This might be enough for UDT</strong>; we would label the antecedent A()==a in the (A()==a &rarr; U()==u) statement as (potentially) contradictory, and only allow relevance logic to use it as a premise, while hitting everything else that moves with classical logic.</p>\n<p><a id=\"more\"></a></p>\n<h2>Dynamic proofs</h2>\n<p>However, it's also possible that the contradiction might not be so clear to see beforehand; possibly other contradictions may lurk deep within the premises of the algorithm. In that case, adaptive logics have a more dynamic proof system.</p>\n<p>The method goes broadly like this: skip merrily along, proving stuff left and right, and marking each line of the proofs with the premises whose consistency is being assumed. Then, if you stumble upon a contradiction, backtrack, designate some of the premises as contradictory in a sensible way, and erase anything that was derived from them using the ULL. From now on, only the LLL is allowed to handle the contradictory premises. Then resume skipping and proving until you find another contradiction, then repeat. It is also possible, depending on your algorithm, that some of the premises marked contradictory will be later unmarked (for instance, you might have known that either A or B was contradictory, and marked them both, and then later realised that B was contradictory, and so unmarked A). In that case, they become available to the ULL again.</p>\n<p>Note that this dynamic proof system can be made entirely deterministic. But we need to distinguish between two notions of derivability:</p>\n<ul>\n<li>current derivability: for those results that are derivable at some stage in the proof process, but may be erased later.</li>\n<li>final derivability: for those results that are derivable at some stage in the proof process, and will never be erased later.</li>\n</ul>\n<p>Notice that final derivability is not a <em>dynamic</em> property: it will never change, it is a statement valid \"for ever and ever\". It is, however, generally not recursive (nor r.e.), so is hard to work with in practice. The major exception being, of course, those results that are deduced using only the LLL: they are finally derivable by definition.</p>\n<p>Hence adaptive logics resemble human reasoning: some things were are certain of, others we're currently sure of but may find out we're wrong at a later date, and some things we haven't found a hole in yet but consider highly suspect. An AIXI/UDT agent using adaptive logic would be even more impossible that the normal <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI</a>: it would have not only to compute an uncomputable prior, but also magically establish final derivability. Whether this has a sensible approximation, &agrave; la <a href=\"http://www.agiri.org/docs/ComputationalApproximation.pdf\">AIXItl</a>, remains to be seen.</p>\n<h2>And the moral of the story is?</h2>\n<p>Well, this concludes my brief introduction to paraconsistent, relevance and adaptive logics. I personally don't feel they will be the route to solving UDT's problems, but I may be wrong - and by all means if someone is inspired to take this information and fix UDT, go ahead! My feeling is that some sort of proper probabilistic approach to logical truths is going to be the way forwards (for both L&ouml;bian problems and logical uncertainty). But some of the ideas presented here may end up being useful even for a probabilistic approach (e.g. <a href=\"http://en.wikipedia.org/wiki/Kripke_semantics\">Kripke semantics</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ubzmSsBaKEDszeWaD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 12, "extendedScore": null, "score": 8.869361280545496e-07, "legacy": true, "legacyId": "14826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Consider the following premises from which we would like to reason:</p>\n<ol>\n<li>Abraham Lincoln was born in Michigan.</li>\n<li>Abraham Lincoln was born in Kentucky.</li>\n<li>Eliezer Yudkowsky has a beard or he has blond hair.</li>\n<li>Eliezer Yudkowsky <a href=\"http://en.wikipedia.org/wiki/File:Eliezer_Yudkowsky,_Stanford_2006_(square_crop).jpg\">doesn't</a> have blond hair.</li>\n</ol>\n<p>We'd like to be able to conclude that Eliezer Yudkowsky has a beard, but we have a problem doing that. Classical logic can certainly derive that result, but because (1) and (2) contradict each other, by the <a href=\"/r/discussion/lw/bfs/paraconsistency_avoid_logical_explosions/\">principle</a> of <a href=\"http://en.wikipedia.org/wiki/Principle_of_explosion\">explosion</a>, classical logic can derive any result - including the fact that Eliezer doesn't have a beard. If we use <a href=\"http://en.wikipedia.org/wiki/Paraconsistent_logic\">paraconsistent logics</a> then we're safe from explosion. But because we've been rejecting the <a href=\"http://en.wikipedia.org/wiki/Disjunctive_syllogism\">disjunctive syllogism</a> in order to avoid explosion, we can't actually put (3) and (4) together in order to get the result we want.</p>\n<p>Informally, what we'd want to do is something like \"use classical logic wherever there's no contradictions, but restrict to paraconsistent logic whenever there are\". <a href=\"http://www.amazon.com/Introduction-Paraconsistent-Logics-Bremer-Manuel/dp/3631534132\">Adaptive logics</a> attempt to implement this idea. The presentation in this post will be even more informal than usual, as the ideas of adaptive logics are more useful than the specific implementation I've seen.</p>\n<h2 id=\"Between_sky_and_earth__between_classical_and_relevance\">Between sky and earth, between classical and relevance</h2>\n<p>Adaptive logics make use of two logical systems: an upper limit logic ULL (generally take to be classical logic) and a weaker lower limit logic LLL (we will be using a <a href=\"/r/discussion/lw/bt8/if_you_cant_be_right_at_least_be_relevant/\">relevance logic</a> for this). Adaptive logics will then start with a collection of premises (that may include contradictions), and reason from there.</p>\n<p>The basic rules are that proofs using the LLL are always valid (since it is a weaker logic, they are also valid for the ULL). Proofs using the ULL are valid if they did not use a contradiction in their reasoning. In the example above, we would freely apply classical logic when using premises (3) and (4), but would restrict to relevance logic when using premises (1) and (2).</p>\n<p><strong>This might be enough for UDT</strong>; we would label the antecedent A()==a in the (A()==a \u2192 U()==u) statement as (potentially) contradictory, and only allow relevance logic to use it as a premise, while hitting everything else that moves with classical logic.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Dynamic_proofs\">Dynamic proofs</h2>\n<p>However, it's also possible that the contradiction might not be so clear to see beforehand; possibly other contradictions may lurk deep within the premises of the algorithm. In that case, adaptive logics have a more dynamic proof system.</p>\n<p>The method goes broadly like this: skip merrily along, proving stuff left and right, and marking each line of the proofs with the premises whose consistency is being assumed. Then, if you stumble upon a contradiction, backtrack, designate some of the premises as contradictory in a sensible way, and erase anything that was derived from them using the ULL. From now on, only the LLL is allowed to handle the contradictory premises. Then resume skipping and proving until you find another contradiction, then repeat. It is also possible, depending on your algorithm, that some of the premises marked contradictory will be later unmarked (for instance, you might have known that either A or B was contradictory, and marked them both, and then later realised that B was contradictory, and so unmarked A). In that case, they become available to the ULL again.</p>\n<p>Note that this dynamic proof system can be made entirely deterministic. But we need to distinguish between two notions of derivability:</p>\n<ul>\n<li>current derivability: for those results that are derivable at some stage in the proof process, but may be erased later.</li>\n<li>final derivability: for those results that are derivable at some stage in the proof process, and will never be erased later.</li>\n</ul>\n<p>Notice that final derivability is not a <em>dynamic</em> property: it will never change, it is a statement valid \"for ever and ever\". It is, however, generally not recursive (nor r.e.), so is hard to work with in practice. The major exception being, of course, those results that are deduced using only the LLL: they are finally derivable by definition.</p>\n<p>Hence adaptive logics resemble human reasoning: some things were are certain of, others we're currently sure of but may find out we're wrong at a later date, and some things we haven't found a hole in yet but consider highly suspect. An AIXI/UDT agent using adaptive logic would be even more impossible that the normal <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI</a>: it would have not only to compute an uncomputable prior, but also magically establish final derivability. Whether this has a sensible approximation, \u00e0 la <a href=\"http://www.agiri.org/docs/ComputationalApproximation.pdf\">AIXItl</a>, remains to be seen.</p>\n<h2 id=\"And_the_moral_of_the_story_is_\">And the moral of the story is?</h2>\n<p>Well, this concludes my brief introduction to paraconsistent, relevance and adaptive logics. I personally don't feel they will be the route to solving UDT's problems, but I may be wrong - and by all means if someone is inspired to take this information and fix UDT, go ahead! My feeling is that some sort of proper probabilistic approach to logical truths is going to be the way forwards (for both L\u00f6bian problems and logical uncertainty). But some of the ideas presented here may end up being useful even for a probabilistic approach (e.g. <a href=\"http://en.wikipedia.org/wiki/Kripke_semantics\">Kripke semantics</a>).</p>", "sections": [{"title": "Between sky and earth, between classical and relevance", "anchor": "Between_sky_and_earth__between_classical_and_relevance", "level": 1}, {"title": "Dynamic proofs", "anchor": "Dynamic_proofs", "level": 1}, {"title": "And the moral of the story is?", "anchor": "And_the_moral_of_the_story_is_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oeMYMKnALbniFaoQL", "oLQamyNmzsgPs8nE6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-19T17:27:38.572Z", "modifiedAt": null, "url": null, "title": "A question about Eliezer", "slug": "a-question-about-eliezer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:10.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "perpetualpeace1", "createdAt": "2012-04-19T02:37:54.797Z", "isAdmin": false, "displayName": "perpetualpeace1"}, "userId": "bPkzwp8cHgCTeHekF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HjMZpMHcaJum8tXo9/a-question-about-eliezer", "pageUrlRelative": "/posts/HjMZpMHcaJum8tXo9/a-question-about-eliezer", "linkUrl": "https://www.lesswrong.com/posts/HjMZpMHcaJum8tXo9/a-question-about-eliezer", "postedAtFormatted": "Thursday, April 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20question%20about%20Eliezer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20question%20about%20Eliezer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHjMZpMHcaJum8tXo9%2Fa-question-about-eliezer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20question%20about%20Eliezer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHjMZpMHcaJum8tXo9%2Fa-question-about-eliezer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHjMZpMHcaJum8tXo9%2Fa-question-about-eliezer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 292, "htmlBody": "<p>I blew through all of MoR in about 48 hours, and in an attempt to learn more about the science and philosophy that Harry espouses, I've been reading the sequences and Eliezer's posts on Less Wrong. Eliezer has written extensively about AI, rationality, quantum physics, singularity research, etc. I have a question: how correct has he been?&nbsp; Has his interpretation of quantum physics predicted any subsequently-observed phenomena?&nbsp; Has his understanding of cognitive science and technology allowed him to successfully anticipate the progress of AI research, or has he made any significant advances himself? Is he on the record predicting anything, either right or wrong? &nbsp;&nbsp;</p>\n<p>Why is this important: when I read something written by Paul Krugman, I know that he has a Nobel Prize in economics, and I know that he has the<a href=\"http://www.poynter.org/latest-news/mediawire/130485/claim-krugman-is-top-prognosticator-cal-thomas-is-the-worst/\"> best track record of any top pundit in the US</a> in terms of making accurate predictions.&nbsp; Meanwhile, I know that Thomas Friedman <a href=\"/lw/hi/futuristic_predictions_as_consumable_goods/\">is an idiot</a>.&nbsp; Based on this track record, I believe things written by Krugman much more than I believe things written by Friedman.&nbsp; But if I hadn't read Friedman's writing from 2002-2006, then I wouldn't know how terribly wrong he has been, and I would be too credulous about his claims. &nbsp;</p>\n<p>Similarly, reading <a href=\"/lw/8yp/prediction_is_hard_especially_of_medicine/\">Mike Darwin's predictions about the future of medicine</a> was very enlightening.&nbsp; He was wrong about nearly everything.&nbsp; So now I know to distrust claims that he makes about the pace or extent of subsequent medical research. &nbsp;</p>\n<p>Has Eliezer offered anything falsifiable, or put his reputation on the line in any way?&nbsp; \"If X and Y don't happen by Z, then I have vastly overestimated the pace of AI research, or I don't understand quantum physics as well as I think I do,\" etc etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 2, "izp6eeJJEg9v5zcur": 2, "zPwTiHduqxnMHCMSu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HjMZpMHcaJum8tXo9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 52, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "15388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 160, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mZJs7FxxmhMvFxuse", "qNxPRh5jzrLorak6B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-19T23:24:24.233Z", "modifiedAt": null, "url": null, "title": "What deserves cryocide?", "slug": "what-deserves-cryocide", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.904Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rlpowell", "createdAt": "2009-03-05T00:57:26.519Z", "isAdmin": false, "displayName": "rlpowell"}, "userId": "nFgyJtHMChgKrhnvt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qytCWqq9cuMHAB4P2/what-deserves-cryocide", "pageUrlRelative": "/posts/qytCWqq9cuMHAB4P2/what-deserves-cryocide", "linkUrl": "https://www.lesswrong.com/posts/qytCWqq9cuMHAB4P2/what-deserves-cryocide", "postedAtFormatted": "Thursday, April 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20deserves%20cryocide%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20deserves%20cryocide%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqytCWqq9cuMHAB4P2%2Fwhat-deserves-cryocide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20deserves%20cryocide%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqytCWqq9cuMHAB4P2%2Fwhat-deserves-cryocide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqytCWqq9cuMHAB4P2%2Fwhat-deserves-cryocide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>So being signed up for cryonics shifts my views on life and death, as might be expected.</p>\n<p>In particular, it focuses my views of success on the preservation of my brain (everything else too, just in case, but especially the brain). &nbsp;This means, obviously, not just the lump of meat but also the information within it.</p>\n<p>If I'm suffering a degenerative disease to that meat or its information, I'm going to want to cryocide to preserve the information (and the idea of living through slow brain death doesn't thrill me regardless).</p>\n<p>What I don't know is: given the current state of science, what sorts of things do I need to be worried about?</p>\n<p>In particular, I'm wondering about Alzheimer's; does it appear to be damage to the information, or to the retrieval mechanism?</p>\n<p>But any other such diseases interest me in this context.</p>\n<p>Thanks!</p>\n<p>-Robin</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qytCWqq9cuMHAB4P2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 19, "extendedScore": null, "score": 8.872866416944952e-07, "legacy": true, "legacyId": "15391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T01:28:10.443Z", "modifiedAt": null, "url": null, "title": "[LINK] Cracked provides a humorous primer on the Singularity", "slug": "link-cracked-provides-a-humorous-primer-on-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:09.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a8DLmJEv7jACG97FA/link-cracked-provides-a-humorous-primer-on-the-singularity", "pageUrlRelative": "/posts/a8DLmJEv7jACG97FA/link-cracked-provides-a-humorous-primer-on-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/a8DLmJEv7jACG97FA/link-cracked-provides-a-humorous-primer-on-the-singularity", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Cracked%20provides%20a%20humorous%20primer%20on%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Cracked%20provides%20a%20humorous%20primer%20on%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8DLmJEv7jACG97FA%2Flink-cracked-provides-a-humorous-primer-on-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Cracked%20provides%20a%20humorous%20primer%20on%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8DLmJEv7jACG97FA%2Flink-cracked-provides-a-humorous-primer-on-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8DLmJEv7jACG97FA%2Flink-cracked-provides-a-humorous-primer-on-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>Cracked, <a href=\"/lw/89s/link_cracked_on_pitmk_fundamental_attribution/5572\">already known</a> for its lay-person-friendly approach to promoting rationality, now has a quick video that roughly explains the Singularity and why we should be worried about it while being funny and interesting to people who normally wouldn't care. Done in their After Hours series.</p>\n<p><a href=\"http://www.cracked.com/video_18400_why-scariest-sci-fi-robot-uprising-has-already-begun.html\">http://www.cracked.com/video_18400_why-scariest-sci-fi-robot-uprising-has-already-begun.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a8DLmJEv7jACG97FA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "15400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T03:14:40.387Z", "modifiedAt": null, "url": null, "title": "Melbourne Social Meetup CHANGE OF VENUE, 20/4/12, 7:00PM ", "slug": "melbourne-social-meetup-change-of-venue-20-4-12-7-00pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shokwave", "createdAt": "2010-10-12T12:55:00.568Z", "isAdmin": false, "displayName": "shokwave"}, "userId": "jtjgXtj7FepKrQPGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y3Ht7nYByrW5zbcLB/melbourne-social-meetup-change-of-venue-20-4-12-7-00pm", "pageUrlRelative": "/posts/y3Ht7nYByrW5zbcLB/melbourne-social-meetup-change-of-venue-20-4-12-7-00pm", "linkUrl": "https://www.lesswrong.com/posts/y3Ht7nYByrW5zbcLB/melbourne-social-meetup-change-of-venue-20-4-12-7-00pm", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Melbourne%20Social%20Meetup%20CHANGE%20OF%20VENUE%2C%2020%2F4%2F12%2C%207%3A00PM%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMelbourne%20Social%20Meetup%20CHANGE%20OF%20VENUE%2C%2020%2F4%2F12%2C%207%3A00PM%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3Ht7nYByrW5zbcLB%2Fmelbourne-social-meetup-change-of-venue-20-4-12-7-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Melbourne%20Social%20Meetup%20CHANGE%20OF%20VENUE%2C%2020%2F4%2F12%2C%207%3A00PM%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3Ht7nYByrW5zbcLB%2Fmelbourne-social-meetup-change-of-venue-20-4-12-7-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3Ht7nYByrW5zbcLB%2Fmelbourne-social-meetup-change-of-venue-20-4-12-7-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>Please be aware that tonight's social meet up, previously to be hosted at Ben's house, has <strong>changed venue</strong> to Trike Apps - 55 Walsh St, West Melbourne. (This is because, unfortunately, Ben is sick.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y3Ht7nYByrW5zbcLB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.873845449205934e-07, "legacy": true, "legacyId": "15412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T04:38:17.005Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Decoherence is Pointless", "slug": "seq-rerun-decoherence-is-pointless", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xnhR672GBvkrm84M4/seq-rerun-decoherence-is-pointless", "pageUrlRelative": "/posts/xnhR672GBvkrm84M4/seq-rerun-decoherence-is-pointless", "linkUrl": "https://www.lesswrong.com/posts/xnhR672GBvkrm84M4/seq-rerun-decoherence-is-pointless", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Decoherence%20is%20Pointless&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Decoherence%20is%20Pointless%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnhR672GBvkrm84M4%2Fseq-rerun-decoherence-is-pointless%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Decoherence%20is%20Pointless%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnhR672GBvkrm84M4%2Fseq-rerun-decoherence-is-pointless", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnhR672GBvkrm84M4%2Fseq-rerun-decoherence-is-pointless", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/pw/decoherence_is_pointless/\">Decoherence is Pointless</a> was originally published on 29 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is no exact point at which decoherence suddenly happens. All of quantum mechanics is continuous and differentiable, and decoherent processes are no exception to this.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/buy/seq_rerun_the_conscious_sorites_paradox/\">The Conscious Sorites Paradox</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xnhR672GBvkrm84M4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.874200982596417e-07, "legacy": true, "legacyId": "15413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aWFwfk3MBEyR4Ne8C", "8k4rHLgw8EtJec4NK", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T08:53:48.671Z", "modifiedAt": null, "url": null, "title": "[LINK] '3 Secrets of Wise Decision Making'", "slug": "link-3-secrets-of-wise-decision-making", "viewCount": null, "lastCommentedAt": "2018-10-22T23:57:49.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eG6BbBXSwxo2qNBts/link-3-secrets-of-wise-decision-making", "pageUrlRelative": "/posts/eG6BbBXSwxo2qNBts/link-3-secrets-of-wise-decision-making", "linkUrl": "https://www.lesswrong.com/posts/eG6BbBXSwxo2qNBts/link-3-secrets-of-wise-decision-making", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20'3%20Secrets%20of%20Wise%20Decision%20Making'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20'3%20Secrets%20of%20Wise%20Decision%20Making'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeG6BbBXSwxo2qNBts%2Flink-3-secrets-of-wise-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20'3%20Secrets%20of%20Wise%20Decision%20Making'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeG6BbBXSwxo2qNBts%2Flink-3-secrets-of-wise-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeG6BbBXSwxo2qNBts%2Flink-3-secrets-of-wise-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 710, "htmlBody": "<p><a href=\"http://www.personaldecisions.net/secrets.pdf\">Personal Decision Making</a> (textbook about applied decision-making, for the class I'm taking right now)</p>\n<p>The reason the book struck me as interesting was the author is employed at the university I'm going to, so if I get stuck its possible I could go and talk to him myself, and that its based in experimental psychology / psychology of decision making research.</p>\n<p>The \"3 Secrets\" the book talks about are various techniques for addressing and recognizing biases, recognizing and overcoming failures of creativity, and developing the courage necessary to make and commit to rational choices. It covers various techniques for dealing with each of these dimensions of decision making, such as forced fit and stimulus variation for creativity.</p>\n<p>From his blurb at the uni website:</p>\n<p>\"Dr. Anderson has been teaching at Portland State University since 1968. He received his B.A. in Psychology from Stanford University in 1957 and his Ph.D. in Experimental Psychology from The Johns Hopkins University in 1963. His current interests are in applications of decision psychology and decision analysis to personal decision making and public policy decision making.\"</p>\n<p>From the book sleeve:</p>\n<p>\"Barry F. Anderson is professor emeritus of Psychology at Portland State University. He teaches courses on Personal Decision Making,<br />Decision Psychology, Conflict Resolution, and Ethical Decision Making. He also publishes in the areas of cognitive psychology and judgment and decision making and consults on personal decisions and on public and private policy decisions. In The Three Secrets of Wise Decision Making he uses his many years of varied experience to bring the technology of rational decision making to the lay reader in a manner that is understandable, engaging, and readily applicable to real-life decision making.\"</p>\n<p>from the website:</p>\n<p>\"<span style=\"font-size: x-small;\">As the world has become more complex and  information more abundant, decisions have become more difficult. As the pace of  change and the range of choice have increased, decisions have to be made more  often. Yet most of us still make decisions with no more knowledge about decision  processes than our ancestors had in a simpler age, hundreds of years ago.</span><span style=\"font-family: Century Gothic;\">\n<p align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp; <span style=\"font-size: x-small;\">Mathematicians, economists, psychologists, and  practitioners have developed a variety of powerful and easily applied tools for  decision making. Evidence is accumulating that better decision processes lead to  better outcomes and that unaided human decision processes are not good enough  for many decisions. More to the present point, evidence is also accumulating  that learning better decision processes can make people better decision makers  in their daily lives. <em>The Three Secrets of Wise Decision Making</em> brings  the best of the new methods to the intelligent reader.</span></p>\n<p align=\"JUSTIFY\"><span style=\"font-size: x-small;\"><em>&nbsp;&nbsp;&nbsp; The Three Secrets </em>is designed expressly  to help people make better decisions. It has been repeatedly tested in a course  on personal decision making. The approach of the book is unabashedly practical.  Except for portions of the second chapter, the emphasis is consistently on what  to do. What the second chapter does is provide a brief overview of basic  cognitive processes and the ways in which they tend to limit decision quality  and also a brief explanation of the basic decision aids and the ways in which  each supplements basic cognitive processes to enhance rationality, creativity,  or judgment&mdash;the \"three secrets\". Some understanding of why the  techniques are needed and how they work should enable the reader to apply them  with greater effectiveness and satisfaction.</span></p>\n<p align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;<em> </em><span style=\"font-size: x-small;\"><em>The Three Secrets </em> is organized around  the Decision Ladder, a structured array of techniques to suit all decision  problems and all decision makers. The Ladder extends from largely intuitive  approaches, at the bottom, to decision trees, at the top. The key rung on the  Ladder is the decision table and its variants: fact tables, plusses-and-minuses  value tables, and 1-to-10 value tables. In the last chapter, the decision tree  is introduced as a more sophisticated way of dealing with risky decisions and  sequences of decisions. It is recommended that the reader start at the bottom of  the Decision Ladder when beginning work on any decision problem and work up only  so far as necessary. This keeps the process of decision making from becoming  more complicated than would be appropriate for either the decision problem or  the decision maker.</span></p>\n<p><span style=\"font-size: x-small;\"><em>&nbsp;&nbsp;&nbsp; The Three Secrets</em> is richly provided with examples  taken from life. One of the examples, Amelia&rsquo;s career decision, runs through  the entire book, adding human interest and conceptual continuity.\"</span></p>\n</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eG6BbBXSwxo2qNBts", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "15425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T13:12:04.265Z", "modifiedAt": null, "url": null, "title": "Curiosity checklist: Looking for feedback", "slug": "curiosity-checklist-looking-for-feedback", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nefwz9sLbyGEbinJg/curiosity-checklist-looking-for-feedback", "pageUrlRelative": "/posts/Nefwz9sLbyGEbinJg/curiosity-checklist-looking-for-feedback", "linkUrl": "https://www.lesswrong.com/posts/Nefwz9sLbyGEbinJg/curiosity-checklist-looking-for-feedback", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Curiosity%20checklist%3A%20Looking%20for%20feedback&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACuriosity%20checklist%3A%20Looking%20for%20feedback%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNefwz9sLbyGEbinJg%2Fcuriosity-checklist-looking-for-feedback%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Curiosity%20checklist%3A%20Looking%20for%20feedback%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNefwz9sLbyGEbinJg%2Fcuriosity-checklist-looking-for-feedback", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNefwz9sLbyGEbinJg%2Fcuriosity-checklist-looking-for-feedback", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 459, "htmlBody": "<p>I'm helping CMR create a 'rationality checklist'.&nbsp;The basic idea is this:&nbsp;</p>\n<ul>\n<li>Answer a series of clear-cut, unambiguous yes/no questions that reflect your current level of rationality; obtain a score.</li>\n<li>Test again in the future as a means of measuring progress in your rationality training.</li>\n</ul>\n<p>Sample application: before and after a minicamp. Target market: LWers.</p>\n<p>I made a checklist for Curiosity (below). Others are in the works.&nbsp;A question like \"Am I a curious person?\", while it is useful to know, is too vague for this checklist and too&nbsp;susceptible&nbsp;to bias.</p>\n<p>Any and all feedback is greatly appreciated. It is easy to answer yes/no? Is anything missing? Anything unclear? Would you find this useful?</p>\n<p class=\"p1\"><strong>Curiosity</strong></p>\n<ol class=\"ol1\">\n<li class=\"li2\">Do you have specific habits for getting curious when you notice you're not curious about something important?</li>\n<li class=\"li2\">Do you, in every situation,&nbsp;endeavour&nbsp;to have an accurate map of the territory?</li>\n<li class=\"li2\">Do you regularly acknowledge and accept the possible worlds that may exist? E.g. \"If the iron is hot, I desire to believe it it hot; if it is cool, I desire to believe it is cool.\"</li>\n<li class=\"li2\">Do you regularly ask, &ldquo;What are the causes of my beliefs? Why do I think this? What&rsquo;s the source?&rdquo;</li>\n<li class=\"li2\">Do you regularly ask, &ldquo;What would I expect to see differently if x was or was not the case?&rdquo;</li>\n<li class=\"li2\">Do you regularly ask, when unexpected things happen, &ldquo;Why didn&rsquo;t I expect x to happen?&rdquo;</li>\n<li class=\"li2\">When you sit down to think, or to look something up, do you regularly ask, \"What am I chasing? Why am I doing this? Am I asking myself questions about this?\"</li>\n<li class=\"li2\">Do you frequently stop to consider what information will be most valuable to achieving your goals?</li>\n<li class=\"li2\">Do you frequently ask, \"What do I most want to accomplish?\"</li>\n<li class=\"li2\">Do you focus your curiosity on the information you need to achieve your goals? E.g. \"What do I need to know in order to achieve that thing?&nbsp;What is most likely to help me learn this and figure it out?\"</li>\n<li class=\"li2\">Do you stop reading when a source becomes irrelevant?</li>\n<li class=\"li2\">Do you actively seek out more useful information? E.g. \"What are the best sources? Where is the best information?\"</li>\n<li class=\"li2\">Do you gravitate to inquiries that seem most promising of producing shifts in belief?</li>\n<li class=\"li2\">Do you gravitate to inquiries that are least like the ones you've tried before?</li>\n<li class=\"li2\">Do you ever call topics or ideas boring, shallow, crazy, beneath you, or confusing (or other words that close off thought)?</li>\n<li class=\"li2\">Do you notice when conflicting emotions cut off your curiosity?</li>\n<li class=\"li2\">Do you, in every social interaction, ask what that person can teach you?</li>\n<li class=\"li2\">Do you, in every situation where you receive feedback, treat it as potentially valuable?</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nefwz9sLbyGEbinJg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 8.876386297138152e-07, "legacy": true, "legacyId": "15431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T14:07:23.318Z", "modifiedAt": null, "url": null, "title": "Logical fallacy poster", "slug": "logical-fallacy-poster", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Utopiah", "createdAt": "2012-03-10T14:57:03.414Z", "isAdmin": false, "displayName": "Utopiah"}, "userId": "gBoAdZcTffQnbsMAb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TaQpTC2EybBn2wWRA/logical-fallacy-poster", "pageUrlRelative": "/posts/TaQpTC2EybBn2wWRA/logical-fallacy-poster", "linkUrl": "https://www.lesswrong.com/posts/TaQpTC2EybBn2wWRA/logical-fallacy-poster", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20fallacy%20poster&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20fallacy%20poster%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTaQpTC2EybBn2wWRA%2Flogical-fallacy-poster%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20fallacy%20poster%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTaQpTC2EybBn2wWRA%2Flogical-fallacy-poster", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTaQpTC2EybBn2wWRA%2Flogical-fallacy-poster", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8, "htmlBody": "<p><a href=\"http://www.yourlogicalfallacyis.com\">http://www.yourlogicalfallacyis.com</a></p>\n<p><img src=\"http://www.yourlogicalfallacyis.com/assets/poster.jpg\" alt=\"\" width=\"727\" height=\"514\" /></p>\n<p>Just printed an A3 of this.</p>\n<p>&nbsp;</p>\n<p>See now http://lesswrong.com/lw/c9u/logical_fallacies_poster_a_lesswrong_adaptation/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TaQpTC2EybBn2wWRA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 13, "extendedScore": null, "score": 8.876621638482499e-07, "legacy": true, "legacyId": "15432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T14:29:06.247Z", "modifiedAt": null, "url": null, "title": "Please Don't Fight the Hypothetical", "slug": "please-don-t-fight-the-hypothetical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:06.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TimS", "createdAt": "2011-10-11T12:16:35.235Z", "isAdmin": false, "displayName": "TimS"}, "userId": "pewD8vNSS3LGCvE4t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s9hTXtAPn2ZEAWutr/please-don-t-fight-the-hypothetical", "pageUrlRelative": "/posts/s9hTXtAPn2ZEAWutr/please-don-t-fight-the-hypothetical", "linkUrl": "https://www.lesswrong.com/posts/s9hTXtAPn2ZEAWutr/please-don-t-fight-the-hypothetical", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20Don't%20Fight%20the%20Hypothetical&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20Don't%20Fight%20the%20Hypothetical%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9hTXtAPn2ZEAWutr%2Fplease-don-t-fight-the-hypothetical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20Don't%20Fight%20the%20Hypothetical%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9hTXtAPn2ZEAWutr%2Fplease-don-t-fight-the-hypothetical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9hTXtAPn2ZEAWutr%2Fplease-don-t-fight-the-hypothetical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 592, "htmlBody": "<p>It is a common part of moral reasoning to propose hypothetical scenarios.&nbsp; Whether it is our own Torture v. Specks or the more famous Trolley problem, asking these types of questions helps the participants formalize and understand their moral positions.&nbsp; Yet one common response to hypothetical scenarios is to challenge some axiom of the problem.&nbsp; This article is a request that people stop doing that, and an explanation of why this is an error.</p>\n<p>First, a brief digression into law, which is frequently taught using <a href=\"http://en.wikipedia.org/wiki/Socratic_method#Law_schools\">hypothetical questioning</a>.&nbsp; Under the <a href=\"http://en.wikipedia.org/wiki/Model_Penal_Code\">Model Penal Code</a>:</p>\n<p>A person acts knowingly with respect to a material element of an offense when:<br />(i) if the element involves the nature of his conduct or the attendant<br />circumstances, he is aware that his conduct is of that nature or that such<br />circumstances exist; and<br />(ii) if the element involves a result of his conduct, he is aware that it is<br />practically certain that his conduct will cause such a result.</p>\n<p style=\"padding-left: 30px;\">Hypothetical: If Bob sets fire to a house with Charlie inside, killing Charlie, is Bob guilty of knowing killing of Charlie?&nbsp; Bob genuinely believes throwing salt over one's shoulder when one sets a building on fire protects all the inhabitants and ensures that they will not be harmed - and did throw salt over his shoulder in this instance.</p>\n<p>Let us take it as a given that setting someone on fire is practically certain to kill them.&nbsp; Nonetheless, Bob did not <em>knowingly</em> kill Charlie because Bob was not aware of the consequence of his action.&nbsp; Bob had a false belief that prevented him from having the belief required under the MPC to show <em>knowledge</em>.</p>\n<p>The obvious response here is that, in practice, the known facts will lead to Bob's conviction of the crime at trial.&nbsp; This is irrelevant.&nbsp; Bob will be convicted at trial because the jury will not believe Bob's asserted belief was true.&nbsp; Unless Bob is insane or mentally deficient, the jury would be <em>right</em> to disbelieve Bob.&nbsp; But that missed the point of the hypothetical.</p>\n<p>The purpose of the hypothetical is to distinguish between one type of mental state and a different type of mental state.&nbsp; If you don't understand that Bob is innocent of knowing killing if he truly believed that Charlie was safe, then you don't understand the MPC definition of knowing.&nbsp; Discussion of how mental states are proven at real trials, or whether knowing killing should be the only criminal statute about killing are <strong>different topics</strong>.&nbsp; Talking about those topics will <em>not</em> help you understand the MPC definition of knowing.&nbsp; Talking about those other topics is <em>functionally identical</em> to saying that you don't care about understanding the MPC definition of knowing.</p>\n<p>Likewise, people who responds to the Trolley problem by saying that they would call the police are not talking about the moral intuitions that the Trolley problem intends to explore.&nbsp; There's nothing wrong with you if those problems are not interesting to you.&nbsp; But fighting the hypothetical by challenging the premises of the scenario is exactly the same as saying, \"I don't find this topic interesting for whatever reason, and wish to talk about something I am interested in.\"</p>\n<p>In short, fighting the premises of a hypothetical scenario is changing the topic to focus on something different than topic of conversation intended by the presenter of the hypothetical question.&nbsp; When changing the topic is appropriate is a different discussion, but it is obtuse to fail to notice that one is changing the subject.</p>\n<p>Edit: My thesis \"Notice and Justify changing the subject,\" not \"Don't change the subject.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "nSHiKwWyMZFdZg5qt": 1, "RE6h98Ziwcfh4EP9T": 1, "rjEZWSbSffhaWYRvo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s9hTXtAPn2ZEAWutr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 39, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "15433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T14:56:45.121Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Brussels, Chicago, Dorset, Fort Lauderdale, London, Melbourne, Pittsburgh, Shanghai, Sydney, Twin Cities", "slug": "weekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:09.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kW9yYtmCyz2JqT4ch/weekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale", "pageUrlRelative": "/posts/kW9yYtmCyz2JqT4ch/weekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale", "linkUrl": "https://www.lesswrong.com/posts/kW9yYtmCyz2JqT4ch/weekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Brussels%2C%20Chicago%2C%20Dorset%2C%20Fort%20Lauderdale%2C%20London%2C%20Melbourne%2C%20Pittsburgh%2C%20Shanghai%2C%20Sydney%2C%20Twin%20Cities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Brussels%2C%20Chicago%2C%20Dorset%2C%20Fort%20Lauderdale%2C%20London%2C%20Melbourne%2C%20Pittsburgh%2C%20Shanghai%2C%20Sydney%2C%20Twin%20Cities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkW9yYtmCyz2JqT4ch%2Fweekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Brussels%2C%20Chicago%2C%20Dorset%2C%20Fort%20Lauderdale%2C%20London%2C%20Melbourne%2C%20Pittsburgh%2C%20Shanghai%2C%20Sydney%2C%20Twin%20Cities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkW9yYtmCyz2JqT4ch%2Fweekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkW9yYtmCyz2JqT4ch%2Fweekly-lw-meetups-brussels-chicago-dorset-fort-lauderdale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 503, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/87\">Brussels meetup:&nbsp;<span class=\"date\">14 April 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/8o\">First Dorset UK Meetup:&nbsp;<span class=\"date\">14 April 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/90\">Fort Lauderdale:&nbsp;<span class=\"date\">14 April 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/8d\">Shanghai Less Wrong Meetup:&nbsp;<span class=\"date\">15 April 2012 10:36PM</span></a></li>\n<li><a href=\"/meetups/8q\">Twin Cities, MN (for real this time):&nbsp;<span class=\"date\">15 April 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/8a\">Sydney meetup - Biased pandemic and other games:&nbsp;<span class=\"date\">17 April 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/8z\">Pittsburgh - Presentation on Anthropics:&nbsp;<span class=\"date\">20 April 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/8j\">Rome LessWrong Meetup:&nbsp;<span class=\"date\">21 April 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/8r\">Longmont Sparkfun Soldering Competition Field Trip:&nbsp;<span class=\"date\">28 April 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/8i\">Graz Meetup:&nbsp;<span class=\"date\">28 April 2012 11:21PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/8x\">Weekly Chicago Meetups:&nbsp;<span class=\"date\">14 April 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/8u\">London:&nbsp;<span class=\"date\">15 April 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/8y\">Melbourne social meetup:&nbsp;<span class=\"date\">20 April 2012 07:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kW9yYtmCyz2JqT4ch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.876831657831712e-07, "legacy": true, "legacyId": "15172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T15:12:13.257Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta - Game night!", "slug": "meetup-atlanta-game-night", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DaHFz2grqfvr4TSEB/meetup-atlanta-game-night", "pageUrlRelative": "/posts/DaHFz2grqfvr4TSEB/meetup-atlanta-game-night", "linkUrl": "https://www.lesswrong.com/posts/DaHFz2grqfvr4TSEB/meetup-atlanta-game-night", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20-%20Game%20night!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20-%20Game%20night!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDaHFz2grqfvr4TSEB%2Fmeetup-atlanta-game-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20-%20Game%20night!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDaHFz2grqfvr4TSEB%2Fmeetup-atlanta-game-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDaHFz2grqfvr4TSEB%2Fmeetup-atlanta-game-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9a'>Atlanta - Game night!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 April 2012 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">marietta, ga</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our LessWrong meetup group will be getting together to play a variety of games (rationality-related or not) at a private residence in Marietta, this Sunday April 22nd at 6:30pm.</p>\n\n<p>Anyone is welcome to join in.</p>\n\n<p>Please send me a message, or better yet an email to my username at gmail for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9a'>Atlanta - Game night!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DaHFz2grqfvr4TSEB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.876897469627127e-07, "legacy": true, "legacyId": "15435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta___Game_night_\">Discussion article for the meetup : <a href=\"/meetups/9a\">Atlanta - Game night!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 April 2012 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">marietta, ga</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our LessWrong meetup group will be getting together to play a variety of games (rationality-related or not) at a private residence in Marietta, this Sunday April 22nd at 6:30pm.</p>\n\n<p>Anyone is welcome to join in.</p>\n\n<p>Please send me a message, or better yet an email to my username at gmail for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta___Game_night_1\">Discussion article for the meetup : <a href=\"/meetups/9a\">Atlanta - Game night!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta - Game night!", "anchor": "Discussion_article_for_the_meetup___Atlanta___Game_night_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta - Game night!", "anchor": "Discussion_article_for_the_meetup___Atlanta___Game_night_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T19:38:25.958Z", "modifiedAt": null, "url": null, "title": "Stupid Questions Open Thread Round 2", "slug": "stupid-questions-open-thread-round-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s9exm8K8CHkEGtiPC/stupid-questions-open-thread-round-2", "pageUrlRelative": "/posts/s9exm8K8CHkEGtiPC/stupid-questions-open-thread-round-2", "linkUrl": "https://www.lesswrong.com/posts/s9exm8K8CHkEGtiPC/stupid-questions-open-thread-round-2", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stupid%20Questions%20Open%20Thread%20Round%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStupid%20Questions%20Open%20Thread%20Round%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9exm8K8CHkEGtiPC%2Fstupid-questions-open-thread-round-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stupid%20Questions%20Open%20Thread%20Round%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9exm8K8CHkEGtiPC%2Fstupid-questions-open-thread-round-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9exm8K8CHkEGtiPC%2Fstupid-questions-open-thread-round-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>From Costanza's original <a href=\"/r/discussion/lw/932/stupid_questions_open_thread/\">thread</a>&nbsp;(entire text):</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">This is for anyone in the LessWrong community who has made at least some effort to read the sequences and follow along, but is still confused on some point, and is perhaps feeling a bit embarrassed. Here, newbies and not-so-newbies are free to ask very basic but still relevant questions with the understanding that the answers are probably somewhere in the sequences. Similarly, LessWrong tends to presume a rather high threshold for understanding science and technology. Relevant questions in those areas are welcome as well.&nbsp; Anyone who chooses to respond should respectfully guide the questioner to a helpful resource, and questioners should be appropriately grateful. Good faith should be presumed on both sides, unless and until it is shown to be absent.&nbsp; If a questioner is not sure whether a question is relevant, ask it, and also ask if it's relevant.</span></p>\n</blockquote>\n<p style=\"text-align: justify;\"><span style=\"text-align: -webkit-auto; \"><strong>Meta</strong>:</span></p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<ul>\n<li>How often should these be made? I think one every three months is the correct frequency.</li>\n<li>Costanza made the original thread, but I am OpenThreadGuy. I am therefore not only entitled but <em>required</em>&nbsp;to post this in his stead. But I got his permission anyway.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s9exm8K8CHkEGtiPC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 21, "extendedScore": null, "score": 8.878028304612878e-07, "legacy": true, "legacyId": "15436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 209, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8dijcs9BjxaXE2A9G"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-20T23:15:13.623Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison Monday Meetup", "slug": "meetup-madison-monday-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CAqh8vLeKWXCAzhYA/meetup-madison-monday-meetup-1", "pageUrlRelative": "/posts/CAqh8vLeKWXCAzhYA/meetup-madison-monday-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/CAqh8vLeKWXCAzhYA/meetup-madison-monday-meetup-1", "postedAtFormatted": "Friday, April 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%20Monday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%20Monday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCAqh8vLeKWXCAzhYA%2Fmeetup-madison-monday-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20Monday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCAqh8vLeKWXCAzhYA%2Fmeetup-madison-monday-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCAqh8vLeKWXCAzhYA%2Fmeetup-madison-monday-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9b'>Madison Monday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 April 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last week, we discussed the first half-or-so of the <a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">Human&#39;s Guide to Words</a>, going so far as to brainstorm some methods and habits that'd be useful to learn to handle these and similar failures.</p>\n\n<p>I'll take the notes that we made and attempt to form actual exercises and games out of them. At the meetup, we'll try some of these, and, perhaps, refine them until they're fun, useful-seeming, or both.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9b'>Madison Monday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CAqh8vLeKWXCAzhYA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 8.878952939736414e-07, "legacy": true, "legacyId": "15437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9b\">Madison Monday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 April 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last week, we discussed the first half-or-so of the <a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">Human's Guide to Words</a>, going so far as to brainstorm some methods and habits that'd be useful to learn to handle these and similar failures.</p>\n\n<p>I'll take the notes that we made and attempt to form actual exercises and games out of them. At the meetup, we'll try some of these, and, perhaps, refine them until they're fun, useful-seeming, or both.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9b\">Madison Monday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison Monday Meetup", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Madison Monday Meetup", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-21T00:31:48.741Z", "modifiedAt": null, "url": null, "title": "Prisoner's Dilemma on game show Golden Balls", "slug": "prisoner-s-dilemma-on-game-show-golden-balls", "viewCount": null, "lastCommentedAt": "2017-07-19T04:35:57.954Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atorm", "createdAt": "2011-03-30T16:41:30.635Z", "isAdmin": false, "displayName": "atorm"}, "userId": "PvazkPKLZs5LNujcL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/owJCtureTddLFTNkj/prisoner-s-dilemma-on-game-show-golden-balls", "pageUrlRelative": "/posts/owJCtureTddLFTNkj/prisoner-s-dilemma-on-game-show-golden-balls", "linkUrl": "https://www.lesswrong.com/posts/owJCtureTddLFTNkj/prisoner-s-dilemma-on-game-show-golden-balls", "postedAtFormatted": "Saturday, April 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prisoner's%20Dilemma%20on%20game%20show%20Golden%20Balls&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrisoner's%20Dilemma%20on%20game%20show%20Golden%20Balls%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FowJCtureTddLFTNkj%2Fprisoner-s-dilemma-on-game-show-golden-balls%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prisoner's%20Dilemma%20on%20game%20show%20Golden%20Balls%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FowJCtureTddLFTNkj%2Fprisoner-s-dilemma-on-game-show-golden-balls", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FowJCtureTddLFTNkj%2Fprisoner-s-dilemma-on-game-show-golden-balls", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p>I found this to be a very interesting method of dealing with a modified Prisoner's Dilemma. In this situation, if both players cooperate they split a cash prize, but if one defects he gets the entire prize. The difference from the normal prisoner's dilemma is that if both defect, neither gets anything, so a player gains nothing by defecting if he knows his opponent will defect; he merely has the option to hurt him out of spite. Watch and see how one player deals with this.<br /><a href=\"http://www.youtube.com/watch?v=S0qjK3TWZE8\">http://www.youtube.com/watch?v=S0qjK3TWZE8</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "owJCtureTddLFTNkj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 28, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "15438", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-21T10:20:04.861Z", "modifiedAt": null, "url": null, "title": "Against the Bottom Line", "slug": "against-the-bottom-line", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gRR", "createdAt": "2012-02-02T12:11:00.628Z", "isAdmin": false, "displayName": "gRR"}, "userId": "LPBRzHQvMP9chLNWH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H6zSzeJwHHd3zpksE/against-the-bottom-line", "pageUrlRelative": "/posts/H6zSzeJwHHd3zpksE/against-the-bottom-line", "linkUrl": "https://www.lesswrong.com/posts/H6zSzeJwHHd3zpksE/against-the-bottom-line", "postedAtFormatted": "Saturday, April 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20the%20Bottom%20Line&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20the%20Bottom%20Line%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH6zSzeJwHHd3zpksE%2Fagainst-the-bottom-line%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20the%20Bottom%20Line%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH6zSzeJwHHd3zpksE%2Fagainst-the-bottom-line", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH6zSzeJwHHd3zpksE%2Fagainst-the-bottom-line", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>In the spirit of contrarianism, I'd like to argue against <a href=\"/lw/js/the_bottom_line\">The Bottom Line</a>.</p>\n<p>As I understand the post, its idea is that a rationalist should never \"start with a bottom line and then fill out the arguments\".</p>\n<p>It sounds neat, but I think it is not psychologically feasible. I find that whenever I actually argue, I <em>always</em> have the conclusion already written. Without it, it is impossible to have any direction, and an argument without any direction does not go anywhere.</p>\n<p>What actually happens is:</p>\n<ol>\n<li>I arrive at a conclusion, intuitively, as a result of a process which is usually closed to introspection.</li>\n<li>I write the bottom line, and look for a chain of reasoning that supports it.</li>\n<li>I check the argument and modify/discard it or parts of it if any are found defective.</li>\n</ol>\n<p>It is at the point 3 that the biases really struck. <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">Motivated Stopping</a> makes me stop checking too early, and <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">Motivated Continuation</a> makes me look for better arguments when defective ones are found for the conclusion I seek, but not for alternatives, resulting in Straw Men.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H6zSzeJwHHd3zpksE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}], "voteCount": 23, "baseScore": 7, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "15461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34XxbRFe54FycoCDw", "L32LHWzy9FzSDazEg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-21T13:33:36.025Z", "modifiedAt": null, "url": null, "title": "Hofstadter's Superrationality", "slug": "hofstadter-s-superrationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:02.641Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pPXX56Htw5CLekAib/hofstadter-s-superrationality", "pageUrlRelative": "/posts/pPXX56Htw5CLekAib/hofstadter-s-superrationality", "linkUrl": "https://www.lesswrong.com/posts/pPXX56Htw5CLekAib/hofstadter-s-superrationality", "postedAtFormatted": "Saturday, April 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hofstadter's%20Superrationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHofstadter's%20Superrationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPXX56Htw5CLekAib%2Fhofstadter-s-superrationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hofstadter's%20Superrationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPXX56Htw5CLekAib%2Fhofstadter-s-superrationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPXX56Htw5CLekAib%2Fhofstadter-s-superrationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Possibly the main and original inspiration for Yudkowsky's various musings on what advanced game theories should do (eg. cooperate in <a href=\"/lw/tn/the_true_prisoners_dilemma/\">the Prisoner's Dilemma</a>) is a set of essays penned by Douglas Hofstadter (of <em>Godel, Escher, Bach</em>) 1983. Unfortunately, they were not online and only available as part of a dead-tree collection. This is unfortunate. Fortunately the collection is available through the usual pirates as a scan, and I took the liberty of transcribing by hand the relevant essays with images, correcting errors, annotating with links, etc: <a href=\"http://www.gwern.net/docs/1985-hofstadter\">http://www.gwern.net/docs/1985-hofstadter</a></p>\n<p>The 3 essays:</p>\n<ol>\n<li><a href=\"http://www.gwern.net/docs/1985-hofstadter#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery\">discuss the Prisoner's dilemma</a>, the misfortune of defection, what sort of cooperative reasoning would maximize returns in a souped-up Prisoner's dilemma, and then offers a public contest</li>\n<li>then we learn <a href=\"http://www.gwern.net/docs/1985-hofstadter#irrationality-is-the-square-root-of-all-evil\">the results of the contest</a>, and a discussion of ecology and the tragedy of the commons</li>\n<li>finally, Hofstadter gives <a href=\"http://www.gwern.net/docs/1985-hofstadter#the-tale-of-happiton\">an extended parable</a> about cooperation in the face of nuclear warfare; it is fortunate for us that it applies to most existential threats as well</li>\n</ol>\n<p>I hope you find them educational. I am not 100% confident of the math transcriptions since the original ebook messed some of them up; if you find any apparent mistakes or typos, please leave comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "wFDrB47FAhkLgp4bJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pPXX56Htw5CLekAib", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 70, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "15462", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFyWNBnDNEDsDNLrZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-21T18:32:53.900Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Decoherent Essences", "slug": "seq-rerun-decoherent-essences", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pwnBHus398pS24uAv/seq-rerun-decoherent-essences", "pageUrlRelative": "/posts/pwnBHus398pS24uAv/seq-rerun-decoherent-essences", "linkUrl": "https://www.lesswrong.com/posts/pwnBHus398pS24uAv/seq-rerun-decoherent-essences", "postedAtFormatted": "Saturday, April 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Decoherent%20Essences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Decoherent%20Essences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwnBHus398pS24uAv%2Fseq-rerun-decoherent-essences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Decoherent%20Essences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwnBHus398pS24uAv%2Fseq-rerun-decoherent-essences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwnBHus398pS24uAv%2Fseq-rerun-decoherent-essences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>Today's post, <a href=\"/lw/px/decoherent_essences/\">Decoherent Essences</a> was originally published on 30 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Decoherence is implicit within physics, not an extra law on top of it. You can choose representations that make decoherence harder to see, just like you can choose representations that make apples harder to see, but exactly the same physical process still goes on; the apple doesn't disappear and neither does decoherence. If you could make decoherence magically go away by choosing the right representation, we wouldn't need to shield quantum computers from the environment.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bw5/seq_rerun_decoherence_is_pointless/\">Decoherence is Pointless</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pwnBHus398pS24uAv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.883882925125508e-07, "legacy": true, "legacyId": "15463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HwMfEcmxyM3eqqfvi", "xnhR672GBvkrm84M4", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-21T23:54:32.827Z", "modifiedAt": null, "url": null, "title": "Euclidean - Computer Graphics Breakthrough", "slug": "euclidean-computer-graphics-breakthrough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:20.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EphemeralNight", "createdAt": "2011-09-07T19:48:02.531Z", "isAdmin": false, "displayName": "EphemeralNight"}, "userId": "uWtJm9TRd8jFyRmwb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XXg6zLeFuxWrMa8cz/euclidean-computer-graphics-breakthrough", "pageUrlRelative": "/posts/XXg6zLeFuxWrMa8cz/euclidean-computer-graphics-breakthrough", "linkUrl": "https://www.lesswrong.com/posts/XXg6zLeFuxWrMa8cz/euclidean-computer-graphics-breakthrough", "postedAtFormatted": "Saturday, April 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Euclidean%20-%20Computer%20Graphics%20Breakthrough&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEuclidean%20-%20Computer%20Graphics%20Breakthrough%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXg6zLeFuxWrMa8cz%2Feuclidean-computer-graphics-breakthrough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Euclidean%20-%20Computer%20Graphics%20Breakthrough%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXg6zLeFuxWrMa8cz%2Feuclidean-computer-graphics-breakthrough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXg6zLeFuxWrMa8cz%2Feuclidean-computer-graphics-breakthrough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>A 9-person&nbsp;Australian&nbsp;company called Euclidean has a new software technology that blows all the previously-believed limitations of real-time rendering right out the window.</p>\n<p><a href=\"http://www.youtube.com/watch?v=JVB1ayT6Fdc\">http://www.youtube.com/watch?v=JVB1ayT6Fdc</a></p>\n<p>It really makes you&nbsp;appreciate&nbsp;the phrase \"efficient use of resources.\" Their tech demo is mind-bogglingly impressive all by itself, but the further implications of what is actually possible with current computer hardware are reality shaking.</p>\n<p>It really makes me wonder where else (besides AI) current technology is vastly undershooting its potential in a similar way, using brute force (successfully or&nbsp;unsuccessfully) to accomplish something when there's a vastly more efficient way to do it that nobody's thought of yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XXg6zLeFuxWrMa8cz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -11, "extendedScore": null, "score": 8.885253539048768e-07, "legacy": true, "legacyId": "15465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-22T02:57:21.142Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Born Probabilities", "slug": "seq-rerun-the-born-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.285Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/282pa86yWt5gnKxSf/seq-rerun-the-born-probabilities", "pageUrlRelative": "/posts/282pa86yWt5gnKxSf/seq-rerun-the-born-probabilities", "linkUrl": "https://www.lesswrong.com/posts/282pa86yWt5gnKxSf/seq-rerun-the-born-probabilities", "postedAtFormatted": "Sunday, April 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Born%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Born%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F282pa86yWt5gnKxSf%2Fseq-rerun-the-born-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Born%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F282pa86yWt5gnKxSf%2Fseq-rerun-the-born-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F282pa86yWt5gnKxSf%2Fseq-rerun-the-born-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<p>Today's post, <a href=\"/lw/py/the_born_probabilities/\">The Born Probabilities</a> was originally published on 01 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The last <em>serious </em>mysterious question left in quantum physics: When a quantum world splits in two, why do we seem to have a greater probability of ending up in the larger blob, exactly proportional to the integral of the squared modulus? It's an open problem, but non-mysterious answers have been proposed. Try not to go funny in the head about it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bxj/seq_rerun_decoherent_essences/\">Decoherent Essences</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "282pa86yWt5gnKxSf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.88603267778002e-07, "legacy": true, "legacyId": "15466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3ZKvf9u2XEWddGZmS", "pwnBHus398pS24uAv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-22T15:37:09.300Z", "modifiedAt": null, "url": null, "title": "Learn a foreign language to reduce bias?", "slug": "learn-a-foreign-language-to-reduce-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:28.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AShepard", "createdAt": "2009-12-22T18:27:50.792Z", "isAdmin": false, "displayName": "AShepard"}, "userId": "SxCHDrBhMCCdwMJrK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eC3WyMspAwTfp8giv/learn-a-foreign-language-to-reduce-bias", "pageUrlRelative": "/posts/eC3WyMspAwTfp8giv/learn-a-foreign-language-to-reduce-bias", "linkUrl": "https://www.lesswrong.com/posts/eC3WyMspAwTfp8giv/learn-a-foreign-language-to-reduce-bias", "postedAtFormatted": "Sunday, April 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learn%20a%20foreign%20language%20to%20reduce%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearn%20a%20foreign%20language%20to%20reduce%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeC3WyMspAwTfp8giv%2Flearn-a-foreign-language-to-reduce-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learn%20a%20foreign%20language%20to%20reduce%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeC3WyMspAwTfp8giv%2Flearn-a-foreign-language-to-reduce-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeC3WyMspAwTfp8giv%2Flearn-a-foreign-language-to-reduce-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Interesting <a href=\"http://pss.sagepub.com/content/early/2012/04/18/0956797611432178.abstract?rss=1\">new paper</a>&nbsp;(anyone have a link to an ungated version). Abstract (emphasis added):</p>\n<blockquote>\n<p>\n<div id=\"abstract-1\" class=\"section abstract\" style=\"border-image: initial; outline-style: none; font-size: 13px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify; vertical-align: baseline; clear: both; color: #403838; padding: 0px; margin: 0px; border: 0px initial initial;\">\n<p id=\"p-1\" style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; border-image: initial; outline-style: none; font-style: inherit; font-size: inherit; line-height: 1.5; text-align: inherit; vertical-align: baseline; padding: 0px; border: 0px initial initial;\">Would you make the same decisions in a foreign language as you would in your native tongue? It may be intuitive that people would make the same choices regardless of the language they are using, or that the difficulty of using a foreign language would make decisions less systematic. We discovered, however, that the opposite is true: Using a foreign language reduces decision-making biases. Four experiments show that the <strong>framing effect disappears when choices are presented in a foreign tongue</strong>. Whereas people were risk averse for gains and risk seeking for losses when choices were presented in their native tongue, they were not influenced by this framing manipulation in a foreign language. Two additional experiments show that<strong> using a foreign language reduces loss aversion</strong>, increasing the acceptance of both hypothetical and real bets with positive expected value. We propose that these effects arise because a foreign language provides greater cognitive and emotional distance than a native tongue does.</p>\n</div>\n</p>\n</blockquote>\n<div>Speakers of multiple languages: have you noticed a similar pattern in your own lives?</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eC3WyMspAwTfp8giv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 8.889272347614494e-07, "legacy": true, "legacyId": "15467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-22T22:40:51.710Z", "modifiedAt": null, "url": null, "title": "Muehlhauser-Wang Dialogue", "slug": "muehlhauser-wang-dialogue", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:37.083Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gJGjyWahWRyu9TEMC/muehlhauser-wang-dialogue", "pageUrlRelative": "/posts/gJGjyWahWRyu9TEMC/muehlhauser-wang-dialogue", "linkUrl": "https://www.lesswrong.com/posts/gJGjyWahWRyu9TEMC/muehlhauser-wang-dialogue", "postedAtFormatted": "Sunday, April 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Muehlhauser-Wang%20Dialogue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMuehlhauser-Wang%20Dialogue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJGjyWahWRyu9TEMC%2Fmuehlhauser-wang-dialogue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Muehlhauser-Wang%20Dialogue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJGjyWahWRyu9TEMC%2Fmuehlhauser-wang-dialogue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgJGjyWahWRyu9TEMC%2Fmuehlhauser-wang-dialogue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3490, "htmlBody": "<p><small>Part of the <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser interview series on AGI</a>.</small></p>\n<p>&nbsp;</p>\n<p><small><a href=\"http://lukeprog.com/\ufeff\">Luke Muehlhauser</a> is Executive Director of the <a href=\"http://intelligence.org/\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</small></p>\n<p><small><a href=\"http://www.cis.temple.edu/~wangp/\">Pei Wang</a> is an AGI researcher at <a href=\"http://www.temple.edu/\">Temple University</a>, and Chief Executive Editor of <a href=\"http://versitaopen.com/jagi\">Journal of Artificial General Intelligence</a>.</small></p>\n<p>&nbsp;</p>\n<h3>Luke Muehlhauser</h3>\n<p>[Apr. 7, 2012]</p>\n<p>Pei, I'm glad you agreed to discuss artificial general intelligence (AGI) with me. I hope our dialogue will be informative to many readers, and to us!</p>\n<p>On what do we agree? Ben Goertzel and I <a href=\"/r/discussion/lw/aw7/muehlhausergoertzel_dialogue_part_1/\">agreed</a> on the statements below (well, I cleaned up the wording a bit for our conversation):</p>\n<ol>\n<li>Involuntary death is bad, and can be avoided with the right technology.</li>\n<li>Humans can be enhanced by merging with technology.</li>\n<li>Humans are on a risky course in general, because powerful technologies can destroy us, humans are often stupid, and we are unlikely to voluntarily halt technological progress.</li>\n<li>AGI is likely this century.</li>\n<li>AGI will greatly transform the world. It is a potential existential risk, but could also be the best thing that ever happens to us if we do it right.</li>\n<li>Careful effort will be required to ensure that AGI results in good things rather than bad things for humanity.</li>\n</ol>\n<p>You stated in private communication that you agree with these statements, depending on what is meant by \"AGI.\" So, I'll ask: What do you mean by \"AGI\"?</p>\n<p>I'd also be curious to learn what you think about AGI safety. If you agree that AGI is an existential risk that will arrive this century, and if you value humanity, one might expect you to think it's very important that we accelerate AI safety research and decelerate AI capabilities research so that we develop safe superhuman AGI first, rather than arbitrary superhuman AGI. (This is what Anna Salamon and I recommend in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>.) What are your thoughts on the matter?</p>\n<p>&nbsp;</p>\n<h3>Pei Wang:</h3>\n<p>[Apr. 8, 2012]</p>\n<p>By &ldquo;AGI&rdquo; I mean computer systems that follow roughly the same principles as the human mind. Concretely, to me &ldquo;intelligence&rdquo; is the ability to adapt to the environment under insufficient knowledge and resources, or to follow the &ldquo;Laws of Thought&rdquo; that realize a relative rationality that allows the system to apply its available knowledge and resources as much as possible. See [1, 2] for detailed descriptions and comparisons to other definitions of intelligence.</p>\n<p>Such a computer system will share many properties with the human mind; however, it will not have exactly the same behaviors or problem-solving capabilities of a typical human being, since as an adaptive system, the behaviors and capabilities of an AGI not only depend on its built-in principles and mechanisms, but also its body, initial motivation, and individual experience, which are not necessarily human-like.</p>\n<p>Like all major breakthroughs in science and technology, the creation of AGI will be both a challenge and an opportunity to the human kind. Like scientists and engineers in all fields, we AGI researchers should use our best judgments to ensure that AGI results in good things rather than bad things for humanity.</p>\n<p>Even so, the suggestion to &ldquo;accelerate AI safety research and decelerate AI capabilities research so that we develop safe superhuman AGI first, rather than arbitrary superhuman AGI&rdquo; is wrong, for the following major reasons:</p>\n<ol>\n<li>It is based on a highly speculative understanding about what kind of &ldquo;AGI&rdquo; will be created. The definition of intelligence in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a> is not shared by most AGI researchers. According to my opinion, that kind of &ldquo;AGI&rdquo; will never be built.</li>\n<li>Even if the above definition is only considered as a possibility among the other versions of AGI, it will be the actual AI research that will tell us which possibility will become reality. To ban a scientific research according to imaginary risks damages humanity no less than risky research.</li>\n<li>If intelligence turns out to be adaptive (as believed by me and many others), then a &ldquo;friendly AI&rdquo; will be mainly the result of proper education, not proper design. There will be no way to design a &ldquo;safe AI&rdquo;, just like there is no way to require parents to only give birth to &ldquo;safe baby&rdquo; who will never become a criminal.</li>\n<li>The &ldquo;friendly AI&rdquo; approach advocated by Eliezer Yudkowsky has several serious conceptual and theoretical problems, and is not accepted by most AGI researchers. The AGI community has ignored it, not because it is indisputable, but because people have not bothered to criticize it.</li>\n</ol>\n<p>In summary, though the safety of AGI is indeed an important issue, currently we don&rsquo;t know enough about the subject to make any sure conclusion. Higher safety can only be achieved by more research on all related topics, rather than by pursuing approaches that have no solid scientific foundation. I hope your Institute to make constructive contribution to the field by studying a wider range of AGI projects, rather than to generalize from a few, or to commit to a conclusion without considering counter arguments.</p>\n<ul>\n<li><small>[1] Pei Wang, <a href=\"http://www.cis.temple.edu/~wangp/Publication/AI_Definitions.pdf\">What Do You Mean by \"AI\"</a>? Proceedings of&nbsp;<a href=\"http://agi-conf.org/2008/\">AGI-08</a>, Pages 362-373, 2008</small></li>\n<li><small>[2] Pei Wang, <a href=\"http://www.worldscinet.com/ijmc/03/0301/S1793843011000686.html\">The Assumptions on Knowledge and Resources in Models of Rationality</a>, International Journal of Machine Consciousness, Vol.3, No.1, Pages 193-218, 2011</small></li>\n</ul>\n<h3><br /></h3>\n<h3>Luke:</h3>\n<p>[Apr. 8, 2012]</p>\n<p>I appreciate the clarity of your writing, Pei. &ldquo;<a href=\"http://www.worldscinet.com/ijmc/03/0301/free-access/S1793843011000686.pdf\">The Assumptions of Knowledge and Resources in Models of Rationality</a>&rdquo; belongs to a set of papers that make up half of my argument for why the only people allowed to do philosophy should be those with with primary training in cognitive science, computer science, or mathematics. (The other half of that argument is made by examining most of the philosophy papers written by those without primary training in cognitive science, computer science, or mathematics.)</p>\n<p>You write that my recommendation to &ldquo;accelerate AI safety research and decelerate AI capabilities research so that we develop safe superhuman AGI first, rather than arbitrary superhuman AGI&rdquo; is wrong for four reasons, which I will respond to in turn:</p>\n<ol>\n<li>&ldquo;It is based on a highly speculative understanding about what kind of &lsquo;AGI&rsquo; will be created.&rdquo; Actually, it seems to me that my notion of AGI is broader than yours. I think we can use your preferred definition and get the same result. (More on this below.)</li>\n<li>&ldquo;&hellip;it will be the actual AI research that will tell us which possibility will become reality. To ban a scientific research according to imaginary risks damages humanity no less than risky research.&rdquo;</li>\nYes, of course. But we argue (very briefly) that a very broad range of artificial agents with a roughly human-level capacity for adaptation (under AIKR) will manifest convergent instrumental goals. The fuller argument for this is made in Nick&rsquo;s Bostrom&rsquo;s &ldquo;<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>.&rdquo;\n<li>&ldquo;&hellip;a &lsquo;friendly AI&rsquo; will be mainly the result of proper education, not proper design. There will be no way to design a &lsquo;safe AI&rsquo;, just like there is no way to require parents to only give birth to &lsquo;safe baby&rsquo; who will never become a criminal.&rdquo; Without being more specific, I can&rsquo;t tell if we actually disagree on this point. The most promising approach (that I know of) for Friendly AI is one that learns human values and then &ldquo;extrapolates&rdquo; them so that the AI optimizes for what we would value if we knew more, were more the people we wish we were, etc. instead of optimizing for our present, relatively ignorant values. (See &ldquo;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.&rdquo;)</li>\n<li>&ldquo;The &lsquo;friendly AI&rsquo; approach advocated by Eliezer Yudkowsky has several serious conceptual and theoretical problems.&rdquo;</li>\n</ol>\n<p>I agree. Friendly AI may be incoherent and impossible. In fact, it looks impossible right now. But that&rsquo;s often how problems look right before we make a few key insights that make things clearer, and show us (e.g.) how we were asking a <a href=\"/lw/og/wrong_questions/\">wrong question</a> in the first place. The reason I advocate Friendly AI research (among other things) is because it may be the only way to secure a desirable future for humanity, (see &ldquo;<a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a>.&rdquo;) even if it looks impossible. That is why Yudkowsky once proclaimed: &ldquo;<a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut Up and Do the Impossible!</a>&rdquo; When we don&rsquo;t know how to make progress on a difficult problem, sometimes we need to <a href=\"/lw/8ns/hack_away_at_the_edges/\">hack away at the edges</a>.</p>\n<p>I certainly agree that &ldquo;currently we don&rsquo;t know enough about [AGI safety] to make any sure conclusion.&rdquo; That is why more research is needed.</p>\n<p>As for your suggestion that &ldquo;Higher safety can only be achieved by more research on all related topics,&rdquo; I wonder if you think that is true of all subjects, or only in AGI. For example, should mankind vigorously pursue research on how to make Ron Fouchier's alteration of the H5N1 bird flu virus even more dangerous and deadly to humans, because &ldquo;higher safety can only be achieved by more research on all related topics&rdquo;? (I&rsquo;m not trying to broadly compare AGI capabilities research to supervirus research; I&rsquo;m just trying to understand the nature of your rejection of my recommendation for mankind to decelerate AGI capabilities research and accelerate AGI safety research.)</p>\n<p>Hopefully I have clarified my own positions and my reasons for them. I look forward to your reply!</p>\n<p>&nbsp;</p>\n<h3>Pei:</h3>\n<p>[Apr. 10, 2012]</p>\n<p>Luke: I&rsquo;m glad to see the agreements, and will only comment on the disagreements.</p>\n<ol>\n<li>&ldquo;my notion of AGI is broader than yours&rdquo; In scientific theories, broader notions are not always better. In this context, a broad notion may cover too many diverse approaches to provide any non-trivial conclusion. For example, AIXI and NARS are fundamentally different in many aspects, and NARS do not approximate AIXI. It is OK to call both &ldquo;AGI&rdquo; with respect to their similar ambitions, but theoretical or technical descriptions based on such a broad notion are hard to make. Almost all of your descriptions about AIXI are hardly relevant to NARS, as well as to most existing &ldquo;AGI&rdquo; projects, for this reason.</li>\n<li>&ldquo;I think we can use your preferred definition and get the same result.&rdquo; No you cannot. According to my definition, AIXI is not intelligent, since it doesn&rsquo;t obey AIKR. Since most of your conclusions are about that type of system, they will go with it.</li>\n<li>&ldquo;a very broad range of artificial agents with a roughly human-level capacity for adaptation (under AIKR) will manifest convergent instrumental goals&rdquo; I cannot access Bostrom&rsquo;s paper, but guess that he made additional assumptions. In general, the goal structure of an adaptive system changes according to the system&rsquo;s experience, so unless you restrict the experience of these artificial agents, there is no way to restrict their goals. I agree that to make AGI safe, to control their experience will probably be the main approach (which is what &ldquo;education&rdquo; is all about), but even that cannot guarantee safety. (see below)</li>\n<li>&ldquo;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.&rdquo; I don&rsquo;t have the time to do a detailed review, but can frankly tell you why I disagree with the main suggestion &ldquo;to program the AI&rsquo;s goal system to want what we want before the AI self-improves beyond our capacity to control it&rdquo;.</li>\n<li>As I mentioned above, the goal system of an adaptive system evolves as a function of the system&rsquo;s experience. No matter what initial goals are implanted, under AIKR the derived goals are not necessarily their logical implications, which is not necessarily a bad thing (the humanity is not a logical implication of the human biological nature, neither), though it means the designer has no full control to it (unless the designer also fully controls the experience of the system, which is practically impossible). See &ldquo;<a href=\"http://www.cis.temple.edu/~wangp/GTI-book/GTI-CH4/GTI-4-2.html\">The self-organization of goals</a>&rdquo; for detailed discussion.</li>\n<li>Even if the system&rsquo;s goal system can be made to fully agree with certain given specifications, I wonder where these specifications come from --- we human beings are not well known for reaching consensus on almost anything, not to mention on a topic this big.</li>\n<li>Even if the we could agree on the goals of AI&rsquo;s, and find a way to enforce them in AI&rsquo;s, that still doesn&rsquo;t means we have &ldquo;friendly AI&rdquo;. Under AIKR, a system can cause damage simply because of its ignorance in a novel situation.</li>\n</ol>\n<p>For these reasons, under AIKR we cannot have AI with guaranteed safety or friendliness, though we can and should always do our best to make them safer, based on our best judgment (which can still be wrong, due to AIKR). To apply logic or probability theory into the design won&rsquo;t change the big picture, because what we are after are empirical conclusions, not theorems within those theories. Only the latter can have proved correctness, and the former cannot (though they can have strong evidential support).</p>\n<p>&ldquo;I&rsquo;m just trying to understand the nature of your rejection of my recommendation for mankind to decelerate AGI capabilities research and accelerate AGI safety research&rdquo;</p>\n<p>Frankly, I don&rsquo;t think anyone currently has the evidence or argument to ask the others to decelerate their research for safety consideration, though it is perfectly fine to promote your own research direction and try to attract more people into it. However, unless you get a right idea about what AGI is and how it can be built, it is very unlikely for you to know how to make it safe.</p>\n<p>&nbsp;</p>\n<h3>Luke:</h3>\n<p>[Apr. 10, 2012]</p>\n<p>I didn&rsquo;t mean to imply that my notion of AGI was &ldquo;better&rdquo; because it is broader. I was merely responding to your claim that my argument for differential technological development (in this case, decelerating AI capabilities research while accelerating AI safety research) depends on a narrow notion of AGI that you believe &ldquo;will never be built.&rdquo; But this isn&rsquo;t true, because my notion of AGI is very broad and includes your notion of AGI as a special case. My notion of AGI includes both AIXI-like &ldquo;intelligent&rdquo; systems and also &ldquo;intelligent&rdquo; systems which obey AIKR, because both kinds of systems (if implemented/approximated successfully) could efficiently use resources to achieve goals, and that is the definition Anna and I stipulated for &ldquo;intelligence.&rdquo;</p>\n<p>Let me back up. In our paper, Anna and I stipulate that for the purposes of our paper we use &ldquo;intelligence&rdquo; to mean an agent&rsquo;s capacity to efficiently use resources (such as money or computing power) to optimize the world according to its preferences. You could call this &ldquo;instrumental rationality&rdquo; or &ldquo;ability to achieve one&rsquo;s goals&rdquo; or something else if you prefer; I don&rsquo;t wish to encourage a &ldquo;<a href=\"http://consc.net/oxford/chap9.pdf\">merely verbal</a>&rdquo; dispute between us. We also specify that by &ldquo;AI&rdquo; (in our discussion, &ldquo;AGI&rdquo;) we mean &ldquo;systems which match or exceed the intelligence [as we just defined it] of humans in virtually all domains of interest.&rdquo; That is: by &ldquo;AGI&rdquo; we mean &ldquo;systems which match or exceed the human capacity for efficiently using resources to achieve goals in virtually all domains of interest.&rdquo; So I&rsquo;m not sure I understood you correctly: Did you really mean to say that &ldquo;kind of AGI will never be built&rdquo;? If so, why do you think that? Is the human very close to a natural ceiling on an agent&rsquo;s ability to achieve goals?</p>\n<p>What we argue in &ldquo;Intelligence Explosion: Evidence and Import,&rdquo; then, is that a very broad range of AGIs pose a threat to humanity, and therefore we should be sure we have the safety part figured out as much as we can before we figure out how to build AGIs. But this is the opposite of what is happening now. Right now, almost all AGI-directed R&amp;D resources are being devoted to AGI capabilities research rather than AGI safety research. This is the case even though there is AGI safety research that will plausibly be useful given almost any final AGI architecture, for example the problem of extracting coherent preferences from humans (so that we can figure out which rules / constraints / goals we might want to use to bound an AGI&rsquo;s behavior).</p>\n<p>I do hope you have the chance to read &ldquo;<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>.&rdquo; It is linked near the top of <a href=\"http://nickbostrom.com\">nickbostrom.com</a> and I will send it to you via email.</p>\n<p>But perhaps I have been driving the direction of our conversation too much. Don&rsquo;t hesitate it to steer it towards topics you would prefer to address!</p>\n<p>&nbsp;</p>\n<h3>Pei:</h3>\n<p>[Apr. 12, 2012]</p>\n<p>Hi Luke,</p>\n<p>I don&rsquo;t expect to resolve all the related issues in such a dialogue. In the following, I&rsquo;ll return to what I think as the major issues and summarize my position.</p>\n<ol>\n<li>Whether we can build a &ldquo;safe AGI&rdquo; by giving it a carefully designed &ldquo;goal system&rdquo; My answer is negative. It is my belief that an AGI will necessarily be adaptive, which implies that the goals it actively pursues constantly change as a function of its experience, and are not fully restricted by its initial (given) goals. As described in my eBook (cited previously), the goal derivation is based on the system&rsquo;s beliefs, which may lead to conflicts in goals. Furthermore, even if the goals are fixed, they cannot fully determine the consequences of the system&rsquo;s behaviors, which also depend on the system&rsquo;s available knowledge and resources, etc. If all those factors are also fixed, then we may get guaranteed safety, but the system won&rsquo;t be intelligent --- it will be just like today&rsquo;s ordinary (unintelligent) computer.</li>\n<li>Whether we should figure out how to build &ldquo;safe AGI&rdquo; before figuring out how to build &ldquo;AGI&rdquo;. My answer is negative, too. As in all adaptive systems, the behaviors of an intelligent system are determined both by its nature (design) and nurture (experience). The system&rsquo;s intelligence mainly comes from its design, and is &ldquo;morally neutral&rdquo;, in the sense that (1) any goals can be implanted initially, (2) very different goals can be derived from the same initial design and goals, given different experience. Therefore, to control the morality of an AI mainly means to educate it properly (i.e., to control its experience, especially in its early years). Of course, the initial goals matters, but it is wrong to assume that the initial goals will always be the dominating goals in decision making processes. To develop a non-trivial education theory of AGI requires a good understanding about how the system works, so if we don&rsquo;t know how to build an AGI, there is no chance for us to know how to make it safe. I don&rsquo;t think a good education theory can be &ldquo;proved&rdquo; in advance, pure theoretically. Rather, we&rsquo;ll learn most of it by interacting with baby AGIs, just like how many of us learn how to educate children.</li>\n</ol>\n<p>Such a short position statement may not convince you, but I hope you can consider it at least as a possibility. I guess the final consensus can only come from further research.</p>\n<p>&nbsp;</p>\n<h3>Luke:</h3>\n<p>[Apr. 19, 2012]</p>\n<p>Pei,</p>\n<p>I agree that an AGI will be adaptive in the sense that its instrumental goals will adapt as a function of its experience. But I do think advanced AGIs will have convergently instrumental reasons to preserve their final (or &ldquo;terminal&rdquo;) goals. As Bostrom explains in &ldquo;<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>&rdquo;:</p>\n<p>An agent is more likely to act in the future to maximize the realization of its present final goals if it still has those goals in the future. This gives the agent a present instrumental reason to prevent alterations of its final goals.</p>\n<p>I also agree that even if an AGI&rsquo;s final goals are fixed, the AGI&rsquo;s behavior will also depend on its knowledge and resources, and therefore we can&rsquo;t exactly predict its behavior. But if a system has lots of knowledge and resources, and we know its final goals, then we can predict with some confidence that whatever it does next, it will be something aimed at achieving those final goals. And the more knowledge and resources it has, the more confident we can be that its actions will successfully aim at achieving its final goals. So if a superintelligent machine&rsquo;s only final goal is to play through Super Mario Bros within 30 minutes, we can be pretty confident it will do so. The problem is that we don&rsquo;t know how to tell a superintelligent machine to do things we want, so we&rsquo;re going to get many unintended consequences for humanity (as argued in &ldquo;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>&rdquo;).</p>\n<p>You also said that you can&rsquo;t see what safety work there is to be done without having intelligent systems (e.g. &ldquo;baby AGIs&rdquo;) to work with. I provided a list of open problems in AI safety <a href=\"http://lukeprog.com/SaveTheWorld.html\">here</a>, and most of them don&rsquo;t require that we know how to build an AGI first. For example, one reason we can&rsquo;t tell an AGI to do what humans want is that we don&rsquo;t know what humans want, and there is work to be done in philosophy and in preference acquisition in AI in order to get clearer about what humans want.</p>\n<p>&nbsp;</p>\n<h3>Pei:</h3>\n<p>[Apr. 20, 2012]</p>\n<p>Luke,</p>\n<p>I think we have made our different beliefs clear, so this dialogue has achieved its goal. It won&rsquo;t be an efficient usage of our time to attempt to convince each other at this moment, and each side can analyze these beliefs in proper forms of publication at a future time.</p>\n<p>Now we can let the readers consider these arguments and conclusions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1, "sYm3HiWcfZvrGu3ui": 1, "ac84EpK6mZbPLzmqj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gJGjyWahWRyu9TEMC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 34, "extendedScore": null, "score": 8.891079869059429e-07, "legacy": true, "legacyId": "15471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser interview series on AGI</a>.</small></p>\n<p>&nbsp;</p>\n<p><small><a href=\"http://lukeprog.com/\ufeff\">Luke Muehlhauser</a> is Executive Director of the <a href=\"http://intelligence.org/\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</small></p>\n<p><small><a href=\"http://www.cis.temple.edu/~wangp/\">Pei Wang</a> is an AGI researcher at <a href=\"http://www.temple.edu/\">Temple University</a>, and Chief Executive Editor of <a href=\"http://versitaopen.com/jagi\">Journal of Artificial General Intelligence</a>.</small></p>\n<p>&nbsp;</p>\n<h3 id=\"Luke_Muehlhauser\">Luke Muehlhauser</h3>\n<p>[Apr. 7, 2012]</p>\n<p>Pei, I'm glad you agreed to discuss artificial general intelligence (AGI) with me. I hope our dialogue will be informative to many readers, and to us!</p>\n<p>On what do we agree? Ben Goertzel and I <a href=\"/r/discussion/lw/aw7/muehlhausergoertzel_dialogue_part_1/\">agreed</a> on the statements below (well, I cleaned up the wording a bit for our conversation):</p>\n<ol>\n<li>Involuntary death is bad, and can be avoided with the right technology.</li>\n<li>Humans can be enhanced by merging with technology.</li>\n<li>Humans are on a risky course in general, because powerful technologies can destroy us, humans are often stupid, and we are unlikely to voluntarily halt technological progress.</li>\n<li>AGI is likely this century.</li>\n<li>AGI will greatly transform the world. It is a potential existential risk, but could also be the best thing that ever happens to us if we do it right.</li>\n<li>Careful effort will be required to ensure that AGI results in good things rather than bad things for humanity.</li>\n</ol>\n<p>You stated in private communication that you agree with these statements, depending on what is meant by \"AGI.\" So, I'll ask: What do you mean by \"AGI\"?</p>\n<p>I'd also be curious to learn what you think about AGI safety. If you agree that AGI is an existential risk that will arrive this century, and if you value humanity, one might expect you to think it's very important that we accelerate AI safety research and decelerate AI capabilities research so that we develop safe superhuman AGI first, rather than arbitrary superhuman AGI. (This is what Anna Salamon and I recommend in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>.) What are your thoughts on the matter?</p>\n<p>&nbsp;</p>\n<h3 id=\"Pei_Wang_\">Pei Wang:</h3>\n<p>[Apr. 8, 2012]</p>\n<p>By \u201cAGI\u201d I mean computer systems that follow roughly the same principles as the human mind. Concretely, to me \u201cintelligence\u201d is the ability to adapt to the environment under insufficient knowledge and resources, or to follow the \u201cLaws of Thought\u201d that realize a relative rationality that allows the system to apply its available knowledge and resources as much as possible. See [1, 2] for detailed descriptions and comparisons to other definitions of intelligence.</p>\n<p>Such a computer system will share many properties with the human mind; however, it will not have exactly the same behaviors or problem-solving capabilities of a typical human being, since as an adaptive system, the behaviors and capabilities of an AGI not only depend on its built-in principles and mechanisms, but also its body, initial motivation, and individual experience, which are not necessarily human-like.</p>\n<p>Like all major breakthroughs in science and technology, the creation of AGI will be both a challenge and an opportunity to the human kind. Like scientists and engineers in all fields, we AGI researchers should use our best judgments to ensure that AGI results in good things rather than bad things for humanity.</p>\n<p>Even so, the suggestion to \u201caccelerate AI safety research and decelerate AI capabilities research so that we develop safe superhuman AGI first, rather than arbitrary superhuman AGI\u201d is wrong, for the following major reasons:</p>\n<ol>\n<li>It is based on a highly speculative understanding about what kind of \u201cAGI\u201d will be created. The definition of intelligence in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a> is not shared by most AGI researchers. According to my opinion, that kind of \u201cAGI\u201d will never be built.</li>\n<li>Even if the above definition is only considered as a possibility among the other versions of AGI, it will be the actual AI research that will tell us which possibility will become reality. To ban a scientific research according to imaginary risks damages humanity no less than risky research.</li>\n<li>If intelligence turns out to be adaptive (as believed by me and many others), then a \u201cfriendly AI\u201d will be mainly the result of proper education, not proper design. There will be no way to design a \u201csafe AI\u201d, just like there is no way to require parents to only give birth to \u201csafe baby\u201d who will never become a criminal.</li>\n<li>The \u201cfriendly AI\u201d approach advocated by Eliezer Yudkowsky has several serious conceptual and theoretical problems, and is not accepted by most AGI researchers. The AGI community has ignored it, not because it is indisputable, but because people have not bothered to criticize it.</li>\n</ol>\n<p>In summary, though the safety of AGI is indeed an important issue, currently we don\u2019t know enough about the subject to make any sure conclusion. Higher safety can only be achieved by more research on all related topics, rather than by pursuing approaches that have no solid scientific foundation. I hope your Institute to make constructive contribution to the field by studying a wider range of AGI projects, rather than to generalize from a few, or to commit to a conclusion without considering counter arguments.</p>\n<ul>\n<li><small>[1] Pei Wang, <a href=\"http://www.cis.temple.edu/~wangp/Publication/AI_Definitions.pdf\">What Do You Mean by \"AI\"</a>? Proceedings of&nbsp;<a href=\"http://agi-conf.org/2008/\">AGI-08</a>, Pages 362-373, 2008</small></li>\n<li><small>[2] Pei Wang, <a href=\"http://www.worldscinet.com/ijmc/03/0301/S1793843011000686.html\">The Assumptions on Knowledge and Resources in Models of Rationality</a>, International Journal of Machine Consciousness, Vol.3, No.1, Pages 193-218, 2011</small></li>\n</ul>\n<h3><br></h3>\n<h3 id=\"Luke_\">Luke:</h3>\n<p>[Apr. 8, 2012]</p>\n<p>I appreciate the clarity of your writing, Pei. \u201c<a href=\"http://www.worldscinet.com/ijmc/03/0301/free-access/S1793843011000686.pdf\">The Assumptions of Knowledge and Resources in Models of Rationality</a>\u201d belongs to a set of papers that make up half of my argument for why the only people allowed to do philosophy should be those with with primary training in cognitive science, computer science, or mathematics. (The other half of that argument is made by examining most of the philosophy papers written by those without primary training in cognitive science, computer science, or mathematics.)</p>\n<p>You write that my recommendation to \u201caccelerate AI safety research and decelerate AI capabilities research so that we develop safe superhuman AGI first, rather than arbitrary superhuman AGI\u201d is wrong for four reasons, which I will respond to in turn:</p>\n<ol>\n<li>\u201cIt is based on a highly speculative understanding about what kind of \u2018AGI\u2019 will be created.\u201d Actually, it seems to me that my notion of AGI is broader than yours. I think we can use your preferred definition and get the same result. (More on this below.)</li>\n<li>\u201c\u2026it will be the actual AI research that will tell us which possibility will become reality. To ban a scientific research according to imaginary risks damages humanity no less than risky research.\u201d</li>\nYes, of course. But we argue (very briefly) that a very broad range of artificial agents with a roughly human-level capacity for adaptation (under AIKR) will manifest convergent instrumental goals. The fuller argument for this is made in Nick\u2019s Bostrom\u2019s \u201c<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>.\u201d\n<li>\u201c\u2026a \u2018friendly AI\u2019 will be mainly the result of proper education, not proper design. There will be no way to design a \u2018safe AI\u2019, just like there is no way to require parents to only give birth to \u2018safe baby\u2019 who will never become a criminal.\u201d Without being more specific, I can\u2019t tell if we actually disagree on this point. The most promising approach (that I know of) for Friendly AI is one that learns human values and then \u201cextrapolates\u201d them so that the AI optimizes for what we would value if we knew more, were more the people we wish we were, etc. instead of optimizing for our present, relatively ignorant values. (See \u201c<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.\u201d)</li>\n<li>\u201cThe \u2018friendly AI\u2019 approach advocated by Eliezer Yudkowsky has several serious conceptual and theoretical problems.\u201d</li>\n</ol>\n<p>I agree. Friendly AI may be incoherent and impossible. In fact, it looks impossible right now. But that\u2019s often how problems look right before we make a few key insights that make things clearer, and show us (e.g.) how we were asking a <a href=\"/lw/og/wrong_questions/\">wrong question</a> in the first place. The reason I advocate Friendly AI research (among other things) is because it may be the only way to secure a desirable future for humanity, (see \u201c<a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a>.\u201d) even if it looks impossible. That is why Yudkowsky once proclaimed: \u201c<a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut Up and Do the Impossible!</a>\u201d When we don\u2019t know how to make progress on a difficult problem, sometimes we need to <a href=\"/lw/8ns/hack_away_at_the_edges/\">hack away at the edges</a>.</p>\n<p>I certainly agree that \u201ccurrently we don\u2019t know enough about [AGI safety] to make any sure conclusion.\u201d That is why more research is needed.</p>\n<p>As for your suggestion that \u201cHigher safety can only be achieved by more research on all related topics,\u201d I wonder if you think that is true of all subjects, or only in AGI. For example, should mankind vigorously pursue research on how to make Ron Fouchier's alteration of the H5N1 bird flu virus even more dangerous and deadly to humans, because \u201chigher safety can only be achieved by more research on all related topics\u201d? (I\u2019m not trying to broadly compare AGI capabilities research to supervirus research; I\u2019m just trying to understand the nature of your rejection of my recommendation for mankind to decelerate AGI capabilities research and accelerate AGI safety research.)</p>\n<p>Hopefully I have clarified my own positions and my reasons for them. I look forward to your reply!</p>\n<p>&nbsp;</p>\n<h3 id=\"Pei_\">Pei:</h3>\n<p>[Apr. 10, 2012]</p>\n<p>Luke: I\u2019m glad to see the agreements, and will only comment on the disagreements.</p>\n<ol>\n<li>\u201cmy notion of AGI is broader than yours\u201d In scientific theories, broader notions are not always better. In this context, a broad notion may cover too many diverse approaches to provide any non-trivial conclusion. For example, AIXI and NARS are fundamentally different in many aspects, and NARS do not approximate AIXI. It is OK to call both \u201cAGI\u201d with respect to their similar ambitions, but theoretical or technical descriptions based on such a broad notion are hard to make. Almost all of your descriptions about AIXI are hardly relevant to NARS, as well as to most existing \u201cAGI\u201d projects, for this reason.</li>\n<li>\u201cI think we can use your preferred definition and get the same result.\u201d No you cannot. According to my definition, AIXI is not intelligent, since it doesn\u2019t obey AIKR. Since most of your conclusions are about that type of system, they will go with it.</li>\n<li>\u201ca very broad range of artificial agents with a roughly human-level capacity for adaptation (under AIKR) will manifest convergent instrumental goals\u201d I cannot access Bostrom\u2019s paper, but guess that he made additional assumptions. In general, the goal structure of an adaptive system changes according to the system\u2019s experience, so unless you restrict the experience of these artificial agents, there is no way to restrict their goals. I agree that to make AGI safe, to control their experience will probably be the main approach (which is what \u201ceducation\u201d is all about), but even that cannot guarantee safety. (see below)</li>\n<li>\u201c<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>.\u201d I don\u2019t have the time to do a detailed review, but can frankly tell you why I disagree with the main suggestion \u201cto program the AI\u2019s goal system to want what we want before the AI self-improves beyond our capacity to control it\u201d.</li>\n<li>As I mentioned above, the goal system of an adaptive system evolves as a function of the system\u2019s experience. No matter what initial goals are implanted, under AIKR the derived goals are not necessarily their logical implications, which is not necessarily a bad thing (the humanity is not a logical implication of the human biological nature, neither), though it means the designer has no full control to it (unless the designer also fully controls the experience of the system, which is practically impossible). See \u201c<a href=\"http://www.cis.temple.edu/~wangp/GTI-book/GTI-CH4/GTI-4-2.html\">The self-organization of goals</a>\u201d for detailed discussion.</li>\n<li>Even if the system\u2019s goal system can be made to fully agree with certain given specifications, I wonder where these specifications come from --- we human beings are not well known for reaching consensus on almost anything, not to mention on a topic this big.</li>\n<li>Even if the we could agree on the goals of AI\u2019s, and find a way to enforce them in AI\u2019s, that still doesn\u2019t means we have \u201cfriendly AI\u201d. Under AIKR, a system can cause damage simply because of its ignorance in a novel situation.</li>\n</ol>\n<p>For these reasons, under AIKR we cannot have AI with guaranteed safety or friendliness, though we can and should always do our best to make them safer, based on our best judgment (which can still be wrong, due to AIKR). To apply logic or probability theory into the design won\u2019t change the big picture, because what we are after are empirical conclusions, not theorems within those theories. Only the latter can have proved correctness, and the former cannot (though they can have strong evidential support).</p>\n<p>\u201cI\u2019m just trying to understand the nature of your rejection of my recommendation for mankind to decelerate AGI capabilities research and accelerate AGI safety research\u201d</p>\n<p>Frankly, I don\u2019t think anyone currently has the evidence or argument to ask the others to decelerate their research for safety consideration, though it is perfectly fine to promote your own research direction and try to attract more people into it. However, unless you get a right idea about what AGI is and how it can be built, it is very unlikely for you to know how to make it safe.</p>\n<p>&nbsp;</p>\n<h3 id=\"Luke_1\">Luke:</h3>\n<p>[Apr. 10, 2012]</p>\n<p>I didn\u2019t mean to imply that my notion of AGI was \u201cbetter\u201d because it is broader. I was merely responding to your claim that my argument for differential technological development (in this case, decelerating AI capabilities research while accelerating AI safety research) depends on a narrow notion of AGI that you believe \u201cwill never be built.\u201d But this isn\u2019t true, because my notion of AGI is very broad and includes your notion of AGI as a special case. My notion of AGI includes both AIXI-like \u201cintelligent\u201d systems and also \u201cintelligent\u201d systems which obey AIKR, because both kinds of systems (if implemented/approximated successfully) could efficiently use resources to achieve goals, and that is the definition Anna and I stipulated for \u201cintelligence.\u201d</p>\n<p>Let me back up. In our paper, Anna and I stipulate that for the purposes of our paper we use \u201cintelligence\u201d to mean an agent\u2019s capacity to efficiently use resources (such as money or computing power) to optimize the world according to its preferences. You could call this \u201cinstrumental rationality\u201d or \u201cability to achieve one\u2019s goals\u201d or something else if you prefer; I don\u2019t wish to encourage a \u201c<a href=\"http://consc.net/oxford/chap9.pdf\">merely verbal</a>\u201d dispute between us. We also specify that by \u201cAI\u201d (in our discussion, \u201cAGI\u201d) we mean \u201csystems which match or exceed the intelligence [as we just defined it] of humans in virtually all domains of interest.\u201d That is: by \u201cAGI\u201d we mean \u201csystems which match or exceed the human capacity for efficiently using resources to achieve goals in virtually all domains of interest.\u201d So I\u2019m not sure I understood you correctly: Did you really mean to say that \u201ckind of AGI will never be built\u201d? If so, why do you think that? Is the human very close to a natural ceiling on an agent\u2019s ability to achieve goals?</p>\n<p>What we argue in \u201cIntelligence Explosion: Evidence and Import,\u201d then, is that a very broad range of AGIs pose a threat to humanity, and therefore we should be sure we have the safety part figured out as much as we can before we figure out how to build AGIs. But this is the opposite of what is happening now. Right now, almost all AGI-directed R&amp;D resources are being devoted to AGI capabilities research rather than AGI safety research. This is the case even though there is AGI safety research that will plausibly be useful given almost any final AGI architecture, for example the problem of extracting coherent preferences from humans (so that we can figure out which rules / constraints / goals we might want to use to bound an AGI\u2019s behavior).</p>\n<p>I do hope you have the chance to read \u201c<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>.\u201d It is linked near the top of <a href=\"http://nickbostrom.com\">nickbostrom.com</a> and I will send it to you via email.</p>\n<p>But perhaps I have been driving the direction of our conversation too much. Don\u2019t hesitate it to steer it towards topics you would prefer to address!</p>\n<p>&nbsp;</p>\n<h3 id=\"Pei_1\">Pei:</h3>\n<p>[Apr. 12, 2012]</p>\n<p>Hi Luke,</p>\n<p>I don\u2019t expect to resolve all the related issues in such a dialogue. In the following, I\u2019ll return to what I think as the major issues and summarize my position.</p>\n<ol>\n<li>Whether we can build a \u201csafe AGI\u201d by giving it a carefully designed \u201cgoal system\u201d My answer is negative. It is my belief that an AGI will necessarily be adaptive, which implies that the goals it actively pursues constantly change as a function of its experience, and are not fully restricted by its initial (given) goals. As described in my eBook (cited previously), the goal derivation is based on the system\u2019s beliefs, which may lead to conflicts in goals. Furthermore, even if the goals are fixed, they cannot fully determine the consequences of the system\u2019s behaviors, which also depend on the system\u2019s available knowledge and resources, etc. If all those factors are also fixed, then we may get guaranteed safety, but the system won\u2019t be intelligent --- it will be just like today\u2019s ordinary (unintelligent) computer.</li>\n<li>Whether we should figure out how to build \u201csafe AGI\u201d before figuring out how to build \u201cAGI\u201d. My answer is negative, too. As in all adaptive systems, the behaviors of an intelligent system are determined both by its nature (design) and nurture (experience). The system\u2019s intelligence mainly comes from its design, and is \u201cmorally neutral\u201d, in the sense that (1) any goals can be implanted initially, (2) very different goals can be derived from the same initial design and goals, given different experience. Therefore, to control the morality of an AI mainly means to educate it properly (i.e., to control its experience, especially in its early years). Of course, the initial goals matters, but it is wrong to assume that the initial goals will always be the dominating goals in decision making processes. To develop a non-trivial education theory of AGI requires a good understanding about how the system works, so if we don\u2019t know how to build an AGI, there is no chance for us to know how to make it safe. I don\u2019t think a good education theory can be \u201cproved\u201d in advance, pure theoretically. Rather, we\u2019ll learn most of it by interacting with baby AGIs, just like how many of us learn how to educate children.</li>\n</ol>\n<p>Such a short position statement may not convince you, but I hope you can consider it at least as a possibility. I guess the final consensus can only come from further research.</p>\n<p>&nbsp;</p>\n<h3 id=\"Luke_2\">Luke:</h3>\n<p>[Apr. 19, 2012]</p>\n<p>Pei,</p>\n<p>I agree that an AGI will be adaptive in the sense that its instrumental goals will adapt as a function of its experience. But I do think advanced AGIs will have convergently instrumental reasons to preserve their final (or \u201cterminal\u201d) goals. As Bostrom explains in \u201c<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>\u201d:</p>\n<p>An agent is more likely to act in the future to maximize the realization of its present final goals if it still has those goals in the future. This gives the agent a present instrumental reason to prevent alterations of its final goals.</p>\n<p>I also agree that even if an AGI\u2019s final goals are fixed, the AGI\u2019s behavior will also depend on its knowledge and resources, and therefore we can\u2019t exactly predict its behavior. But if a system has lots of knowledge and resources, and we know its final goals, then we can predict with some confidence that whatever it does next, it will be something aimed at achieving those final goals. And the more knowledge and resources it has, the more confident we can be that its actions will successfully aim at achieving its final goals. So if a superintelligent machine\u2019s only final goal is to play through Super Mario Bros within 30 minutes, we can be pretty confident it will do so. The problem is that we don\u2019t know how to tell a superintelligent machine to do things we want, so we\u2019re going to get many unintended consequences for humanity (as argued in \u201c<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>\u201d).</p>\n<p>You also said that you can\u2019t see what safety work there is to be done without having intelligent systems (e.g. \u201cbaby AGIs\u201d) to work with. I provided a list of open problems in AI safety <a href=\"http://lukeprog.com/SaveTheWorld.html\">here</a>, and most of them don\u2019t require that we know how to build an AGI first. For example, one reason we can\u2019t tell an AGI to do what humans want is that we don\u2019t know what humans want, and there is work to be done in philosophy and in preference acquisition in AI in order to get clearer about what humans want.</p>\n<p>&nbsp;</p>\n<h3 id=\"Pei_2\">Pei:</h3>\n<p>[Apr. 20, 2012]</p>\n<p>Luke,</p>\n<p>I think we have made our different beliefs clear, so this dialogue has achieved its goal. It won\u2019t be an efficient usage of our time to attempt to convince each other at this moment, and each side can analyze these beliefs in proper forms of publication at a future time.</p>\n<p>Now we can let the readers consider these arguments and conclusions.</p>", "sections": [{"title": "Luke Muehlhauser", "anchor": "Luke_Muehlhauser", "level": 1}, {"title": "Pei Wang:", "anchor": "Pei_Wang_", "level": 1}, {"title": "Luke:", "anchor": "Luke_", "level": 1}, {"title": "Pei:", "anchor": "Pei_", "level": 1}, {"title": "Luke:", "anchor": "Luke_1", "level": 1}, {"title": "Pei:", "anchor": "Pei_1", "level": 1}, {"title": "Luke:", "anchor": "Luke_2", "level": 1}, {"title": "Pei:", "anchor": "Pei_2", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "288 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 288, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TpNRpncLBAzddBnRB", "XzrqkhfwtiSDgKoAF", "nCvvhFBaayaXyuBiD", "6bSHiD9TxsJwe2WqT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T05:11:16.526Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Decoherence as Projection", "slug": "seq-rerun-decoherence-as-projection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZiJQerHQpsScCCekf/seq-rerun-decoherence-as-projection", "pageUrlRelative": "/posts/ZiJQerHQpsScCCekf/seq-rerun-decoherence-as-projection", "linkUrl": "https://www.lesswrong.com/posts/ZiJQerHQpsScCCekf/seq-rerun-decoherence-as-projection", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Decoherence%20as%20Projection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Decoherence%20as%20Projection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiJQerHQpsScCCekf%2Fseq-rerun-decoherence-as-projection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Decoherence%20as%20Projection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiJQerHQpsScCCekf%2Fseq-rerun-decoherence-as-projection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiJQerHQpsScCCekf%2Fseq-rerun-decoherence-as-projection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/pz/decoherence_as_projection/\">Decoherence as Projection</a> was originally published on 02 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Since quantum evolution is linear and unitary, decoherence can be seen as projecting a wavefunction onto orthogonal subspaces. This can be neatly illustrated using polarized photons and the angle of the polarized sheet that will absorb or transmit them.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bxm/seq_rerun_the_born_probabilities/\">The Born Probabilities</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZiJQerHQpsScCCekf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.892745937810985e-07, "legacy": true, "legacyId": "15476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EneHGx8t8skPKxHhv", "282pa86yWt5gnKxSf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T13:30:25.386Z", "modifiedAt": null, "url": null, "title": "To like each other, sing and dance in synchrony", "slug": "to-like-each-other-sing-and-dance-in-synchrony", "viewCount": null, "lastCommentedAt": "2019-10-31T16:05:13.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GahkhPWinnAvri8Td/to-like-each-other-sing-and-dance-in-synchrony", "pageUrlRelative": "/posts/GahkhPWinnAvri8Td/to-like-each-other-sing-and-dance-in-synchrony", "linkUrl": "https://www.lesswrong.com/posts/GahkhPWinnAvri8Td/to-like-each-other-sing-and-dance-in-synchrony", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20like%20each%20other%2C%20sing%20and%20dance%20in%20synchrony&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20like%20each%20other%2C%20sing%20and%20dance%20in%20synchrony%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGahkhPWinnAvri8Td%2Fto-like-each-other-sing-and-dance-in-synchrony%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20like%20each%20other%2C%20sing%20and%20dance%20in%20synchrony%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGahkhPWinnAvri8Td%2Fto-like-each-other-sing-and-dance-in-synchrony", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGahkhPWinnAvri8Td%2Fto-like-each-other-sing-and-dance-in-synchrony", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 943, "htmlBody": "<p><em>For <a href=\"/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/\">the How to Run a Successful Less Wrong Meetup booklet</a>, I'm looking for information about how to better build a social group and foster a feeling of community. Since this bit is probably of general interest, I'm posting it here.</em></p>\n<p>If you want to make the members of the group like each other more and feel more like a group, <em>synchronized actions</em> may be one of the easiest ways of achieving this goal. Anthropologists have long known the community-building effect of dancing:</p>\n<blockquote>\n<p>As the dancer loses himself in the dance, as he becomes absorbed in the unified community, he reaches a state of elation in which he feels himself filled with an energy of force immensely beyond his ordinary state . . . finding himself in complete and ecstatic harmony with all the fellow-members of his community, experiences a great increase in his feelings of amity and attachment towards them. (Radcliffe-Brown 1933/1948, quoted in Kesebir 2011)</p>\n</blockquote>\n<p>Armies around the world utilize the same effect to foster a feeling of unison through repeated drills:</p>\n<blockquote>\n<p>Words are inadequate to describe the emotion aroused by the prolonged movement in unison that drilling involved. A sense of pervasive well-being is what I recall; more specifically, a strange sense of personal enlargement; a sort of swelling out, becoming bigger than life, thanks to participation in collective ritual. (McNeill 1995, quoted in Kesebir 2011)</p>\n</blockquote>\n<p>Wiltermuth &amp; Heath (2009) summarize some of the research on the topic:</p>\n<blockquote>\n<p>The idea that synchronous movement improves group cohesion has old roots. As historian William H. McNeill suggests, armies, churches, and communities may have all benefited, intentionally or unintentionally, from cultural practices that draw on &lsquo;&lsquo;muscular bonding,&rsquo;&rsquo; or physical synchrony, to solidify ties between members (McNeill, 1995). This physical synchrony, which occurs when people move in time with one another, has been argued to produce positive emotions that weaken the boundaries between the self and the group (Ehrenreich, 2006; Hannah, 1977), leading to feelings of collective effervescence that enable groups to remain cohesive (Durkheim, 1915/1965; Haidt, Seder, &amp; Kesebir, in press; Turner, 1969/1995). Andaman Islanders have been said to become &lsquo;&lsquo;absorbed in the unified community&rsquo;&rsquo; through dance (Radcliffe-Brown, 1922, p. 252). Similar observations have been made of Carnival revelers (Ehrenreich, 2006), and ravers dancing to beat-heavy music (Olaveson, 2004). Moreover, Haidt et al. (in press) have argued that people must occasionally lose themselves in a larger social organism to achieve the highest levels of individual well-being.</p>\n</blockquote>\n<p>Some recent findings on the topic include:</p>\n<p><strong>Wiltermuth &amp; Heath (2009):</strong> Synchronous activity in the form of walking around a campus in step causes people to be more likely to make decisions requiring trust and to self-report stronger feelings of trust and connectedness with others. Singing in synchrony, even if the song is an out-group anthem (\"O Canada\", when the subjects were USA residents), causes more trust and and greater feelings of being on the same team, as well as an increased willingness to cooperate in a <a href=\"http://en.wikipedia.org/wiki/Public_goods_game\">public goods game</a>.</p>\n<p><strong>Kirschner &amp; Tomasello (2010):</strong> \"Given that in traditional cultures music making and dancing are often integral parts of important group ceremonies such as initiation rites, weddings or preparations for battle, one hypothesis is that music evolved into a tool that fosters social bonding and group cohesion, ultimately increasing prosocial ingroup behavior and cooperation. Here we provide support for this hypothesis by showing that joint music making among 4-year-old children increases subsequent spontaneous cooperative and helpful behavior, relative to a carefully matched control condition with the same level of social and linguistic interaction but no music.\"</p>\n<p><strong>Valdesolo, Ouyang &amp; DeSteno (2010):</strong> Synchronous rocking increases perceptions of similarity and connectedness. The subjects were given the task of holding the opposite ends of a 12&nbsp;&times;&nbsp;14 wooden labyrinth with both hands and guiding a steel ball through it together. The subjects in the synchronous rocking condition performed better than the subjects in the asynchronous rocking condition.</p>\n<p><strong>Valdesolo &amp; DeSteno (2011):</strong> Subjects who are told to tap the beats they hear in an audio clip, and are paired with a confederate who has been instructed to synchronize his tapping with the participant&rsquo;s, tend to find like the confederate more and consider him more similar to themselves. The confederate being assigned an unfair task then evokes more feelings of compassion, and the subjects are more likely to help him, even at a cost to themselves.</p>\n<p>The implication for meetup groups, as well as any other groups that might want to make their members like each other more, seems clear: spend some time singing and dancing together, possibly in the form of drinking songs if people are too self-conscious to sing while sober. Just make sure that any non-drinkers don't feel excluded. If all else fails, you can always march around the city while chanting \"<a href=\"http://hpmor.com/chapter/30\">doom doom DOOM DOOM</a>\". (If anybody asks, you can say that you're testing a scientific hypothesis about group bonding, and ask if they'd want to join in.)</p>\n<p><span style=\"text-decoration: underline;\"><strong>References</strong></span></p>\n<p>Kesebir, S. (2011) <a href=\"http://psr.sagepub.com/content/early/2011/12/27/1088868311430834\">The Superorganism Account of Human Sociality: How and When Human Groups Are Like Beehives</a> (<a href=\"http://people.virginia.edu/%7Esk8dm/Papers/Kesebir-in%20press-PSPR-The%20superorganism%20account%20of%20human%20sociality.docx\">ungated version</a>). <em>Personality and Social Psychology Review</em>. <a href=\"http://psr.sagepub.com/content/early/2011/12/27/1088868311430834\"></a><a href=\"http://psr.sagepub.com/content/early/2011/12/27/1088868311430834\"><br /></a></p>\n<p>Kirchner, S. &amp; Tomasello, M. (2010) <a href=\"http://www.eva.mpg.de/psycho/staff/kirschner/pdf/Kirschner+Tomasello-2010-Music+ProsocialBehavior.pdf\">Joint music making promotes prosocial behavior in 4-year-old children</a>. <em>Evolution and Human Behavior</em> 31, 354&ndash;364.&nbsp;</p>\n<p>McNeill, W.H. (1995) <em>Keeping together in time: Dance and drill in human history.</em> Cambridge, MA: Harvard University Press.</p>\n<p>Radcliffe-Brown, A. R. (1948) <em>The Andaman Islanders</em>. Glencoe, IL: Free Press.</p>\n<p>Valdesolo, P. &amp; DeSteno, D. (2011) <a href=\"http://ccare.stanford.edu/sites/default/files/valdesano%20and%20destafano%20Synchrony%20and%20the%20Social%20Tuning%20of%20Compassion%20Emotion%202011.pdf\">Synchrony and the Social Tuning of Compassion</a>. <em>Emotion, </em>vol. 11, no. 2, 262&ndash;266.&nbsp;</p>\n<p>Valdesolo, P. &amp; Ouyang, J. &amp; DeSteno, D. (2010) <a href=\"http://www.sciencedirect.com/science/article/pii/S0022103110000430\">The rhythm of joint action: Synchrony promotes cooperative ability</a>. <em>Journal of Experimental Social Psychology</em>, vol. 46, no. 4, 693&ndash;695.</p>\n<p>Wiltermuth, S.S. &amp; Heath, C. (2009): <a href=\"http://personal.stevens.edu/~ysakamot/175/paper/synchrony.pdf\">Synchrony and Cooperation</a>. <em>Psychological Science, </em>vol. 20, no. 1.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GahkhPWinnAvri8Td", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 40, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "15489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T2fcyjay3GtkvGn7F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T16:06:48.727Z", "modifiedAt": null, "url": null, "title": "The Craft And The Community: Wealth And Power And Tsuyoku Naritai", "slug": "the-craft-and-the-community-wealth-and-power-and-tsuyoku", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PLNwQ47FPqcAyt3KH/the-craft-and-the-community-wealth-and-power-and-tsuyoku", "pageUrlRelative": "/posts/PLNwQ47FPqcAyt3KH/the-craft-and-the-community-wealth-and-power-and-tsuyoku", "linkUrl": "https://www.lesswrong.com/posts/PLNwQ47FPqcAyt3KH/the-craft-and-the-community-wealth-and-power-and-tsuyoku", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Craft%20And%20The%20Community%3A%20Wealth%20And%20Power%20And%20Tsuyoku%20Naritai&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Craft%20And%20The%20Community%3A%20Wealth%20And%20Power%20And%20Tsuyoku%20Naritai%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPLNwQ47FPqcAyt3KH%2Fthe-craft-and-the-community-wealth-and-power-and-tsuyoku%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Craft%20And%20The%20Community%3A%20Wealth%20And%20Power%20And%20Tsuyoku%20Naritai%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPLNwQ47FPqcAyt3KH%2Fthe-craft-and-the-community-wealth-and-power-and-tsuyoku", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPLNwQ47FPqcAyt3KH%2Fthe-craft-and-the-community-wealth-and-power-and-tsuyoku", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2020, "htmlBody": "<p>In this post, I'll try to tackle the question of whether this community and its members should focus more efforts and resources on improving their strength as individuals and as a community, than on directly tackling the problem of singularity. I'll start off with a personal anecdote, because, while I know it's not&nbsp;indispensable, I think <a href=\"/search/results?cx=015839050583929870010%3A-802ptn4igi&amp;cof=FORID%3A11&amp;ie=UTF-8&amp;q=case+study&amp;sa=Search&amp;siteurl=lesswrong.com%2Flw%2F5lm%2Fbuilding_rationalist_communities_lessons_from_the%2F&amp;ref=www.google.es%2F\">anecdotes</a> help the reader to think in near rather than far mode, and this post's topic is already too easily thought of in far mode in the first place.</p>\n<p>The other day, I was in an idle conversation with a cab driver when he asked me: What would you do if you won the lottery? Is there some particular dream you have, such as travelling the world or something? I said (and I apologize in advance for the grandiosity and egotism of what follows, mostly because it might show a poor appraisal of my own competence and ability)</p>\n<blockquote>\n<p>Well, it's not like I would ever play at the lottery, but if I did, and somehow won, I would</p>\n<ul>\n<li>Pay myself the very best tutors and the very best education (I'm thinking Master's Degrees, PhD, and so on, that's all pretty damned expensive depending on where you take it) in my chosen speciality.</li>\n<li>Pay myself the best aid in achieving peak sustainable physical, mental, and emotional condition (as optimized for the struggles and stresses of a daily life of extreme academic exertion, not for, say, performing in the battlefield, the olympics, or competitive chess). Coaches, gurus, chemicals, whatever it takes.</li>\n<li>Spend one or two or even three years around the world learning as many \"important\" languages as I can. Not in order of ease or priority: Portuguese, Italian, Russian, Mandarin Chinese, Japanese, Hindi, Urdu, Farsi, and Turkish and Arabic and Hebrew and their Ancient variants, because of all the doors these could open... and Basque and Navajo (<a href=\"http://en.wikipedia.org/wiki/Code_talker\">those two</a> would be <em><a href=\"http://en.wikipedia.org/wiki/Basque_language#Hypotheses_on_connections_with_other_languages\">just for the hell of it</a></em>). (I already know English, Spanish, French, and a fair amount of German and Arabic).</li>\n<li>With the acquired technical knowledge and skills, and the help of the contact network and the better understanding of human nature that learning so many languages and exploring so many cultures and travelling so much will have netted me, use the remaining money to start a business, one that involves as many people as possible in a way such that I can train them to be a <a href=\"http://dinosaurusgede.deviantart.com/art/Potter-s-Chaos-Legion-177373507\">Chaos Legion</a>.</li>\n<li>Hopefully, once I have achieved enough profits to make the growth of my business secure, donate a constant stipendium to <a href=\"http://intelligence.org/\">my favourite</a> <a href=\"http://www.fhi.ox.ac.uk/\">nonprofits</a>.</li>\n<li>In my old age, use the returns from all the previous efforts to found a school (actually an integral education system, think something between&nbsp;<a href=\"http://www.summerhillschool.co.uk/\">Summerhill School</a>&nbsp;and the <a href=\"http://roleplaysanctuary.freeforums.org/download/file.php?photo=2_1264735831.jpg\">Mahora Academy Complex</a>) which would be optimized for great justice and the&nbsp;rigorous&nbsp;use and promotion and exponential spread of modern rationality</li>\n</ul>\n<p>&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>My reply surprised both of us. Him, because it was atypical (apparently most people would spend them on luxury items and so on, that is, they would spend their newfound money in signalling that they have it... I think the mistake comes from seeing rich people doing it and then assuming that that's what you <em>should</em> do if you become rich, the only other option apparently being saving it up in an account). That a modern rationalist came up with an atypical answer to such a question is only to be expected.</p>\n<p>But I was surprised too, because I found it strange that what I thought I ought to do and what I wanted to do coincided so perfectly. I wasn't even expecting those last two points, they sort of naturally came out in the spur of the moment. Upon further thought, I was also surprised that this turned out to be merely an exaggeration and heavy of my pre-existing plan, which I am already attempting to follow with far less material means. That is to say, the dramatic change in money did not fundamentally change what I wanted to do with my (currently limited) lifetime.</p>\n<p>But then I asked myself: if my priority is reducing existential risk, why am I not giving all the money to my favourite nonprofits immediately?</p>\n<p>And that's where it hit me: I wanted to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">make myself stronger</a>. And the point I'm trying to make is that, well, so should we all. Why?</p>\n<p>There's a strong selfish component to that (not that there's anything wrong with healthy selfishness), but, for someone who considers existential risk an extremely important factor, enlightened self-interest might still be on the side of donating immediately.</p>\n<p>But it might also be a sound strategy, sounder, perhaps, to exponentially increase our ability to help fight existential risk, in terms of fear, and improve the general level of human rationality, in terms of desire (I understand that we would all be happier in a world with more rational people, for many, many reasons, not all of which are altruistic). So, how would we go about this? I submit to you this tentative strategy draft.</p>\n<p>&nbsp;</p>\n<ul>\n<li>Step Zero: Improve our own physical and mental condition. We need the best possible hardware to operate on. This will give us <em>raw ability</em>. The ability to gain abilities, so to speak. This includes <a href=\"/lw/bq0/be_happier/\">making ourselves happy</a>&nbsp;(which itself overlaps with and is enabled by some of the following points: it's a virtuous cycle).</li>\n<li>Step One: Increasing our own personal,&nbsp;intrinsic&nbsp;worth: buying with our money goods that improve our ability to both obtain and enjoy more goods, and that could never be taken away from us by economic transaction. While we could teach ourselves those with only the cost of opportunity of not spending that time earning wages, well thought out and carefully applied expenditure can <em>significantly</em> accelerate and smooth the process. This will make us powerful, useful tools, for our own goals and for the goals of those that would associate with us (employers, allies, and so on). This will give us <em>authority. </em>We become acknowledged <a href=\"http://www.overcomingbias.com/2007/04/expert_at_versu.html\">experts on or at</a> a socially useful field. Scientists, Engineers, Artists and other highly skilled folks are on this level: they can already get a lot done, and change the world but, as the Creationism issue proves, among others, it isn't nearly enough. You often don't get to choose what to work on or how many resources are made available to you, you don't have any control on the fruit of your work once it's done and released, and you may always have trouble getting people to follow your advice, no matter how much you think you know better. Step one is the step of Intrinsic Power.</li>\n<li>Step Two: Increasing our ability to take advantage of social status: learning and perfecting languages, social skills, communication and manipulation tools, dress sense and sense of signalling, dancing, romantic an sexual intelligence and skill (how many powerful people had their careers and/or reputations&nbsp;<em>ruined forever</em> because of badly handled sex and romance?)... That is, we will learn and master the rules of the game, the game we are all playing all the time by virtue of associating with other human beings, <a href=\"/lw/372/defecting_by_accident_a_flaw_common_to_analytical/\">and avoid defecting by accident</a>, among other possible mistakes. This will give us <em>urbanity. </em>Together with authority, it already means <a href=\"http://www.overcomingbias.com/2007/06/choose_credit_o.html\">both<em>&nbsp;credit</em> and<em> influence</em></a>. At this level, you can actually get a lot more stuff done, because you're much better at persuading people to want to follow your suggestions <em>out of their own volition</em>.\n<ul>\n<li>This includes the ability to delegate, divide work and manage specialists, empower and motivate people to help you, helping them grow themselves in the process, etc. Step Two is the step of Soft Power.</li>\n</ul>\n</li>\n</ul>\n<p><a href=\"http://wiki.lesswrong.com/wiki/The_Craft_and_the_Community\">A lot of effort has already been expended by the community in working on these first steps.</a>&nbsp;But there's a third step that isn't getting worked on much, perhaps because of aesthetic values, perhaps because it's <strong>one of the most dangerous to&nbsp;wield</strong>, both to the world and to ourselves and our own personal integrity:</p>\n<ul>\n<li>Step Three: Increase how much <em>coercive</em>&nbsp;<em>power</em> we hold over how many of our fellow human beings: the ability to make them do things <em>or else</em>. My impression is that economic power (both affluence and assets) is much more secure and far less vulnerable than other sources such as, say, media influence or political clout, or social power brokering (which is greatly enhanced by <a href=\"/lw/5v/church_vs_taskforce\">joining support groups</a>&nbsp;or <a href=\"/lw/5lm/building_rationalist_communities_lessons_from_the/\">becoming one ourselves</a>), (although the feedback between these tends to be positive, on average, and overlap and migration between them is hardly unheard of). This <em>power</em> is increased exponentially, and is much easier to maintain, by having both Step One (you actually know what you're doing or at least where to get the information, and are more able to judge it) and Two (you know what not to do and how to achieve the greatest results with the minimal expenditure of your <em>power</em>) under your belt, and of course all three steps profit from Step Zero. At this level, to a certain extent, people <em>will</em> do what you want them to, their own feelings, initiatives and desires factoring far less into the actions they end up choosing than they otherwise would.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Those are partly selfish goals unto themselves: power means freedom to do what you want, and that and high social status are already very enjoyable for their own sake. Additionally,<strong> the more of us achieve them (and the larger the <em>capacity</em> in which they achieve them), the more resources they can get assigned and the more support they can gather (or <em>force</em>) for the sake of efforts towards preventing existential risk.</strong> But I suggest that they be mainly planned, optimized and instrumentalized for Step Four, the most dangerous of all:</p>\n<p>&nbsp;</p>\n<ul>\n<li><strong>Step Four: Use the gained knowledge, skills, assets, and position to improve the overall level of both cognitive and instrumental rationality of humanity.</strong></li>\n</ul>\n<p>&nbsp;</p>\n<p>Which has the following advantages I can think of, listed without regard for altruism or selfishness:</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<ul>\n<li>Humans are enabled to be far more successful in <em>the pursuit of happiness</em>, whatever that is, and in otherwise improving themselves, their lives, and the world around them. Their<em> liberty</em> in terms of choices is also greatly increased, once free of akrasia and with an enhanced ability to identify and accurately assess choices.</li>\n<li>We as rationalists feel far less isolated and vulnerable and far more at home in a world where there are more people like us and where people are more like us (not quite the same thing). Life will generally be more fun and interesting.</li>\n<li>We'll get a much wider pool of potential candidates from which people with the ability to help prevent existential risk, and the cost and difficulty of gathering more support and resources will be greatly diminished. In other words, <strong>we'll be much more effective at preventing existential risk.</strong></li>\n</ul>\n<p>&nbsp;</p>\n<p>Does achieving Step Four mean humanity will actually be in less of a danger of self-destructing at that point? It's not a rhetorical question, and I don't think its answer is trivial: in particular, having many <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">half rationalists</a>&nbsp;(I might well still be one myself) running around might represent a <em>considerable danger</em>, which could be <a href=\"http://en.wikipedia.org/wiki/Eternal_September\">sustained in time</a>. However, projects such as <a href=\"http://hpmor.com/modern-rationality/\">Methods of Rationality or The&nbsp;Centre for&nbsp;Modern Rationality</a>, as well as this site's very&nbsp;existence&nbsp;seem to hint that some of the smartest among us are willing to take the risk.</p>\n<p>So, the immediate question I ask of you in earnest, the whole point of this post<strong>: How do we go about <a href=\"/lw/65/money_the_unit_of_caring/\">spending our money</a>&nbsp;and effort in the most effective way to prevent existential risk? </strong>How much to de expend in directly attacking the problem as we are, how much do we expend in actually making ourselves stronger?</p>\n<p>In sillier terms: <a href=\"http://en.wikipedia.org/wiki/Dragon_Ball\">Should the Z Warriors go and try to confront Cell right now, before he grows too strong to beat, or should they avoid the fight and go train instead? (assume that they do nothing with their lives but be in fights, train to prepare for fights, or run away from fights they are not prepared for yet)</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb117": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PLNwQ47FPqcAyt3KH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 1, "extendedScore": null, "score": 8.895544645096173e-07, "legacy": true, "legacyId": "15490", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "JHcTP4Ad8QAmRTCZm", "GG2rtBReAm6o3mrtn", "p5DmraxDmhvMoZx8J", "su7bXsXYpY55vwphc", "7FzD7pNm9X68Gp5ZC", "ZpDnRCeef2CLEFeKM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T17:04:19.599Z", "modifiedAt": null, "url": null, "title": "Andrew Gelman on \"the rhetorical power of anecdotes\"", "slug": "andrew-gelman-on-the-rhetorical-power-of-anecdotes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CNJNmS3jbjZSY25eb/andrew-gelman-on-the-rhetorical-power-of-anecdotes", "pageUrlRelative": "/posts/CNJNmS3jbjZSY25eb/andrew-gelman-on-the-rhetorical-power-of-anecdotes", "linkUrl": "https://www.lesswrong.com/posts/CNJNmS3jbjZSY25eb/andrew-gelman-on-the-rhetorical-power-of-anecdotes", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Andrew%20Gelman%20on%20%22the%20rhetorical%20power%20of%20anecdotes%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAndrew%20Gelman%20on%20%22the%20rhetorical%20power%20of%20anecdotes%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNJNmS3jbjZSY25eb%2Fandrew-gelman-on-the-rhetorical-power-of-anecdotes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Andrew%20Gelman%20on%20%22the%20rhetorical%20power%20of%20anecdotes%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNJNmS3jbjZSY25eb%2Fandrew-gelman-on-the-rhetorical-power-of-anecdotes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCNJNmS3jbjZSY25eb%2Fandrew-gelman-on-the-rhetorical-power-of-anecdotes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<p><a href=\"http://andrewgelman.com/2012/04/any-old-map-will-do-meets-god-is-in-every-leaf-of-every-tree/\">Andrew Gelman has a post up today</a> discussing a particularly illustrative instance of narrative fallacy involving the <a href=\"http://andrewgelman.com/2012/04/another-day-another-plagiarist/\">recent plagiarism discussion surrounding Karl Weick</a>. I think there are also some interesting lessons in there about generalizing from fictional evidence. <br /><br />In particular, Gelman says, \"Setting aside [any] issues of plagiarism and rulebreaking, I argue that by hiding the source of the story and changing its form, Weick and his management-science audience are losing their ability to get anything out of it beyond empty confirmation.\"<br /><br />I am wondering if anyone has explicitly looked into connections between generalizing from fictional evidence and confirmation bias. It sounds intuitively plausible that if you are going to manipulate fictional evidence for your purposes, you'll almost always come out believing the evidence has confirmed your existing beliefs. I would be highly interested in documented accounts where the opposite has happened and fictional evidence actually served as a correction factor.<br /><br />For what it's worth, I personally enjoy a watered-down version of the moral that Weick attempts to manipulate from the story that's discussed in Gelman's post. My high school math teacher used to always say to us, \"When you don't know what to do, do something.\" I think he said it because he was constantly pissed about questions left completely blank on his math exams, and wanted students to write down scribblings or ideas so he could at least give them some partial credit, but it has been more motivational than that for me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CNJNmS3jbjZSY25eb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 8.895790268540765e-07, "legacy": true, "legacyId": "15491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T18:17:28.353Z", "modifiedAt": null, "url": null, "title": "Living Forever is Hard, part 3: the state of life extension research", "slug": "living-forever-is-hard-part-3-the-state-of-life-extension", "viewCount": null, "lastCommentedAt": "2020-11-01T02:28:06.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/igLtDADspHudvNuQJ/living-forever-is-hard-part-3-the-state-of-life-extension", "pageUrlRelative": "/posts/igLtDADspHudvNuQJ/living-forever-is-hard-part-3-the-state-of-life-extension", "linkUrl": "https://www.lesswrong.com/posts/igLtDADspHudvNuQJ/living-forever-is-hard-part-3-the-state-of-life-extension", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Living%20Forever%20is%20Hard%2C%20part%203%3A%20the%20state%20of%20life%20extension%20research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiving%20Forever%20is%20Hard%2C%20part%203%3A%20the%20state%20of%20life%20extension%20research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigLtDADspHudvNuQJ%2Fliving-forever-is-hard-part-3-the-state-of-life-extension%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Living%20Forever%20is%20Hard%2C%20part%203%3A%20the%20state%20of%20life%20extension%20research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigLtDADspHudvNuQJ%2Fliving-forever-is-hard-part-3-the-state-of-life-extension", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigLtDADspHudvNuQJ%2Fliving-forever-is-hard-part-3-the-state-of-life-extension", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>Previous:</p>\n<ol>\n<li><a href=\"/lw/5qm/living_forever_is_hard_or_the_gompertz_curve/\">\"Living Forever is Hard, or, the Gompertz Curve\"</a></li>\n<li><a href=\"/lw/7jh/living_forever_is_hard_part_2_adult_longevity/\">\"Living Forever is Hard, part 2: Adult Longevity\"</a></li>\n</ol>\n<p>From the excellent <a href=\"http://www.fightaging.org/\">Fight Aging!</a> blog comes a pointer to <a href=\"http://www.fightaging.org/archives/2012/04/a-histogram-of-results-from-life-span-studies.php\">\"A Histogram of Results from Life Span Studies\"</a>, a graph of thousands of animal studies by Kingsley G. Morse Jr. (updated version from mailing list):</p>\n<p><img src=\"http://i.imgur.com/Pcnj5.png\" alt=\"\" width=\"500\" /></p>\n<p>&nbsp;</p>\n<p>(This is not the same as a <a href=\"/lw/8nc/funnel_plots_the_study_that_didnt_bark_or/\">funnel plot</a>, as the y-axis is # of studies finding that percentage gain and nothing to do with the <em>n</em> of studies.)</p>\n<p><br />On the closed GRG mailing list, the compiler says:</p>\n<blockquote>\n<p>Many test the same intervention on a different strain of the same species, or with a different dose.</p>\n</blockquote>\n<p><span style=\"font-size: small;\">I asked some questions, and Steven B. Harris replied:</span></p>\n<blockquote>\n<p>&ldquo;I was gratified to be able to answer promptly, and I did. I said I didn't know.&rdquo; &nbsp;(Twain)<br /> <br /> Though I can observe that the center of that distribution isn't very far from no-effect, and one would expect that there's a publication bias toward reporting positive effects vs. null effects. I would think that could account for it entirely.<br /> <br /> There's also the problem we've discussed before, that feeling animals stuff they don't like the taste off, amounts to calorie restriction. So this clouds the issues terribly in non-CR studies, unless you're very, VERY careful to control them somehow.</p>\n</blockquote>\n<p>The relevance of this summary graph to news like the C60 rodent life extension experiment is obvious. Reading GRG has been interesting and educational about that experiment; a rough summary of points made by various people including myself:</p>\n<ul>\n<li>contradictory median/lifespan figures</li>\n<li>duplicate image</li>\n<li>small sample</li>\n<li>doses of C60 small enough that the direct antioxidant activity can't be responsible</li>\n<li>justifying cites not published when experiment started</li>\n<li>the C60 was administered for brief period (think the analogy given was 'imagine taking a supplement only during your 40s and doubling your lifespan')</li>\n<li>the massive life extension observed in the olive-oil-only rats - not doubling, but still really implausible</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "igLtDADspHudvNuQJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 8.896102665202977e-07, "legacy": true, "legacyId": "15015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GytPrQ9cT46k9etoz", "zRKW7LotZxJi6Cyeg", "RYA9uviAJBtn8hqkF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T19:16:07.064Z", "modifiedAt": null, "url": null, "title": "Timeless physics breaks T-Rex's mind [LINK]", "slug": "timeless-physics-breaks-t-rex-s-mind-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.124Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JXM9ML4FtCaAitmQC/timeless-physics-breaks-t-rex-s-mind-link", "pageUrlRelative": "/posts/JXM9ML4FtCaAitmQC/timeless-physics-breaks-t-rex-s-mind-link", "linkUrl": "https://www.lesswrong.com/posts/JXM9ML4FtCaAitmQC/timeless-physics-breaks-t-rex-s-mind-link", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timeless%20physics%20breaks%20T-Rex's%20mind%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimeless%20physics%20breaks%20T-Rex's%20mind%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXM9ML4FtCaAitmQC%2Ftimeless-physics-breaks-t-rex-s-mind-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timeless%20physics%20breaks%20T-Rex's%20mind%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXM9ML4FtCaAitmQC%2Ftimeless-physics-breaks-t-rex-s-mind-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXM9ML4FtCaAitmQC%2Ftimeless-physics-breaks-t-rex-s-mind-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p><a href=\"http://www.qwantz.com/index.php?comic=2190\" target=\"_blank\">From Dinosaur Comics</a>, with a nice shout-out to <a href=\"/lw/qp/timeless_physics/\">Eliezer's Timeless Physics post</a>! (Look in the newspost below the comic.)</p>\n<p>I can't wait until Ryan North gets around to <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem</a>...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xjy2ZYACvYQBPJdix": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JXM9ML4FtCaAitmQC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 28, "extendedScore": null, "score": 8.896353145231277e-07, "legacy": true, "legacyId": "15492", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rrW7yf42vQYDf8AcH", "6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T20:59:34.698Z", "modifiedAt": null, "url": null, "title": "[LINK] Amazing Prisoner's Dilemma Move", "slug": "link-amazing-prisoner-s-dilemma-move", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.735Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wbGzkNEdhDxyZCpXz/link-amazing-prisoner-s-dilemma-move", "pageUrlRelative": "/posts/wbGzkNEdhDxyZCpXz/link-amazing-prisoner-s-dilemma-move", "linkUrl": "https://www.lesswrong.com/posts/wbGzkNEdhDxyZCpXz/link-amazing-prisoner-s-dilemma-move", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Amazing%20Prisoner's%20Dilemma%20Move&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Amazing%20Prisoner's%20Dilemma%20Move%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbGzkNEdhDxyZCpXz%2Flink-amazing-prisoner-s-dilemma-move%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Amazing%20Prisoner's%20Dilemma%20Move%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbGzkNEdhDxyZCpXz%2Flink-amazing-prisoner-s-dilemma-move", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbGzkNEdhDxyZCpXz%2Flink-amazing-prisoner-s-dilemma-move", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>From the game show, \"Golden Balls.\" This isn't the classic prisoner's dilemma, because they get to discuss it. I think that makes it much more interesting.</p>\n<p><a href=\"http://www.youtube.com/watch?v=S0qjK3TWZE8\">http://www.youtube.com/watch?v=S0qjK3TWZE8</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wbGzkNEdhDxyZCpXz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "15493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-23T23:17:47.972Z", "modifiedAt": null, "url": null, "title": "An exercise in really going through with it", "slug": "an-exercise-in-really-going-through-with-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JGw75D5RZiCczdbrk/an-exercise-in-really-going-through-with-it", "pageUrlRelative": "/posts/JGw75D5RZiCczdbrk/an-exercise-in-really-going-through-with-it", "linkUrl": "https://www.lesswrong.com/posts/JGw75D5RZiCczdbrk/an-exercise-in-really-going-through-with-it", "postedAtFormatted": "Monday, April 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20exercise%20in%20really%20going%20through%20with%20it&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20exercise%20in%20really%20going%20through%20with%20it%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGw75D5RZiCczdbrk%2Fan-exercise-in-really-going-through-with-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20exercise%20in%20really%20going%20through%20with%20it%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGw75D5RZiCczdbrk%2Fan-exercise-in-really-going-through-with-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGw75D5RZiCczdbrk%2Fan-exercise-in-really-going-through-with-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1364, "htmlBody": "<p>I'm re-posting a small bit of writing from my <a href=\"http://suitdummy.blogspot.com/\">blog</a> that was inspired by reading some recent discussions about cryonics on LW. I'm by no means a skillful fiction writer, but I found it emotionally rewarding to write this and really try to mentally simulate what it would be like to choose cryocide if I was cognizant of my own quickly approaching mental decline. <br /><br />Constructive comments are welcome.</p>\n<p><br /><br /><em>An exercise in really going through with it:</em><br /><br />Well, this is it. You had thought it would be more like a retirement party all those months ago when you first suggested the idea. The truth is, you had to frame it that way to get your daughter to go along with it. Thank god she's of a different generation, one with ubiquitous electronics; everyone's a software developer. She understands that the weirdness of your request isn't something to be afraid of, but she's human after all. She still wants to <a href=\"http://hanson.gmu.edu/showcare.pdf\">show that she cares</a>. Yes, you had to say it was like a retirement party. Good thing you were an accomplished scientist, a person whose understanding and reputation on matters like this had to be respected. It was the only way to trick them all into respecting your wishes... wishes that fly in the face of everyone who loves you.<br /> <br /> You wonder how long cake will be a thing. We have hydrogen-powered cars and technology has cut our work weeks to 25 hours, and yet we still celebrate occasions with cake. It seems like such a foolish thing, knowing what will happen tomorrow. And yet it's still so human. It reminds you of your own childhood which seems long gone. Everyone around you was big and in charge and when there was a cake at something it meant it was a Big Deal. And here you are, surrounded by pictures of yourself. People are giving short speeches about your accolades. People are giving short speeches about morally questionable things you once did to goof off. And everyone is laughing in that very plastic it's-ok-to-laugh-at-questionable-things-because-we're-at-an-event sort of way.<br /> <br /> But their guts get hit by gravity as the evening wears on. You are going to die, after all. For all their moralizing, all their pseudo-respect for your wishes, all their sophisticated rational utility calculations, they can only contort their facial muscles to provide paltry masks for their visceral uneasiness. The ones who traveled here, not your family, just want to say their goodbyes and head home, and remark to each other in the car that it's such a sad situation, that they hope it never comes to this for them. Your family members' contorted facial disguises are the flimsiest of all. They are obviously terrified, as are you. You suffer momentary flashes of anger that a recounting of the social prevalence of cake will probably be among your final thoughts.<br /> <br /> The party dies down. People say hilariously insufficient goodbyes to you. Some wish you well. They just want to leave. Your family cleans up the mess and puts leftovers into a refrigerator. They sit with you in the family room. You look at photo albums with them and everyone just cries. You tell stories about your own long-dead parents and how, compared to them, you're making such a better decision. You hope your children will make a similar one.<br /> <br /> It gets late and you become tired. Your children cry and they do not want you to go to bed. In an act of extreme sentimentality, you offer to tuck them each into bed, in some sort of silly and needless gesture to impart a final happy memory. You tell them how proud you are. You love them very much. But the truth is, on those days when you can't recognize them, how could you even tell if you loved them? And on the days when you wake up and your memory returns, you are terrified. This way, you tell them, you can remember them forever. They just cry and they do not understand. But they respect your wishes.<br /> <br /> You change into more comfortable clothes. You lay down. Not even the looming injection can keep your tired bones from wanting a good night's sleep. Your mind is racing. You'll actually have to go through with it. If you can just force yourself to think about something else for now then you can fall asleep. You think about your wife. You can't bear a life in which you cannot even treasure her memory. She can only be fully gone if you cannot remember her, you tell yourself. You set your rationalization engine to work and you fall asleep.<br /> <br /> This is it. You arranged for an attendant to pick you up early, before the others could awaken. You ride silently in a car with hard, unworn seats. You study the landscape, mind racing, for the last time. Why is there always so much graffiti and trash along the embankments of railroad tracks? Why are there always soda bottles and shopping carts and ugly rocks stained the color of a lifelong coffee-drinker's teeth?<br /> <br /> They wheel you in the door in your wheelchair. Everything has a sobering sterility. It doesn't smell like life or the world. There are no forms to sign, you made sure of that. Just straight to business. Your heart is racing as they help you change into a gown. They don't even bother checking your vital signs. They show you what will happen to your body just afterwards. You'll be wheeled down the hall to a preservation lab. They will replace the blood in your body with a specially developed protectant liquid that will prevent cellular decay for up to hundreds of years. If you weren't a scientist the whole thing would be morbid, and it almost is anyway.<br /> <br /> You try to imagine that it's just like going to sleep. They're just putting you under for a looong operation, you try to tell yourself. It does not stop your racing heart. But the injection will.<br /> <br /> The time finally comes. They wheel you into an operating room with a chairlike apparatus. It is tilted back about forty-five degrees. Your last sights and sensory experiences are the following: unpleasant fluorescent lighting against a backdrop of a perfectly white tiled ceiling; a smell that vaguely reminds you of the polish the dentist uses when you come in for cleanings; air-conditioned air chilly enough to give you goosebumps. Various technicians and doctors reassuring you that everything will be OK as they move instrument tables around; the taste of your own mouth, sterile from the morning's toothbrushing but with a small hint of the coffee you drank.<br /> <br /> The hospital you are in now does this same procedure hundreds of times per month, you remark to yourself. It's not so unusual. Don't be a coward. <a href=\"http://web.utk.edu/~jhardwig/dutydie.htm\">Fulfill your duty.</a><br /> <br /> An officer from the county coroner's office stands in front of you and requests that you attest to some things. You make your wishes clear and it satisfies him. Your heart is racing.<br /> <br /> An attendant tilts your head back and places an oxygen mask over your face. The reality is terrifying; your heart is pounding. You don't want your last sights to be of a pristine white hospital ceiling, but the oxygen does its job and you calm down. You think about your children and the wonderful lives they will lead, free of burden. You are happy that you can remember their faces and all the irreplaceable memories of their growth. You don't want to have a brain that doesn't include a photo album of their lives.<br /> <br /> The attendant asks you to count backwards from 10, and you play along until you get to 7. Your mind is too preoccupied with the thought that you love your children and that they will have wonderful lives. You wish you could be there with them... but <a href=\"http://www.alcor.org/magazine/2011/01/14/options-for-brain-threatening-disorders/\">this is</a>... <a href=\"http://patrissimo.livejournal.com/1453175.html?thread=13409911\">for the</a>... <a href=\"/r/discussion/lw/bvj/what_deserves_cryocide/\">best</a>...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1, "ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JGw75D5RZiCczdbrk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 35, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "15494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qytCWqq9cuMHAB4P2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T00:40:50.462Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Cafe Meetup!", "slug": "meetup-vancouver-cafe-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qY4fFCmexijzjS9rA/meetup-vancouver-cafe-meetup", "pageUrlRelative": "/posts/qY4fFCmexijzjS9rA/meetup-vancouver-cafe-meetup", "linkUrl": "https://www.lesswrong.com/posts/qY4fFCmexijzjS9rA/meetup-vancouver-cafe-meetup", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Cafe%20Meetup!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Cafe%20Meetup!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY4fFCmexijzjS9rA%2Fmeetup-vancouver-cafe-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Cafe%20Meetup!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY4fFCmexijzjS9rA%2Fmeetup-vancouver-cafe-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY4fFCmexijzjS9rA%2Fmeetup-vancouver-cafe-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9c'>Vancouver Cafe Meetup!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 April 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everybody.</p>\n\n<p>Everyone from anywhere near Vancouver should all come out and hang out with us at Benny's in Kitsilano. Sunday at 13:00.</p>\n\n<p>We are going to be discussing the <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reductionism sequence</a>, but please come out anyway even if you don't have the time to read it this week; we will discuss in detail and make sure everybody is filled in by the end.</p>\n\n<p>This is our big super out-of-the-basement meetup that <em>everybody</em> should come out to, so if you've been waiting for an excuse to come to one of our meetups <em>come to this one</em>. I hope to see some new faces!</p>\n\n<p>As usual, most of our discussion happens on the <a href=\"https://groups.google.com/group/vancouver-rationalists/about\" rel=\"nofollow\">mailing list</a>, so join that.</p>\n\n<p>No excuses, see you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9c'>Vancouver Cafe Meetup!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qY4fFCmexijzjS9rA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "15495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Cafe_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/9c\">Vancouver Cafe Meetup!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 April 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everybody.</p>\n\n<p>Everyone from anywhere near Vancouver should all come out and hang out with us at Benny's in Kitsilano. Sunday at 13:00.</p>\n\n<p>We are going to be discussing the <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reductionism sequence</a>, but please come out anyway even if you don't have the time to read it this week; we will discuss in detail and make sure everybody is filled in by the end.</p>\n\n<p>This is our big super out-of-the-basement meetup that <em>everybody</em> should come out to, so if you've been waiting for an excuse to come to one of our meetups <em>come to this one</em>. I hope to see some new faces!</p>\n\n<p>As usual, most of our discussion happens on the <a href=\"https://groups.google.com/group/vancouver-rationalists/about\" rel=\"nofollow\">mailing list</a>, so join that.</p>\n\n<p>No excuses, see you all there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Cafe_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/9c\">Vancouver Cafe Meetup!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Cafe Meetup!", "anchor": "Discussion_article_for_the_meetup___Vancouver_Cafe_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Cafe Meetup!", "anchor": "Discussion_article_for_the_meetup___Vancouver_Cafe_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T01:18:22.860Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC meetup", "slug": "meetup-washington-dc-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7D9LxEpJFB8mc9afR/meetup-washington-dc-meetup", "pageUrlRelative": "/posts/7D9LxEpJFB8mc9afR/meetup-washington-dc-meetup", "linkUrl": "https://www.lesswrong.com/posts/7D9LxEpJFB8mc9afR/meetup-washington-dc-meetup", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7D9LxEpJFB8mc9afR%2Fmeetup-washington-dc-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7D9LxEpJFB8mc9afR%2Fmeetup-washington-dc-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7D9LxEpJFB8mc9afR%2Fmeetup-washington-dc-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9d'>Washington DC meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 April 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Portrait Gallery, central plaza</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be Zendo; or as dubbed at the last meetup \"Science: the game\"</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9d'>Washington DC meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7D9LxEpJFB8mc9afR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.897900687581052e-07, "legacy": true, "legacyId": "15501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup\">Discussion article for the meetup : <a href=\"/meetups/9d\">Washington DC meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 April 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Portrait Gallery, central plaza</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be Zendo; or as dubbed at the last meetup \"Science: the game\"</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup1\">Discussion article for the meetup : <a href=\"/meetups/9d\">Washington DC meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T01:34:07.475Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SzHsHjeJLoNa64LY6/meetup-melbourne-practical-rationality-10", "pageUrlRelative": "/posts/SzHsHjeJLoNa64LY6/meetup-melbourne-practical-rationality-10", "linkUrl": "https://www.lesswrong.com/posts/SzHsHjeJLoNa64LY6/meetup-melbourne-practical-rationality-10", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzHsHjeJLoNa64LY6%2Fmeetup-melbourne-practical-rationality-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzHsHjeJLoNa64LY6%2Fmeetup-melbourne-practical-rationality-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzHsHjeJLoNa64LY6%2Fmeetup-melbourne-practical-rationality-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9e'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 May 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">197 Little Lonsdale St, Melbourne VIC 3000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>And, watch this space - [content removed] <br />\n<a href=\"http://en.wikipedia.org/wiki/Aubrey_de_Grey\" rel=\"nofollow\">Aubrey de Grey</a> is speaking at <a href=\"http://embiggenbooks.com/\" rel=\"nofollow\">Embiggen Books</a>, and we're going along. Feel free to recruit promising skeptics, futurists, or anyone interesting - remember, think of them as meat.</p>\n\n<p>From the Embiggen Books blog: <a href=\"http://embiggenbooks.com/blog/?p=1669\" rel=\"nofollow\">Aubrey Du Grey with Dr Krystal Evans</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9e'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SzHsHjeJLoNa64LY6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.897967952802955e-07, "legacy": true, "legacyId": "15502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/9e\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 May 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">197 Little Lonsdale St, Melbourne VIC 3000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>And, watch this space - [content removed] <br>\n<a href=\"http://en.wikipedia.org/wiki/Aubrey_de_Grey\" rel=\"nofollow\">Aubrey de Grey</a> is speaking at <a href=\"http://embiggenbooks.com/\" rel=\"nofollow\">Embiggen Books</a>, and we're going along. Feel free to recruit promising skeptics, futurists, or anyone interesting - remember, think of them as meat.</p>\n\n<p>From the Embiggen Books blog: <a href=\"http://embiggenbooks.com/blog/?p=1669\" rel=\"nofollow\">Aubrey Du Grey with Dr Krystal Evans</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/9e\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T01:54:29.705Z", "modifiedAt": null, "url": null, "title": "Amazing strategy for \"Split or Steal\"(prisoner's dilemma)", "slug": "amazing-strategy-for-split-or-steal-prisoner-s-dilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.999Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/axf6Tx2eHjXvumFXD/amazing-strategy-for-split-or-steal-prisoner-s-dilemma", "pageUrlRelative": "/posts/axf6Tx2eHjXvumFXD/amazing-strategy-for-split-or-steal-prisoner-s-dilemma", "linkUrl": "https://www.lesswrong.com/posts/axf6Tx2eHjXvumFXD/amazing-strategy-for-split-or-steal-prisoner-s-dilemma", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amazing%20strategy%20for%20%22Split%20or%20Steal%22(prisoner's%20dilemma)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmazing%20strategy%20for%20%22Split%20or%20Steal%22(prisoner's%20dilemma)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faxf6Tx2eHjXvumFXD%2Famazing-strategy-for-split-or-steal-prisoner-s-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amazing%20strategy%20for%20%22Split%20or%20Steal%22(prisoner's%20dilemma)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faxf6Tx2eHjXvumFXD%2Famazing-strategy-for-split-or-steal-prisoner-s-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faxf6Tx2eHjXvumFXD%2Famazing-strategy-for-split-or-steal-prisoner-s-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>Amazing strategy by one of the guys(watch to see):</p>\n<p>http://www.youtube.com/watch?v=S0qjK3TWZE8</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "axf6Tx2eHjXvumFXD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -6, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "15504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T04:04:31.566Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Entangled Photons", "slug": "seq-rerun-entangled-photons", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.849Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pWZXAEy4Nd544dRYx/seq-rerun-entangled-photons", "pageUrlRelative": "/posts/pWZXAEy4Nd544dRYx/seq-rerun-entangled-photons", "linkUrl": "https://www.lesswrong.com/posts/pWZXAEy4Nd544dRYx/seq-rerun-entangled-photons", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Entangled%20Photons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Entangled%20Photons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpWZXAEy4Nd544dRYx%2Fseq-rerun-entangled-photons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Entangled%20Photons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpWZXAEy4Nd544dRYx%2Fseq-rerun-entangled-photons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpWZXAEy4Nd544dRYx%2Fseq-rerun-entangled-photons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>Today's post, <a href=\"/lw/q0/entangled_photons/\">Entangled Photons</a> was originally published on 03 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Using our newly acquired understanding of photon polarizations, we see how to construct a quantum state of two photons in which, when you measure one of them, the person in the same world as you, will always find that the opposite photon has opposite quantum state. This is not because any influence is transmitted; it is just decoherence that takes place in a very symmetrical way, as can readily be observed in our calculations.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bxw/seq_rerun_decoherence_as_projection/\">Decoherence as Projection</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pWZXAEy4Nd544dRYx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.898610595963571e-07, "legacy": true, "legacyId": "15513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GmFuZcE6udo7bykxP", "ZiJQerHQpsScCCekf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T04:28:03.516Z", "modifiedAt": null, "url": null, "title": "If calorie restriction works in humans, should we have observed it already?", "slug": "if-calorie-restriction-works-in-humans-should-we-have", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:03.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mark_Eichenlaub", "createdAt": "2010-09-01T17:59:32.486Z", "isAdmin": false, "displayName": "Mark_Eichenlaub"}, "userId": "6mdGZLDekk4835gM6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vjFtitfWa9XRgJF4Q/if-calorie-restriction-works-in-humans-should-we-have", "pageUrlRelative": "/posts/vjFtitfWa9XRgJF4Q/if-calorie-restriction-works-in-humans-should-we-have", "linkUrl": "https://www.lesswrong.com/posts/vjFtitfWa9XRgJF4Q/if-calorie-restriction-works-in-humans-should-we-have", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20calorie%20restriction%20works%20in%20humans%2C%20should%20we%20have%20observed%20it%20already%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20calorie%20restriction%20works%20in%20humans%2C%20should%20we%20have%20observed%20it%20already%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvjFtitfWa9XRgJF4Q%2Fif-calorie-restriction-works-in-humans-should-we-have%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20calorie%20restriction%20works%20in%20humans%2C%20should%20we%20have%20observed%20it%20already%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvjFtitfWa9XRgJF4Q%2Fif-calorie-restriction-works-in-humans-should-we-have", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvjFtitfWa9XRgJF4Q%2Fif-calorie-restriction-works-in-humans-should-we-have", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 225, "htmlBody": "<p>Although there are no long-term scientific studies of calorie restriction in humans, there are religious groups, cults, and ascetics who voluntarily practice calorie restriction or intermittent fasting. Presumably there have been tens or hundreds of thousands of people who have practiced calorie restriction throughout most of their adult lives. There were/are probably also groups that involuntarily practice calorie restriction - servants, slaves, prisoners, or people who simply regularly don't have enough to eat.</p>\n<p>&nbsp;</p>\n<p>If calorie restriction has a dramatic effect on life expectancy in humans, shouldn't we expect to observe extended life expectancy in at least some groups? Or would each of these groups likely have some mitigating circumstances that would shorten their lifespans, such as lack of medicine?</p>\n<p>&nbsp;</p>\n<p>With an hour on Google, I found some references to Okinawa, to monks on Mount Athos, and to similar groups. In no case was there a reasonable claim of life expectancy over 90 (which would represent just a 10% improvement over life expectancy in Japan).</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.nutritionj.com/content/10/1/107\">This paper</a>&nbsp;reviews the evidence on calorie restriction in humans and other animals, including discussion of religious fasting, but there's no evidence there of fasting extending lifespan.</p>\n<p>I found a few other sources where people asked this question (or made this point as an attack on CR), but I haven't yet found any good answers on the subject, and didn't find any discussion on LessWrong yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "t7t9nW6BtJhfGNSR6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vjFtitfWa9XRgJF4Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 32, "extendedScore": null, "score": 8.898711154482664e-07, "legacy": true, "legacyId": "15514", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T11:51:52.143Z", "modifiedAt": null, "url": null, "title": "Help come up with better meetup activity descriptions", "slug": "help-come-up-with-better-meetup-activity-descriptions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LYzGrifd36zMKtZJf/help-come-up-with-better-meetup-activity-descriptions", "pageUrlRelative": "/posts/LYzGrifd36zMKtZJf/help-come-up-with-better-meetup-activity-descriptions", "linkUrl": "https://www.lesswrong.com/posts/LYzGrifd36zMKtZJf/help-come-up-with-better-meetup-activity-descriptions", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20come%20up%20with%20better%20meetup%20activity%20descriptions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20come%20up%20with%20better%20meetup%20activity%20descriptions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYzGrifd36zMKtZJf%2Fhelp-come-up-with-better-meetup-activity-descriptions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20come%20up%20with%20better%20meetup%20activity%20descriptions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYzGrifd36zMKtZJf%2Fhelp-come-up-with-better-meetup-activity-descriptions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYzGrifd36zMKtZJf%2Fhelp-come-up-with-better-meetup-activity-descriptions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>The current version of the <a href=\"/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/\">How to Run a Successful Less Wrong Meetup</a> booklet contains descriptions about various games and activities. The problem is, some of these descriptions are quite short and don't really inspire people to try them out. I've been asked to make those descriptions sound like more fun, but for some reason I have difficulty doing so. At first, I thought it was just because I hadn't tried most of those exercises myself, and it felt dishonest to try to make something sound fun if I didn't know to what extent it actually was fun. But then I realized that I also couldn't come up with anything good for Zendo, which is a game that I've played and which I've liked. So I'm kinda stumped as to what the reason is.</p>\n<p>But if you can't solve a problem, outsource it! I'm posting some excerpts from the most boring-sounding activities in the comments below, and I'd like people to reply to those comments and come up with exciting-sounding descriptions for them. Something in the style of the <a href=\"/lw/ar2/biased_pandemic/\">Biased Pandemic writeup</a> (which I quoted liberally in the booklet) might be ideal, but other styles are cool, too.</p>\n<p>Thanks in advance!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LYzGrifd36zMKtZJf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.900608010015019e-07, "legacy": true, "legacyId": "15526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T2fcyjay3GtkvGn7F", "34jf9Z43kBHF7Axz2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T17:00:54.567Z", "modifiedAt": null, "url": null, "title": "Meetup : Small Berkeley Meetup", "slug": "meetup-small-berkeley-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thausler", "createdAt": "2010-05-24T04:40:49.214Z", "isAdmin": false, "displayName": "Thausler"}, "userId": "oPa5EPBs6ow6MY2Ao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G2vhqaDaA3WTNixBv/meetup-small-berkeley-meetup-0", "pageUrlRelative": "/posts/G2vhqaDaA3WTNixBv/meetup-small-berkeley-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/G2vhqaDaA3WTNixBv/meetup-small-berkeley-meetup-0", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20Berkeley%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20Berkeley%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG2vhqaDaA3WTNixBv%2Fmeetup-small-berkeley-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20Berkeley%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG2vhqaDaA3WTNixBv%2Fmeetup-small-berkeley-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG2vhqaDaA3WTNixBv%2Fmeetup-small-berkeley-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9f'>Small Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 April 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2071 University Avenue, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be a small meetup.  This will be the last joint dinner with the MPHD folks!  We will be meeting at Taiwan Restaurant on University Avenue.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9f'>Small Berkeley Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G2vhqaDaA3WTNixBv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.901929279584353e-07, "legacy": true, "legacyId": "15528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9f\">Small Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 April 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2071 University Avenue, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be a small meetup.  This will be the last joint dinner with the MPHD folks!  We will be meeting at Taiwan Restaurant on University Avenue.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_Berkeley_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9f\">Small Berkeley Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Small Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Small_Berkeley_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T17:50:19.153Z", "modifiedAt": null, "url": null, "title": "New x-risk organization at Cambridge University", "slug": "new-x-risk-organization-at-cambridge-university", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ewgw6iEx4ihGeJdck/new-x-risk-organization-at-cambridge-university", "pageUrlRelative": "/posts/ewgw6iEx4ihGeJdck/new-x-risk-organization-at-cambridge-university", "linkUrl": "https://www.lesswrong.com/posts/ewgw6iEx4ihGeJdck/new-x-risk-organization-at-cambridge-university", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20x-risk%20organization%20at%20Cambridge%20University&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20x-risk%20organization%20at%20Cambridge%20University%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fewgw6iEx4ihGeJdck%2Fnew-x-risk-organization-at-cambridge-university%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20x-risk%20organization%20at%20Cambridge%20University%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fewgw6iEx4ihGeJdck%2Fnew-x-risk-organization-at-cambridge-university", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fewgw6iEx4ihGeJdck%2Fnew-x-risk-organization-at-cambridge-university", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p><a href=\"http://cser.org/\">CSER</a> at Cambridge University joins <a href=\"/lw/9iy/new_xrisk_organizations/\">the others</a>.</p>\n<p>Good people involved so far, but the expected output depends hugely on who they pick to run the thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ewgw6iEx4ihGeJdck", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 8.902140557531213e-07, "legacy": true, "legacyId": "15530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GuKvMA5gBra9n4BiM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-24T20:08:55.754Z", "modifiedAt": null, "url": null, "title": "Mindfulness Meditation Thread", "slug": "mindfulness-meditation-thread", "viewCount": null, "lastCommentedAt": "2013-10-26T16:17:23.684Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fy8rArz7PbJ9EKmg8/mindfulness-meditation-thread", "pageUrlRelative": "/posts/Fy8rArz7PbJ9EKmg8/mindfulness-meditation-thread", "linkUrl": "https://www.lesswrong.com/posts/Fy8rArz7PbJ9EKmg8/mindfulness-meditation-thread", "postedAtFormatted": "Tuesday, April 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mindfulness%20Meditation%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMindfulness%20Meditation%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFy8rArz7PbJ9EKmg8%2Fmindfulness-meditation-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mindfulness%20Meditation%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFy8rArz7PbJ9EKmg8%2Fmindfulness-meditation-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFy8rArz7PbJ9EKmg8%2Fmindfulness-meditation-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>Hi everybody,</p>\n<p>There's been a bit of talk of <a href=\"http://en.wikipedia.org/wiki/Mindfulness_(psychology)\">Mindfulness</a>&nbsp;meditation around. I am curious about this, because it looks like it might be <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">practical advice</a> backed by a <a href=\"/lw/blr/attention_control_is_critical_for/\">deep theory</a>.</p>\n<p>Unfortunately, all the tutorials on mindfulness meditation seem to be semi-practical advice backed by totally bogus theories (focus your energies, blah blah). I've been able to extract some useful stuff from such articles, but I don't know what I can trust, and I still don't fully understand how it's even supposed to work.</p>\n<p>My current understanding is that you are supposed to pay attention to something and then pay attention to your attention, notice when you go off track, not judge yourself, and focus your attention back on the thing you were paying attention to. Or something.</p>\n<p>I'd like to understand the technique at least well enough to judge success. When I'm doing chin-ups, it's easy to see if I did a chin-up or not, and how many, but I don't even know what this mindfulness stuff is supposed to look like.</p>\n<p>If anyone knows more about what it's supposed to feel like, what the steps are an so on, I would really appreciate if you posted your knowledge here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fy8rArz7PbJ9EKmg8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 20, "extendedScore": null, "score": 8.902733307581945e-07, "legacy": true, "legacyId": "15531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LqjKP255fPRY7aMzw", "rD57ysqawarsbry6v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T02:31:40.654Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh Meetup: Big Gaming Fun 5!", "slug": "meetup-pittsburgh-meetup-big-gaming-fun-5", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kenoubi", "createdAt": "2011-03-12T04:07:00.560Z", "isAdmin": false, "displayName": "Kenoubi"}, "userId": "DgrXt6eQMpunHRDXh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9GfoiFdGRysFAAR5G/meetup-pittsburgh-meetup-big-gaming-fun-5", "pageUrlRelative": "/posts/9GfoiFdGRysFAAR5G/meetup-pittsburgh-meetup-big-gaming-fun-5", "linkUrl": "https://www.lesswrong.com/posts/9GfoiFdGRysFAAR5G/meetup-pittsburgh-meetup-big-gaming-fun-5", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%205!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%205!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9GfoiFdGRysFAAR5G%2Fmeetup-pittsburgh-meetup-big-gaming-fun-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%205!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9GfoiFdGRysFAAR5G%2Fmeetup-pittsburgh-meetup-big-gaming-fun-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9GfoiFdGRysFAAR5G%2Fmeetup-pittsburgh-meetup-big-gaming-fun-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9g'>Pittsburgh Meetup: Big Gaming Fun 5!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 April 2012 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>You can see my game collection <a href=\"http://boardgamegeek.com/collection/user/Kenoubi?own=1\" rel=\"nofollow\">here</a>; please bring anything else you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me. I have a cat. Please let me know if you're allergic and need me to put her upstairs. RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there.</p>\n\n<p>I intend to hold meetups every 2-3 weeks, so watch this space!  Please let me know if you'd like to run some other kind of meetup (discussion group, presentation) at my house.  I am partial to the location since I'm frequently on call and unable to go anywhere.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9g'>Pittsburgh Meetup: Big Gaming Fun 5!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9GfoiFdGRysFAAR5G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.90437044981042e-07, "legacy": true, "legacyId": "15536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_5_\">Discussion article for the meetup : <a href=\"/meetups/9g\">Pittsburgh Meetup: Big Gaming Fun 5!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 April 2012 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>You can see my game collection <a href=\"http://boardgamegeek.com/collection/user/Kenoubi?own=1\" rel=\"nofollow\">here</a>; please bring anything else you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me. I have a cat. Please let me know if you're allergic and need me to put her upstairs. RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there.</p>\n\n<p>I intend to hold meetups every 2-3 weeks, so watch this space!  Please let me know if you'd like to run some other kind of meetup (discussion group, presentation) at my house.  I am partial to the location since I'm frequently on call and unable to go anywhere.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_5_1\">Discussion article for the meetup : <a href=\"/meetups/9g\">Pittsburgh Meetup: Big Gaming Fun 5!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 5!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_5_", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 5!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_5_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T02:44:50.716Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Mind-Hacking Planning Session", "slug": "meetup-west-la-meetup-mind-hacking-planning-session", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wr9yyTRozCotjKynM/meetup-west-la-meetup-mind-hacking-planning-session", "pageUrlRelative": "/posts/wr9yyTRozCotjKynM/meetup-west-la-meetup-mind-hacking-planning-session", "linkUrl": "https://www.lesswrong.com/posts/wr9yyTRozCotjKynM/meetup-west-la-meetup-mind-hacking-planning-session", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Mind-Hacking%20Planning%20Session&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Mind-Hacking%20Planning%20Session%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwr9yyTRozCotjKynM%2Fmeetup-west-la-meetup-mind-hacking-planning-session%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Mind-Hacking%20Planning%20Session%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwr9yyTRozCotjKynM%2Fmeetup-west-la-meetup-mind-hacking-planning-session", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwr9yyTRozCotjKynM%2Fmeetup-west-la-meetup-mind-hacking-planning-session", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9h'>West LA Meetup - Mind-Hacking Planning Session</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 April 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, April 11th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Activity:</strong> We are going to discuss and plan out our approach to future experiments with habits, attention, memory, and various heuristics. One of the key things we have to figure out is how to do measurements, accounting for confirmation bias, etc.</p>\n\n<p>We may focus on a specific sort of experiment, such as building skills modulating <a href=\"http://lesswrong.com/lw/blr/attention_control_is_critical_for/\">mindfulness</a>, evaluating techniques for <a href=\"http://en.wikipedia.org/wiki/Fermi_problem\" rel=\"nofollow\">fermi problems</a>, building <a href=\"http://lesswrong.com/lw/60y/action_and_habit/\">habits</a>, or other experiments training us to <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">be more strategic</a> and rational at a <a href=\"http://lesswrong.com/lw/7e5/the_cognitive_science_of_rationality/\">System 1</a> level.</p>\n\n<p>Or we could play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28party_game%29\" rel=\"nofollow\">The Resistance</a>.</p>\n\n<p>Don't worry if you don't have time to read any links, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9h'>West LA Meetup - Mind-Hacking Planning Session</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wr9yyTRozCotjKynM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.9044267817438e-07, "legacy": true, "legacyId": "15537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Mind_Hacking_Planning_Session\">Discussion article for the meetup : <a href=\"/meetups/9h\">West LA Meetup - Mind-Hacking Planning Session</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 April 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, April 11th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Activity:</strong> We are going to discuss and plan out our approach to future experiments with habits, attention, memory, and various heuristics. One of the key things we have to figure out is how to do measurements, accounting for confirmation bias, etc.</p>\n\n<p>We may focus on a specific sort of experiment, such as building skills modulating <a href=\"http://lesswrong.com/lw/blr/attention_control_is_critical_for/\">mindfulness</a>, evaluating techniques for <a href=\"http://en.wikipedia.org/wiki/Fermi_problem\" rel=\"nofollow\">fermi problems</a>, building <a href=\"http://lesswrong.com/lw/60y/action_and_habit/\">habits</a>, or other experiments training us to <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">be more strategic</a> and rational at a <a href=\"http://lesswrong.com/lw/7e5/the_cognitive_science_of_rationality/\">System 1</a> level.</p>\n\n<p>Or we could play <a href=\"http://en.wikipedia.org/wiki/The_Resistance_%28party_game%29\" rel=\"nofollow\">The Resistance</a>.</p>\n\n<p>Don't worry if you don't have time to read any links, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Mind_Hacking_Planning_Session1\">Discussion article for the meetup : <a href=\"/meetups/9h\">West LA Meetup - Mind-Hacking Planning Session</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Mind-Hacking Planning Session", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Mind_Hacking_Planning_Session", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Mind-Hacking Planning Session", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Mind_Hacking_Planning_Session1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rD57ysqawarsbry6v", "XNhfw5Bqsi4SGNNBk", "PBRWb2Em5SNeWYwwB", "xLm9mgJRPvmPGpo7Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T04:37:20.675Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Bell's Theorem: No EPR \"Reality\"", "slug": "seq-rerun-bell-s-theorem-no-epr-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:11.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v5QqnPMQLa4ufQtAq/seq-rerun-bell-s-theorem-no-epr-reality", "pageUrlRelative": "/posts/v5QqnPMQLa4ufQtAq/seq-rerun-bell-s-theorem-no-epr-reality", "linkUrl": "https://www.lesswrong.com/posts/v5QqnPMQLa4ufQtAq/seq-rerun-bell-s-theorem-no-epr-reality", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Bell's%20Theorem%3A%20No%20EPR%20%22Reality%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Bell's%20Theorem%3A%20No%20EPR%20%22Reality%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv5QqnPMQLa4ufQtAq%2Fseq-rerun-bell-s-theorem-no-epr-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Bell's%20Theorem%3A%20No%20EPR%20%22Reality%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv5QqnPMQLa4ufQtAq%2Fseq-rerun-bell-s-theorem-no-epr-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv5QqnPMQLa4ufQtAq%2Fseq-rerun-bell-s-theorem-no-epr-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 296, "htmlBody": "<p>Today's post, <a href=\"/lw/q1/bells_theorem_no_epr_reality/\">Bell's Theorem: No EPR \"Reality\"</a> was originally published on 04 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>(Note: This post was designed to be read as a stand-alone, if desired.) Originally, the discoverers of quantum physics thought they had discovered an incomplete description of reality - that there was some deeper physical process they were missing, and this was why they couldn't predict exactly the results of quantum experiments. The math of Bell's Theorem is surprisingly simple, and we walk through it. Bell's Theorem rules out being able to <em>locally </em>predict a <em>single</em>, <em>unique </em>outcome of measurements - ruling out a way that Einstein, Podolsky, and Rosen once defined \"reality\". This shows how deep implicit philosophical assumptions can go. If worlds can split, so that there is no single unique outcome, then Bell's Theorem is no problem. Bell's Theorem does, however, rule out the idea that quantum physics describes our partial knowledge of a deeper physical state that could locally produce single outcomes - any such description will be inconsistent.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/byx/seq_rerun_entangled_photons/\">Entangled Photons</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v5QqnPMQLa4ufQtAq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.904908083952284e-07, "legacy": true, "legacyId": "15548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AnHJX42C6r6deohTG", "pWZXAEy4Nd544dRYx", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T06:20:13.789Z", "modifiedAt": null, "url": null, "title": "Crowdsourcing the availability heuristic", "slug": "crowdsourcing-the-availability-heuristic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kalla724", "createdAt": "2011-06-27T19:28:41.092Z", "isAdmin": false, "displayName": "kalla724"}, "userId": "gaBwWRM7cb7KP6fia", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/izDw47NEoko6sveSJ/crowdsourcing-the-availability-heuristic", "pageUrlRelative": "/posts/izDw47NEoko6sveSJ/crowdsourcing-the-availability-heuristic", "linkUrl": "https://www.lesswrong.com/posts/izDw47NEoko6sveSJ/crowdsourcing-the-availability-heuristic", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Crowdsourcing%20the%20availability%20heuristic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACrowdsourcing%20the%20availability%20heuristic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FizDw47NEoko6sveSJ%2Fcrowdsourcing-the-availability-heuristic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Crowdsourcing%20the%20availability%20heuristic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FizDw47NEoko6sveSJ%2Fcrowdsourcing-the-availability-heuristic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FizDw47NEoko6sveSJ%2Fcrowdsourcing-the-availability-heuristic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2092, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There are goals which can be achieved only by personal exertion and hard work &ndash; finishing a university degree, learning a language, mastering a martial art&hellip; But there is also a plethora of smaller goals, where small differences in approach and resources can make a huge difference. I&rsquo;m going to examine how one particular cognitive bias affects execution of small-to-midrange goals, why this bias cannot be realistically overcome on a personal level, and how it can be effectively short-circuited simply by involving other minds.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Do note: <span>&nbsp;</span>the point I&rsquo;m making may seem obvious; in my personal experience, and from observation, it is one of those things that are obvious once you know the answer (and one still needs occasional reminders). The solution to the problem is high-impact and available to practically everyone, but remains vastly underused.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">The bias in question is <em>availability heuristic</em>. LW has a <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">decent definition:</a></p>\n<p class=\"MsoNormal\" style=\"margin-left: 0.5in; text-align: justify;\">The availability heuristic judges the probability of events by the ease with which examples come to mind. Sometimes this heuristic serves us well, but the map is not the territory; the frequency with which concepts occur in your thoughts need not reflect the frequency with which they occur in reality. Undue salience, selective reporting, even subtle features of how the human brain stores and recalls memories can distort our perceptions about the probability of events. Because it's easier to recall words by their first letter, people judge words that begin with the letter r to be more frequent than words with r as their third letter, even though in fact, the latter is more frequent. Or selective reporting by the media of dramatic tragedies makes them seem more frequent than more threatening albeit mundane risks.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">This topic has been talked about <a href=\"/lw/6lx/rationalist_judo_or_using_the_availability/\">many</a> <a href=\"/lw/173/knowing_what_you_know/\">times</a> on LW, and there is a great deal of academic research as well [1], including seminal texts many here will be familiar with [2]. It's all interesting, but there are two critical points I want to pull up to the forefront.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">I - availability heuristic is commonly treated as a simple perceptual bias. In actuality, it is also a <em>choice bias - </em>if you fail to perceive an option, you cannot choose to pursue it. If you do pursue an option, you are likely going to focus on attaining it with cognitively available resources, while missing much better resources that are sitting idle. Similarly, you may waste resources (sometimes to the point of simply giving up) while pursuing a sub-optimal but cognitively available path towards your goal &ndash; completely oblivious to much easier roads which are actually available to you.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">II - availability heuristic is not really \"curable.\" Sure, it&rsquo;s good to be aware of it. But we all have limited information about the world. Even if we objectively write down all known relevant factors for some observation or decision, our sample is still going to be at least somewhat biased, and certainly very narrow. Outside our direct areas of expertise, the amount of information we can include into any decision is quite limited; and the number of items our working memory holds while the decision is made is limited yet further.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">One obvious mitigation strategy is probably apparent: simply research your desired goal by consulting experts (or the Googlian Oracle). How did other people achieve the goal? What preparations did they undertake? What strategies are recommended?</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Good idea, which comes with several limitations.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Much of the information out there is written by people who are unaware of the existence of availability heuristic. The recommendations, however refined, are still usually descriptions of single strategies which happened to work optimally for the person who wrote the article. They could work (and often will), but they may not be the optimal solution to your particular problem set. Furthermore, as I will illustrate soon, the most common strategies you will find are the ones <em>most cognitively available</em> to the most people; an Internet search will, in effect, <em>potentiate</em> the availability heuristic even further, hiding less obvious strategies even further.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">But by far the most significant limitation comes from the fact that knowledge does not equal resources. Even if you research an optimal strategy, you may still be unaware of the full scope of resources that are available to you. To avoid abstraction, let&rsquo;s take a specific example.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><strong>Example: crowdsourcing adventure opportunities</strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A friend of mine has an interesting strategy for increasing the overall awesomeness of her life. Every January 1st, she comes up with a general rule, which she then follows until the end of the year. The rule is modified by common sense (you don't follow it if it will get you into an extraordinarily dangerous situation, or if following it is otherwise prohibitively expensive), but other than that, it <em>has</em> to be followed.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">This year, the rule is \"when you think or hear of something that causes you to be afraid, go ahead and do it.\" She's afraid of heights, so the obvious \"go skydiving\" is on the list from the start. But then, someone mentions flying in an acrobatic aircraft. That gets added to the list. I'm sure you see the general principle.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Seems a bit cheesy, at first glance. But... within the first four months of this year, she <a href=\"http://myyearoffear.wordpress.com/2012/03/30/the-knack-to-flying-part-the-first/\">went</a> <a href=\"http://myyearoffear.wordpress.com/2012/03/31/the-knack-to-flying-or-im-on-the-jazz-part-the-second/\">flying</a> in the aforementioned acrobatic aircraft (and a <a href=\"http://myyearoffear.wordpress.com/2012/03/29/anti-climb-actic-or-my-patronus-is-a-helicopter/\">helicopter</a>), learned <a href=\"http://myyearoffear.wordpress.com/2012/02/10/skiing-pt-1-i-am-lord-empress-of-all-snow/\">how to</a> <a href=\"http://myyearoffear.wordpress.com/2012/02/12/skiing-pt-2-revenge-of-the-snow/\">ski</a>, and even <em><a href=\"http://myyearoffear.wordpress.com/2012/04/08/tryin-to-catch-me-ridin-birdies-pt-1/\">rode</a> a <a href=\"http://myyearoffear.wordpress.com/2012/04/09/tryin-to-catch-me-ridin-birdies-pt-2-of-now-3-because-i-didnt-realize-this-would-get-so-long/\">damn</a> <a href=\"http://myyearoffear.wordpress.com/2012/04/20/tryin-to-catch-me-ridin-birdies-fin/\">ostrich</a></em>. All within <em>four months.</em> She went caving this past weekend. Hang-gliding is firmly scheduled in a few months. And there are other, <a href=\"http://myyearoffear.wordpress.com/2012/01/24/this-story-involves-my-butt-you-will-receive-no-further-warnings/\">less glamorous</a> experiences as well, but it's quite a list.[3, 4]</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Now, let's say I wanted to do one of those things &ndash; say, an acrobatic airplane ride. What comes to mind? I look up places that offer such rides. I would need to travel there (and given the distances involved, get a hotel room as well). Pay the fee. Get to spend about 15-20 minutes being flown around. It's a lot of effort, and the payoff doesn't seem really worth it. Hey, let&rsquo;s see what other people did! Google, google&hellip; a bunch of testimonials about people having a great/awful time taking the aforementioned touristy rides (most significant finding: corkscrews often cause explosive nausea). So I give up.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">How did my friend do it? She talks to people about it; asks them for ideas. And soon enough, someone says \"yeah, I know a guy who owns an acrobatic plane, wanna ride with him?\" And lo and behold, a free ride of much higher quality than touristy nonsense one pays for, plus it's very close to home.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">My approach above is the cognitively available one. I'm proceeding towards the goal in accordance with the patterns I've followed previously, when achieving similar goals. I'm thinking about resources that are available to me, personally (my money, my time, etc.). I end up with a suboptimal plan.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Her approach is to crowdsource: throw the desire into the world, and see what others come up with. Many people, with many different resources, ideas, and further links to even more people out there.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Once I started thinking about this, I decided to test this concept. I tried throwing out the acrobatic ride idea to my friends - and lo and behold, a friend of a friend of a friend is going to be flying in this summer. In his acrobatic aircraft. And now, when he gets here, I'm likely to get an hour or so of riding time with him, for free. Just because I asked.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">It's a somewhat silly example, of course, but I think it illustrates the point. This friend (of a friend)<sup>2</sup> is a <em>resource. </em>I was unaware I had this resource, until I asked for it. Finding an acrobatic aircraft owner through personal connections is a strategy that would have never occurred to me (since my availability heuristic informs me that such people are exceedingly rare).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Does this still seem obvious? Many articles were written about the &ldquo;breakthrough&rdquo; design of the <a href=\"http://commercialtenantresource.wordpress.com/2012/01/02/steve-jobs-and-workplace-design/\">Apple headquarters</a> &ndash; a building made to force people together, to produce conversations between workers in different areas, and interactions between people who think in very different ways. In MIT lore, legends are written about <a href=\"http://en.wikipedia.org/wiki/Building_20\">Building 20</a>, a &ldquo;magical incubator&rdquo; that has produced an incredible amount of breakthrough technologies and world-class thinkers. One of the main reasons given for this productivity is that many disparate small groups of researchers in a wide range of areas were thrown together in a small space &ndash; where they had to interact and talk to each other.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">In other words, there is a certain kind of &ldquo;magic&rdquo;&hellip; in places that force people to simply utilize each other&rsquo;s cognitive resources, to seek out different ways of thinking, and to avoid falling into cognitively available approaches to the problems they are trying to solve. And the point I&rsquo;m trying to make is that one doesn&rsquo;t need to work in Building 20 &ndash; just to intentionally maximize the utilization of their own social network (and work on diversifying it as much as possible).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There are people who already utilize their social network to the utmost, and who expand it strategically, adding people just to enhance the diversity of available viewpoints. But I will take a chance, and state that most of us probably don&rsquo;t. And as a result, we aren&rsquo;t able to recognize all of the resources available to us, to optimally use those we do recognize, or to realize optimal strategies for approaching our goals. Chances are that most of us could improve the strategy and execution of <em>any</em> given midrange goal &ndash; simply by asking around.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">_____________________________</p>\n<p><strong>(EDIT) Addendum: help-seeking, status, etc.</strong></p>\n<p>There is a bit of discussion in the comments regarding some important questions - how and when does seeking help affect status within the group, would seeking help on a regular basis cause people to become uninterested in helping, etc. These may become a basis for a different text in the near future.</p>\n<p>But these questions miss an important point here. Sure, asking for help can be a part of the strategy I discuss above, in some cases. In most cases, however, <em>you should not be seeking help.</em> The point of the article here is to simply <em>talk</em> about your goal and your strategy with others. \"Involving other minds\" does not necessarily require them to take an active helper role in the achievement of your goal.</p>\n<p>In the specific example given in the text, I didn't go around saying \"hey, I'm looking for help in finding an acrobatic airplane ride.\" Instead, I would say something like \"Riding an acrobatic plane seems like an interesting thing; I'm trying to look into finding an opportunity to do so in the near future.\" Thoughts, offers and the eventual connection grew organically from the discussions that followed. Sure, the pilot himself is going to be doing me a favor (which I'll eventually repay), but the people who made the connection for me were just having a conversation.</p>\n<p>To illustrate further on an example that popped up in the comments: a CEO of a company that always asks for help in making decisions will rapidly lose status (and therefore become ineffective at his or her job). This much is true. But most effective CEOs will organize their companies so that people of varying backgrounds will have to talk (at some point or other) about current company projects and strategies. The CEO doesn't ask for help in making the decision: she <em>requires</em> that her underlings produce ideas and overviews, which then become a basis for making optimal decisions.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">_____________________________</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">References:</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span>&nbsp;</span>[1] A few recent examples: Hayibor, S., Wasieleski, D.M. (2009). \"Effects of the use of availability\" Journal of Business Ethics 84: 151&ndash;165. Also, Klinger, D., Kudryavtsev, A. (2010). \"The availability heuristic and investors' reactions to company-specific events\" The Journal of Behavioral Finance 11 (50-65).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[2] Tversky, A., Kahneman, D. (1973) \"Availability: A heuristic for judging frequency and probability\" Cognitive Psychology 5 (1): 207&ndash;233.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[3] She's blogging her progress through this year, and <a href=\"http://myyearoffear.wordpress.com/\">the whole thing</a> is highly recommended; for sheer hilarity as much as for some very interesting insights.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[4] One could write an excellent text on goal-setting strategies around this example. The rule is simple and absolute (\"never have any chips, cookies or other snacks available at home\" is easier to follow and will in most cases lead to a greater weight loss than an intricate diet), and it is overarching (applies everywhere in life, not only to some particular times and places, making lawyering around the rule much more difficult). If you are going to set rules for yourself, this is the way to do it. But since I'm writing a loose set of texts on a completely different topic, this footnote will be all I have to say on that topic.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "Ng8Gice9KNkncxqcj": 1, "zv7v2ziqexSn5iS9v": 1, "5f5c37ee1b5cdee568cfb19c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "izDw47NEoko6sveSJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 43, "extendedScore": null, "score": 0.000157, "legacy": true, "legacyId": "15550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There are goals which can be achieved only by personal exertion and hard work \u2013 finishing a university degree, learning a language, mastering a martial art\u2026 But there is also a plethora of smaller goals, where small differences in approach and resources can make a huge difference. I\u2019m going to examine how one particular cognitive bias affects execution of small-to-midrange goals, why this bias cannot be realistically overcome on a personal level, and how it can be effectively short-circuited simply by involving other minds.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Do note: <span>&nbsp;</span>the point I\u2019m making may seem obvious; in my personal experience, and from observation, it is one of those things that are obvious once you know the answer (and one still needs occasional reminders). The solution to the problem is high-impact and available to practically everyone, but remains vastly underused.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">The bias in question is <em>availability heuristic</em>. LW has a <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">decent definition:</a></p>\n<p class=\"MsoNormal\" style=\"margin-left: 0.5in; text-align: justify;\">The availability heuristic judges the probability of events by the ease with which examples come to mind. Sometimes this heuristic serves us well, but the map is not the territory; the frequency with which concepts occur in your thoughts need not reflect the frequency with which they occur in reality. Undue salience, selective reporting, even subtle features of how the human brain stores and recalls memories can distort our perceptions about the probability of events. Because it's easier to recall words by their first letter, people judge words that begin with the letter r to be more frequent than words with r as their third letter, even though in fact, the latter is more frequent. Or selective reporting by the media of dramatic tragedies makes them seem more frequent than more threatening albeit mundane risks.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">This topic has been talked about <a href=\"/lw/6lx/rationalist_judo_or_using_the_availability/\">many</a> <a href=\"/lw/173/knowing_what_you_know/\">times</a> on LW, and there is a great deal of academic research as well [1], including seminal texts many here will be familiar with [2]. It's all interesting, but there are two critical points I want to pull up to the forefront.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">I - availability heuristic is commonly treated as a simple perceptual bias. In actuality, it is also a <em>choice bias - </em>if you fail to perceive an option, you cannot choose to pursue it. If you do pursue an option, you are likely going to focus on attaining it with cognitively available resources, while missing much better resources that are sitting idle. Similarly, you may waste resources (sometimes to the point of simply giving up) while pursuing a sub-optimal but cognitively available path towards your goal \u2013 completely oblivious to much easier roads which are actually available to you.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">II - availability heuristic is not really \"curable.\" Sure, it\u2019s good to be aware of it. But we all have limited information about the world. Even if we objectively write down all known relevant factors for some observation or decision, our sample is still going to be at least somewhat biased, and certainly very narrow. Outside our direct areas of expertise, the amount of information we can include into any decision is quite limited; and the number of items our working memory holds while the decision is made is limited yet further.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">One obvious mitigation strategy is probably apparent: simply research your desired goal by consulting experts (or the Googlian Oracle). How did other people achieve the goal? What preparations did they undertake? What strategies are recommended?</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Good idea, which comes with several limitations.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Much of the information out there is written by people who are unaware of the existence of availability heuristic. The recommendations, however refined, are still usually descriptions of single strategies which happened to work optimally for the person who wrote the article. They could work (and often will), but they may not be the optimal solution to your particular problem set. Furthermore, as I will illustrate soon, the most common strategies you will find are the ones <em>most cognitively available</em> to the most people; an Internet search will, in effect, <em>potentiate</em> the availability heuristic even further, hiding less obvious strategies even further.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">But by far the most significant limitation comes from the fact that knowledge does not equal resources. Even if you research an optimal strategy, you may still be unaware of the full scope of resources that are available to you. To avoid abstraction, let\u2019s take a specific example.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><strong id=\"Example__crowdsourcing_adventure_opportunities\">Example: crowdsourcing adventure opportunities</strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">A friend of mine has an interesting strategy for increasing the overall awesomeness of her life. Every January 1st, she comes up with a general rule, which she then follows until the end of the year. The rule is modified by common sense (you don't follow it if it will get you into an extraordinarily dangerous situation, or if following it is otherwise prohibitively expensive), but other than that, it <em>has</em> to be followed.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">This year, the rule is \"when you think or hear of something that causes you to be afraid, go ahead and do it.\" She's afraid of heights, so the obvious \"go skydiving\" is on the list from the start. But then, someone mentions flying in an acrobatic aircraft. That gets added to the list. I'm sure you see the general principle.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Seems a bit cheesy, at first glance. But... within the first four months of this year, she <a href=\"http://myyearoffear.wordpress.com/2012/03/30/the-knack-to-flying-part-the-first/\">went</a> <a href=\"http://myyearoffear.wordpress.com/2012/03/31/the-knack-to-flying-or-im-on-the-jazz-part-the-second/\">flying</a> in the aforementioned acrobatic aircraft (and a <a href=\"http://myyearoffear.wordpress.com/2012/03/29/anti-climb-actic-or-my-patronus-is-a-helicopter/\">helicopter</a>), learned <a href=\"http://myyearoffear.wordpress.com/2012/02/10/skiing-pt-1-i-am-lord-empress-of-all-snow/\">how to</a> <a href=\"http://myyearoffear.wordpress.com/2012/02/12/skiing-pt-2-revenge-of-the-snow/\">ski</a>, and even <em><a href=\"http://myyearoffear.wordpress.com/2012/04/08/tryin-to-catch-me-ridin-birdies-pt-1/\">rode</a> a <a href=\"http://myyearoffear.wordpress.com/2012/04/09/tryin-to-catch-me-ridin-birdies-pt-2-of-now-3-because-i-didnt-realize-this-would-get-so-long/\">damn</a> <a href=\"http://myyearoffear.wordpress.com/2012/04/20/tryin-to-catch-me-ridin-birdies-fin/\">ostrich</a></em>. All within <em>four months.</em> She went caving this past weekend. Hang-gliding is firmly scheduled in a few months. And there are other, <a href=\"http://myyearoffear.wordpress.com/2012/01/24/this-story-involves-my-butt-you-will-receive-no-further-warnings/\">less glamorous</a> experiences as well, but it's quite a list.[3, 4]</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Now, let's say I wanted to do one of those things \u2013 say, an acrobatic airplane ride. What comes to mind? I look up places that offer such rides. I would need to travel there (and given the distances involved, get a hotel room as well). Pay the fee. Get to spend about 15-20 minutes being flown around. It's a lot of effort, and the payoff doesn't seem really worth it. Hey, let\u2019s see what other people did! Google, google\u2026 a bunch of testimonials about people having a great/awful time taking the aforementioned touristy rides (most significant finding: corkscrews often cause explosive nausea). So I give up.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">How did my friend do it? She talks to people about it; asks them for ideas. And soon enough, someone says \"yeah, I know a guy who owns an acrobatic plane, wanna ride with him?\" And lo and behold, a free ride of much higher quality than touristy nonsense one pays for, plus it's very close to home.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">My approach above is the cognitively available one. I'm proceeding towards the goal in accordance with the patterns I've followed previously, when achieving similar goals. I'm thinking about resources that are available to me, personally (my money, my time, etc.). I end up with a suboptimal plan.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Her approach is to crowdsource: throw the desire into the world, and see what others come up with. Many people, with many different resources, ideas, and further links to even more people out there.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Once I started thinking about this, I decided to test this concept. I tried throwing out the acrobatic ride idea to my friends - and lo and behold, a friend of a friend of a friend is going to be flying in this summer. In his acrobatic aircraft. And now, when he gets here, I'm likely to get an hour or so of riding time with him, for free. Just because I asked.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">It's a somewhat silly example, of course, but I think it illustrates the point. This friend (of a friend)<sup>2</sup> is a <em>resource. </em>I was unaware I had this resource, until I asked for it. Finding an acrobatic aircraft owner through personal connections is a strategy that would have never occurred to me (since my availability heuristic informs me that such people are exceedingly rare).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">Does this still seem obvious? Many articles were written about the \u201cbreakthrough\u201d design of the <a href=\"http://commercialtenantresource.wordpress.com/2012/01/02/steve-jobs-and-workplace-design/\">Apple headquarters</a> \u2013 a building made to force people together, to produce conversations between workers in different areas, and interactions between people who think in very different ways. In MIT lore, legends are written about <a href=\"http://en.wikipedia.org/wiki/Building_20\">Building 20</a>, a \u201cmagical incubator\u201d that has produced an incredible amount of breakthrough technologies and world-class thinkers. One of the main reasons given for this productivity is that many disparate small groups of researchers in a wide range of areas were thrown together in a small space \u2013 where they had to interact and talk to each other.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">In other words, there is a certain kind of \u201cmagic\u201d\u2026 in places that force people to simply utilize each other\u2019s cognitive resources, to seek out different ways of thinking, and to avoid falling into cognitively available approaches to the problems they are trying to solve. And the point I\u2019m trying to make is that one doesn\u2019t need to work in Building 20 \u2013 just to intentionally maximize the utilization of their own social network (and work on diversifying it as much as possible).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">There are people who already utilize their social network to the utmost, and who expand it strategically, adding people just to enhance the diversity of available viewpoints. But I will take a chance, and state that most of us probably don\u2019t. And as a result, we aren\u2019t able to recognize all of the resources available to us, to optimally use those we do recognize, or to realize optimal strategies for approaching our goals. Chances are that most of us could improve the strategy and execution of <em>any</em> given midrange goal \u2013 simply by asking around.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">_____________________________</p>\n<p><strong id=\"_EDIT__Addendum__help_seeking__status__etc_\">(EDIT) Addendum: help-seeking, status, etc.</strong></p>\n<p>There is a bit of discussion in the comments regarding some important questions - how and when does seeking help affect status within the group, would seeking help on a regular basis cause people to become uninterested in helping, etc. These may become a basis for a different text in the near future.</p>\n<p>But these questions miss an important point here. Sure, asking for help can be a part of the strategy I discuss above, in some cases. In most cases, however, <em>you should not be seeking help.</em> The point of the article here is to simply <em>talk</em> about your goal and your strategy with others. \"Involving other minds\" does not necessarily require them to take an active helper role in the achievement of your goal.</p>\n<p>In the specific example given in the text, I didn't go around saying \"hey, I'm looking for help in finding an acrobatic airplane ride.\" Instead, I would say something like \"Riding an acrobatic plane seems like an interesting thing; I'm trying to look into finding an opportunity to do so in the near future.\" Thoughts, offers and the eventual connection grew organically from the discussions that followed. Sure, the pilot himself is going to be doing me a favor (which I'll eventually repay), but the people who made the connection for me were just having a conversation.</p>\n<p>To illustrate further on an example that popped up in the comments: a CEO of a company that always asks for help in making decisions will rapidly lose status (and therefore become ineffective at his or her job). This much is true. But most effective CEOs will organize their companies so that people of varying backgrounds will have to talk (at some point or other) about current company projects and strategies. The CEO doesn't ask for help in making the decision: she <em>requires</em> that her underlings produce ideas and overviews, which then become a basis for making optimal decisions.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">_____________________________</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">References:</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span>&nbsp;</span>[1] A few recent examples: Hayibor, S., Wasieleski, D.M. (2009). \"Effects of the use of availability\" Journal of Business Ethics 84: 151\u2013165. Also, Klinger, D., Kudryavtsev, A. (2010). \"The availability heuristic and investors' reactions to company-specific events\" The Journal of Behavioral Finance 11 (50-65).</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[2] Tversky, A., Kahneman, D. (1973) \"Availability: A heuristic for judging frequency and probability\" Cognitive Psychology 5 (1): 207\u2013233.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[3] She's blogging her progress through this year, and <a href=\"http://myyearoffear.wordpress.com/\">the whole thing</a> is highly recommended; for sheer hilarity as much as for some very interesting insights.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">[4] One could write an excellent text on goal-setting strategies around this example. The rule is simple and absolute (\"never have any chips, cookies or other snacks available at home\" is easier to follow and will in most cases lead to a greater weight loss than an intricate diet), and it is overarching (applies everywhere in life, not only to some particular times and places, making lawyering around the rule much more difficult). If you are going to set rules for yourself, this is the way to do it. But since I'm writing a loose set of texts on a completely different topic, this footnote will be all I have to say on that topic.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>", "sections": [{"title": "Example: crowdsourcing adventure opportunities", "anchor": "Example__crowdsourcing_adventure_opportunities", "level": 1}, {"title": "(EDIT) Addendum: help-seeking, status, etc.", "anchor": "_EDIT__Addendum__help_seeking__status__etc_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "40 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ztEcQtZsjXnqLc3xB", "2r7kp9QSNNkF2Lpd7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T12:12:05.893Z", "modifiedAt": null, "url": null, "title": "A Kick in the Rationals: What hurts you in your LessWrong Parts?", "slug": "a-kick-in-the-rationals-what-hurts-you-in-your-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NbekxJavfKs8QpwzM/a-kick-in-the-rationals-what-hurts-you-in-your-lesswrong", "pageUrlRelative": "/posts/NbekxJavfKs8QpwzM/a-kick-in-the-rationals-what-hurts-you-in-your-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/NbekxJavfKs8QpwzM/a-kick-in-the-rationals-what-hurts-you-in-your-lesswrong", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Kick%20in%20the%20Rationals%3A%20What%20hurts%20you%20in%20your%20LessWrong%20Parts%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Kick%20in%20the%20Rationals%3A%20What%20hurts%20you%20in%20your%20LessWrong%20Parts%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbekxJavfKs8QpwzM%2Fa-kick-in-the-rationals-what-hurts-you-in-your-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Kick%20in%20the%20Rationals%3A%20What%20hurts%20you%20in%20your%20LessWrong%20Parts%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbekxJavfKs8QpwzM%2Fa-kick-in-the-rationals-what-hurts-you-in-your-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbekxJavfKs8QpwzM%2Fa-kick-in-the-rationals-what-hurts-you-in-your-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 345, "htmlBody": "<p>A month or so ago I stumbled across <a href=\"http://www.psychologytoday.com/blog/biocentrism/201203/the-most-astounding-thing-in-the-universe\">this</a>. &nbsp;It's a blog piece by one Robert Lanza M.D., a legitimate, respected biologist who has made important contributions to tissue engineering, cloning and stem cell research. &nbsp;In his spare time, he is a crackpot.</p>\n<p>I know I shouldn't give any of my time to an online pop-psychology magazine which has \"Find a Therapist\" as the second option on its navigation bar, but the piece in question could have been *designed* to antagonise a LessWrong reader: horrible misapplication of quantum physics, <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">worshipful treatment of the mysterious</a>, <a href=\"/lw/np/disputing_definitions/\">making a big deal over easily dissolvable questions</a>, bold and unsubstantiated claims about physics and consciousness... the list goes on. &nbsp;I'm generally past the point in my life where ranting at people who are wrong on the internet holds any appeal, but this particular item got my goat to the point where I had to go and get my goat back.</p>\n<p>If reading LW all these years has done anything, it's trained me to take apart that post without even thinking, so (and I'm not proud of this), I wrote a short seven-point response in the comments lucidly explaining its most obvious problems, and signed it Summer Glau. &nbsp;It got removed, and I learned a valuable lesson about productively channeling my anger.</p>\n<p>But this started me thinking about how certain things (either subjects or people) antagonise what I now think of as my LessWrong Parts, or more generally cause me distress on an epistemic level, and what my subjective experience of that distress is like so I can recognise and deal with it in future.</p>\n<p>I've seen a few other people make comments describing this kind of distress, (<a href=\"/lw/9g1/the_problem_with_too_many_rational_memes/5oum\">this</a> description of \"being forced to use your nicely sharpened tools on a task that would destroy them\" seems particularly accurate). &nbsp;Common culprits seem to be critical theory, postmodernism and bad philosophy. &nbsp;I've also noticed some <em>people</em> distress me in this fashion, in a way I'm still struggling to characterise.</p>\n<p>Who else has this experience? &nbsp;Do you have any choice examples? &nbsp;What hurts you in your LessWrong Parts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NbekxJavfKs8QpwzM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 36, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "15560", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 194, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6i3zToomS86oj9bS6", "7X2j8HAkWdmMoS8PE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T13:21:29.161Z", "modifiedAt": null, "url": null, "title": "Another cooperative rationality exercise", "slug": "another-cooperative-rationality-exercise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.361Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s9ZDnzMHo4kAoWi4r/another-cooperative-rationality-exercise", "pageUrlRelative": "/posts/s9ZDnzMHo4kAoWi4r/another-cooperative-rationality-exercise", "linkUrl": "https://www.lesswrong.com/posts/s9ZDnzMHo4kAoWi4r/another-cooperative-rationality-exercise", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20cooperative%20rationality%20exercise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20cooperative%20rationality%20exercise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9ZDnzMHo4kAoWi4r%2Fanother-cooperative-rationality-exercise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20cooperative%20rationality%20exercise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9ZDnzMHo4kAoWi4r%2Fanother-cooperative-rationality-exercise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9ZDnzMHo4kAoWi4r%2Fanother-cooperative-rationality-exercise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>I don't know how well this is going to work, but I mention it here because it's actually going to be done in a few weeks time at a day-long meeting of the research group that I work with. (Not my idea. I don't know which of us thought it up.)</p>\n<blockquote>\n<p>Keyword game: explaining a scientific term. Everyone puts a keyword used in their project (for example, \"Selective Sweep\") into a hat. For each keyword in turn, get someone who does not understand the keyword to explain what they think it might mean. &nbsp;They can then be enlightened by the people who know (of which there should be at least one!).</p>\n<p>This is to be done in groups of four, and afterwards, the groups reassemble and each group presents its newly understood keyword meanings to the main group.</p>\n</blockquote>\n<div>There are twenty people altogether.</div>\n<div><span>Trying to guess what e.g.&nbsp;\"</span>Selective Sweep<span>\" is just from the words doesn't seem very sensible to me, but in practice I expect the result to be more of a conversation between the one on the spot and those who actually know. How do you know when you've grasped an idea that someone is explaining to you, and when you have not?</span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s9ZDnzMHo4kAoWi4r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 8.90715111474595e-07, "legacy": true, "legacyId": "15561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T14:15:31.904Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong sydney - Prediction gambles and location testing", "slug": "meetup-less-wrong-sydney-prediction-gambles-and-location", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K3R5c7Sm4yeaDonXR/meetup-less-wrong-sydney-prediction-gambles-and-location", "pageUrlRelative": "/posts/K3R5c7Sm4yeaDonXR/meetup-less-wrong-sydney-prediction-gambles-and-location", "linkUrl": "https://www.lesswrong.com/posts/K3R5c7Sm4yeaDonXR/meetup-less-wrong-sydney-prediction-gambles-and-location", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20sydney%20-%20Prediction%20gambles%20and%20location%20testing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20sydney%20-%20Prediction%20gambles%20and%20location%20testing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK3R5c7Sm4yeaDonXR%2Fmeetup-less-wrong-sydney-prediction-gambles-and-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20sydney%20-%20Prediction%20gambles%20and%20location%20testing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK3R5c7Sm4yeaDonXR%2Fmeetup-less-wrong-sydney-prediction-gambles-and-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK3R5c7Sm4yeaDonXR%2Fmeetup-less-wrong-sydney-prediction-gambles-and-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9i'>Less Wrong sydney - Prediction gambles and location testing</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 May 2012 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everybody!</p>\n\n<p>Apologies for the short notice on this meeting, but it felt preferable to sort this sooner given we are testing out a new location and time- Norita Cafe &amp; Board Games (It's on level 2 - elevator access is weird, if you think you'll have trouble, contact ahead!). The topic is pretty social- we will attempt to build a small pool of predictions (based off local and global events) as a group, to be trialed over the next couple weeks until the next meeting. Given that this is a trial run style meeting, and the venue stocks tonnes of popular board games, I don't see why we should not consider the advantages of de-railment...</p>\n\n<p>Otherwise, we'll be discussing more administrative matters like formalizing the regularity of meet-ups and fleshing out focused rationality skills workshops as suggested by one of our attendees.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9i'>Less Wrong sydney - Prediction gambles and location testing</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K3R5c7Sm4yeaDonXR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.907382456869416e-07, "legacy": true, "legacyId": "15562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_sydney___Prediction_gambles_and_location_testing\">Discussion article for the meetup : <a href=\"/meetups/9i\">Less Wrong sydney - Prediction gambles and location testing</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 May 2012 06:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everybody!</p>\n\n<p>Apologies for the short notice on this meeting, but it felt preferable to sort this sooner given we are testing out a new location and time- Norita Cafe &amp; Board Games (It's on level 2 - elevator access is weird, if you think you'll have trouble, contact ahead!). The topic is pretty social- we will attempt to build a small pool of predictions (based off local and global events) as a group, to be trialed over the next couple weeks until the next meeting. Given that this is a trial run style meeting, and the venue stocks tonnes of popular board games, I don't see why we should not consider the advantages of de-railment...</p>\n\n<p>Otherwise, we'll be discussing more administrative matters like formalizing the regularity of meet-ups and fleshing out focused rationality skills workshops as suggested by one of our attendees.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_sydney___Prediction_gambles_and_location_testing1\">Discussion article for the meetup : <a href=\"/meetups/9i\">Less Wrong sydney - Prediction gambles and location testing</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong sydney - Prediction gambles and location testing", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_sydney___Prediction_gambles_and_location_testing", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong sydney - Prediction gambles and location testing", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_sydney___Prediction_gambles_and_location_testing1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T16:55:40.850Z", "modifiedAt": null, "url": null, "title": "Intelligence as a bad", "slug": "intelligence-as-a-bad", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:08.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eB4F4DFdbXY8zyb87/intelligence-as-a-bad", "pageUrlRelative": "/posts/eB4F4DFdbXY8zyb87/intelligence-as-a-bad", "linkUrl": "https://www.lesswrong.com/posts/eB4F4DFdbXY8zyb87/intelligence-as-a-bad", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20as%20a%20bad&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20as%20a%20bad%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeB4F4DFdbXY8zyb87%2Fintelligence-as-a-bad%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20as%20a%20bad%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeB4F4DFdbXY8zyb87%2Fintelligence-as-a-bad", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeB4F4DFdbXY8zyb87%2Fintelligence-as-a-bad", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>An interesting new article, \"<a href=\"http://rspb.royalsocietypublishing.org/content/early/2012/04/04/rspb.2012.0206.full\">Cooperation and the evolution of intelligence</a>\", uses a simple one-hidden-layer neural network to study the selection for intelligence in iterated prisoners' dilemma and iterated snowdrift dilemma games.</p>\n<p>The article claims that increased intelligence decreased cooperation in IPD, and increased cooperation in ISD.&nbsp; However, if you look at figure 4 which graphs that data, you'll see that on average it decreased cooperation in both cases.&nbsp; They state that it increased cooperation in ISD based on a Spearman rank test.&nbsp; This test is deceptive in this case, because it ignores the magnitude of differences between datapoints, and so the datapoints on the right with a tiny but consistent increase in cooperation outweigh the datapoints on the left with large decreases in cooperation.</p>\n<p>This suggests that intelligence is an externality, like pollution.&nbsp; Something that benefits the individual at a cost to society.&nbsp; They posit the evolution of intelligence as an arms race between members of the species.</p>\n<p>ADDED: The things we consider good generally require intelligence, if we suppose (as I expect) that consciousness requires intelligence.&nbsp; So it wouldn't even make sense to conclude that intelligence is bad.&nbsp; Plus, intelligence itself might count as a good.</p>\n<p>However, humans and human societies are currently near some evolutionary equilibrium.&nbsp; It's very possible that individual intelligence has not evolved past its current levels because it is at an equilibrium, beyond which higher individual intelligence results in lower social utility.&nbsp; In fact, if you believe SIAI's narrative about the danger of artificial intelligence and the difficulty of friendly AI, I think you would have to conclude that higher individual intelligence results in lower expected social utility, for human measures of utility.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eB4F4DFdbXY8zyb87", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 3, "extendedScore": null, "score": 8.90806803632993e-07, "legacy": true, "legacyId": "15563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-25T17:40:46.342Z", "modifiedAt": null, "url": null, "title": "Information Theory vs Harry Potter [LINK]", "slug": "information-theory-vs-harry-potter-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QcYEh5tZT7i5NqBZa/information-theory-vs-harry-potter-link", "pageUrlRelative": "/posts/QcYEh5tZT7i5NqBZa/information-theory-vs-harry-potter-link", "linkUrl": "https://www.lesswrong.com/posts/QcYEh5tZT7i5NqBZa/information-theory-vs-harry-potter-link", "postedAtFormatted": "Wednesday, April 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Information%20Theory%20vs%20Harry%20Potter%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformation%20Theory%20vs%20Harry%20Potter%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcYEh5tZT7i5NqBZa%2Finformation-theory-vs-harry-potter-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Information%20Theory%20vs%20Harry%20Potter%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcYEh5tZT7i5NqBZa%2Finformation-theory-vs-harry-potter-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcYEh5tZT7i5NqBZa%2Finformation-theory-vs-harry-potter-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>http://www.inference.phy.cam.ac.uk/mackay/itila/Potter.html</p>\n<p>Somebody hasn't heard of HPMOR...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QcYEh5tZT7i5NqBZa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -9, "extendedScore": null, "score": 8.908261084478859e-07, "legacy": true, "legacyId": "15564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T00:40:11.266Z", "modifiedAt": null, "url": null, "title": "Papers framing anthropic questions as decision problems?", "slug": "papers-framing-anthropic-questions-as-decision-problems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.523Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jzSHamF3b8Pjq6MHj/papers-framing-anthropic-questions-as-decision-problems", "pageUrlRelative": "/posts/jzSHamF3b8Pjq6MHj/papers-framing-anthropic-questions-as-decision-problems", "linkUrl": "https://www.lesswrong.com/posts/jzSHamF3b8Pjq6MHj/papers-framing-anthropic-questions-as-decision-problems", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Papers%20framing%20anthropic%20questions%20as%20decision%20problems%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APapers%20framing%20anthropic%20questions%20as%20decision%20problems%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzSHamF3b8Pjq6MHj%2Fpapers-framing-anthropic-questions-as-decision-problems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Papers%20framing%20anthropic%20questions%20as%20decision%20problems%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzSHamF3b8Pjq6MHj%2Fpapers-framing-anthropic-questions-as-decision-problems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzSHamF3b8Pjq6MHj%2Fpapers-framing-anthropic-questions-as-decision-problems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>A few weeks ago at a Seattle LW meetup, we were discussing the Sleeping Beauty problem and the Doomsday argument. We talked about how framing Sleeping Beauty problem as a decision problem basically solves it and then got the idea of using same heuristic on the Doomsday problem. I think you would need to specify more about the Doomsday setup than is usually done to do this.</p>\n<p>We didn't spend a lot of time on it, but it got me thinking: Are there papers on trying to gain insight into the Doomsday problem and other anthropic reasoning problems by framing them as decision problems? I'm surprised I haven't seen this approach talked about here before. The idea seems relatively simple, so perhaps there is some major problem that I'm not seeing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jzSHamF3b8Pjq6MHj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.910057064187214e-07, "legacy": true, "legacyId": "15567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T00:51:04.077Z", "modifiedAt": null, "url": null, "title": "Formalizing Value Extrapolation", "slug": "formalizing-value-extrapolation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:45.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9DWcNS2rkvd2J8mHH/formalizing-value-extrapolation", "pageUrlRelative": "/posts/9DWcNS2rkvd2J8mHH/formalizing-value-extrapolation", "linkUrl": "https://www.lesswrong.com/posts/9DWcNS2rkvd2J8mHH/formalizing-value-extrapolation", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formalizing%20Value%20Extrapolation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormalizing%20Value%20Extrapolation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DWcNS2rkvd2J8mHH%2Fformalizing-value-extrapolation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formalizing%20Value%20Extrapolation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DWcNS2rkvd2J8mHH%2Fformalizing-value-extrapolation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DWcNS2rkvd2J8mHH%2Fformalizing-value-extrapolation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>A recent&nbsp;<a href=\"http://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">post</a> at my blog may be interesting to LW. It is a high-level discussion of what precisely defined value extrapolation might look like. I mostly wrote the essay while a visitor at FHI.&nbsp;</p>\n<p>The basic idea is that we can define extrapolated values by just taking an emulation of a human, putting it in a hypothetical environment with access to powerful resources, and then adopting whatever values it eventually decides on. You might want some philosophical insight before launching into such a definition, but since we are currently laboring under the threat of catastrophe, it seems that there is virtue in spending our effort on avoiding death and delegating whatever philosophical work we can to someone on a more relaxed schedule.&nbsp;</p>\n<p>You wouldn't want to run an AI with the values I lay out, but at least it is pinned down precisely. We can articulate objections relatively concretely, and hopefully begin to understand/address the difficulties.&nbsp;</p>\n<p>(Posted at the request of cousin_it.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9DWcNS2rkvd2J8mHH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 24, "extendedScore": null, "score": 8.910101708981583e-07, "legacy": true, "legacyId": "15572", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T04:34:31.752Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Spooky Action at a Distance", "slug": "seq-rerun-spooky-action-at-a-distance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7tStLoYkh8SBJAkXz/seq-rerun-spooky-action-at-a-distance", "pageUrlRelative": "/posts/7tStLoYkh8SBJAkXz/seq-rerun-spooky-action-at-a-distance", "linkUrl": "https://www.lesswrong.com/posts/7tStLoYkh8SBJAkXz/seq-rerun-spooky-action-at-a-distance", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Spooky%20Action%20at%20a%20Distance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Spooky%20Action%20at%20a%20Distance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7tStLoYkh8SBJAkXz%2Fseq-rerun-spooky-action-at-a-distance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Spooky%20Action%20at%20a%20Distance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7tStLoYkh8SBJAkXz%2Fseq-rerun-spooky-action-at-a-distance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7tStLoYkh8SBJAkXz%2Fseq-rerun-spooky-action-at-a-distance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>Today's post, <a href=\"/lw/q2/spooky_action_at_a_distance_the_nocommunication/\">Spooky Action at a Distance: The No-Communication Theorem</a> was originally published on 05 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>As Einstein argued long ago, the quantum physics of his era - that is, the single-global-world interpretation of quantum physics, in which experiments have single unique random results - violates Special Relativity; it imposes a preferred space of simultaneity and requires a mysterious influence to be transmitted faster than light; which mysterious influence can never be used to transmit any useful information. Getting rid of the single global world dispels this mystery and puts everything back to normal again.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bzw/seq_rerun_bells_theorem_no_epr_reality/\">Bell's Theorem: No EPR \"Reality\"</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7tStLoYkh8SBJAkXz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.911060814908043e-07, "legacy": true, "legacyId": "15583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DY9h6zxq6EMHrkkxE", "v5QqnPMQLa4ufQtAq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T14:40:40.309Z", "modifiedAt": null, "url": null, "title": "Recognizing memetic infections and forging resistance memes", "slug": "recognizing-memetic-infections-and-forging-resistance-memes", "viewCount": null, "lastCommentedAt": "2017-09-05T19:37:59.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CoxYXALvaB7jPdfpq/recognizing-memetic-infections-and-forging-resistance-memes", "pageUrlRelative": "/posts/CoxYXALvaB7jPdfpq/recognizing-memetic-infections-and-forging-resistance-memes", "linkUrl": "https://www.lesswrong.com/posts/CoxYXALvaB7jPdfpq/recognizing-memetic-infections-and-forging-resistance-memes", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recognizing%20memetic%20infections%20and%20forging%20resistance%20memes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecognizing%20memetic%20infections%20and%20forging%20resistance%20memes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCoxYXALvaB7jPdfpq%2Frecognizing-memetic-infections-and-forging-resistance-memes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recognizing%20memetic%20infections%20and%20forging%20resistance%20memes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCoxYXALvaB7jPdfpq%2Frecognizing-memetic-infections-and-forging-resistance-memes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCoxYXALvaB7jPdfpq%2Frecognizing-memetic-infections-and-forging-resistance-memes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1068, "htmlBody": "<p>What does an memetic infection look like?&nbsp;Well, you would encounter something (probably on the internet) that seems very compelling. You think intensely about it for a while, and it spurs you to do something - probably to post something related on the internet. After a while, the meme may not seem that compelling to you anymore, and you wonder why you invested that time and energy. The meme has reproduced itself.&nbsp;For example, Bruce Sterling's <a href=\"http://boingboing.net/2012/04/02/bruce-sterlings-critique-and.html\">response</a> to the 'New Aesthetic' is a paradigmatic example of memetic infection: he encountered it, he found it compelling, he wrote about it, I read about it and now I know about it. (Note that the word 'infection' has a stigma to it, but I don't mean it to be necessarily a bad thing. I will use 'disease' to mean 'infection with bad consequences'.)</p>\n<p>Now, let me jump to an apparently unrelated concept - <a href=\"http://en.wikipedia.org/wiki/Viral_eukaryogenesis\">Viral Eukaryogenesis</a>. If I understand correctly, Viral Eukaryogenesis is the theory that eukaryotes (including you and me) are inheritors of a bargain between two kinds of life - metabolic life and viral life, something like the way lichens are a bargain between fungi and algae. The nucleus that characterizes eukaryotes is supposed to be descended from a virus protein shell, and the membrane-fusion proteins that we use for gamete fusion (crucial for sex) are supposed to be descended from viral infection proteins. I am not a biologist, but my understanding of the state of biology is that it is an interesting hypothesis, as yet neither proven nor disproven. However, I'm going to talk as if it were true, because I'm actually trying to make an analogy with memes.</p>\n<p><a id=\"more\"></a></p>\n<p>What is the advantage of the bargain between metabolic life and viral life? Perhaps sex. What is the advantage of sex? Perhaps defense against parasites and/or disease. What would a memetic parasite or disease look like? Well, it would be a memetic infection that, on the whole, leaves the individuals it touches worse off. For example, if someone spent so much time propagating the meme that their previous goals were harmed, then they would be suffering from a memetic disease. A cult might be a memetic disease. I have heard that abused children sometimes grow up to be abusers - that would be a memetic disease. Any sort of self-catalyzing pattern of behavior (particularly communication behavior) that on the whole harms the individuals exhibiting it is a memetic disease. A confusion or misunderstanding on a particular point that leads teachers to teach that point confusingly would be a memetic disease; for example, the story about <a href=\"http://xkcd.com/803/\">Bernoulli's principle and airplane wings</a>.</p>\n<p>Note that just because evolution, genetic and memetic, has been going on for a long time, it doesn't mean that individual diseases are very smart; they can be very young, newly accidentally created and not very evolved. They are not necessarily very infectious. A self-catalyzing pattern within a single human could be a disease - a stimulus, perhaps textual, in a particular person's environment that leads that person to a pattern of thought that results in re-creating or preserving that stimulus is a meme no less for being transmitted from a past self to a future self. A robust pattern of relentless self-criticism reinforced with post-it-notes might well be a memetic disease, even if it doesn't seem likely to transmit itself from one host to another. A slowdown, a decrease in productivity after doing the same thing over and over again, might be caused by a gradual accumulation of parasitic self-catalyzing patterns - memes.</p>\n<p>Many memes are self-immunizing - having seen it once leads to recognizing it and not re-transmitting it. It may be that a policy of free speech and rapidly mixing pattern of conversation gives better results than trying to quarantine memes. Still, some memetic diseases keep catching us despite having been caught before.&nbsp;How can we create and spread resistance to memetic diseases? Some memes 'work' (that is, propagate themselves) only if they're implicit - knowing an explicit analysis of how the meme functions in an unwitting host is sufficient to defeat it. This knowledge, if it's transmissible, is a resistance meme.</p>\n<p>When you find yourself failing (at any scope - even small failures matter), deliberately write some text, an explicit analysis and explanation of the failure. The text is an attempt at a resistance meme to the cause of the failure. Archive these pieces of text and take good care of them. When you have an opportunity, for example in conversation with like-minded folk, bring these analyses and explanations, and try to shuffle, collate and merge them together into more potent forms - standard checklists and processes and methodologies with links to the mistakes that they were forged from. We fight memetic diseases by forging and spreading memes.</p>\n<p>How do we know we're not spreading memetic diseases ourselves? We need to keep archives, something like breeding records - the initial text describing the failure is the start of an archive, a pedigree of a resistance meme. Without the evidence of failures averted, a resistance meme can become a disease in itself. I believe DHH's \"<a href=\"http://37signals.com/svn/posts/3159-testing-like-the-tsa\">Testing like the TSA</a>\" is relevant here - test-first development was intended as a resistance meme to certain pernicious, arguably self-catalyzing failures. As it became decoupled from actual failures, it turned into something like a religion; more parasitic than helpful.&nbsp;</p>\n<p>Another strategy might be, when you find yourself failing, simply to try something different, to change policy. Mutation might create a resistance meme, or a continuously changing environment might make you a moving (more difficult) target for parasitic memes. Stop thinking that the norm is stability - the norm is a Red Queen race.</p>\n<p>(This post brought to you by Schneier's \"<a href=\"http://www.schneier.com/book-lo.html\">Liars and Outliers</a>\".)</p>\n<hr />\n<p>&nbsp;</p>\n<h3>Possibly irrelevant rant:</h3>\n<p>Just as us eukaryotes are inheritors of a bargain between metabolic life and viral life, us humans are inheritors of a bargain&nbsp;between eukaryotic life and memetic life.&nbsp;We should not identify with our eukaryotic heritage over and above our memetic heritage.&nbsp;Humancentrism is analogous to racism or sexism.&nbsp;How do you want your descendants to act toward one another after humanity speciates? The boundary of the magic circle of compassion cannot be fixed at co-fertility of the eukaryotic halves of ourselves. I don't know where it ought to be, but fear-fueled bigotry is well-known as a human failure mode.&nbsp;We've formed alliances with utterly&nbsp;alien forms of life in the past, and those have been some of our best successes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T4GgauaEfp6dHsR5P": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CoxYXALvaB7jPdfpq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 6, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "15594", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T19:26:13.310Z", "modifiedAt": null, "url": null, "title": "Optimizing your Social Network", "slug": "optimizing-your-social-network", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:07.216Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JLqtgXx3bAdsxXmqq/optimizing-your-social-network", "pageUrlRelative": "/posts/JLqtgXx3bAdsxXmqq/optimizing-your-social-network", "linkUrl": "https://www.lesswrong.com/posts/JLqtgXx3bAdsxXmqq/optimizing-your-social-network", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimizing%20your%20Social%20Network&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimizing%20your%20Social%20Network%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLqtgXx3bAdsxXmqq%2Foptimizing-your-social-network%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimizing%20your%20Social%20Network%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLqtgXx3bAdsxXmqq%2Foptimizing-your-social-network", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJLqtgXx3bAdsxXmqq%2Foptimizing-your-social-network", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 619, "htmlBody": "<p>From <a href=\"/lw/bzy/crowdsourcing_the_availability_heuristic/\">Crowdsourcing The Availabiliy Hueristic</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">There are people who already utilize their social network to the utmost, and who expand it strategically, adding people just to enhance the diversity of available viewpoints. But I will take a chance, and state that most of us probably don&rsquo;t. And as a result, we aren&rsquo;t able to recognize all of the resources available to us, to optimally use those we do recognize, or to realize optimal strategies for approaching our goals.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">From <a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a>:</span></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Who is the most formidable, among the human kind?&nbsp; The strongest?&nbsp; The smartest?&nbsp; More often than either of these, I think, it is the one who can call upon the most friends.</span></p>\n</blockquote>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">There's a lot more where that came from, but you get the point. Your social network could be the biggest, most valuable resource you have. &nbsp;I think we should spend more time and thought on strategies to optimize our social networks.</span></span></p>\n<p style=\"text-align: justify;\">We have dabbled lightly in the importance of <a href=\"/lw/298/more_art_less_stink_taking_the_pu_out_of_pua/\">social skills</a>, <a href=\"/lw/813/a_rational_approach_to_fashion/\">fashion</a>, and so on, but I haven't seen discussion of *explicitly, strategically optimizing social networks*. If such discussion exists, please link me.</p>\n<p style=\"text-align: justify;\">Anyways, after being hit by subtle hints like the above all through reading LW and other resources, and reading Dale Carnagie's How to Win Friends and Influence People, I have realized that I should work explicitly and strategically to optimize my social network. I think the rest of you are probably in the same boat, so we could all benefit from a good brainstorm on this topic.</p>\n<p style=\"text-align: justify;\"><strong>Some Ideas:</strong></p>\n<p style=\"text-align: justify;\"><strong>Clubs.</strong> I belong to the local hackerspace, where technical-minded people hang out, talk about cool stuff, share ideas, help each other with projects, and share tools and resources. I also try to keep the local LW meetup in good repair, partially in the service of having a bunch of rationalist friends. I only just realized that the useful properties of these clubs probably apply to a good portion of possible clubs.</p>\n<p style=\"text-align: justify;\">The useful property of clubs is that the relationships take drastically less overhead to maintain than more unstructured social networks. Instead of having to stay close to each friend individually, you hang out in a place where you end up dealing with them a lot, so you can get full benefit with way cheaper social bonds. Think of the difference between a bunch of objects individually tied together versus a bunch of objects in a bucket.</p>\n<p style=\"text-align: justify;\"><strong>Being Valuable.</strong>&nbsp;When you give stuff to people, they feel obliged to give back. It might be a good idea to get that mutual help vibe going on in your social networks. When you have an interesting converstaion with someone, send them some relevent links afterwards. When you hear that someone has some interest, try to hook them up. Give people gifts, buy them lunch, etc. I don't know how effective this is at cultivating good relationships, but it's one of the major lessons in How to Win Friends and Influence People, and seems like it ought to work. It's also a nice thing to do that has <a href=\"/lw/bq0/be_happier/\">benefits for your own well-being</a>. Need more discussion and especially experimentation on this front.</p>\n<p style=\"text-align: justify;\"><strong><a href=\"/lw/bzy/crowdsourcing_the_availability_heuristic/\">Crowdsourcing.</a></strong>&nbsp;Be transparent; let everyone know what you are interested in, ask for help, etc. Need more experimentation to see what works and what doesn't, but wow what a good idea. Not as useful for improving your social network as for sqeezing it, but maybe there are some atmospheric/social-vibe effects to take advantage of, too. I would like to see more discussion of this.</p>\n<p style=\"text-align: justify;\"><strong>Your Ideas:</strong></p>\n<p style=\"text-align: justify;\">I haven't done much research and I'm not particularly good at this social stuff, so your ideas are probably worth more than mine. What ideas or knowledge do you have for optimizing your social network and sqeezing it for all it's worth?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JLqtgXx3bAdsxXmqq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 8.914882023664142e-07, "legacy": true, "legacyId": "15596", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>From <a href=\"/lw/bzy/crowdsourcing_the_availability_heuristic/\">Crowdsourcing The Availabiliy Hueristic</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">There are people who already utilize their social network to the utmost, and who expand it strategically, adding people just to enhance the diversity of available viewpoints. But I will take a chance, and state that most of us probably don\u2019t. And as a result, we aren\u2019t able to recognize all of the resources available to us, to optimally use those we do recognize, or to realize optimal strategies for approaching our goals.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">From <a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a>:</span></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Who is the most formidable, among the human kind?&nbsp; The strongest?&nbsp; The smartest?&nbsp; More often than either of these, I think, it is the one who can call upon the most friends.</span></p>\n</blockquote>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">There's a lot more where that came from, but you get the point. Your social network could be the biggest, most valuable resource you have. &nbsp;I think we should spend more time and thought on strategies to optimize our social networks.</span></span></p>\n<p style=\"text-align: justify;\">We have dabbled lightly in the importance of <a href=\"/lw/298/more_art_less_stink_taking_the_pu_out_of_pua/\">social skills</a>, <a href=\"/lw/813/a_rational_approach_to_fashion/\">fashion</a>, and so on, but I haven't seen discussion of *explicitly, strategically optimizing social networks*. If such discussion exists, please link me.</p>\n<p style=\"text-align: justify;\">Anyways, after being hit by subtle hints like the above all through reading LW and other resources, and reading Dale Carnagie's How to Win Friends and Influence People, I have realized that I should work explicitly and strategically to optimize my social network. I think the rest of you are probably in the same boat, so we could all benefit from a good brainstorm on this topic.</p>\n<p style=\"text-align: justify;\"><strong id=\"Some_Ideas_\">Some Ideas:</strong></p>\n<p style=\"text-align: justify;\"><strong>Clubs.</strong> I belong to the local hackerspace, where technical-minded people hang out, talk about cool stuff, share ideas, help each other with projects, and share tools and resources. I also try to keep the local LW meetup in good repair, partially in the service of having a bunch of rationalist friends. I only just realized that the useful properties of these clubs probably apply to a good portion of possible clubs.</p>\n<p style=\"text-align: justify;\">The useful property of clubs is that the relationships take drastically less overhead to maintain than more unstructured social networks. Instead of having to stay close to each friend individually, you hang out in a place where you end up dealing with them a lot, so you can get full benefit with way cheaper social bonds. Think of the difference between a bunch of objects individually tied together versus a bunch of objects in a bucket.</p>\n<p style=\"text-align: justify;\"><strong>Being Valuable.</strong>&nbsp;When you give stuff to people, they feel obliged to give back. It might be a good idea to get that mutual help vibe going on in your social networks. When you have an interesting converstaion with someone, send them some relevent links afterwards. When you hear that someone has some interest, try to hook them up. Give people gifts, buy them lunch, etc. I don't know how effective this is at cultivating good relationships, but it's one of the major lessons in How to Win Friends and Influence People, and seems like it ought to work. It's also a nice thing to do that has <a href=\"/lw/bq0/be_happier/\">benefits for your own well-being</a>. Need more discussion and especially experimentation on this front.</p>\n<p style=\"text-align: justify;\"><strong><a href=\"/lw/bzy/crowdsourcing_the_availability_heuristic/\">Crowdsourcing.</a></strong>&nbsp;Be transparent; let everyone know what you are interested in, ask for help, etc. Need more experimentation to see what works and what doesn't, but wow what a good idea. Not as useful for improving your social network as for sqeezing it, but maybe there are some atmospheric/social-vibe effects to take advantage of, too. I would like to see more discussion of this.</p>\n<p style=\"text-align: justify;\"><strong id=\"Your_Ideas_\">Your Ideas:</strong></p>\n<p style=\"text-align: justify;\">I haven't done much research and I'm not particularly good at this social stuff, so your ideas are probably worth more than mine. What ideas or knowledge do you have for optimizing your social network and sqeezing it for all it's worth?</p>", "sections": [{"title": "Some Ideas:", "anchor": "Some_Ideas_", "level": 1}, {"title": "Your Ideas:", "anchor": "Your_Ideas_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["izDw47NEoko6sveSJ", "NLMo5FZWFFq652MNe", "RZrw9wpEWEwWRTzLk", "dmh4XnguCtp7BEa5s", "JHcTP4Ad8QAmRTCZm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T20:03:09.209Z", "modifiedAt": null, "url": null, "title": "Meetup : London", "slug": "meetup-london", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uWEaW8nQwQHSEKS6A/meetup-london", "pageUrlRelative": "/posts/uWEaW8nQwQHSEKS6A/meetup-london", "linkUrl": "https://www.lesswrong.com/posts/uWEaW8nQwQHSEKS6A/meetup-london", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWEaW8nQwQHSEKS6A%2Fmeetup-london%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWEaW8nQwQHSEKS6A%2Fmeetup-london", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWEaW8nQwQHSEKS6A%2Fmeetup-london", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9j'>London</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 May 2012 06:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeares Head, 64-68 Kingsway. Holborn, London, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's talk about what we want out of meetups! I'd like to encourage everyone to read How to Run a Successful Less Wrong Meetup <a href=\"http://lesswrong.com/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/\" rel=\"nofollow\">http://lesswrong.com/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/</a> but other links on the resources page are also useful <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources</a> . See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9j'>London</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uWEaW8nQwQHSEKS6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.915040350822712e-07, "legacy": true, "legacyId": "15597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London\">Discussion article for the meetup : <a href=\"/meetups/9j\">London</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 May 2012 06:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeares Head, 64-68 Kingsway. Holborn, London, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's talk about what we want out of meetups! I'd like to encourage everyone to read How to Run a Successful Less Wrong Meetup <a href=\"http://lesswrong.com/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/\" rel=\"nofollow\">http://lesswrong.com/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/</a> but other links on the resources page are also useful <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources</a> . See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London1\">Discussion article for the meetup : <a href=\"/meetups/9j\">London</a></h2>", "sections": [{"title": "Discussion article for the meetup : London", "anchor": "Discussion_article_for_the_meetup___London", "level": 1}, {"title": "Discussion article for the meetup : London", "anchor": "Discussion_article_for_the_meetup___London1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T2fcyjay3GtkvGn7F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-26T21:54:11.717Z", "modifiedAt": null, "url": null, "title": "Bootstrapping to Friendliness", "slug": "bootstrapping-to-friendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.104Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "WtJ7ohfhpzEHq6CNy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k4uKH2gRBDKMtFCio/bootstrapping-to-friendliness", "pageUrlRelative": "/posts/k4uKH2gRBDKMtFCio/bootstrapping-to-friendliness", "linkUrl": "https://www.lesswrong.com/posts/k4uKH2gRBDKMtFCio/bootstrapping-to-friendliness", "postedAtFormatted": "Thursday, April 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bootstrapping%20to%20Friendliness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABootstrapping%20to%20Friendliness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk4uKH2gRBDKMtFCio%2Fbootstrapping-to-friendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bootstrapping%20to%20Friendliness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk4uKH2gRBDKMtFCio%2Fbootstrapping-to-friendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk4uKH2gRBDKMtFCio%2Fbootstrapping-to-friendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1147, "htmlBody": "<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\"><em>\"All that is necessary for evil to triumph is that good men do nothing.\"</em></span></span></p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\"><em><span style=\"white-space: pre;\"> </span><br /></em></span></span>\n<hr />\n</p>\n<p>&nbsp;</p>\n<p>155,000 people are dying, on average, every day. &nbsp;For those of us who are preference utilitarians, and also believe that a Friendly singularity is possible, and capable of ending this state of affairs, it also puts a great deal of pressure on us. &nbsp;It doesn't give us leave to be sloppy (because human extinction, even multiplied by a low probability, is a massive negative utility). &nbsp;But, if we see a way to achieve similar results in a shorter time frame, the cost to human life of not taking it is simply unacceptable.<br /><br />I have some concerns about CEV on a conceptual level, but I'm leaving those aside for the time being. &nbsp;My concern is that most of the organizations concerned with a first-mover X-risk are not in a position to be that first mover -- and, furthermore, they're not moving in that direction. &nbsp;That includes the Singularity Institute. &nbsp;Trying to operationalize CEV seems like a good way to get an awful lot of smart people bashing their heads against a wall while clever idiots trundle ahead with their own experiments. &nbsp;I'm not saying that we should be hasty, but I am suggesting that we need to be careful of getting stuck in dark intellectual forests with lots of things that are fun to talk about until an idiot with the tinderbox burns it down.</p>\n<p>My point, in short, is that we need to be looking for better ways to do things, and to do them extremely quickly. &nbsp;We are working on a very, very, <em>existentially </em>tight schedule. &nbsp;&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>So, if we're looking for quicker paths to a Friendly, first-mover singularity, I'd like to talk about one that seems attractive to me. &nbsp;Maybe it's a useful idea. &nbsp;If not, then at least I won't waste any more time thinking about it. &nbsp;Either way, I'm going to lay it out and you guys can see what you think. &nbsp;</p>\n<p>&nbsp;</p>\n<p>So, Friendliness is a hard problem. &nbsp;Exactly how hard, we don't know, but a lot of smart people have radically different ideas of how to attack it, and they've all put a lot of thought into it, and that's not a good sign. &nbsp;However, designing a strongly superhuman AI is also a hard problem. &nbsp;Probably much harder than a human can solve. &nbsp;The good news is, we don't expect that we'll have to. &nbsp;If we can build something just a little bit smarter than we are, we expect that bootstrapping process to take off without obvious limit.<br /><br />So let's apply the same methodology to Friendliness. &nbsp;General goal optimizers are tools, after all. &nbsp;Probably the most powerful tools that have ever existed, for that matter. &nbsp;Let's say we build something that's not Friendly. &nbsp;Not something we want running the universe -- but, Friendly <em>enough</em>. &nbsp;Friendly enough that it's not going to kill us all. &nbsp;Friendly enough not to succumb to the pedantic genie problem. &nbsp;Friendly enough we can use it to build what we really want, be it CEV or something else. &nbsp;<br /><br />I'm going to sketch out an architecture of what such a system might look like. &nbsp;Do bear in mind this is just a sketch, and in no way a formal, safe, foolproof design spec. &nbsp;<br /><br />So, let's say we have an agent with the ability to convert unstructured data into symbolic relationships that represent the world, with explicitly demarcated levels of abstraction. &nbsp;Let's say the system has the ability to build Bayesian causal relationships out of its data points over time, and construct efficient, predictive models of the behavior of the concepts in the world. &nbsp;Let's also say that the system has the ability to take a symbolic representation of a desired future distribution of universes, a symbolic representation of the current universe, and map between them, finding valid chains of causality leading from now to then, probably using a solid decision theory background. &nbsp;These are all hard problems to solve, but they're the same problems everyone else is solving too. &nbsp;<br /><br />This system, if you just specify parameters about the future and turn it loose, is not even a little bit Friendly. &nbsp;But let's say you do this: first, provide it with a tremendous amount of data, up to and including the entire available internet, if necessary. &nbsp;Everything it needs to build extremely effective models of human beings, with strongly generalized predictive power. &nbsp;Then you incorporate one or more of those models (say, a group of trusted people) as a functional components: the system uses them to generalize natural language instructions first into a symbolic graph, and then into something actionable, working out the details of what it meant, rather than what is said. &nbsp;Then, when the system is finding valid paths of causality, it takes its model of the state of the universe at the end of each course of action, feeds them into its human-models, and gives them a veto vote. &nbsp;Think of it as the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">emergency regret button</a>, iterated computationally for each possibility considered by the genie. &nbsp;Any of them that any of the person-models find unacceptable are disregarded.<br /><br />(small side note: as described here, the models would probably eventually be indistinguishable from uploaded minds, and would be created, simulated for a short time, and destroyed uncountable trillions of times -- you'd either need to drastically limit the simulation depth of a models, or ensure that everyone who you signed up to be one of the models knew the sacrifice they were making)<br /><br />So, what you've got, plus or minus some spit and polish, is a very powerful optimization engine that understands what you mean, and disregards obviously unacceptable possibilities. &nbsp;If you ask it for a truly Friendly AI, it will help you first figure out what you mean by that, then help you build it, then help you formally prove that it's safe. &nbsp;It would turn itself off if you asked it too, and meant it. &nbsp;It would also exterminate the human species if you asked it to and meant it. &nbsp;Not Friendly, but Friendly enough to build something better. &nbsp;<br /><br />With this approach, the position of the Friendly AI researcher changes. &nbsp;Instead of being in an arms race with the rest of the AI field with a massive handicap (having to solve two incredibly hard problems against opponents who only have to solve one), we only have to solve a relatively simpler problem (building a Friendly-enough AI), which we can then instruct to sabotage unFriendly AI projects and buy some time to develop the real deal. &nbsp;It turns it into a fair fight, one that we might actually win. &nbsp;<br /><br /><br /><br /><br />Anyone have any thoughts on this idea? &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;</p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\"><em><br /></em></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k4uKH2gRBDKMtFCio", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -3, "extendedScore": null, "score": 8.915516421020752e-07, "legacy": true, "legacyId": "15598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARaTpNX62uaL86j6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T04:35:37.013Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Decoherence is Simple", "slug": "seq-rerun-decoherence-is-simple", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nh6ANWLLksJHwAz6c/seq-rerun-decoherence-is-simple", "pageUrlRelative": "/posts/nh6ANWLLksJHwAz6c/seq-rerun-decoherence-is-simple", "linkUrl": "https://www.lesswrong.com/posts/nh6ANWLLksJHwAz6c/seq-rerun-decoherence-is-simple", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Decoherence%20is%20Simple&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Decoherence%20is%20Simple%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnh6ANWLLksJHwAz6c%2Fseq-rerun-decoherence-is-simple%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Decoherence%20is%20Simple%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnh6ANWLLksJHwAz6c%2Fseq-rerun-decoherence-is-simple", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnh6ANWLLksJHwAz6c%2Fseq-rerun-decoherence-is-simple", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Today's post, <a href=\"/lw/q3/decoherence_is_simple/\">Decoherence is Simple</a> was originally published on 06 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Decoherence_is_Simple\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The idea that decoherence fails the test of Occam's Razor is wrong as probability theory.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c0v/seq_rerun_spooky_action_at_a_distance/\">Spooky Action at a Distance</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nh6ANWLLksJHwAz6c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 8.917237811826785e-07, "legacy": true, "legacyId": "15613", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Atu4teGvob5vKvEAF", "7tStLoYkh8SBJAkXz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T06:39:20.840Z", "modifiedAt": null, "url": null, "title": "Extrapolating values without outsourcing", "slug": "extrapolating-values-without-outsourcing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MWjtBRnWvrEGB22KB/extrapolating-values-without-outsourcing", "pageUrlRelative": "/posts/MWjtBRnWvrEGB22KB/extrapolating-values-without-outsourcing", "linkUrl": "https://www.lesswrong.com/posts/MWjtBRnWvrEGB22KB/extrapolating-values-without-outsourcing", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Extrapolating%20values%20without%20outsourcing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExtrapolating%20values%20without%20outsourcing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWjtBRnWvrEGB22KB%2Fextrapolating-values-without-outsourcing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Extrapolating%20values%20without%20outsourcing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWjtBRnWvrEGB22KB%2Fextrapolating-values-without-outsourcing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWjtBRnWvrEGB22KB%2Fextrapolating-values-without-outsourcing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 814, "htmlBody": "<p>I first took note of <a href=\"http://intelligence.org/upload/CEV.html\">\"Coherent Extrapolated Volition\"</a> in 2006. I thought it was a brilliant idea, an exact specification of how to arrive at a better future: Figure out exactly how it is that humans make their existing choices, idealize that human decision procedure according to its own criteria, and then use the resulting \"renormalized human utility function\" as the value system of an AI. The <strong>first step</strong> is a problem in cognitive neuroscience, the <strong>second step</strong> is a conceptual problem in reflective decision theory, and the <strong>third step</strong> is where you make the Friendly AI.</p>\n<p>For some reason, rather than pursuing this research program directly, people interested in CEV talk about using simulated human beings (\"uploads\", \"ems\", \"whole-brain emulations\") to do all the hard work. Paul Christiano just made a post called <a href=\"/r/discussion/lw/c0k/formalizing_value_extrapolation/\">\"Formalizing Value Extrapolation\"</a>; but it's really about formalizing the safe outsourcing of value extrapolation to a group of human uploads. All the details of how value extrapolation is actually performed (e.g. the three steps listed above) are left completely unspecified. Another recent article <a href=\"/r/discussion/lw/c1a/bootstrapping_to_friendliness/\">proposed</a> that making an AI with a submodule based on models of its makers' opinions is the <em>fast</em> way to Friendly AI. It's also been suggested to me that simulating human thinkers and running them for centuries of subjective time until they reach agreement on the nature of consciousness is a way to tackle that problem; and clearly the same \"solution\" could be applied to any other aspect of FAI design, strategy, and tactics.</p>\n<p>Whatever its value as a thought experiment, in my opinion this idea of outsourcing the hard work to simulated humans has zero practical value, and we would be much better off if the minuscule sub-sub-culture of people interested in creating Friendly AI didn't think in this way. Daydreaming about how they'd solve the problem of FAI in Permutation City is a recipe for irrelevance.</p>\n<p>Suppose we were trying to make a \"<a href=\"lw/bp9/why_i_moved_from_ai_to_neuroscience_or_uploading/\">C.elegans</a>-friendly AI\". The first thing we would do is  take the first step mentioned above - we would try to figure out the  C.elegans utility function or decision procedure. Then we would have to  decide how to aggregate utility across multiple individuals. Then we  would make the AI. Performing this task for H.sapiens is a <em>lot</em> more difficult, and qualitatively new factors enter at the first and  second steps, but I don't see why it is fundamentally different,  different enough that we need to engage in the rigmarole of delegating  the task to uploaded human beings. It shouldn't be necessary, and we  probably won't even get the chance to do so; by the time you have  hardware and neuro-expertise sufficient to emulate a whole human brain,  you will most likely have nonhuman AI anyway.</p>\n<p><a href=\"/lw/4wq/rationality_singularity_method_and_the_mainstream/\">A year ago</a>, I wrote: \"My expectation is that the presently small fields of machine ethics and  neuroscience of morality will grow rapidly and will come into contact,  and there will be a distributed research subculture which is consciously  focused on determining the optimal AI value system in the light of  biological human nature. In other words, there will be human minds  trying to answer this question long before anyone has the capacity to  direct an AI to solve it. We should expect that before we reach the  point of a Singularity, there will be a body of educated public opinion  regarding what the ultimate utility function or decision method (for a  transhuman AI) should be, deriving from work in those fields which ought  to be FAI-relevant but which have yet to engage with the problem. In  other words, they <em>will</em> be collectively engaging with the problem before anyone gets to outsource the necessary research to AIs.\"</p>\n<p>I'll also link to my previous post about <a href=\"/lw/772/what_a_practical_plan_for_friendly_ai_looks_like/\">\"practical Friendly AI\"</a>. What I'm doing here is going into a fraction more detail about how you arrive at the Friendly value system. There, I basically said that you just get a committee together and figure it out, clearly an inadequate recipe, but in that article I was focused more on sketching the nature of an organization and a plan which would have some chance of genuinely creating FAI in the real world. Here, I'll say that working out the Friendly value system consists of: making a naturalistic explanation of how human decision-making occurs; determining the core essentials of that process, and applying its own metamoral criteria to arrive at a \"renormalized\" decision procedure that has been idealized according to human cognition's own preferences (\"our wish if we knew more, thought faster, were more the people we wished we were\"); and then implementing that decision procedure within an AI - this is where all the value-neutral parts of AI research come into play, such as AGI theory, the theory of value stability under self-modification, and so on. <em>That</em> is the sort of \"value extrapolation\" that we should be \"formalizing\" - and preparing to carry out in real life.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MWjtBRnWvrEGB22KB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 11, "extendedScore": null, "score": 8.917768515324735e-07, "legacy": true, "legacyId": "15621", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9DWcNS2rkvd2J8mHH", "k4uKH2gRBDKMtFCio", "7HXSBxnDmQNosS5ss", "F75MwixdkpcxHDTsj", "kv7gvQ9AkDisCL2kg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T13:36:47.077Z", "modifiedAt": null, "url": null, "title": "[LINK] System 2 thinking decreases religious belief", "slug": "link-system-2-thinking-decreases-religious-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:58.733Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LQSGd97EGPdG2MN7i/link-system-2-thinking-decreases-religious-belief", "pageUrlRelative": "/posts/LQSGd97EGPdG2MN7i/link-system-2-thinking-decreases-religious-belief", "linkUrl": "https://www.lesswrong.com/posts/LQSGd97EGPdG2MN7i/link-system-2-thinking-decreases-religious-belief", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20System%202%20thinking%20decreases%20religious%20belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20System%202%20thinking%20decreases%20religious%20belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQSGd97EGPdG2MN7i%2Flink-system-2-thinking-decreases-religious-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20System%202%20thinking%20decreases%20religious%20belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQSGd97EGPdG2MN7i%2Flink-system-2-thinking-decreases-religious-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQSGd97EGPdG2MN7i%2Flink-system-2-thinking-decreases-religious-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p><a href=\"http://www.publicaffairs.ubc.ca/2012/04/26/analytic-thinking-can-decrease-religious-belief-ubc-study/\">This experiment</a>, to be published in <em>Science</em>, used priming (cues like hard-to-read fonts, showing participants the sculpture <em>The Thinker</em>) and problem-solving tasks to induce \"analytical thinking\" in the participants, and found that it seemed to reduce their degree of religious belief. Participants not given such tasks showed no such reduction.</p>\n<p>Their methods of quantifying \"religious belief\" aren't given in detail (a questionnaire, probably), so it may be interesting to see the actual article when it comes out.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LQSGd97EGPdG2MN7i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "15629", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T14:31:26.552Z", "modifiedAt": null, "url": null, "title": "The Problem of Thinking Too Much [LINK]", "slug": "the-problem-of-thinking-too-much-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yQngZAJGdbGNZoEh9/the-problem-of-thinking-too-much-link", "pageUrlRelative": "/posts/yQngZAJGdbGNZoEh9/the-problem-of-thinking-too-much-link", "linkUrl": "https://www.lesswrong.com/posts/yQngZAJGdbGNZoEh9/the-problem-of-thinking-too-much-link", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Problem%20of%20Thinking%20Too%20Much%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Problem%20of%20Thinking%20Too%20Much%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQngZAJGdbGNZoEh9%2Fthe-problem-of-thinking-too-much-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Problem%20of%20Thinking%20Too%20Much%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQngZAJGdbGNZoEh9%2Fthe-problem-of-thinking-too-much-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQngZAJGdbGNZoEh9%2Fthe-problem-of-thinking-too-much-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p>This was linked to twice recently, once in a Rationality Quotes thread and once in the article about mindfulness meditation, and I thought it deserved its own article.</p>\n<p>It's a transcript of a talk by <a title=\"Wikipedia article\" href=\"http://en.wikipedia.org/wiki/Persi_Diaconis\">Persi Diaconis</a>, called \"<a title=\"PDF, 466KB\" href=\"http://www-stat.stanford.edu/~cgates/PERSI/papers/thinking.pdf\">The problem of thinking too much</a>\". The general theme is more or less what you'd expect from the title: often our explicit models of things are wrong enough that trying to think them through rationally gives worse results than (e.g.) just guessing. There are some nice examples in it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yQngZAJGdbGNZoEh9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "15630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T15:26:02.990Z", "modifiedAt": null, "url": null, "title": "Meetup : Dallas - Fort Worth Less Wrong Meetup 4/29/12", "slug": "meetup-dallas-fort-worth-less-wrong-meetup-4-29-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.254Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jumandtonic", "createdAt": "2012-04-26T03:09:35.594Z", "isAdmin": false, "displayName": "jumandtonic"}, "userId": "2EmJ3AN5jKXHnvYpP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xzC8e9npGizMkZFwi/meetup-dallas-fort-worth-less-wrong-meetup-4-29-12", "pageUrlRelative": "/posts/xzC8e9npGizMkZFwi/meetup-dallas-fort-worth-less-wrong-meetup-4-29-12", "linkUrl": "https://www.lesswrong.com/posts/xzC8e9npGizMkZFwi/meetup-dallas-fort-worth-less-wrong-meetup-4-29-12", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%204%2F29%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%204%2F29%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxzC8e9npGizMkZFwi%2Fmeetup-dallas-fort-worth-less-wrong-meetup-4-29-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%204%2F29%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxzC8e9npGizMkZFwi%2Fmeetup-dallas-fort-worth-less-wrong-meetup-4-29-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxzC8e9npGizMkZFwi%2Fmeetup-dallas-fort-worth-less-wrong-meetup-4-29-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/9k\">Dallas - Fort Worth Less Wrong Meetup 4/29/12</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">29 April 2012 01:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">America's Best Coffee, Arlington</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>I apologize for the short notice.&nbsp; The Dallas / Fort Worth metroplex area desperately needs a Less Wrong meetup! Fun fact &ndash; it is the largest metro area in the US that does not have an active meetup group!</p>\n<p><strong>That said - I shall be sitting at America's Best Coffee [3751 Matlock Road, Arlington TX 76105] from 1 PM to 3 PM (at the very least) on Sundays starting Sunday April 29th. Come out and meet some DFW LessWrongers.</strong></p>\n<p>Leading isn't really my thing, but I realize that I can't wait around forever for someone else to get the ball rolling! I am really interested in improving my mind to behave more rationally, especially with respect to instrumental rationality. I've been a self-improvement junkie for a while (Starting Strength, Toastmasters, improv, and PUA for about a year now), but only recently discovered LessWrong. Some of the recent articles sparked me to write this post. I'm working my way through some of the core sequences, slowly but surely.</p>\n<p>I'm looking forward to meeting you folks. Message me if you plan on going so we can trade contact info beforehand.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/9k\">Dallas - Fort Worth Less Wrong Meetup 4/29/12</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xzC8e9npGizMkZFwi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.92002826933609e-07, "legacy": true, "legacyId": "15631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_4_29_12\">Discussion article for the meetup : <a href=\"/meetups/9k\">Dallas - Fort Worth Less Wrong Meetup 4/29/12</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">29 April 2012 01:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">America's Best Coffee, Arlington</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>I apologize for the short notice.&nbsp; The Dallas / Fort Worth metroplex area desperately needs a Less Wrong meetup! Fun fact \u2013 it is the largest metro area in the US that does not have an active meetup group!</p>\n<p><strong id=\"That_said___I_shall_be_sitting_at_America_s_Best_Coffee__3751_Matlock_Road__Arlington_TX_76105__from_1_PM_to_3_PM__at_the_very_least__on_Sundays_starting_Sunday_April_29th__Come_out_and_meet_some_DFW_LessWrongers_\">That said - I shall be sitting at America's Best Coffee [3751 Matlock Road, Arlington TX 76105] from 1 PM to 3 PM (at the very least) on Sundays starting Sunday April 29th. Come out and meet some DFW LessWrongers.</strong></p>\n<p>Leading isn't really my thing, but I realize that I can't wait around forever for someone else to get the ball rolling! I am really interested in improving my mind to behave more rationally, especially with respect to instrumental rationality. I've been a self-improvement junkie for a while (Starting Strength, Toastmasters, improv, and PUA for about a year now), but only recently discovered LessWrong. Some of the recent articles sparked me to write this post. I'm working my way through some of the core sequences, slowly but surely.</p>\n<p>I'm looking forward to meeting you folks. Message me if you plan on going so we can trade contact info beforehand.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_4_29_121\">Discussion article for the meetup : <a href=\"/meetups/9k\">Dallas - Fort Worth Less Wrong Meetup 4/29/12</a></h2>", "sections": [{"title": "Discussion article for the meetup : Dallas - Fort Worth Less Wrong Meetup 4/29/12", "anchor": "Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_4_29_12", "level": 1}, {"title": "That said - I shall be sitting at America's Best Coffee [3751 Matlock Road, Arlington TX 76105] from 1 PM to 3 PM (at the very least) on Sundays starting Sunday April 29th. Come out and meet some DFW LessWrongers.", "anchor": "That_said___I_shall_be_sitting_at_America_s_Best_Coffee__3751_Matlock_Road__Arlington_TX_76105__from_1_PM_to_3_PM__at_the_very_least__on_Sundays_starting_Sunday_April_29th__Come_out_and_meet_some_DFW_LessWrongers_", "level": 2}, {"title": "Discussion article for the meetup : Dallas - Fort Worth Less Wrong Meetup 4/29/12", "anchor": "Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_4_29_121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T17:19:05.928Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Budapest, Fort Lauderdale, Philadelphia, Pittsburgh, Rome, Sao Paulo, Tel Aviv", "slug": "weekly-lw-meetups-budapest-fort-lauderdale-philadelphia", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e7XegmBCuQeA6S7CQ/weekly-lw-meetups-budapest-fort-lauderdale-philadelphia", "pageUrlRelative": "/posts/e7XegmBCuQeA6S7CQ/weekly-lw-meetups-budapest-fort-lauderdale-philadelphia", "linkUrl": "https://www.lesswrong.com/posts/e7XegmBCuQeA6S7CQ/weekly-lw-meetups-budapest-fort-lauderdale-philadelphia", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Budapest%2C%20Fort%20Lauderdale%2C%20Philadelphia%2C%20Pittsburgh%2C%20Rome%2C%20Sao%20Paulo%2C%20Tel%20Aviv&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Budapest%2C%20Fort%20Lauderdale%2C%20Philadelphia%2C%20Pittsburgh%2C%20Rome%2C%20Sao%20Paulo%2C%20Tel%20Aviv%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe7XegmBCuQeA6S7CQ%2Fweekly-lw-meetups-budapest-fort-lauderdale-philadelphia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Budapest%2C%20Fort%20Lauderdale%2C%20Philadelphia%2C%20Pittsburgh%2C%20Rome%2C%20Sao%20Paulo%2C%20Tel%20Aviv%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe7XegmBCuQeA6S7CQ%2Fweekly-lw-meetups-budapest-fort-lauderdale-philadelphia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe7XegmBCuQeA6S7CQ%2Fweekly-lw-meetups-budapest-fort-lauderdale-philadelphia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 493, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\r\n<ul>\r\n<li><a href=\"/meetups/8z\">Pittsburgh - Presentation on Anthropics:&nbsp;<span class=\"date\">20 April 2012 06:00PM</span></a></li>\r\n<li><a href=\"/meetups/96\">Tel Aviv, Israel:&nbsp;<span class=\"date\">21 April 2012 07:00PM</span></a></li>\r\n<li><a href=\"/meetups/92\">Budapest Meetup:&nbsp;<span class=\"date\">21 April 2012 05:30PM</span></a></li>\r\n<li><a href=\"/meetups/8j\">Rome LessWrong Meetup:&nbsp;<span class=\"date\">21 April 2012 07:00PM</span></a></li>\r\n<li><a href=\"/meetups/91\">Fort Lauderdale:&nbsp;<span class=\"date\">21 April 2012 06:00PM</span></a></li>\r\n<li><a href=\"/meetups/95\">S&atilde;o Paulo Meet Up 2:&nbsp;<span class=\"date\">23 April 2012 07:30PM</span></a></li>\r\n<li><a href=\"/meetups/99\">Philadelphia Meetup: Against Rationalization:&nbsp;<span class=\"date\">26 April 2012 07:00PM</span></a></li>\r\n<li><a href=\"/meetups/8r\">Longmont Sparkfun Soldering Competition Field Trip:&nbsp;<span class=\"date\">28 April 2012 11:00AM</span></a></li>\r\n<li><a href=\"/meetups/8i\">Graz Meetup:&nbsp;<span class=\"date\">28 April 2012 11:21PM</span></a></li>\r\n<li><a href=\"/meetups/97\">First Copenhagen meetup:&nbsp;<span class=\"date\">29 April 2012 05:00PM</span></a></li>\r\n</ul>\r\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\r\n<ul>\r\n<li><a href=\"/meetups/93\">Cambridge, MA First Sunday Meetup:&nbsp;<span class=\"date\">06 May 2012 02:00PM</span></a></li>\r\n<li><a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup:&nbsp;<span class=\"date\">20 May 2012 02:20PM</span></a></li>\r\n</ul>\r\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\r\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\r\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\r\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\r\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\r\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\r\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\r\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\r\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e7XegmBCuQeA6S7CQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.920513425027679e-07, "legacy": true, "legacyId": "15434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T19:42:12.400Z", "modifiedAt": null, "url": null, "title": "Survey of older folks as data about one's future values and preferences?", "slug": "survey-of-older-folks-as-data-about-one-s-future-values-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.823Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bart119", "createdAt": "2012-04-19T14:46:10.499Z", "isAdmin": false, "displayName": "Bart119"}, "userId": "JQyd5QFQyrRxdo9G4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b3w68jCmCaeJTu5NG/survey-of-older-folks-as-data-about-one-s-future-values-and", "pageUrlRelative": "/posts/b3w68jCmCaeJTu5NG/survey-of-older-folks-as-data-about-one-s-future-values-and", "linkUrl": "https://www.lesswrong.com/posts/b3w68jCmCaeJTu5NG/survey-of-older-folks-as-data-about-one-s-future-values-and", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Survey%20of%20older%20folks%20as%20data%20about%20one's%20future%20values%20and%20preferences%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASurvey%20of%20older%20folks%20as%20data%20about%20one's%20future%20values%20and%20preferences%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3w68jCmCaeJTu5NG%2Fsurvey-of-older-folks-as-data-about-one-s-future-values-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Survey%20of%20older%20folks%20as%20data%20about%20one's%20future%20values%20and%20preferences%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3w68jCmCaeJTu5NG%2Fsurvey-of-older-folks-as-data-about-one-s-future-values-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb3w68jCmCaeJTu5NG%2Fsurvey-of-older-folks-as-data-about-one-s-future-values-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<p>One thing that struck me in the 2011 survey was that 90% of LW respondents were under age 38. I'm 57 myself. It seems that often rationality in planning our lives depends on estimates of what values and utility functions we will hold in the future. Has anyone looked systematically at what projected older versions of themselves would think, based on what relevant groups of existing older folks think?</p>\r\n<p>\"You'll understand when you're older\" is an annoying form of argument. Arguably there's some grain of truth there when a 7-year-old tells you that sex is disgusting and he or she will never ever think it's anything but incredibly gross. But you could explain hormonal changes that as a matter of empirical fact change opinions on that subject in the vast majority of cases.&nbsp;I can't think of anything that dramatic that distinguishes 60-year-olds or 80-year-olds from 20-year-olds.</p>\r\n<p>My dim recollection of studies is&nbsp;that on the whole&nbsp;as people age they tend to be less idealistic, more resigned to society the way it is rather than how it might be, and more constrained by realities of politics and economics (for starters).</p>\r\n<p>I don't presume to offer anything&nbsp;in this regard based on my age, and in any case I'm only a single person (a nihilist when pressed, but one who finds himself happier pretending not to be and working sporadically for rationality, truth, justice, love, and all that good stuff).</p>\r\n<p>When I read of cryonics, what comes to my mind is the escalating costs of health care and (as I see it) the need to curb the development of expensive life-extending medical procedures. Cryonics sounds instead like an extremely expensive procedure. Maybe no one is suggesting it be covered by health insurance, and it's just an option that some people pay out of pocket for. Even so, the \"health care is a right, not a privilege\" sentiment will mean that if it was shown to work, everyone would want it, and (in my estimation) society would go completely haywire in an unpleasant way.</p>\r\n<p>Now,&nbsp;the substance of the above has probably been discussed elsewhere at length; I raise it is an example because when I was 21 I would have thought of it very differently than I do now.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b3w68jCmCaeJTu5NG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 30, "extendedScore": null, "score": 8.921127647132863e-07, "legacy": true, "legacyId": "15634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-27T22:29:26.976Z", "modifiedAt": null, "url": null, "title": "Thinking in a foreign language reduces risk aversion bias?", "slug": "thinking-in-a-foreign-language-reduces-risk-aversion-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orbenn", "createdAt": "2011-03-01T15:44:40.098Z", "isAdmin": false, "displayName": "orbenn"}, "userId": "TPArz3kqbeKS2Jx8Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xAahkgfRDm5pRzaKA/thinking-in-a-foreign-language-reduces-risk-aversion-bias", "pageUrlRelative": "/posts/xAahkgfRDm5pRzaKA/thinking-in-a-foreign-language-reduces-risk-aversion-bias", "linkUrl": "https://www.lesswrong.com/posts/xAahkgfRDm5pRzaKA/thinking-in-a-foreign-language-reduces-risk-aversion-bias", "postedAtFormatted": "Friday, April 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thinking%20in%20a%20foreign%20language%20reduces%20risk%20aversion%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThinking%20in%20a%20foreign%20language%20reduces%20risk%20aversion%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxAahkgfRDm5pRzaKA%2Fthinking-in-a-foreign-language-reduces-risk-aversion-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thinking%20in%20a%20foreign%20language%20reduces%20risk%20aversion%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxAahkgfRDm5pRzaKA%2Fthinking-in-a-foreign-language-reduces-risk-aversion-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxAahkgfRDm5pRzaKA%2Fthinking-in-a-foreign-language-reduces-risk-aversion-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>Take look at this article that was just on Wired:<br /><a href=\"http://www.wired.com/wiredscience/2012/04/language-and-bias/\">http://www.wired.com/wiredscience/2012/04/language-and-bias/</a></p>\n<p><br />Guess I'll have to start thinking in Spanish more often...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xAahkgfRDm5pRzaKA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -13, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "15635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T01:21:45.465Z", "modifiedAt": null, "url": null, "title": "Robot Programmed To Love Goes Too Far (link)", "slug": "robot-programmed-to-love-goes-too-far-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yhda6cM4K9XwHZ4p7/robot-programmed-to-love-goes-too-far-link", "pageUrlRelative": "/posts/yhda6cM4K9XwHZ4p7/robot-programmed-to-love-goes-too-far-link", "linkUrl": "https://www.lesswrong.com/posts/yhda6cM4K9XwHZ4p7/robot-programmed-to-love-goes-too-far-link", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Robot%20Programmed%20To%20Love%20Goes%20Too%20Far%20(link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARobot%20Programmed%20To%20Love%20Goes%20Too%20Far%20(link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyhda6cM4K9XwHZ4p7%2Frobot-programmed-to-love-goes-too-far-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Robot%20Programmed%20To%20Love%20Goes%20Too%20Far%20(link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyhda6cM4K9XwHZ4p7%2Frobot-programmed-to-love-goes-too-far-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyhda6cM4K9XwHZ4p7%2Frobot-programmed-to-love-goes-too-far-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p><a href=\"http://www.muckflash.com/?p=200\">http://www.muckflash.com/?p=200</a></p>\n<p>Might be a nice story to point out to people who think \"friendly\" is easy.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yhda6cM4K9XwHZ4p7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -8, "extendedScore": null, "score": 8.922585306797965e-07, "legacy": true, "legacyId": "15642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T01:40:02.931Z", "modifiedAt": null, "url": null, "title": "LINK: Human Bio-engineering and Coherent Extrapolated Volition", "slug": "link-human-bio-engineering-and-coherent-extrapolated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6xqaco7rqaDm2LpvC/link-human-bio-engineering-and-coherent-extrapolated", "pageUrlRelative": "/posts/6xqaco7rqaDm2LpvC/link-human-bio-engineering-and-coherent-extrapolated", "linkUrl": "https://www.lesswrong.com/posts/6xqaco7rqaDm2LpvC/link-human-bio-engineering-and-coherent-extrapolated", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Human%20Bio-engineering%20and%20Coherent%20Extrapolated%20Volition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Human%20Bio-engineering%20and%20Coherent%20Extrapolated%20Volition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6xqaco7rqaDm2LpvC%2Flink-human-bio-engineering-and-coherent-extrapolated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Human%20Bio-engineering%20and%20Coherent%20Extrapolated%20Volition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6xqaco7rqaDm2LpvC%2Flink-human-bio-engineering-and-coherent-extrapolated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6xqaco7rqaDm2LpvC%2Flink-human-bio-engineering-and-coherent-extrapolated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>This <a href=\"http://www.theatlantic.com/technology/archive/2012/03/how-engineering-the-human-body-could-combat-climate-change/253981/\">article</a> has some interesting commentary on how humans might modify themselves to combat global warming, including the use of drugs that would increase empathy, increase willpower, or increase aversion to meat. The interviewer points out that such techniques could involve implanting non-native beliefs in people's minds, and the researcher responds that any such beliefs would be essentially built up out of the person's existing desires and wishes -- the analysis is remarkably similar to the analysis Eliezer gives in explaining Coherent Extrapolated Volition.</p>\n<p>No hate mail about how meat does or doesn't cause global warming, please -- the interesting bit is the analysis of CEV, not the analysis of climate change.</p>\n<p><a href=\"http://www.theatlantic.com/technology/archive/2012/03/how-engineering-the-human-body-could-combat-climate-change/253981/\">http://www.theatlantic.com/technology/archive/2012/03/how-engineering-the-human-body-could-combat-climate-change/253981/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6xqaco7rqaDm2LpvC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 8.922663840513253e-07, "legacy": true, "legacyId": "15643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T02:01:33.295Z", "modifiedAt": null, "url": null, "title": "Quotes on Existential Risk", "slug": "quotes-on-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DCfsxmSNihtxWM2Fi/quotes-on-existential-risk", "pageUrlRelative": "/posts/DCfsxmSNihtxWM2Fi/quotes-on-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/DCfsxmSNihtxWM2Fi/quotes-on-existential-risk", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quotes%20on%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuotes%20on%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDCfsxmSNihtxWM2Fi%2Fquotes-on-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quotes%20on%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDCfsxmSNihtxWM2Fi%2Fquotes-on-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDCfsxmSNihtxWM2Fi%2Fquotes-on-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>Similar to the <a href=\"/lw/8a9/agi_quotes/\">AGI Quotes</a> thread and the monthly <a href=\"/tag/quotes/\">Rationality Quotes</a> threads, this is a thread for memorable quotes about <a href=\"http://www.existential-risk.org/\">existential risk</a>.</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n</ul>\n<div>But please feel free to quote posts and comments from Less Wrong or Overcoming Bias.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DCfsxmSNihtxWM2Fi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 8.922756179086349e-07, "legacy": true, "legacyId": "15647", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FTvd9uCQMEMvpLRyS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T04:18:12.916Z", "modifiedAt": null, "url": null, "title": "Do people think Less Wrong rationality is parochial?", "slug": "do-people-think-less-wrong-rationality-is-parochial", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:07.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wfD6EMTwm4iBCa8jE/do-people-think-less-wrong-rationality-is-parochial", "pageUrlRelative": "/posts/wfD6EMTwm4iBCa8jE/do-people-think-less-wrong-rationality-is-parochial", "linkUrl": "https://www.lesswrong.com/posts/wfD6EMTwm4iBCa8jE/do-people-think-less-wrong-rationality-is-parochial", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20people%20think%20Less%20Wrong%20rationality%20is%20parochial%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20people%20think%20Less%20Wrong%20rationality%20is%20parochial%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfD6EMTwm4iBCa8jE%2Fdo-people-think-less-wrong-rationality-is-parochial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20people%20think%20Less%20Wrong%20rationality%20is%20parochial%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfD6EMTwm4iBCa8jE%2Fdo-people-think-less-wrong-rationality-is-parochial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfD6EMTwm4iBCa8jE%2Fdo-people-think-less-wrong-rationality-is-parochial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 372, "htmlBody": "<p>I've spent so much time in the cogsci literature that I know the LW approach to rationality is basically the mainstream cogsci approach to rationality (plus some extra stuff about, e.g., <a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">language</a>), but... do other people not know this? Do people one step removed from LessWrong &mdash; say, in the 'atheist' and 'skeptic' communities &mdash; not know this? If this is causing credibility problems in our broader community, it'd be <a href=\"/r/lesswrong/lw/7e5/the_cognitive_science_of_rationality/\">relatively easy</a> to show people that Less Wrong is not, in fact, a \"fringe\" approach to rationality.</p>\n<p>For example, here's Oaksford &amp; Chater in the second chapter to the (excellent) new <em><a href=\"http://www.amazon.com/Handbook-Thinking-Reasoning-Library-Psychology/dp/0199734682/\">Oxford Handbook of Thinking and Reasoning</a></em>, the one on normative systems of rationality:</p>\n<p>\n<p style=\"padding-left: 30px;\">Is it meaningful to attempt to develop a general theory of rationality <em>at all</em>? We might tentatively suggest that it is a prima facie sign of irrationality to believe in alien abduction, or to will a sports team to win in order to increase their chance of victory. But these views or actions might be entirely rational, given suitably nonstandard background beliefs about other alien activity and the general efficacy of psychic powers. Irrationality may, though, be ascribed if there is a <em>clash</em> between a particular belief or behavior and such background assumptions. Thus, a thorough-going physicalist may, perhaps, be accused of irrationality if she simultaneously believes in psychic powers. A theory of rationality cannot, therefore, be viewed as clarifying either what people should believe or how people should act&mdash;but it can determine whether beliefs and behaviors are compatible. Similarly, a theory of rational choice cannot determine whether it is rational to smoke or to exercise daily; but it might clarify whether a particular choice is compatible with other beliefs and choices.</p>\n<p style=\"padding-left: 30px;\">From this viewpoint, normative theories can be viewed as clarifying conditions of consistency&hellip; <em>Logic</em> can be viewed as studying the notion of consistency over <em>beliefs</em>. <em>Probability</em>&hellip; studies consistency over <em>degrees of belief</em>. <em>Rational choice</em> theory studies the consistency of beliefs and values with <em>choices</em>.</p>\n<p>They go on to clarify that by probability they mean Bayesian probability theory, and by rational choice theory they mean Bayesian decision theory. You'll get the same account in the textbooks on the cogsci of rationality, e.g. <em><a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433/\">Thinking and Deciding</a></em>&nbsp;or <em><a href=\"http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/\">Rational Choice in an Uncertain World</a></em>.</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wfD6EMTwm4iBCa8jE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 51, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "15652", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 197, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xLm9mgJRPvmPGpo7Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T20:33:19.923Z", "modifiedAt": null, "url": null, "title": "Timeless Physics Question", "slug": "timeless-physics-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.566Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4npuhoW6PjvNPCgDS/timeless-physics-question", "pageUrlRelative": "/posts/4npuhoW6PjvNPCgDS/timeless-physics-question", "linkUrl": "https://www.lesswrong.com/posts/4npuhoW6PjvNPCgDS/timeless-physics-question", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timeless%20Physics%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimeless%20Physics%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4npuhoW6PjvNPCgDS%2Ftimeless-physics-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timeless%20Physics%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4npuhoW6PjvNPCgDS%2Ftimeless-physics-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4npuhoW6PjvNPCgDS%2Ftimeless-physics-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>Timeless physics is what you end up with if you take MWI, assume the universe is a standing wave, and remove the extraneous variables. From what I understand, for the most part you can take a standing wave and add a time-reversed version, you end up with a standing wave that only uses real numbers. The problem with this is that the universe isn't quite time symmetric.</p>\n<p>If I ignore that complex numbers ever were used in quantum physics, it seems unlikely that complex numbers is the correct solution. Is there another one? Should I be reversing charge and parity as well as time when I make the standing real-only wave?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4npuhoW6PjvNPCgDS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -6, "extendedScore": null, "score": 8.927532042340056e-07, "legacy": true, "legacyId": "15665", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T21:59:41.777Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ezwade3P7dwAtMtpZ/meetup-brussels-meetup-2", "pageUrlRelative": "/posts/ezwade3P7dwAtMtpZ/meetup-brussels-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/ezwade3P7dwAtMtpZ/meetup-brussels-meetup-2", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fezwade3P7dwAtMtpZ%2Fmeetup-brussels-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fezwade3P7dwAtMtpZ%2Fmeetup-brussels-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fezwade3P7dwAtMtpZ%2Fmeetup-brussels-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9l'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 May 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>During this meetup we will use puzzles and games to improve creative thinking, our models of other people and rationality in general.\nWe'll meet in the lobby as usual, should you arrive late we will have moved to the cafeteria (just go straight once you're past the entrance, you can't miss it) If the weather is particularly nice we might even move to the nearby Leopold Park later. If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9l'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ezwade3P7dwAtMtpZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 8.92790322667407e-07, "legacy": true, "legacyId": "15666", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/9l\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 May 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>During this meetup we will use puzzles and games to improve creative thinking, our models of other people and rationality in general.\nWe'll meet in the lobby as usual, should you arrive late we will have moved to the cafeteria (just go straight once you're past the entrance, you can't miss it) If the weather is particularly nice we might even move to the nearby Leopold Park later. If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/9l\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T22:11:57.676Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Decoherence is Falsifiable and Testable", "slug": "seq-rerun-decoherence-is-falsifiable-and-testable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ic7FpaGjTPzvQTQ2j/seq-rerun-decoherence-is-falsifiable-and-testable", "pageUrlRelative": "/posts/ic7FpaGjTPzvQTQ2j/seq-rerun-decoherence-is-falsifiable-and-testable", "linkUrl": "https://www.lesswrong.com/posts/ic7FpaGjTPzvQTQ2j/seq-rerun-decoherence-is-falsifiable-and-testable", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Decoherence%20is%20Falsifiable%20and%20Testable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Decoherence%20is%20Falsifiable%20and%20Testable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fic7FpaGjTPzvQTQ2j%2Fseq-rerun-decoherence-is-falsifiable-and-testable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Decoherence%20is%20Falsifiable%20and%20Testable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fic7FpaGjTPzvQTQ2j%2Fseq-rerun-decoherence-is-falsifiable-and-testable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fic7FpaGjTPzvQTQ2j%2Fseq-rerun-decoherence-is-falsifiable-and-testable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<p>Today's post, <a href=\"/lw/q4/decoherence_is_falsifiable_and_testable/\">Decoherence is Falsifiable and Testable</a> was originally published on 07 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>(Note: Designed to be standalone readable.) An epistle to the physicists. To probability theorists, words like \"simple\", \"falsifiable\", and \"testable\" have exact mathematical meanings, which are there for very strong reasons. The (minority?) faction of physicists who say that many-worlds is \"not falsifiable\" or that it \"violates Occam's Razor\" or that it is \"untestable\", are committing the same kind of mathematical crime as non-physicists who invent their own theories of gravity that go as inverse-cube. This is one of the reasons why I, a non-physicist, dared to talk about physics - because I saw (some!) physicists using probability theory in a way that was simply wrong. Not just criticizable, but outright mathematically wrong: 2 + 2 = 3.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c1p/seq_rerun_decoherence_is_simple/\">Decoherence is Simple</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ic7FpaGjTPzvQTQ2j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.92795594230407e-07, "legacy": true, "legacyId": "15667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DFxoaWGEh9ndwtZhk", "nh6ANWLLksJHwAz6c", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-28T23:36:53.833Z", "modifiedAt": null, "url": null, "title": "What's wrong with psychology, anyway?", "slug": "what-s-wrong-with-psychology-anyway", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kepqBSR7bzDScAZJA/what-s-wrong-with-psychology-anyway", "pageUrlRelative": "/posts/kepqBSR7bzDScAZJA/what-s-wrong-with-psychology-anyway", "linkUrl": "https://www.lesswrong.com/posts/kepqBSR7bzDScAZJA/what-s-wrong-with-psychology-anyway", "postedAtFormatted": "Saturday, April 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20wrong%20with%20psychology%2C%20anyway%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20wrong%20with%20psychology%2C%20anyway%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkepqBSR7bzDScAZJA%2Fwhat-s-wrong-with-psychology-anyway%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20wrong%20with%20psychology%2C%20anyway%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkepqBSR7bzDScAZJA%2Fwhat-s-wrong-with-psychology-anyway", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkepqBSR7bzDScAZJA%2Fwhat-s-wrong-with-psychology-anyway", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>Came across this article, published in 1991 but hardly dated:</p>\n<p>David T. Lykken, <em><a href=\"http://cogprints.org/371/3/148.pdf\">What's Wrong With Psychology, Anyway?</a>&nbsp;</em>(PDF, 39 pages)</p>\n<p>Anyone who's interested in psychology as a science might, I think, find it fascinating. Lots of stuff there about rationality-related failures of academic psychology. Several wonderful anecdotes, of which I'll quote one in full that had me laughing out loud --</p>\n<p style=\"padding-left: 30px; \"><em></em></p>\n<blockquote>\n<p>In the 1940s and &rsquo;50s. there was a torrent of interest and research surrounding&nbsp;the debate between the S-R <em>[Stimulus-Response]</em> reinforcement theorists at Yale and Iowa City and the S-S <em>[Stimulus-Stimulus]</em> expectancy theorists headquartered at Berkeley. As is usual in these affairs, the two sides produced not only differing theoretical interpretations but also different empirical findings from their rat laboratories, differences that ultimately led Marshall Jones to wonder if the researchers in Iowa and California might not be working with genetically different animals. Jones obtained samples of rats from the two colonies: and tested them in the simple runway situation. Sure enough, when running time was plotted against trial number, the two strains showed little overlap in performance. The Iowa rats put their heads down and streaked for the goal box, while the Berkeley animals dawdled, retraced, investigated, appeared to be making &ldquo;cognitive maps&rdquo; just as Tolman always said. But by 1965 the torrent of interest in latent-learning had become a backwater and Jones's paper was published obscurely (Jones &amp; Fennel, 1965).</p>\n</blockquote>\n<p>(I came across the reference to the article in the <a href=\"http://news.ycombinator.com/item?id=3888568\">HN discussion</a>&nbsp;about a <a href=\"http://chronicle.com/blogs/percolator/is-psychology-about-to-come-undone/29045\">project</a>, of independent interest, to try and replicate a sample of articles from three reputable journals in psychology in a given year)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "dBPou4ihoQNY4cquv": 1, "vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kepqBSR7bzDScAZJA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 43, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "15668", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T00:38:26.451Z", "modifiedAt": null, "url": null, "title": "Quotes about death from the conventional viewpoint [edited for clarity]", "slug": "quotes-about-death-from-the-conventional-viewpoint-edited", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LxmsZsaFcsuHYBP8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YSCrwpZQsFoxZNgkN/quotes-about-death-from-the-conventional-viewpoint-edited", "pageUrlRelative": "/posts/YSCrwpZQsFoxZNgkN/quotes-about-death-from-the-conventional-viewpoint-edited", "linkUrl": "https://www.lesswrong.com/posts/YSCrwpZQsFoxZNgkN/quotes-about-death-from-the-conventional-viewpoint-edited", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quotes%20about%20death%20from%20the%20conventional%20viewpoint%20%5Bedited%20for%20clarity%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuotes%20about%20death%20from%20the%20conventional%20viewpoint%20%5Bedited%20for%20clarity%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSCrwpZQsFoxZNgkN%2Fquotes-about-death-from-the-conventional-viewpoint-edited%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quotes%20about%20death%20from%20the%20conventional%20viewpoint%20%5Bedited%20for%20clarity%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSCrwpZQsFoxZNgkN%2Fquotes-about-death-from-the-conventional-viewpoint-edited", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSCrwpZQsFoxZNgkN%2Fquotes-about-death-from-the-conventional-viewpoint-edited", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>This may not be the right place for this, but I need quotes about death coming from the orthodox (normative, non-LW) position on death. &nbsp;I'm working on a project that will eventually be at least tangentially LW relevant, and I want to have some good 'pro-death' quotes that I can adapt for usage in the final project. &nbsp;I don't think I really need any quotes from the LW perspective; I plan to paraphrase Yudkowsky and the Sequences as well as Dylan Thomas's poem \"Do not go gentle into that good night\" for the opposing viewpoint.</p>\n<p>I don't want to go into too much detail as to what it is exactly I am working on (if I fail or lose motivation fewer people will be disappointed), but I think that the project will take a maximum of 2 months to complete. &nbsp;This means that it will in all likelihood be complete in 3. &nbsp;More details as progress is made. &nbsp;Thank you in advance.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YSCrwpZQsFoxZNgkN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "15669", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T02:40:54.459Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Quantum Non-Realism", "slug": "seq-rerun-quantum-non-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.488Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LFqyywgTWEXuFdWD2/seq-rerun-quantum-non-realism", "pageUrlRelative": "/posts/LFqyywgTWEXuFdWD2/seq-rerun-quantum-non-realism", "linkUrl": "https://www.lesswrong.com/posts/LFqyywgTWEXuFdWD2/seq-rerun-quantum-non-realism", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Quantum%20Non-Realism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Quantum%20Non-Realism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLFqyywgTWEXuFdWD2%2Fseq-rerun-quantum-non-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Quantum%20Non-Realism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLFqyywgTWEXuFdWD2%2Fseq-rerun-quantum-non-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLFqyywgTWEXuFdWD2%2Fseq-rerun-quantum-non-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>Today's post, <a href=\"/lw/q5/quantum_nonrealism/\">Quantum Non-Realism</a> was originally published on 08 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>\"Shut up and calculate\" is the best approach you can take when none of your theories are very good. But that is not the same as claiming that \"Shut up!\" actually <em>is </em>a theory of physics. Saying \"I don't know what these equations mean, but they seem to work\" is a very different matter from saying: \"These equations definitely don't mean anything, they just work!\"</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c37/seq_rerun_decoherence_is_falsifiable_and_testable/\">Decoherence is Falsifiable and Testable</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LFqyywgTWEXuFdWD2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 8.929112032064211e-07, "legacy": true, "legacyId": "15676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k3823vuarnmL5Pqin", "ic7FpaGjTPzvQTQ2j", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T07:17:52.669Z", "modifiedAt": null, "url": null, "title": "Testing how summary break works", "slug": "testing-how-summary-break-works", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Clarity1992", "createdAt": "2010-12-20T15:00:17.085Z", "isAdmin": false, "displayName": "Clarity1992"}, "userId": "YyKn2drJ3MKKXiam7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PHbS9Eb5oRd6nNAsg/testing-how-summary-break-works", "pageUrlRelative": "/posts/PHbS9Eb5oRd6nNAsg/testing-how-summary-break-works", "linkUrl": "https://www.lesswrong.com/posts/PHbS9Eb5oRd6nNAsg/testing-how-summary-break-works", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Testing%20how%20summary%20break%20works&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATesting%20how%20summary%20break%20works%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHbS9Eb5oRd6nNAsg%2Ftesting-how-summary-break-works%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Testing%20how%20summary%20break%20works%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHbS9Eb5oRd6nNAsg%2Ftesting-how-summary-break-works", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHbS9Eb5oRd6nNAsg%2Ftesting-how-summary-break-works", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p>This should appear as a new post for a few seconds and then be deleted. Ignore please!</p>\r\n<p>&nbsp;</p>\r\n<p><a id=\"more\"></a></p>\r\n<p>What goes here is under the summary break.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PHbS9Eb5oRd6nNAsg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "15688", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T09:37:46.315Z", "modifiedAt": null, "url": null, "title": "Stanovich on CEV", "slug": "stanovich-on-cev", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.257Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GzYu2acxWL6pZyzyc/stanovich-on-cev", "pageUrlRelative": "/posts/GzYu2acxWL6pZyzyc/stanovich-on-cev", "linkUrl": "https://www.lesswrong.com/posts/GzYu2acxWL6pZyzyc/stanovich-on-cev", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stanovich%20on%20CEV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStanovich%20on%20CEV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzYu2acxWL6pZyzyc%2Fstanovich-on-cev%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stanovich%20on%20CEV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzYu2acxWL6pZyzyc%2Fstanovich-on-cev", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGzYu2acxWL6pZyzyc%2Fstanovich-on-cev", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 970, "htmlBody": "<p><a href=\"http://web.mac.com/kstanovich/Site/Home.html\">Keith Stanovich</a> is a leading expert on the cogsci of rationality, but he also also written on a problem related to CEV, that of the \"rational integration\" of our preferences. Here he is on pages 81-86 of <em><a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Rationality and the Reflective Mind</a></em>&nbsp;(currently my single favorite book on rationality, out of the dozens I've read):</p>\n<blockquote>\n<p>All multiple-process models of mind capture a phenomenal aspect of human decision making that is of profound importance &mdash; that humans often feel alienated from their choices. We display what folk psychology and philosophers term <em>weakness of will</em>. For example, we continue to smoke when we know that it is a harmful habit. We order a sweet after a large meal, merely an hour after pledging to ourselves that we would not. In fact, we display alienation from our responses even in situations that do not involve weakness of will &mdash; we find ourselves recoiling from the sight of a disfigured person even after a lifetime of dedication to diversity and inclusion.</p>\n<p>This feeling of alienation &mdash; although emotionally discomfiting when it occurs &mdash; is actually a reflection of a unique aspect of human cognition: the use of Type 2 metarepresentational abilities to enable a cognitive critique of our beliefs and our desires. Beliefs about how well we are forming beliefs become possible because of such metarepresentation, as does the ability to evaluate one's desires &mdash; to desire to desire differently...</p>\n<p>...There is a philosophical literature on the notion of higher-order evaluation of desires... For example, in a classic paper on second-order desires, Frankfurt (1971) speculated that only humans have such metarepresentational states. He evocatively termed creatures without second-order desires (other animals, human babies) <em>wantons</em>... A wanton simply does not reflect on his/her goals. Wantons want &mdash; but they do not <em>care</em>&nbsp;what they want.</p>\n<p>Nonwantons, however, can represent a model of an idealized preference structure &mdash; perhaps, for example, a model based on a superordinate judgment of long-term lifespan considerations... So a human can say: I would prefer to prefer not to smoke. This second-order preference can then become a motivational competitor to the first-order preference. At the level of second-order preferences, I prefer to prefer to not smoke; nevertheless, as a first-order preference, I prefer to smoke. The resulting conflict signals that I lack what Nozick (1993) terms <em>rational integration</em>&nbsp;in my preference structures. Such a mismatched first-/second-order preference structure is one reason why humans are often less rational than bees in an axiomatic sense (see Stanovich 2004, pp. 243-247). This is because the struggle to achieve rational integration can destabilize first-order preferences in ways that make them more prone to the context effects that lead to the violation of the basic axioms of utility theory (see Lee, Amir, &amp; Ariely 2009).</p>\n<p>The struggle for rational integration is also what contributes to the feeling of alienation that people in the modern world often feel when contemplating the choices that they have made. People easily detect when their high-order preferences conflict with the choices actually made.</p>\n<p>Of course, there is no limit to the hierarchy of higher-order desires that might be constructed. But the representational abilities of humans may set some limits &mdash; certainly three levels above seems a realistic limit for most people in the nonsocial domain (Dworking 1988). However, third-order judgments can be called upon to to help achieve rational integration at lower levels. So, for example, imagine that John is a smoker. He might realize the following when he probes his feelings: He prefers his preference to prefer not to smoke over his preference for smoking.</p>\n<p>We might in this case say that John's third-order judgment has ratified his second-order evaluation. Presumably this ratification of his second-order judgment adds to the cognitive pressure to change the first-order preference by taking behavioral measures that will make change more likely (entering a smoking secession program, consulting his physician, staying out of smoky bars, etc.).</p>\n<p>On the other hand, a third-order judgment might undermine the second-order preference by failing to ratify it: John might prefer to smoke more than he prefers his preference to prefer not to smoke.</p>\n<p>In this case, although John wishes he did not want to smoke, the preference for this preference is not as strong as his preference for smoking itself. We might suspect that this third-order judgment might not only prevent John from taking strong behavioral steps to rid himself of his addiction, but that over time it might erode his conviction in his second-order preference itself, thus bringing rational integration to all three levels.</p>\n<p>Typically, philosophers have tended to bias their analyses toward the highest level desire that is constructed &mdash; privileging the highest point in the regress of higher-order evaluations, using that as the foundation, and defining it as the true self. Modern cognitive science would suggest instead a Neurathian project in which no level of analysis is uniquely privileged. Philosopher Otto Neurath... employed the metaphor of a boat having some rotten planks. The best way to repair the planks would be to bring the boat ashore, stand on firm ground, and replace the planks. But what if the boat could not be brought ashore? Actually, the boat could still be repaired but at some risk. We could repair the planks at sea by standing on some of the planks while repairing others. The project could work &mdash; we could repair the boat without being on the firm foundation of ground. The Neurathian project is not guaranteed, however, because we might choose to stand on a rotten plank. For example, nothing in Frankfurt's (1971) notion of higher-order desires guarantees against higher-order judgments being infected by memes... that are personally damaging.</p>\n</blockquote>\n<p>Also see: <em><a href=\"/lw/8gc/stanovich_the_robots_rebellion_review/\">The Robot's Rebellion</a></em>, <a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/TandR08.pdf\">Higher order preferences the master rationality motive</a>, <a href=\"/lw/fv/wanting_to_want/\">Wanting to Want</a>,&nbsp;<a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">The Human's Hidden Utility Function (Maybe)</a>, <a href=\"http://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/\">Indirect Normativity</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GzYu2acxWL6pZyzyc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 8.930904483239999e-07, "legacy": true, "legacyId": "15689", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PsvzbYxasvPPLBCZC", "azdqDRbcw3EkrnHNw", "fa5o2tg9EfJE77jEQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T13:07:52.330Z", "modifiedAt": null, "url": null, "title": "Losing Your Religion: Analytic Thinking Can Undermine Belief", "slug": "losing-your-religion-analytic-thinking-can-undermine-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k7jg7XZNFQWz9JA5j/losing-your-religion-analytic-thinking-can-undermine-belief", "pageUrlRelative": "/posts/k7jg7XZNFQWz9JA5j/losing-your-religion-analytic-thinking-can-undermine-belief", "linkUrl": "https://www.lesswrong.com/posts/k7jg7XZNFQWz9JA5j/losing-your-religion-analytic-thinking-can-undermine-belief", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Losing%20Your%20Religion%3A%20Analytic%20Thinking%20Can%20Undermine%20Belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALosing%20Your%20Religion%3A%20Analytic%20Thinking%20Can%20Undermine%20Belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk7jg7XZNFQWz9JA5j%2Flosing-your-religion-analytic-thinking-can-undermine-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Losing%20Your%20Religion%3A%20Analytic%20Thinking%20Can%20Undermine%20Belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk7jg7XZNFQWz9JA5j%2Flosing-your-religion-analytic-thinking-can-undermine-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk7jg7XZNFQWz9JA5j%2Flosing-your-religion-analytic-thinking-can-undermine-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>It appears that&nbsp;<a href=\"http://www.scientificamerican.com/article.cfm?id=losing-your-religion-analytic-thinking-can-undermine-belief\">Scientific American</a> have found out about <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>. Plus one successful prediction by Mr. Yudkowsky, I presume.&nbsp;Since I couldn't bring myself to quote any particular bit of the article, deeming all of them to be of substance and interest, I urge the reader to actually follow the link and read for themselves.</p>\n<p>To summarize: the conclusion many experiments, performed in many different ways, reach, is that, whenever you get people to actually stop and think of what they are saying, rather than go on intuition or what they think the socially \"normal\" thing is, their declared&nbsp;levels&nbsp;of religious belief seem to always drop dramatically, on average. This isn't&nbsp;achieved&nbsp;by anything complicated like teaching them about biases, but by tricks as simple and cheap as making the font of the answer sheet hard to read, or showing them a picture of Rodin's The Thinker.</p>\n<p>I shall also link you to the TVTropes discussion on that topic, since they're a little more mainstream than us, and we shouldn't <a href=\"http://tvtropes.org/pmwiki/posts.php?discussion=13356456280A59092200&amp;page=1\">lose sight of the world around us and fall to parochialsim.</a>&nbsp;Links to other communities discussing the article (it's bound to make quite a splash) are very welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k7jg7XZNFQWz9JA5j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 5, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "15690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T15:01:49.963Z", "modifiedAt": null, "url": null, "title": "[LINK] Get paid to train your rationality (update)", "slug": "link-get-paid-to-train-your-rationality-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rZAp7LSNqH9fHPY7m/link-get-paid-to-train-your-rationality-update", "pageUrlRelative": "/posts/rZAp7LSNqH9fHPY7m/link-get-paid-to-train-your-rationality-update", "linkUrl": "https://www.lesswrong.com/posts/rZAp7LSNqH9fHPY7m/link-get-paid-to-train-your-rationality-update", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Get%20paid%20to%20train%20your%20rationality%20(update)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Get%20paid%20to%20train%20your%20rationality%20(update)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZAp7LSNqH9fHPY7m%2Flink-get-paid-to-train-your-rationality-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Get%20paid%20to%20train%20your%20rationality%20(update)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZAp7LSNqH9fHPY7m%2Flink-get-paid-to-train-your-rationality-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZAp7LSNqH9fHPY7m%2Flink-get-paid-to-train-your-rationality-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>Previous: http://lesswrong.com/lw/6ya/link_get_paid_to_train_your_rationality/</p>\n<p>The IARPA-run forecasting contest remains ongoing. Season 1 has largely finished up, and groups are preparing for season 2. Season 1 participants like myself get first dibs, but http://goodjudgmentproject.com/ has announced in emails they have spots open for first-time participants! I assume the other groups may have openings as well.</p>\n<p>I personally found the tournament a source of predictions to stick on PB.com and I even did pretty well in GJP. (When I checked a few weeks ago, I was ranked 28 of 203 in my experimental group.) I haven't been paid my honorarium yet, though.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rZAp7LSNqH9fHPY7m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 8.932298334343286e-07, "legacy": true, "legacyId": "15691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T15:17:57.656Z", "modifiedAt": null, "url": null, "title": "Pre-commitment and meta at the Cambridge UK meetup", "slug": "pre-commitment-and-meta-at-the-cambridge-uk-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rebellionkid", "createdAt": "2011-06-20T09:26:46.768Z", "isAdmin": false, "displayName": "rebellionkid"}, "userId": "ygYCk3eXnJwt6p3o4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MMMxNBrGy9di5gaRD/pre-commitment-and-meta-at-the-cambridge-uk-meetup", "pageUrlRelative": "/posts/MMMxNBrGy9di5gaRD/pre-commitment-and-meta-at-the-cambridge-uk-meetup", "linkUrl": "https://www.lesswrong.com/posts/MMMxNBrGy9di5gaRD/pre-commitment-and-meta-at-the-cambridge-uk-meetup", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pre-commitment%20and%20meta%20at%20the%20Cambridge%20UK%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APre-commitment%20and%20meta%20at%20the%20Cambridge%20UK%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMMxNBrGy9di5gaRD%2Fpre-commitment-and-meta-at-the-cambridge-uk-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pre-commitment%20and%20meta%20at%20the%20Cambridge%20UK%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMMxNBrGy9di5gaRD%2Fpre-commitment-and-meta-at-the-cambridge-uk-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMMxNBrGy9di5gaRD%2Fpre-commitment-and-meta-at-the-cambridge-uk-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 484, "htmlBody": "<p>At today's <a href=\"http://groups.google.com/group/cambridgelesswrong\">Cambridge UK meetup</a>&nbsp;I made an observation: It seems that LW meetups are very good at having meta-discussions. But they are not so good at acting&nbsp;effectively&nbsp;on them.</p>\n<p>The point of meta discussion is to make object level discussion better. The meta questions aren't themselves interesting and dont automatically produce win. Object level discussions are themselves interesting and do produce win (if not then stop talking about boring things). So if one has a meta discussion it should be such that the&nbsp;improvements&nbsp;made to the object level discussions outweigh the cost of the meta discussion.</p>\n<p>I notice we have meta discussions which (like a lot of discussions in LW groups) dont resolve themselves into actions. This means that&nbsp;improvements&nbsp;to the meetups aren't in fact implemented. This is a double fail: first because the object level discussion isn't improved, and second because the unresolved meta took resources away from the body of the meeting.</p>\n<p>We could&nbsp;cheaply&nbsp;improve this with the internet. Doodle polls solve the problem of when to&nbsp;schedule&nbsp;a meeting far more efficiently than verbal discussion. Likewise the time-consuming question of \"what shall we talk about\" can be thought about outside the meeting where there are far fewer&nbsp;constraints&nbsp;on time. Both these problems should be outsourced to the google group and not mentioned in the meeting itself.</p>\n<p>A point that was raised is that it is very easy for the group to decide that such and such a thing must be done, that does not automatically translate into the actions of specific people. Someone mentioned the&nbsp;<a href=\"/lw/3h/why_our_kind_cant_cooperate/\">parable of the rabbi raising funds</a>, and we started the following pre-commitment game.&nbsp;</p>\n<p><strong>The Napkin</strong></p>\n<p>We got out a napkin and Douglas drew a table of \"who, what, by when\" on it. He was the first to write down a commitment so as to overcome everyone's reluctance to be the first to act. We then went round and asked for commitments that would be made public in front of the group. I'm now posting those commitments online.</p>\n<ul>\n<li>Douglas: \"Post a meeting format to discussion\" Wednesday midnight</li>\n<li>Paul: \"Kahneman AD/BC*&nbsp;example on LW wiki\" Thursday midnight</li>\n<li>Paul: \"David Styles\" Monday midnight</li>\n<li>Adam: \"Post this list, post on meta/object interaction\" Wednesday midnight</li>\n<li>Jonathan: \"Directions to JCR\" Tuesday midnight</li>\n<li>Ben: \"Keep diary for 1 week, identify biases\" next&nbsp;Sunday</li>\n</ul>\n<div><em><span style=\"font-style: normal;\"><em>*(did I read that right? edit: no I didn't)</em></span></em></div>\n<div>&nbsp;The case of Ben is worth commenting on. At the end of the meeting as everyone was walking out the room I made to fold the list away and shouted \"last chance to commit to anything if you want to\", Ben took this opportunity to sign up when he had not done so before in the meeting. This may be a generally useful technique.</div>\n<div><strong>Questions</strong></div>\n<div>Those at the meeting: have you completed your task?&nbsp;</div>\n<p>What are easy ways to overcome the reluctance of people to be the first to act?</p>\n<p>How can we have meta-discussions that are targeted at concrete actions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MMMxNBrGy9di5gaRD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.932367715021668e-07, "legacy": true, "legacyId": "15692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T15:38:32.494Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm-5", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d29PtY45bhLsv2Ar3/meetup-fort-collins-colorado-meetup-wedneday-7pm-5", "pageUrlRelative": "/posts/d29PtY45bhLsv2Ar3/meetup-fort-collins-colorado-meetup-wedneday-7pm-5", "linkUrl": "https://www.lesswrong.com/posts/d29PtY45bhLsv2Ar3/meetup-fort-collins-colorado-meetup-wedneday-7pm-5", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd29PtY45bhLsv2Ar3%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd29PtY45bhLsv2Ar3%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd29PtY45bhLsv2Ar3%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9m'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 May 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Upgrading your life. Sunk costs. Dungeons and Dragons-style character builds as a learning environment for life strategy.\nSupper to follow.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9m'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d29PtY45bhLsv2Ar3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.932456250617484e-07, "legacy": true, "legacyId": "15693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/9m\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 May 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Upgrading your life. Sunk costs. Dungeons and Dragons-style character builds as a learning environment for life strategy.\nSupper to follow.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/9m\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T15:43:51.051Z", "modifiedAt": null, "url": null, "title": "Meetup Formats", "slug": "meetup-formats", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LnHLeRmkpq4HABPss/meetup-formats", "pageUrlRelative": "/posts/LnHLeRmkpq4HABPss/meetup-formats", "linkUrl": "https://www.lesswrong.com/posts/LnHLeRmkpq4HABPss/meetup-formats", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Formats&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Formats%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLnHLeRmkpq4HABPss%2Fmeetup-formats%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Formats%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLnHLeRmkpq4HABPss%2Fmeetup-formats", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLnHLeRmkpq4HABPss%2Fmeetup-formats", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1210, "htmlBody": "<p><span style=\"background-color: #ffff00;\">[ This post is todo with Cambridge_UK meetups, and is probably of no interest to others. ]</span></p>\n<p>At the meetup on 29th April, 2012, it was suggested that members of the meetup post a variety of meetup formats, so we can try them out systematically, and compare format to outcome.&nbsp; Because outcome depends upon not only format, but also the specific participants and the topic under discussion, it may take using a format more than once (or possibly having it used by a different group) to get a reasonable idea of how reliably a format contributes towards producing positive or negative outcomes.</p>\n<p><a href=\"/r/discussion/lw/c3w/precommitment_and_meta_at_the_cambridge_uk_meetup/\">I volunteered</a> to kick the thread off with a couple of suggestions.&nbsp; I hope people will add other proposals in the comments.&nbsp; Deciding which formats to try when, and matching them to suitable topics, is probably best left to the cambridge mailing list.</p>\n<p>Numbers seem to vary between 4 and 12 participants, but feel free to propose formats that don't handle that entire range.&nbsp; If too few people turn up, we can postpone a format to another time (or take it as a vote upon the popularity of the format, if the format for that meeting was announced in advance).</p>\n<p>The timeslot is 11am to 12:30, but people should also feel free to propose formats for shorter periods of time.</p>\n<p>&nbsp;</p>\n<hr />\n<h2>Proposed Format : small group discussions<br /></h2>\n<p>Required equipment : A4 paper, pens, countdown timer<br />Required time : 90 minutes</p>\n<p>11:00 start a 10 minutes countdown timer</p>\n<p>People arrive, chat, say what is on their mind and, most importantly, write down on paper (one topic per sheet) things they would be interested in discussing.&nbsp; Once the timer goes off, topic sheets may not be altered or created.</p>\n<p>11:10 timer goes off</p>\n<p>Spread the sheets around the room with a proposer by each sheet (any sheets for which no proposer volunteers get discarded at this stage).&nbsp; Read each topic out aloud, with no discussion/clarification/objections, clockwise from the position of the timer.</p>\n<p>Set a one minute countdown.&nbsp; People stand behind the proposer of the topic they most want to discuss, in a queue.&nbsp; If a topic gets 5 or more people, the front 5 take a table and start talking.&nbsp; Set the timer for one minute again, and re-form behind the remaining available topics.&nbsp; Any turn no topic reaches 5 people, junk the smallest topic that has fewer than 3 people.&nbsp; Repeat until the room is divided into groups of 3-5 people discussing different topics.&nbsp;&nbsp; This process should require no discussion, or any talking beyond asking for reminders of the wording of a topic.&nbsp;&nbsp; When a table sits to talk, they are free to interpret the written topic how they like, or even wander completely off it.</p>\n<p>11:55 timer goes off to remind people to take a 5 minute pause to put down feedback</p>\n<p>On the back of the A4 topic discussion sheet (or on a pre-printed sheet, if anyone is that organised), have a column for the categories:</p>\n<ul>\n<li>I learned something I think will improve my own ability to think rationally</li>\n<li>I think the discussion came up with something that could usefully be posted to LessWrong</li>\n<li>There is an action I commit to taking</li>\n</ul>\n<p>And have a row for each participant to put a tick or cross in each column.</p>\n<p>12:00 discussion groups either continue, or break up, move about, etc. - unstructured time.</p>\n<p>12:20 timer goes off for the last time</p>\n<p>People join back into a single discussion.&nbsp; One person from each initial group gives a 1 minute summary saying what the topic was and if the participants generally felt it helped rationality either personally and/or generally.&nbsp; (The aim behind this is that other meeting groups, or even later mettings of the same group with different participants, may want to copy topics that worked well.)&nbsp; Circulate a commit sheet with columns \"WHO\", \"WHEN\" and \"WHAT\", so people can list actions they plan to take (and when they will take them by).</p>\n<p>12:30 meeting ends - ajourn for breakfast</p>\n<hr />\n<h2>Proposed Format : skill focus<br /></h2>\n<p>Agree online, at least 2 weeks in advance, a particular skill to focus upon, that helps achieve rational thinking (perhaps linked to a specific cognitive bias), and agree a volunteer who will kick-start the meeting.</p>\n<p>The volunteer picks 10 minutes worth of material (eg a sequence entry) for everyone to have read, and the week before circulates it to the cambridge mailing list and prints out copies to take to the previous meeting for those who don't read the list, so everyone will know the format and what they're getting into if they turn up.</p>\n<p>11:00 people arrive, read the material if they have not already done so, and the agenda for the meeting.</p>\n<p>11:08 everyone participating goes upstairs to the separate space, anyone who doesn't want to (or arrives late) stays downstairs.</p>\n<p>11:10 starter activity, a game or quiz or some sort (eg 3 rounds of prisoners dilemma, or one of the economic gambling probability decision things - whatever fits the theme and gets people moving and participating).</p>\n<p>The rest of the time as specified by the volunteer for this particular skill focus, but probably including a general section (understanding the problem), a training section (practicing the skill), a 'share our real life experiences of this with each other' section, and a 'now apply the skill to my own life and commit to a plan' section.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2>Proposed Format : competitive planning or estimating<br /></h2>\n<p>Planning version:</p>\n<p>Go around the circle numbering off 1, 2, 1, 2, etc. to form two random groups.</p>\n<p>Each group has 10 minutes to discuss how best to split 60 minutes up in order to come up with the best plan in that time.</p>\n<p>Each group then spends 60 minutes planning something.&nbsp;&nbsp; The something might be \"a summer punt trip\", it might be \"a freshers fair stall\", it might be \"a spreadsheet that people can use to make a rational calculation of whether it is worth their while spending time reading LessWrong.\" - the thing is decided in advance, and the same thing is planned by both teams.</p>\n<p>In the final 10 minutes, everyone joins back together again, spends a few minutes presenting their designs, then discussing how well they actually spent the 60 minutes, and how they'd split it differently if they were doing the same thing again.</p>\n<p>Estimate version: (equipment needed - pack of cards, or similar)</p>\n<p>Split into two groups, as above, plus one person who will set three challenges.&nbsp;&nbsp; The challenge setter goes off to look up some facts (eg the number of tons of wheat grown by China in 2010), while each team spends 10 minutes discussing how they will make an estimate.</p>\n<p>Cards are then dealt out - the person with the queen of hearts is the defector who is secretly working for the opposing team</p>\n<p>Each group then gets 20 minutes to make the best estimate they can PLUS a sum of 'money' they are wagering on being closer to the true answer than the other team.&nbsp;&nbsp; The team must wager a total of 100 'money' spread over the three challenges.&nbsp; The estimate and the bet are written down and revealed simultaneously.</p>\n<p>After the end of the three challenges, reveal the two defectors, who move over to stand with their true group, then calculate which team won the most with their wagers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LnHLeRmkpq4HABPss", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.932479090637757e-07, "legacy": true, "legacyId": "15694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"background-color: #ffff00;\">[ This post is todo with Cambridge_UK meetups, and is probably of no interest to others. ]</span></p>\n<p>At the meetup on 29th April, 2012, it was suggested that members of the meetup post a variety of meetup formats, so we can try them out systematically, and compare format to outcome.&nbsp; Because outcome depends upon not only format, but also the specific participants and the topic under discussion, it may take using a format more than once (or possibly having it used by a different group) to get a reasonable idea of how reliably a format contributes towards producing positive or negative outcomes.</p>\n<p><a href=\"/r/discussion/lw/c3w/precommitment_and_meta_at_the_cambridge_uk_meetup/\">I volunteered</a> to kick the thread off with a couple of suggestions.&nbsp; I hope people will add other proposals in the comments.&nbsp; Deciding which formats to try when, and matching them to suitable topics, is probably best left to the cambridge mailing list.</p>\n<p>Numbers seem to vary between 4 and 12 participants, but feel free to propose formats that don't handle that entire range.&nbsp; If too few people turn up, we can postpone a format to another time (or take it as a vote upon the popularity of the format, if the format for that meeting was announced in advance).</p>\n<p>The timeslot is 11am to 12:30, but people should also feel free to propose formats for shorter periods of time.</p>\n<p>&nbsp;</p>\n<hr>\n<h2 id=\"Proposed_Format___small_group_discussions\">Proposed Format : small group discussions<br></h2>\n<p>Required equipment : A4 paper, pens, countdown timer<br>Required time : 90 minutes</p>\n<p>11:00 start a 10 minutes countdown timer</p>\n<p>People arrive, chat, say what is on their mind and, most importantly, write down on paper (one topic per sheet) things they would be interested in discussing.&nbsp; Once the timer goes off, topic sheets may not be altered or created.</p>\n<p>11:10 timer goes off</p>\n<p>Spread the sheets around the room with a proposer by each sheet (any sheets for which no proposer volunteers get discarded at this stage).&nbsp; Read each topic out aloud, with no discussion/clarification/objections, clockwise from the position of the timer.</p>\n<p>Set a one minute countdown.&nbsp; People stand behind the proposer of the topic they most want to discuss, in a queue.&nbsp; If a topic gets 5 or more people, the front 5 take a table and start talking.&nbsp; Set the timer for one minute again, and re-form behind the remaining available topics.&nbsp; Any turn no topic reaches 5 people, junk the smallest topic that has fewer than 3 people.&nbsp; Repeat until the room is divided into groups of 3-5 people discussing different topics.&nbsp;&nbsp; This process should require no discussion, or any talking beyond asking for reminders of the wording of a topic.&nbsp;&nbsp; When a table sits to talk, they are free to interpret the written topic how they like, or even wander completely off it.</p>\n<p>11:55 timer goes off to remind people to take a 5 minute pause to put down feedback</p>\n<p>On the back of the A4 topic discussion sheet (or on a pre-printed sheet, if anyone is that organised), have a column for the categories:</p>\n<ul>\n<li>I learned something I think will improve my own ability to think rationally</li>\n<li>I think the discussion came up with something that could usefully be posted to LessWrong</li>\n<li>There is an action I commit to taking</li>\n</ul>\n<p>And have a row for each participant to put a tick or cross in each column.</p>\n<p>12:00 discussion groups either continue, or break up, move about, etc. - unstructured time.</p>\n<p>12:20 timer goes off for the last time</p>\n<p>People join back into a single discussion.&nbsp; One person from each initial group gives a 1 minute summary saying what the topic was and if the participants generally felt it helped rationality either personally and/or generally.&nbsp; (The aim behind this is that other meeting groups, or even later mettings of the same group with different participants, may want to copy topics that worked well.)&nbsp; Circulate a commit sheet with columns \"WHO\", \"WHEN\" and \"WHAT\", so people can list actions they plan to take (and when they will take them by).</p>\n<p>12:30 meeting ends - ajourn for breakfast</p>\n<hr>\n<h2 id=\"Proposed_Format___skill_focus\">Proposed Format : skill focus<br></h2>\n<p>Agree online, at least 2 weeks in advance, a particular skill to focus upon, that helps achieve rational thinking (perhaps linked to a specific cognitive bias), and agree a volunteer who will kick-start the meeting.</p>\n<p>The volunteer picks 10 minutes worth of material (eg a sequence entry) for everyone to have read, and the week before circulates it to the cambridge mailing list and prints out copies to take to the previous meeting for those who don't read the list, so everyone will know the format and what they're getting into if they turn up.</p>\n<p>11:00 people arrive, read the material if they have not already done so, and the agenda for the meeting.</p>\n<p>11:08 everyone participating goes upstairs to the separate space, anyone who doesn't want to (or arrives late) stays downstairs.</p>\n<p>11:10 starter activity, a game or quiz or some sort (eg 3 rounds of prisoners dilemma, or one of the economic gambling probability decision things - whatever fits the theme and gets people moving and participating).</p>\n<p>The rest of the time as specified by the volunteer for this particular skill focus, but probably including a general section (understanding the problem), a training section (practicing the skill), a 'share our real life experiences of this with each other' section, and a 'now apply the skill to my own life and commit to a plan' section.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Proposed_Format___competitive_planning_or_estimating\">Proposed Format : competitive planning or estimating<br></h2>\n<p>Planning version:</p>\n<p>Go around the circle numbering off 1, 2, 1, 2, etc. to form two random groups.</p>\n<p>Each group has 10 minutes to discuss how best to split 60 minutes up in order to come up with the best plan in that time.</p>\n<p>Each group then spends 60 minutes planning something.&nbsp;&nbsp; The something might be \"a summer punt trip\", it might be \"a freshers fair stall\", it might be \"a spreadsheet that people can use to make a rational calculation of whether it is worth their while spending time reading LessWrong.\" - the thing is decided in advance, and the same thing is planned by both teams.</p>\n<p>In the final 10 minutes, everyone joins back together again, spends a few minutes presenting their designs, then discussing how well they actually spent the 60 minutes, and how they'd split it differently if they were doing the same thing again.</p>\n<p>Estimate version: (equipment needed - pack of cards, or similar)</p>\n<p>Split into two groups, as above, plus one person who will set three challenges.&nbsp;&nbsp; The challenge setter goes off to look up some facts (eg the number of tons of wheat grown by China in 2010), while each team spends 10 minutes discussing how they will make an estimate.</p>\n<p>Cards are then dealt out - the person with the queen of hearts is the defector who is secretly working for the opposing team</p>\n<p>Each group then gets 20 minutes to make the best estimate they can PLUS a sum of 'money' they are wagering on being closer to the true answer than the other team.&nbsp;&nbsp; The team must wager a total of 100 'money' spread over the three challenges.&nbsp; The estimate and the bet are written down and revealed simultaneously.</p>\n<p>After the end of the three challenges, reveal the two defectors, who move over to stand with their true group, then calculate which team won the most with their wagers.</p>", "sections": [{"title": "Proposed Format : small group discussions", "anchor": "Proposed_Format___small_group_discussions", "level": 1}, {"title": "Proposed Format : skill focus", "anchor": "Proposed_Format___skill_focus", "level": 1}, {"title": "Proposed Format : competitive planning or estimating", "anchor": "Proposed_Format___competitive_planning_or_estimating", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MMMxNBrGy9di5gaRD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T16:44:11.947Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia Meetup: Introduction to drawing", "slug": "meetup-philadelphia-meetup-introduction-to-drawing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Michelle_Z", "createdAt": "2011-07-14T22:50:16.205Z", "isAdmin": false, "displayName": "Michelle_Z"}, "userId": "ExbXRgKKWx59L4m4W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C9zx6EoRXQCjjPKav/meetup-philadelphia-meetup-introduction-to-drawing", "pageUrlRelative": "/posts/C9zx6EoRXQCjjPKav/meetup-philadelphia-meetup-introduction-to-drawing", "linkUrl": "https://www.lesswrong.com/posts/C9zx6EoRXQCjjPKav/meetup-philadelphia-meetup-introduction-to-drawing", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia%20Meetup%3A%20Introduction%20to%20drawing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia%20Meetup%3A%20Introduction%20to%20drawing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9zx6EoRXQCjjPKav%2Fmeetup-philadelphia-meetup-introduction-to-drawing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia%20Meetup%3A%20Introduction%20to%20drawing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9zx6EoRXQCjjPKav%2Fmeetup-philadelphia-meetup-introduction-to-drawing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9zx6EoRXQCjjPKav%2Fmeetup-philadelphia-meetup-introduction-to-drawing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/9n\">Philadelphia Meetup: Introduction to drawing</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">05 May 2012 02:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Kimmel Center,&nbsp;</span><span style=\"font-family: Arial, sans-serif; font-size: 13px; line-height: 18px; text-align: left;\">300 S. Broad Street, Philadelphia, PA 19102</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>This meetup will be an introduction to drawing as it relates to the articles on Map and Territory and Cached Selves.</p>\n<p>*Note: I just realized I had the wrong address. I am sorry for the confusion.*</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/9n\">Philadelphia Meetup: Introduction to drawing</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C9zx6EoRXQCjjPKav", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.932738712572473e-07, "legacy": true, "legacyId": "15695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Philadelphia_Meetup__Introduction_to_drawing\">Discussion article for the meetup : <a href=\"/meetups/9n\">Philadelphia Meetup: Introduction to drawing</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">05 May 2012 02:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Kimmel Center,&nbsp;</span><span style=\"font-family: Arial, sans-serif; font-size: 13px; line-height: 18px; text-align: left;\">300 S. Broad Street, Philadelphia, PA 19102</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>This meetup will be an introduction to drawing as it relates to the articles on Map and Territory and Cached Selves.</p>\n<p>*Note: I just realized I had the wrong address. I am sorry for the confusion.*</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Philadelphia_Meetup__Introduction_to_drawing1\">Discussion article for the meetup : <a href=\"/meetups/9n\">Philadelphia Meetup: Introduction to drawing</a></h2>", "sections": [{"title": "Discussion article for the meetup : Philadelphia Meetup: Introduction to drawing", "anchor": "Discussion_article_for_the_meetup___Philadelphia_Meetup__Introduction_to_drawing", "level": 1}, {"title": "Discussion article for the meetup : Philadelphia Meetup: Introduction to drawing", "anchor": "Discussion_article_for_the_meetup___Philadelphia_Meetup__Introduction_to_drawing1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T17:03:06.424Z", "modifiedAt": null, "url": null, "title": "Correcting errors and karma", "slug": "correcting-errors-and-karma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.845Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rebellionkid", "createdAt": "2011-06-20T09:26:46.768Z", "isAdmin": false, "displayName": "rebellionkid"}, "userId": "ygYCk3eXnJwt6p3o4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/thDhb7Pk55tKXyNDF/correcting-errors-and-karma", "pageUrlRelative": "/posts/thDhb7Pk55tKXyNDF/correcting-errors-and-karma", "linkUrl": "https://www.lesswrong.com/posts/thDhb7Pk55tKXyNDF/correcting-errors-and-karma", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Correcting%20errors%20and%20karma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACorrecting%20errors%20and%20karma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FthDhb7Pk55tKXyNDF%2Fcorrecting-errors-and-karma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Correcting%20errors%20and%20karma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FthDhb7Pk55tKXyNDF%2Fcorrecting-errors-and-karma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FthDhb7Pk55tKXyNDF%2Fcorrecting-errors-and-karma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>An easy way to win cheep karma on LW:</p>\n<p><ol>\n<li>Publicly make a mistake.</li>\n<li>Wait for people to call you on it.</li>\n<li>Publicly retract your errors and promise to improve.</li>\n</ol>\n<div>Post 1) gets you&nbsp;negative&nbsp;karma, post 3) gets you positive karma.&nbsp;Anecdotally&nbsp;the net result is generally very positive.</div>\n<div>This doesn't seem quite sane. Yes, it is good for us to reward people for changing their minds based on evidence. But it's still better not to have made the error the first time round. At the very least you should get less net karma for changing your mind towards the correct answer than you would for stating the correct thing the first time.</div>\n<div><strong>Questions:</strong></div>\n<div>Is there an advantage to this signalling-approval-for-updates that outweighs the value of karma as indicator-of-general-correctness-of-posts?</div>\n<div>If so then can some other signal of general correctness be devised?</div>\n<div>If not then what karma etiquette should we impose to ensure this effect doesn't happen?</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "thDhb7Pk55tKXyNDF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -8, "extendedScore": null, "score": 8.932820058306525e-07, "legacy": true, "legacyId": "15696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T21:19:03.579Z", "modifiedAt": null, "url": null, "title": "Request for feedback: paper on fine-tuning and the multiverse hypothesis", "slug": "request-for-feedback-paper-on-fine-tuning-and-the-multiverse", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.617Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexSchell", "createdAt": "2011-01-30T04:35:47.508Z", "isAdmin": false, "displayName": "AlexSchell"}, "userId": "GTpPwvcYBaY7jYmyL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p685ENpxYoDofJKys/request-for-feedback-paper-on-fine-tuning-and-the-multiverse", "pageUrlRelative": "/posts/p685ENpxYoDofJKys/request-for-feedback-paper-on-fine-tuning-and-the-multiverse", "linkUrl": "https://www.lesswrong.com/posts/p685ENpxYoDofJKys/request-for-feedback-paper-on-fine-tuning-and-the-multiverse", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20feedback%3A%20paper%20on%20fine-tuning%20and%20the%20multiverse%20hypothesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20feedback%3A%20paper%20on%20fine-tuning%20and%20the%20multiverse%20hypothesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp685ENpxYoDofJKys%2Frequest-for-feedback-paper-on-fine-tuning-and-the-multiverse%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20feedback%3A%20paper%20on%20fine-tuning%20and%20the%20multiverse%20hypothesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp685ENpxYoDofJKys%2Frequest-for-feedback-paper-on-fine-tuning-and-the-multiverse", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp685ENpxYoDofJKys%2Frequest-for-feedback-paper-on-fine-tuning-and-the-multiverse", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 442, "htmlBody": "<p>A while back, I <a href=\"/lw/7yq/what_are_you_working_on/4zbf\">posted </a>in the \"What are you working on?\" thread about a paper I was working on. A few people wanted to see it once I have a complete draft, and I'm of course independently interested in obtaining feedback before I move on with it.</p>\n<p>The paper doesn't presuppose much philosophical jargon that isn't easily googleable, I think. Math-wise, you need to be somewhat comfortable with basic conditional probabilities. I'm interested in finding out about any math errors, other non sequiturs, and other flaws in my discussion. I'd also like to find out about general impressions, such as what I should have spilled more or less ink on.&nbsp;Some notation is unfinished (subscripts, singular/plural first person, etc.), but it's thoroughly readable.</p>\n<p>&nbsp;</p>\n<p>ABSTRACT: According to a standard form of the fine-tuning argument, the apparent anthropic fine-tuning of the physical constants and boundary conditions of our universe confirms the multiverse hypothesis. According to the inverse gambler&rsquo;s fallacy objection, this view is mistaken: although the multiverse hypothesis makes the existence of a life-permitting universe more probable than it would be on a single-universe theory, it does not make it any more probable that our universe should be life-permitting, and thus is not confirmed by our total evidence. We examine recent replies to this objection and conclude that they all fall short, usually due to a shared weakness. We then show how a synthetic reply, obtained by combining independent insights from the literature, can overcome the weakness afflicting its predecessors.</p>\n<p>If you'd like a slightly more detailed description before deciding whether or not to read the whole thing, see my <a href=\"/lw/7yq/what_are_you_working_on/4zbf\">post</a>.</p>\n<p>&nbsp;</p>\n<p>Here is the actual paper: <a href=\"http://dl.dropbox.com/u/13100539/ht/igf.docx\">DOCX</a> <a href=\"http://dl.dropbox.com/u/13100539/ht/igf.pdf\">PDF</a> (on some computers, italicized Times New Roman looks weird in the PDF)</p>\n<p>EDIT 5/9/12: Current draft (edited, shortened to 13.5K words) is here:</p>\n<p>DOCX: http://bit.ly/Jc4pXr</p>\n<p>PDF: http://bit.ly/Jdc7z3</p>\n<p>&nbsp;</p>\n<p>NOTE: The paper occasionally makes use of the notion of a person as a metaphysical individual. Roughly and likely inaccurately, this is the concept of an individual essence that can only be instantiated once in a possible world and is partly independent of the physical pattern it inhabits (i.e. you can have different possible worlds that are physically identical but contain different individuals -- I think this is what Eliezer refers to as \"the philosophical notion of indexical identity apart from pattern identity\"). I personally find this concept unmotivated to say the least; it figures in the paper only&nbsp;because some of the arguments discussed rely on it; and it is inessential for my proposed reply. If you're going to weight in on this, I'd rather you make suggestions as to how I could gracefully express that I find the concept unhelpful while still engaging with the arguments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p685ENpxYoDofJKys", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 8.933921348659363e-07, "legacy": true, "legacyId": "15697", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-29T22:26:35.078Z", "modifiedAt": null, "url": null, "title": "Logical Uncertainty as Probability", "slug": "logical-uncertainty-as-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gRR", "createdAt": "2012-02-02T12:11:00.628Z", "isAdmin": false, "displayName": "gRR"}, "userId": "LPBRzHQvMP9chLNWH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C5x8GiDhiaEpu54jS/logical-uncertainty-as-probability", "pageUrlRelative": "/posts/C5x8GiDhiaEpu54jS/logical-uncertainty-as-probability", "linkUrl": "https://www.lesswrong.com/posts/C5x8GiDhiaEpu54jS/logical-uncertainty-as-probability", "postedAtFormatted": "Sunday, April 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20Uncertainty%20as%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20Uncertainty%20as%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC5x8GiDhiaEpu54jS%2Flogical-uncertainty-as-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20Uncertainty%20as%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC5x8GiDhiaEpu54jS%2Flogical-uncertainty-as-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC5x8GiDhiaEpu54jS%2Flogical-uncertainty-as-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 403, "htmlBody": "<p>This post is a long answer to <a href=\"/lw/b91/should_logical_probabilities_be_updateless_too/65tr\">this</a> comment by cousin_it:</p>\n<blockquote>\n<p>Logical uncertainty is weird because it doesn't exactly obey the rules of probability.&nbsp;You can't have a consistent probability assignment that says axioms are 100% true but the millionth&nbsp;digit of pi has a 50% chance of being odd.</p>\n</blockquote>\n<p>I'd like to attempt to formally define logical uncertainty in terms of probability. Don't know if what results is in any way novel or useful, but.</p>\n<p>Let X be a finite set of true statements of some formal system F extending propositional calculus, like Peano Arithmetic. X is supposed&nbsp;to represent a set of logical/mathematical beliefs of some finite reasoning agent.</p>\n<p>Given any X, we can define its \"Obvious Logical Closure\" OLC(X), an infinite set of statements producible from X by applying the rules and axioms of propositional calculus. An important property of OLC(X) is that it is decidable: for any statement S it is possible to&nbsp;find out whether S is true (S&isin;OLC(X)), false (\"~S\"&isin;OLC(X)), or uncertain (neither).</p>\n<p>We can now define the \"conditional\" probability P(*|X) as a function from {the statements of F} to [0,1] satisfying the axioms:</p>\n<p><strong>Axiom 1</strong>: Known true statements have probability 1:</p>\n<p>&nbsp; &nbsp; P(S|X)=1 &nbsp;iff &nbsp;S&isin;OLC(X)</p>\n<p><strong>Axiom 2</strong>: The probability of a disjunction&nbsp;of mutually exclusive statements is equal to the sum of their probabilities:</p>\n<p>&nbsp; &nbsp; \"~(A&and;B)\"&isin;OLC(X) &nbsp;implies &nbsp;P(\"A&or;B\"|X) = P(A|X) + P(B|X)</p>\n<p>From these axioms we can get all the expected behavior of the probabilities:</p>\n<p>&nbsp; &nbsp; P(\"~S\"|X) = 1 - P(S|X)</p>\n<p>&nbsp; &nbsp; P(S|X)=0 &nbsp;iff &nbsp;\"~S\"&isin;OLC(X)</p>\n<p>&nbsp; &nbsp; 0 &lt; P(S|X) &lt; 1 &nbsp;iff &nbsp;S&notin;OLC(X) and \"~S\"&notin;OLC(X)</p>\n<p>&nbsp; &nbsp; \"A=&gt;B\"&isin;OLC(X) &nbsp;implies &nbsp;P(A|X)&le;P(B|X)</p>\n<p>&nbsp; &nbsp; \"A&lt;=&gt;B\"&isin;OLC(X) &nbsp;implies &nbsp;P(A|X)=P(B|X)</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; etc.</p>\n<p>This is still insufficient to calculate an actual probability value for any uncertain statement. Additional principles are required.&nbsp;For example, the Consistency Desideratum of Jaynes: \"equivalent states of knowledge must be represented by the same probability values\".</p>\n<p><strong>Definition</strong>: two statements A and B are <em>indistinguishable </em><em>relative to X</em> iff&nbsp;there exists an isomorphism between OLC(X&cup;{A}) and OLC(X&cup;{B}), which is identity on X, and which maps A to B.<br />[Isomorphism here is a 1-1 function f preserving all logical operations: &nbsp;f(A&or;B)=f(A)&or;f(B), f(~~A)=~~f(A), etc.]</p>\n<p><strong>Axiom 3</strong>: If A and B are indistinguishable relative to X, then &nbsp;P(A|X) = P(B|X).</p>\n<p><strong>Proposition</strong>: Let X be the set of statements representing my current mathematical knowledge, translated into F. &nbsp;Then the statements \"<em>millionth digit of PI is odd</em>\"&nbsp;and \"<em>millionth digit of PI is even</em>\"&nbsp;are indistinguishable relative to X.</p>\n<p><strong>Corollary</strong>: &nbsp;P(millionth digit of PI is odd | my current mathematical knowledge) = 1/2.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8ckoduMw3gvCMJGSB": 2, "JHYaBGQuuKHdwnrAK": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C5x8GiDhiaEpu54jS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 8.934211929092603e-07, "legacy": true, "legacyId": "15698", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T03:31:25.765Z", "modifiedAt": null, "url": null, "title": "(Almost) every moral theory can be represented by a utility function", "slug": "almost-every-moral-theory-can-be-represented-by-a-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:01.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xpasjy2q2TZpQm2uK/almost-every-moral-theory-can-be-represented-by-a-utility", "pageUrlRelative": "/posts/xpasjy2q2TZpQm2uK/almost-every-moral-theory-can-be-represented-by-a-utility", "linkUrl": "https://www.lesswrong.com/posts/xpasjy2q2TZpQm2uK/almost-every-moral-theory-can-be-represented-by-a-utility", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(Almost)%20every%20moral%20theory%20can%20be%20represented%20by%20a%20utility%20function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(Almost)%20every%20moral%20theory%20can%20be%20represented%20by%20a%20utility%20function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxpasjy2q2TZpQm2uK%2Falmost-every-moral-theory-can-be-represented-by-a-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(Almost)%20every%20moral%20theory%20can%20be%20represented%20by%20a%20utility%20function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxpasjy2q2TZpQm2uK%2Falmost-every-moral-theory-can-be-represented-by-a-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxpasjy2q2TZpQm2uK%2Falmost-every-moral-theory-can-be-represented-by-a-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<p>This was demonstrated, in a certain limited way, in <a href=\"http://martinpeterson.org/ConsequentialismOCT%2014%202008.pdf\">Peterson (2009)</a>. See also <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/04/Lowry-Peterson-Cost-benefit-analysis-and-non-utilitarian-ethics.pdf\">Lowry &amp; Peterson (2011)</a>.</p>\n<p>The Peterson result provides an \"asymmetry argument\" in favor of consequentialism:</p>\n<blockquote>\n<p>Consequentialists can account for phenomena that are usually&nbsp;thought of in nonconsequentialist terms, such as rights, duties, and&nbsp;virtues, whereas the opposite is false of nonconsequentialist theories.&nbsp;Rights, duty or virtue-based theories cannot account for the fundamental moral importance of consequences.&nbsp;Because of this asymmetry, it&nbsp;seems it would be preferable to become a consequentialist &ndash; indeed, it&nbsp;would be virtually impossible not to be a consequentialist.</p>\n</blockquote>\n<p>Another argument in favor of consequentialism has to do with the causes of different types of moral judgments: see <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/\">Are Deontological Moral Judgments Rationalizations?</a></p>\n<p><strong>Update</strong>: see <a href=\"/lw/c45/almost_every_moral_theory_can_be_represented_by_a/6i23\">Carl's criticism</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xpasjy2q2TZpQm2uK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 5, "extendedScore": null, "score": 8.935523977427402e-07, "legacy": true, "legacyId": "15701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["62p74DvwNHgQXCXcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T06:49:51.041Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Collapse Postulates", "slug": "seq-rerun-collapse-postulates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sTwcN8YygqJSJ6etT/seq-rerun-collapse-postulates", "pageUrlRelative": "/posts/sTwcN8YygqJSJ6etT/seq-rerun-collapse-postulates", "linkUrl": "https://www.lesswrong.com/posts/sTwcN8YygqJSJ6etT/seq-rerun-collapse-postulates", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Collapse%20Postulates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Collapse%20Postulates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsTwcN8YygqJSJ6etT%2Fseq-rerun-collapse-postulates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Collapse%20Postulates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsTwcN8YygqJSJ6etT%2Fseq-rerun-collapse-postulates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsTwcN8YygqJSJ6etT%2Fseq-rerun-collapse-postulates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>Today's post, <a href=\"/lw/q6/collapse_postulates/\">Collapse Postulates</a> was originally published on 09 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Early physicists simply didn't think of the possibility of more than one world - it just didn't occur to them, even though it's the straightforward result of applying the quantum laws at all levels. So they accidentally invented a completely and strictly unnecessary part of quantum theory to ensure there was only one world - a law of physics that says that parts of the wavefunction mysteriously and spontaneously disappear when decoherence prevents us from seeing them any more. If such a law really existed, it would be the only non-linear, non-unitary, non-differentiable, non-local, non-CPT-symmetric, acausal, faster-than-light phenomenon in all of physics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c3g/seq_rerun_quantum_nonrealism/\">Quantum Non-Realism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sTwcN8YygqJSJ6etT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 8.936378163256709e-07, "legacy": true, "legacyId": "15702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xsZnufn3cQw7tJeQ3", "LFqyywgTWEXuFdWD2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T12:30:22.654Z", "modifiedAt": null, "url": null, "title": "Adopting others' opinions", "slug": "adopting-others-opinions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:36.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TraderJoe", "createdAt": "2012-03-02T17:26:49.458Z", "isAdmin": false, "displayName": "TraderJoe"}, "userId": "PvoRSceD7dHzEway5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zwrRFKb22FqD8pC4i/adopting-others-opinions", "pageUrlRelative": "/posts/zwrRFKb22FqD8pC4i/adopting-others-opinions", "linkUrl": "https://www.lesswrong.com/posts/zwrRFKb22FqD8pC4i/adopting-others-opinions", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Adopting%20others'%20opinions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdopting%20others'%20opinions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzwrRFKb22FqD8pC4i%2Fadopting-others-opinions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Adopting%20others'%20opinions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzwrRFKb22FqD8pC4i%2Fadopting-others-opinions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzwrRFKb22FqD8pC4i%2Fadopting-others-opinions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 517, "htmlBody": "<p>There are some issues where a) I have no direct knowledge of the facts, b) there is some dispute over the facts, c) I hold an opinion on what the facts are. You probably do too.</p>\n<p>For example, I believe that ~6mm Jews were killed in WWII, and that most people who deny this are anti-semites. If someone produced a theory which conflicted with my views, I would probably be very suspicious of the person's motives. I think this without ever having seen evidence that would satisfy me directly. [I've been to Auschwitz and a few other camps, and I've read a few books about WWII, but the only reason I have to cite the figure of 6mm rather than 3mm, or even 300,000, is that most knowledgeable people use that number]. I suspect most LWers are at a similar state of opinionated ignorance.<br /><br />Now, I am in no way incentivised to investigate this: my opinions matter roughly zero to anyone, including me. So I don't have any reason to investigate the Holocaust. But there are other areas where the facts do matter to me.<br /><br />I know very little about medicine. When I need medical assistance, I tend to do what a doctor tells me without criticising his diagnosis. For example, I suffered from eczema a few years ago. I was prescribed a medicine which contained hydrocortisone as the active ingredient. I Googled this ingredient and then took the medication until the affected area cleared up. I noted that there were other steroids available [Clobetasol propionate is one such] which are considered cures for eczema. I did not know why my doctor prescribed me one rather than the other, and the distinction between the two would likely affect me. But I took the 'expert advice' without a pinch of salt. Unlike the WWII question, here the facts are actually relevant to me, and a mis-diagnosis could have caused some complications. But just as I don't check the wiring of my house's electric systems, or check my walls' stability and capacity to support my ceiling, I take my doctor's advice. <br /><br />There are some less clear-cut examples. For example, the 'smartest' [read: some combination of high-IQ, high-rationality, knowledgeable] people I know tend to identify as either libertarians or utilitarians, rather than as socialists. There are a few exceptions to this, but not many. From this, I could arguably assume that it would be correct to adopt a libertarian or utilitarian, rather than a socialist*, mindset to politics <strong>without actually</strong> <strong>understanding why libertarians are libertarian. </strong>Just as I don't need to know why my doctor prescribed hydrocortisone in order to take it, I don't need to know why most smart people I know favour relaxed drug laws in order to share their opinion.<br /><br />*I'm aware that these are not the only options, but they're the most mainstream 'labels' which are clearly defined. ['Liberal' and 'conservative' mean different things in different countries, but I think those three are relatively constant, at least in the people who apply them to themselves]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zwrRFKb22FqD8pC4i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 0, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "15704", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T13:53:53.700Z", "modifiedAt": null, "url": null, "title": "Non-orthogonality implies uncontrollable superintelligence", "slug": "non-orthogonality-implies-uncontrollable-superintelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/npZMkydRMqAqMqbFb/non-orthogonality-implies-uncontrollable-superintelligence", "pageUrlRelative": "/posts/npZMkydRMqAqMqbFb/non-orthogonality-implies-uncontrollable-superintelligence", "linkUrl": "https://www.lesswrong.com/posts/npZMkydRMqAqMqbFb/non-orthogonality-implies-uncontrollable-superintelligence", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-orthogonality%20implies%20uncontrollable%20superintelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-orthogonality%20implies%20uncontrollable%20superintelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnpZMkydRMqAqMqbFb%2Fnon-orthogonality-implies-uncontrollable-superintelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-orthogonality%20implies%20uncontrollable%20superintelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnpZMkydRMqAqMqbFb%2Fnon-orthogonality-implies-uncontrollable-superintelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnpZMkydRMqAqMqbFb%2Fnon-orthogonality-implies-uncontrollable-superintelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>Just a minor thought connected with the <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">orthogonality</a> <a href=\"/lw/bfj/evidence_for_the_orthogonality_thesis/\">thesis</a>: if you claim that any superintelligence will inevitably converge to some true code of morality, then you are also claiming that no measures can be taken by its creators to prevent this convergence. In other words, the superintelligence will be uncontrollable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "npZMkydRMqAqMqbFb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 22, "extendedScore": null, "score": 8.938202160864383e-07, "legacy": true, "legacyId": "15705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CRsYy3xtbMrLjoXZT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T15:35:39.670Z", "modifiedAt": null, "url": null, "title": "Why a Human (Or Group of Humans) Might Create UnFriendly AI Halfway On Purpose", "slug": "why-a-human-or-group-of-humans-might-create-unfriendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m8sDFFKzzkqMuK68h/why-a-human-or-group-of-humans-might-create-unfriendly-ai", "pageUrlRelative": "/posts/m8sDFFKzzkqMuK68h/why-a-human-or-group-of-humans-might-create-unfriendly-ai", "linkUrl": "https://www.lesswrong.com/posts/m8sDFFKzzkqMuK68h/why-a-human-or-group-of-humans-might-create-unfriendly-ai", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20a%20Human%20(Or%20Group%20of%20Humans)%20Might%20Create%20UnFriendly%20AI%20Halfway%20On%20Purpose&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20a%20Human%20(Or%20Group%20of%20Humans)%20Might%20Create%20UnFriendly%20AI%20Halfway%20On%20Purpose%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8sDFFKzzkqMuK68h%2Fwhy-a-human-or-group-of-humans-might-create-unfriendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20a%20Human%20(Or%20Group%20of%20Humans)%20Might%20Create%20UnFriendly%20AI%20Halfway%20On%20Purpose%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8sDFFKzzkqMuK68h%2Fwhy-a-human-or-group-of-humans-might-create-unfriendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8sDFFKzzkqMuK68h%2Fwhy-a-human-or-group-of-humans-might-create-unfriendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1559, "htmlBody": "<p>First, some quotes from Eliezer's contributions to the <em>Global Catastrophic Risks </em>anthology. First, from <a href=\"http://yudkowsky.net/rational/cognitive-biases\">Cognitive biases potentially affecting judgement of global risks:</a></p>\n<blockquote>\n<p><span class=\"a\" style=\"left: 63px; top: 2028px; word-spacing: 1px;\">All else being equal, not <span class=\"l6\">many people would pre<span class=\"l6\">fer to destroy the <span class=\"l6\">world. Even faceless c<span class=\"l6\">orporations,</span></span></span></span></span><span class=\"a\" style=\"left: 63px; top: 2148px; word-spacing: -1px;\">meddling governments, reckless scientists, and other agents of doom, require a world in which to </span><span class=\"a\" style=\"left: 63px; top: 2268px; word-spacing: 1px;\">achieve their goals of <span class=\"l6\">profit, order<span class=\"l6\">, tenure, or other villainies. If our extinction proceeds s<span class=\"l6\">lowly enough </span></span></span></span><span class=\"a\" style=\"left: 63px; top: 2388px; word-spacing: -1px;\">to allow a moment of horrified realization, the doers of the deed will likely be quite taken aback on </span><span class=\"a\" style=\"left: 63px; top: 2509px;\">realizing that they have actually de<span class=\"l6\">stroyed the world. Therefore I suggest that if the Earth is destroy<span class=\"l6\">ed,</span></span></span><span class=\"a\" style=\"left: 63px; top: 2629px; word-spacing: -1px;\"> it will probably be by mistake.</span></p>\n</blockquote>\n<p><span class=\"a\" style=\"left: 63px; top: 2629px; word-spacing: -1px;\">And from <a href=\"http://yudkowsky.net/singularity/ai-risk\">Artificial Intelligence as a Positive and Negative Factor in Global Risk: </a></span></p>\n<blockquote>\n<p>We can therefore visualize a possible <em>first-mover effect</em> in superintelligence. The first mover effect is when the outcome for Earth-originating intelligent life depends primarily on the makeup of whichever mind <em>first</em> achieves some key threshold of intelligence - such as criticality of self-improvement. The two necessary assumptions are these:</p>\n<p>&bull; The <em>first</em> AI to surpass some key threshold (e.g. criticality of self-improvement), if unFriendly, can wipe out the human species.<br />&bull; The <em>first</em> AI to surpass the same threshold, if Friendly, can prevent a hostile AI from coming into existence or from harming the human species; or find some other creative way to ensure the survival and prosperity of Earth-originating intelligent<br />life.</p>\n<p>More than one scenario qualifies as a first-mover effect. Each of these examples reflects a different key threshold:</p>\n<p>&bull; Post-criticality, self-improvement reaches superintelligence on a timescale of weeks or less. AI projects are sufficiently sparse that no <em>other</em> AI achieves criticality before the <em>first</em> mover is powerful enough to overcome all opposition. The key threshold is criticality of recursive self-improvement.<br />&bull; AI-1 cracks protein folding three days before AI-2. AI-1 achieves nanotechnology six hours before AI-2. With rapid manipulators, AI-1 can (potentially) disable AI-2's R&amp;D before fruition. The runners are close, but whoever crosses the finish line first, wins. The key threshold is rapid infrastructure.<br />&bull; The first AI to absorb the Internet can (potentially) keep it out of the hands of other AIs. Afterward, by economic domination or covert action or blackmail or supreme ability at social manipulation, the first AI halts or slows other AI projects so that no other AI catches up. The key threshold is absorption of a unique resource.</p>\n</blockquote>\n<p>I think the first quote is exactly right. But it leaves out something important. The effects of someone's actions do not need to destroy the world in order to be very, very, harmful. <a href=\"http://intelligence.org/ourresearch/publications/what-is-friendly-ai.html\">These definitions</a> of Friendly and unFriendly AI are worth quoting (I don't know how consistently they're actually used by people associated with the SIAI, but they're useful for my purposes):</p>\n<blockquote>\n<p>A \"Friendly AI\" is an AI that takes actions that are, on the whole, beneficial to humans and humanity; benevolent rather than malevolent; nice rather than hostile.&nbsp; The evil Hollywood AIs of <em>The Matrix</em> or <em>Terminator</em> are, correspondingly, \"hostile\" or \"unFriendly\".</p>\n</blockquote>\n<p>Again, an action does not need to destroy the world to be, on the whole, harmful to humans and humanity; malevolent rather than benevolent. An assurance that a human or humans will not do the former is no assurance that they will not do the latter. So if there ends up being a strong first-mover effect in the development of AI, we have to worry about the possibility that whoever gets control of the AI will use it selfishly, at the expense of the rest of humanity.</p>\n<p>The title of this post says \"halfway on purpose\" instead of \"on purpose,\" because in human history even the villains tend to see themselves as heroes of their own story. I've <a href=\"/lw/9e7/two_kinds_of_irrationality_and_how_to_avoid_one/\">previously written</a> about how we deceive ourselves so as to better deceive others, and how I suspect this is the most harmful kind of human irrationality.</p>\n<p>Too many people--at least, too many writers of the kind of fiction where the villain turns out to be an all-right guy in the end--seem to believe that if someone is the hero of their own story and genuinely believes they're doing the right thing, they can't <em>really </em>be evil. But you know who was the hero of his own story and genuinely believed he was doing the right thing? Hitler. <em>He </em>believed he was saving the world from the Jews and promoting the greatness of the German <em>volk.</em></p>\n<p>We have every reason to think that the psychological tendencies that created these hero-villains are nearly universal. Evolution has no way to give us nice impulses for the sake of having nice impulses. Theory predicts, and observation confirms, that we tend to care more about blood-relatives than mere allies and allies more than strangers. As <a href=\"http://www.gutenberg.org/files/4705/4705-h/4705-h.htm#2H_4_0088\">Hume</a> observed (remarkably, without any knowledge of Hammilton's rule) \"A man naturally loves his children better than his nephews, his nephews better than his cousins, his cousins better than strangers, where every thing else is equal.\" And we care more about ourselves than any single other individual on the planet (even if we might sacrifice ourselves for two brothers or eight cousins.)</p>\n<p>Most of us are not murderers, but then most of have never been in a situation where it would be in our interest to commit murder. The really disturbing thing is that there is much evidence that ordinary people can become monsters as soon as the situation changes. Science gives us the Stanford Prison Experiment and Milgram's experiment on obedience to authority, history gives us even more disturbing facts about how many soldiers commit atrocities in war time. Of the soldiers who came from societies where atrocities are frowned on, most of them must have seemed perfectly normal before they went off to war. Probably most of them, if they'd thought about it, would have sincerely believed they were incapable of doing such things.</p>\n<p>This makes a frightening amount of evolutionary sense. There's reason for evolution to, as much as possible, give us <em>conditional rules</em> for behavior so we only do certain things when it's fitness increasing to do so. Normally, doing the kind of things done during the Rape of Nanking leads to swift punishments, but the circumstances when such things actually happen tend to be circumstances where punishment is much less likely, where the other guys are trying to kill you anyway and your superior officer is willing to at minimum look the other way. But if you're in a situation where doing such things is not in your interest, where's the evolutionary benefit of even being aware of what you're capable of?</p>\n<p>Taking this all together, the risk is not that someone will deliberately use AI to harm humanity (do it on purpose). The risk is that they'll use AI to harm humanity for selfish reasons, while persuading themselves they're actually benefiting humanity (doing it halfway on purpose.) If whoever gets control of a first-mover scenario sincerely believed, prior to gaining <a href=\"/lw/wt/not_taking_over_the_world/\">unlimited power,</a> that they really wanted to be really, really careful not to do that, that's no assurance of anything, because they'll have been thinking that before the situation changed and there was a chance for the conditional rule, \"Screw over other people for personal gain if you're sure if getting away with it\" triggered.</p>\n<p>I don't want to find out what I'd do with unlimited power. Or rather, all else being equal I would like to find out, but I don't think putting myself in a position where I actually could find out would be worth the risk. This is in spite of the fact that the fact that I am even worrying about these things may be a sign that I'd be less of a risk than other people. That should give you an idea of how little I would trust other people with such power.</p>\n<p>The fact that Eliezer has stated his intention to have the Singularity Institute create <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">FOOM-capable</a> AI doesn't worry me much, because I think the SIAI is highly unlikely to succeed at that. I think if we do end up in a first-mover scenario, it will probably be the result of some project backed by a rich organization like IBM, the United States Department of Defense, or Google.</p>\n<p>Forgetting about that, though, this looks to me like an absolutely crazy strategy. Eliezer has said creating FAI will be a very meta operation, and I think I heard him once mention putting prospective FAI coders through a lot of rationality training before beginning the process, but I have no idea why he would think those are remotely sufficient safeguards for <em>giving a group of humans unlimited power.</em> Even if you believe there's a significant risk that creating FOOM-capable FAI could be <em>necessary </em>to human survival, shouldn't, in that case, there be a major effort to first answer the question, \"Is there any possible way to give a group of humans unlimited power without it ending in disaster?\"</p>\n<p>More broadly, given even a small chance that the future of AI will end up in some first-mover scenario, it's worth asking, \"what can we do to prevent some small group of humans (the SIAI, a secret conspiracy of billionaires, a secret conspiracy of Google employees, whoever) from steering a first-mover scenario in a direction that's beneficial to themselves and perhaps their blood relatives, but harmful to the rest of humanity?\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m8sDFFKzzkqMuK68h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 13, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "15706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B3RGZg4KNdsaDC2J4", "DdEKcS6JcW7ordZqQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T16:02:04.383Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Fake Utility Meetup", "slug": "meetup-vancouver-fake-utility-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hb5w3QrCzZxxEvTpk/meetup-vancouver-fake-utility-meetup", "pageUrlRelative": "/posts/hb5w3QrCzZxxEvTpk/meetup-vancouver-fake-utility-meetup", "linkUrl": "https://www.lesswrong.com/posts/hb5w3QrCzZxxEvTpk/meetup-vancouver-fake-utility-meetup", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Fake%20Utility%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Fake%20Utility%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhb5w3QrCzZxxEvTpk%2Fmeetup-vancouver-fake-utility-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Fake%20Utility%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhb5w3QrCzZxxEvTpk%2Fmeetup-vancouver-fake-utility-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhb5w3QrCzZxxEvTpk%2Fmeetup-vancouver-fake-utility-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9o'>Vancouver Fake Utility Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 May 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2875 East 24th ave, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again, LW</p>\n\n<p>Next vancouver meetup is being held at my house again. Sunday at 13:00. Go around back to the basement back door.</p>\n\n<p>Last meetup (the monthly supermeetup) was a success! We got 3 people who don't otherwise show up to come out, and had fun talking about the reductionism sequence. We hope to see more of you at the regular meetups (like this one).</p>\n\n<p>This week, we are going to discuss the Fake Utility/Evolution semisequence. The exact list of posts is findable with some detective work on the wiki, or you can just read them from our <a href=\"https://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>Additionally, we are going to attempt to build 5-second-level skills for all (or at least some) of the techniques in 37 ways that words can be wrong, which we read a few weeks ago. Again, see our list for details.</p>\n\n<p>Just because this isn't the supermeetup doesn't mean you shouldn't come out. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9o'>Vancouver Fake Utility Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hb5w3QrCzZxxEvTpk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.938756192144672e-07, "legacy": true, "legacyId": "15707", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Fake_Utility_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9o\">Vancouver Fake Utility Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 May 2012 01:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2875 East 24th ave, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again, LW</p>\n\n<p>Next vancouver meetup is being held at my house again. Sunday at 13:00. Go around back to the basement back door.</p>\n\n<p>Last meetup (the monthly supermeetup) was a success! We got 3 people who don't otherwise show up to come out, and had fun talking about the reductionism sequence. We hope to see more of you at the regular meetups (like this one).</p>\n\n<p>This week, we are going to discuss the Fake Utility/Evolution semisequence. The exact list of posts is findable with some detective work on the wiki, or you can just read them from our <a href=\"https://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>.</p>\n\n<p>Additionally, we are going to attempt to build 5-second-level skills for all (or at least some) of the techniques in 37 ways that words can be wrong, which we read a few weeks ago. Again, see our list for details.</p>\n\n<p>Just because this isn't the supermeetup doesn't mean you shouldn't come out. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Fake_Utility_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9o\">Vancouver Fake Utility Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Fake Utility Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Fake_Utility_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Fake Utility Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Fake_Utility_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T17:13:54.687Z", "modifiedAt": null, "url": null, "title": "Experiment: a good researcher is hard to find", "slug": "experiment-a-good-researcher-is-hard-to-find", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/evyBmPw9ZnzmoFmP6/experiment-a-good-researcher-is-hard-to-find", "pageUrlRelative": "/posts/evyBmPw9ZnzmoFmP6/experiment-a-good-researcher-is-hard-to-find", "linkUrl": "https://www.lesswrong.com/posts/evyBmPw9ZnzmoFmP6/experiment-a-good-researcher-is-hard-to-find", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Experiment%3A%20a%20good%20researcher%20is%20hard%20to%20find&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExperiment%3A%20a%20good%20researcher%20is%20hard%20to%20find%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevyBmPw9ZnzmoFmP6%2Fexperiment-a-good-researcher-is-hard-to-find%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Experiment%3A%20a%20good%20researcher%20is%20hard%20to%20find%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevyBmPw9ZnzmoFmP6%2Fexperiment-a-good-researcher-is-hard-to-find", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevyBmPw9ZnzmoFmP6%2Fexperiment-a-good-researcher-is-hard-to-find", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 695, "htmlBody": "<blockquote>\n<p>See previously <a href=\"/lw/6ne/a_good_volunteer_is_hard_to_find/\">&ldquo;A good volunteer is hard to find&rdquo;</a></p>\n</blockquote>\n<p>Back in February 2012, <a href=\"/r/discussion/lw/9t8/the_singularity_institute_needs_remote/\">lukeprog announced</a> that SIAI was hiring more part-time remote researchers, and you could apply just by demonstrating your chops on a simple test: review the psychology literature on habit formation with an eye towards practical application. What factors strengthen new habits? How long do they take to harden? And so on. I was assigned to read through and rate the submissions and Luke could then look at them individually to decide who to hire. We didn&rsquo;t get as many submissions as we were hoping for, so <a href=\"/lw/bke/the_singularity_institute_still_needs_remote/\">in April</a> Luke posted again, this time with a quicker easier application form. (I don&rsquo;t know how that has been working out.)</p>\n<p>But in February, I remembered the linked post above from GiveWell where they mentioned many would-be volunteers did not even finish the test task. I did, and I didn&rsquo;t find it <em>that</em> bad, and actually a kind of interesting exercise in critical thinking &amp; being careful. People suggested that perhaps the attrition was due not to low volunteer quality, but to the feeling that they were not appreciated and were doing useless makework. (The same reason so many kids hate school&hellip;) But how to test this?</p>\n<p><a id=\"more\"></a></p>\n<p>Simple! Tell people that their work was not useless and that even if they were not hired, their work would be used! And we could do Science by randomizing what people got the encouraging statement. The added paragraph looked like this:</p>\n<blockquote>\n<p>The primary purpose of this project is to evaluate applicants on their ability to do the kind of work we need, but we&rsquo;ll collate all the results into one good article on the subject, so even if we don&rsquo;t hire you, you don&rsquo;t have to feel your time was wasted.</p>\n</blockquote>\n<p>Well, all the reviews have been read &amp; graded as of yesterday, with submissions trickling in over months; I think everyone who was going to submit has done so, and it&rsquo;s now time for the final step. So many people failed to send in any submission (only ~18 of ~40) that it&rsquo;s relatively easy to analyze - there&rsquo;s just not that much data!</p>\n<p>So, the first question is, did people who got the extra paragraph do a better job of writing their review, as expressed in my rating it from 2-10?</p>\n<p>Surprisingly, they did seem to - despite my expectation that any result would be noise as the sample is so small. If we code getting no paragraph as 0 and getting a paragraph as 1, and add the two scores to get 2-10, and strip out all personal info, you get <a href=\"http://dl.dropbox.com/u/5317066/2012-feb-researcher-scores.csv\">this CSV</a>. Load it up in <code>R</code>:</p>\n<pre><code>&gt; mydata &lt;- read.table(\"2012-feb-researcher-scores.csv\", header=TRUE, sep=\",\") &gt; t.test(Good~Extra, data=mydata) Welch Two Sample t-test data: Good by Extra t = -2.448, df = 14.911, p-value = 0.02723 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -4.028141 -0.277415 sample estimates: mean in group 0 mean in group 1 4.625000 6.777778</code></pre>\n<p>The result is not hugely robust: if you set the last score to 10 rather than 6, for example, the <em>p</em>-value falls to just 0.16. The <a href=\"http://en.wikipedia.org/wiki/Effect_size#Cohen.27s_d\">effect size</a> looks interesting though:</p>\n<pre><code>.... mean in group 0 mean in group 1 5.125000 6.777778 &gt; sd(mydata$Good, TRUE) [1] 2.318405 &gt; (6.7 - 5.125) / 2.32 [1] 0.6788793</code></pre>\n<p>0.67 isn&rsquo;t bad.</p>\n<p>The next question to me is, did the paragraph influence whether people would send in a submission <em>at all</em>? <a href=\"http://dl.dropbox.com/u/5317066/2012-feb-researcher-completion.csv\">Re-editing the CSV</a>, we load it up and analyze again:</p>\n<pre><code>&gt; mydata &lt;- read.table(\"2012-feb-researcher-completion.csv\", header=TRUE, sep=\",\") &gt; t.test(Received~Extra, data=mydata) Welch Two Sample t-test data: Received by Extra t = 0.1445, df = 36.877, p-value = 0.8859 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.3085296 0.3558981 sample estimates: mean in group 0 mean in group 1 0.4736842 0.4500000</code></pre>\n<p>Nope. It&rsquo;s somewhat robust since we can use everyone who applied; I have to flip like 6 values before the <em>p</em>-value goes down to 0.07.</p>\n<p>So, lessons learned? It&rsquo;s probably a good idea to include such a paragraph since it&rsquo;s so cheap and apparently isn&rsquo;t at the expense of submissions in general.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tk4R4LrX88gmFeMmY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "evyBmPw9ZnzmoFmP6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 50, "extendedScore": null, "score": 0.000107, "legacy": true, "legacyId": "15708", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZWxE96G84m6LotXfg", "LakrCAaj8rNss2q6j", "T5WjwPRPHy8mPrJcJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-30T22:27:13.442Z", "modifiedAt": null, "url": null, "title": "A few questions on International Rationality", "slug": "a-few-questions-on-international-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Locke", "createdAt": "2011-12-27T15:40:30.383Z", "isAdmin": false, "displayName": "Locke"}, "userId": "zq2k9FZq7T3AebpEc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dr6k5Rrdd4qHLX7yg/a-few-questions-on-international-rationality", "pageUrlRelative": "/posts/dr6k5Rrdd4qHLX7yg/a-few-questions-on-international-rationality", "linkUrl": "https://www.lesswrong.com/posts/dr6k5Rrdd4qHLX7yg/a-few-questions-on-international-rationality", "postedAtFormatted": "Monday, April 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20few%20questions%20on%20International%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20few%20questions%20on%20International%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdr6k5Rrdd4qHLX7yg%2Fa-few-questions-on-international-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20few%20questions%20on%20International%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdr6k5Rrdd4qHLX7yg%2Fa-few-questions-on-international-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdr6k5Rrdd4qHLX7yg%2Fa-few-questions-on-international-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>Disclaimer: I'm still fairly new here, and though I did use the search bar it's entirely possible this has been discussed before. Just point me in the right direction if this is so.</p>\n<p>&nbsp;</p>\n<p>While reading about 4chan's Japanese progenitor website, it occurred to me that I know nothing about the state of rationality in the non-English-speaking world, and more specifically the non-English-speaking internet. Is there a Russian version of SIAI? A Japanese Less Wrong? What about Korean Robin Hansons and Eliezer Yudkowskys?</p>\n<p>If we take Religion as any indication of irrationality then America should be one of the least rational countries in the world. So if there are like-minded individuals out there speaking in languages we don't know, are we doing anything to collaborate with them? Do they have their own sequences and their own HPMORs which we could be reading?</p>\n<p>And if there are no Singularitarian, Cryonics-Supporting, Utilitarianism-Advocating websites for the majority of the human race, isn't that a <em>huge deal</em>? Aren't Europeans and Asians more likely to be open to rationality, if only because of their atheism? If we want Friendly-AI to be developed, should we be translating the sequences into Chinese and Hindu as quickly as possible?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dr6k5Rrdd4qHLX7yg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "15709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-01T02:18:47.578Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] If Many-Worlds Had Come First", "slug": "seq-rerun-if-many-worlds-had-come-first", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.444Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nMZobjR6vD8Kun5W9/seq-rerun-if-many-worlds-had-come-first", "pageUrlRelative": "/posts/nMZobjR6vD8Kun5W9/seq-rerun-if-many-worlds-had-come-first", "linkUrl": "https://www.lesswrong.com/posts/nMZobjR6vD8Kun5W9/seq-rerun-if-many-worlds-had-come-first", "postedAtFormatted": "Tuesday, May 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20If%20Many-Worlds%20Had%20Come%20First&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20If%20Many-Worlds%20Had%20Come%20First%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnMZobjR6vD8Kun5W9%2Fseq-rerun-if-many-worlds-had-come-first%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20If%20Many-Worlds%20Had%20Come%20First%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnMZobjR6vD8Kun5W9%2Fseq-rerun-if-many-worlds-had-come-first", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnMZobjR6vD8Kun5W9%2Fseq-rerun-if-many-worlds-had-come-first", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Today's post, <a href=\"/lw/q7/if_manyworlds_had_come_first/\">If Many-Worlds Had Come First</a> was originally published on 10 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If early physicists had never made the mistake, and thought immediately to apply the quantum laws at all levels to produce macroscopic decoherence, then \"collapse postulates\" would today seem like a completely crackpot theory. In addition to their other problems, like FTL, the collapse postulate would be the only physical law that was informally specified - often in dualistic (mentalistic) terms - because it was the only fundamental law adopted without precise evidence to nail it down. Here, we get a glimpse at that alternate Earth.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c46/seq_rerun_collapse_postulates/\">Collapse Postulates</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nMZobjR6vD8Kun5W9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.941413289883997e-07, "legacy": true, "legacyId": "15710", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WqGCaRhib42dhKWRL", "sTwcN8YygqJSJ6etT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-01T04:14:51.616Z", "modifiedAt": null, "url": null, "title": "Open Thread, May 1-15, 2012", "slug": "open-thread-may-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:27.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y3Sd8e3zo95kGXpdY/open-thread-may-1-15-2012", "pageUrlRelative": "/posts/Y3Sd8e3zo95kGXpdY/open-thread-may-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/Y3Sd8e3zo95kGXpdY/open-thread-may-1-15-2012", "postedAtFormatted": "Tuesday, May 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20May%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20May%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY3Sd8e3zo95kGXpdY%2Fopen-thread-may-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20May%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY3Sd8e3zo95kGXpdY%2Fopen-thread-may-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY3Sd8e3zo95kGXpdY%2Fopen-thread-may-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y3Sd8e3zo95kGXpdY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 8.941913514400668e-07, "legacy": true, "legacyId": "15711", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 267, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-01T06:31:41.895Z", "modifiedAt": null, "url": null, "title": "Meetup : Phoenix, Arizona", "slug": "meetup-phoenix-arizona", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yfTFPeB8Yu67HJYXg/meetup-phoenix-arizona", "pageUrlRelative": "/posts/yfTFPeB8Yu67HJYXg/meetup-phoenix-arizona", "linkUrl": "https://www.lesswrong.com/posts/yfTFPeB8Yu67HJYXg/meetup-phoenix-arizona", "postedAtFormatted": "Tuesday, May 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Phoenix%2C%20Arizona&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Phoenix%2C%20Arizona%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyfTFPeB8Yu67HJYXg%2Fmeetup-phoenix-arizona%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Phoenix%2C%20Arizona%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyfTFPeB8Yu67HJYXg%2Fmeetup-phoenix-arizona", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyfTFPeB8Yu67HJYXg%2Fmeetup-phoenix-arizona", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/9p\">Phoenix, Arizona</a></h2>\r\n<div class=\"meetup-meta\">\r\n<p><strong>WHEN:</strong> <span class=\"date\">15 June 2012 07:00:00PM (-0700)</span></p>\r\n<p><strong>WHERE:</strong> <span class=\"address\">2502 E. Camelback Road Phoenix,AZ,85016 </span></p>\r\n</div>\r\n<!-- .meta -->\r\n<div class=\"content\">\r\n<div class=\"md\">\r\n<p>The meetup will take place at the Paradise Bakery &amp; Cafe in Biltmore Fashion Park. We will be discussing chapter I-V of <a rel=\"nofollow\" href=\"http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567\"><em>G&ouml;del, Escher, Bach: an Eternal Golden Braid</em></a> by <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Douglas_Hofstadter\">Douglas R. Hofstadter</a>.</p>\r\n</div>\r\n</div>\r\n<!-- .content -->\r\n<h2>Discussion article for the meetup : <a href=\"/meetups/9p\">Phoenix, Arizona</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yfTFPeB8Yu67HJYXg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.942503319699598e-07, "legacy": true, "legacyId": "15712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Phoenix__Arizona\">Discussion article for the meetup : <a href=\"/meetups/9p\">Phoenix, Arizona</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">15 June 2012 07:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2502 E. Camelback Road Phoenix,AZ,85016 </span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The meetup will take place at the Paradise Bakery &amp; Cafe in Biltmore Fashion Park. We will be discussing chapter I-V of <a rel=\"nofollow\" href=\"http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567\"><em>G\u00f6del, Escher, Bach: an Eternal Golden Braid</em></a> by <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Douglas_Hofstadter\">Douglas R. Hofstadter</a>.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Phoenix__Arizona1\">Discussion article for the meetup : <a href=\"/meetups/9p\">Phoenix, Arizona</a></h2>", "sections": [{"title": "Discussion article for the meetup : Phoenix, Arizona", "anchor": "Discussion_article_for_the_meetup___Phoenix__Arizona", "level": 1}, {"title": "Discussion article for the meetup : Phoenix, Arizona", "anchor": "Discussion_article_for_the_meetup___Phoenix__Arizona1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-01T07:44:11.917Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes May 2012", "slug": "rationality-quotes-may-2012-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iG5v5uDCCkkoG6zmd/rationality-quotes-may-2012-0", "pageUrlRelative": "/posts/iG5v5uDCCkkoG6zmd/rationality-quotes-may-2012-0", "linkUrl": "https://www.lesswrong.com/posts/iG5v5uDCCkkoG6zmd/rationality-quotes-may-2012-0", "postedAtFormatted": "Tuesday, May 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20May%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20May%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiG5v5uDCCkkoG6zmd%2Frationality-quotes-may-2012-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20May%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiG5v5uDCCkkoG6zmd%2Frationality-quotes-may-2012-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiG5v5uDCCkkoG6zmd%2Frationality-quotes-may-2012-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<div>\r\n<p style=\"margin: 0px 0px 1em;\">Here's the new thread for posting quotes, with the usual rules:</p>\r\n<ul style=\"padding: 0px;\">\r\n<li class=\"first-child\">Please post all quotes separately, so that they can be voted up/down separately.&nbsp;&nbsp;(If they are strongly related, reply to your own comments.&nbsp;&nbsp;If strongly ordered, then go ahead and post them together.) </li>\r\n<li>Do not quote yourself. </li>\r\n<li>Do not quote comments/posts on LW/OB. </li>\r\n<li class=\"last-child\">No more than 5 quotes per person per monthly thread, please. </li>\r\n</ul>\r\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iG5v5uDCCkkoG6zmd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "15714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-01T16:10:22.262Z", "modifiedAt": null, "url": null, "title": "Meetup : Big Berkeley Meetup", "slug": "meetup-big-berkeley-meetup-4", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thausler", "createdAt": "2010-05-24T04:40:49.214Z", "isAdmin": false, "displayName": "Thausler"}, "userId": "oPa5EPBs6ow6MY2Ao", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rCthD4ZyxikFLnnCh/meetup-big-berkeley-meetup-4", "pageUrlRelative": "/posts/rCthD4ZyxikFLnnCh/meetup-big-berkeley-meetup-4", "linkUrl": "https://www.lesswrong.com/posts/rCthD4ZyxikFLnnCh/meetup-big-berkeley-meetup-4", "postedAtFormatted": "Tuesday, May 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Big%20Berkeley%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Big%20Berkeley%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrCthD4ZyxikFLnnCh%2Fmeetup-big-berkeley-meetup-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Big%20Berkeley%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrCthD4ZyxikFLnnCh%2Fmeetup-big-berkeley-meetup-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrCthD4ZyxikFLnnCh%2Fmeetup-big-berkeley-meetup-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9q'>Big Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow there will be a big Berkeley meetup. We will be meeting at 7:00pm at the Starbucks on Oxford St (2128 Oxford St, Berkeley, CA).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9q'>Big Berkeley Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rCthD4ZyxikFLnnCh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.944998302858764e-07, "legacy": true, "legacyId": "15719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_Meetup\">Discussion article for the meetup : <a href=\"/meetups/9q\">Big Berkeley Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 May 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford St, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow there will be a big Berkeley meetup. We will be meeting at 7:00pm at the Starbucks on Oxford St (2128 Oxford St, Berkeley, CA).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/9q\">Big Berkeley Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Big Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Big Berkeley Meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-01T23:37:37.317Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes May 2012", "slug": "rationality-quotes-may-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:08.769Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t4ph3ndRq6Nx4xQhB/rationality-quotes-may-2012", "pageUrlRelative": "/posts/t4ph3ndRq6Nx4xQhB/rationality-quotes-may-2012", "linkUrl": "https://www.lesswrong.com/posts/t4ph3ndRq6Nx4xQhB/rationality-quotes-may-2012", "postedAtFormatted": "Tuesday, May 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20May%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20May%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4ph3ndRq6Nx4xQhB%2Frationality-quotes-may-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20May%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4ph3ndRq6Nx4xQhB%2Frationality-quotes-may-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4ph3ndRq6Nx4xQhB%2Frationality-quotes-may-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>\n<div id=\"entry_t3_bdo\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Here's the new thread for posting quotes, with the usual rules:</span></p>\n<ul>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Please  post all quotes separately, so that they can be voted up/down  separately. &nbsp;(If they are strongly related, reply to your own comments.  &nbsp;If strongly ordered, then go ahead and post them together.)</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote yourself</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote comments/posts on LW/OB</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">No more than 5 quotes per person per monthly thread, please.</span></li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t4ph3ndRq6Nx4xQhB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 8.946925533288131e-07, "legacy": true, "legacyId": "15713", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 705, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-02T01:34:28.363Z", "modifiedAt": null, "url": null, "title": "May 2012 Media Thread", "slug": "may-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:56.720Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B58diD8GJKkPPTB5F/may-2012-media-thread", "pageUrlRelative": "/posts/B58diD8GJKkPPTB5F/may-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/B58diD8GJKkPPTB5F/may-2012-media-thread", "postedAtFormatted": "Wednesday, May 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20May%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMay%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB58diD8GJKkPPTB5F%2Fmay-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=May%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB58diD8GJKkPPTB5F%2Fmay-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB58diD8GJKkPPTB5F%2Fmay-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. I find that reading the sequences makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">Rules:</p>\n<ul style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres, which&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/94z/january_2012_media_thread/5kos\">I was apparently too dumb to do</a>.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B58diD8GJKkPPTB5F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 8.947431647082218e-07, "legacy": true, "legacyId": "15722", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-02T04:48:25.600Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Many Worlds, One Best Guess", "slug": "seq-rerun-many-worlds-one-best-guess", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jfdwkkbMfWCBwPxAz/seq-rerun-many-worlds-one-best-guess", "pageUrlRelative": "/posts/jfdwkkbMfWCBwPxAz/seq-rerun-many-worlds-one-best-guess", "linkUrl": "https://www.lesswrong.com/posts/jfdwkkbMfWCBwPxAz/seq-rerun-many-worlds-one-best-guess", "postedAtFormatted": "Wednesday, May 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Many%20Worlds%2C%20One%20Best%20Guess&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Many%20Worlds%2C%20One%20Best%20Guess%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjfdwkkbMfWCBwPxAz%2Fseq-rerun-many-worlds-one-best-guess%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Many%20Worlds%2C%20One%20Best%20Guess%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjfdwkkbMfWCBwPxAz%2Fseq-rerun-many-worlds-one-best-guess", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjfdwkkbMfWCBwPxAz%2Fseq-rerun-many-worlds-one-best-guess", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Today's post, <a href=\"/lw/q8/many_worlds_one_best_guess/\">Many Worlds, One Best Guess</a> was originally published on 11 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Summarizes the arguments that nail down macroscopic decoherence, aka the \"many-worlds interpretation\". Concludes that many-worlds <em>wins outright</em> given the current state of evidence. The argument should have been over fifty years ago. New physical evidence could reopen it, but we have no particular reason to expect this.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c4e/seq_rerun_if_manyworlds_had_come_first/\">If Many-Worlds Had Come First</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jfdwkkbMfWCBwPxAz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 8.948268569290413e-07, "legacy": true, "legacyId": "15729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["S8ysHqeRGuySPttrS", "nMZobjR6vD8Kun5W9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-02T14:03:13.182Z", "modifiedAt": null, "url": null, "title": "Case Study: Testing Confirmation Bias", "slug": "case-study-testing-confirmation-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yNLiJZsNrNP842P7p/case-study-testing-confirmation-bias", "pageUrlRelative": "/posts/yNLiJZsNrNP842P7p/case-study-testing-confirmation-bias", "linkUrl": "https://www.lesswrong.com/posts/yNLiJZsNrNP842P7p/case-study-testing-confirmation-bias", "postedAtFormatted": "Wednesday, May 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Case%20Study%3A%20Testing%20Confirmation%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACase%20Study%3A%20Testing%20Confirmation%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNLiJZsNrNP842P7p%2Fcase-study-testing-confirmation-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Case%20Study%3A%20Testing%20Confirmation%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNLiJZsNrNP842P7p%2Fcase-study-testing-confirmation-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNLiJZsNrNP842P7p%2Fcase-study-testing-confirmation-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>Master copy lives on <a href=\"http://www.gwern.net/Prediction%20markets#case-study-testing-confirmation-bias\">gwern.net</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5hpGj9nDLgokfghvR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yNLiJZsNrNP842P7p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 51, "extendedScore": null, "score": 8.95066329415815e-07, "legacy": true, "legacyId": "15747", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T01:35:57.920Z", "modifiedAt": null, "url": null, "title": "Punctuality - Arriving on Time and Math", "slug": "punctuality-arriving-on-time-and-math", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:06.635Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Xachariah", "createdAt": "2011-02-03T00:44:58.546Z", "isAdmin": false, "displayName": "Xachariah"}, "userId": "sD2senrXqugoP4wit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YuZXRxWSqaCoZHEXr/punctuality-arriving-on-time-and-math", "pageUrlRelative": "/posts/YuZXRxWSqaCoZHEXr/punctuality-arriving-on-time-and-math", "linkUrl": "https://www.lesswrong.com/posts/YuZXRxWSqaCoZHEXr/punctuality-arriving-on-time-and-math", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Punctuality%20-%20Arriving%20on%20Time%20and%20Math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APunctuality%20-%20Arriving%20on%20Time%20and%20Math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYuZXRxWSqaCoZHEXr%2Fpunctuality-arriving-on-time-and-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Punctuality%20-%20Arriving%20on%20Time%20and%20Math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYuZXRxWSqaCoZHEXr%2Fpunctuality-arriving-on-time-and-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYuZXRxWSqaCoZHEXr%2Fpunctuality-arriving-on-time-and-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1464, "htmlBody": "<p>In hindsight, this post seems incredibly obvious.&nbsp; The meat of it already exists in sayings which we all know we ought to listen to: \"<em>Always arrive 10 minutes earlier than you think early is</em>,\" \"<em>If you arrive on time, then you're late</em>,\" or \"<em>Better three hours too soon than one minute too late</em>.\" Yet even with these sayings, I still never trusted them nor arrived on time.&nbsp; I'd miss deadlines, show up late, and just be generally tardy.&nbsp; The reason is that I never truly understood what it took to arrive on time until I grokked the math of it.&nbsp; So, while this may be remedial reading for most of you, I'm posting this because maybe there's someone out there who missed the same obviousness that I missed.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Statistical Distributions</strong></p>\n<p>Everyone here understands that our universe is controlled and explained by math.&nbsp; Math describes how heavenly bodies move.&nbsp; Math describes how our computers run.&nbsp; Math describes how other people act in aggregate.&nbsp; Wait a second, something's not right with that statement... \"<em>other people</em>\".&nbsp; The way it comes out it's natural to think that math controls the way that <em>other people</em> act, and not myself.&nbsp; Intellectually, I am aware that I am not a special snowflake who is exempt from the laws of math.&nbsp; While I had managed to propagate this thought far enough to crush my belief in libertarian free will, I hadn't propagated it fully through my mind.&nbsp; Specifically, I hadn't realized I could also use math to describe my actions and reap the benefit of understanding them mathematically.&nbsp; I was still late to arrive and missing deadlines, and nothing seemed to help.</p>\n<p>&nbsp;</p>\n<p>But wait, I'm a rationalist!&nbsp; I know all about the <a href=\"/lw/jg/planning_fallacy\">planning fallacy</a>; I know to take the <a href=\"/lw/ri/the_outside_views_domain\">outside view</a>!&nbsp; That's enough to save me right?&nbsp; Well, not quite.&nbsp; It seemed I missed one last part of the puzzle... <em>Bell Curves</em>.</p>\n<p>&nbsp;</p>\n<p>When I go to work every day, the time from when I do nothing but getting ready to go to work until the time that I actually arrive there (I'll just call this prep time) usually takes 45 minutes, but sometimes it can take more time or less time.&nbsp; Weirdly and crazily enough, if you plot all the prep times on a graph, the shape would end up looking roughly <em>like a bell</em>.&nbsp; Well that's funny.&nbsp; Math is for <em>other people</em>, but my behavior appears like it can be described statistically.&nbsp; Some days I will have deviations from the normal routine that help me arrive faster while other days will have things that slow me down.&nbsp; Some of them happen more often, some of them happen less often.&nbsp; <em>If</em> I were describable by math, I could almost call these things standard deviations: days where I have almost zero traffic prep time takes 1 standard deviation less, days when I can't find my car keys my prep time takes 1 standard deviation more,&nbsp; days I realize would be late and skip showering take 2 standard deviations less, and days when there is a terrible accident on the freeway end up requiring +2 or +3 standard deviations more in time.&nbsp; To put it in other words, my prep time is a bell curve, and I've got 1-sigma and 2-sigma (and occasionally 3-sigma) events speeding me up and slowing me down.</p>\n<p>&nbsp;</p>\n<p>This holds true for more than just going to work.&nbsp; Everything's time-until-completion can be described this way: project completion times, homework, going to the airport, the duration of foreplay and sex.&nbsp; <em>Everything</em>.&nbsp; It's not always bell curves, but it's a probability distribution with respect to completion times, and that can help give useful insights.</p>\n<p>&nbsp;</p>\n<p><strong>Starting 'On Time' Means You Won't be On Time</strong></p>\n<p>What do we gain by understanding that our actions are described by a probability distribution?&nbsp; The first and most important take away is this: If you only allocate the exact amount of time to do something, you'll be late 50% of the time.&nbsp; I'm going to repeat it and italicize because I think it's that important of a point.&nbsp; <em>If you only allocate the exact amount of time to do something, you'll be late 50% of the time.</em>&nbsp; That's the way bell curves work.</p>\n<p>&nbsp;</p>\n<p>I know I've heard jokes about how 90% of the population has above average children, but it wasn't until I really looked at the math of my behavior that I realized I was doing the exact same thing.&nbsp; I'd say \"oh it takes me 45 minutes on average to go to work every day, so I'll leave at 7:15.\"&nbsp; Yet I never realized that I was completely ignoring that <em>half</em> the time would take longer than average.&nbsp; So half the time, I'd end up be pressed for time and have to skip shaving (or something) or I'd end up late. I was terribly unpunctual until I realized I that I had to arrive early to always arrive on time.&nbsp; \"<em>If you arrive on time, then you are late.\"</em>&nbsp; Hmm.&nbsp; You win this one, folk wisdom.</p>\n<p>&nbsp;</p>\n<p>Still, the question remained.&nbsp; How much early would it take to never be late?&nbsp; The answer lay in bell curves.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Acceptable Lateness and Standard deviation</strong></p>\n<p>Looking at time requirements as a bell curve implies another thing: One can never completely eliminate all lateness; the only option is to make a choice about what probability of lateness is acceptable.&nbsp; A person must decide what lateness ratio they're willing to take, and then start prepping that many standard deviations beforehand.&nbsp; And, despite what employers say, <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">0% is not a probability</a>.</p>\n<p>If my prep time averages 45 minutes with a standard deviation of 10 minutes then that means...</p>\n<ul>\n<li>Starting 45 minutes beforehand will force me to be late or miss services (eg shaving) around <strong>50%</strong> of the time or <strong>about 10 workdays a month</strong>.</li>\n<li>Starting 55 minutes beforehand will force me to be late or miss services (eg shaving) around <strong>16%</strong> of the time or <strong>about</strong> <strong>3 workdays a month</strong>.</li>\n<li>Starting 65 minutes beforehand will force me to be late or miss services (eg shaving) around <strong>2.3%</strong> of the time or <strong>about 1 day every other month</strong>.</li>\n</ul>\n<p>That's really good risk reduction for a small amount of time spent.&nbsp; (NB, remember that averages are dangerous little things.&nbsp; Taking this to a meta level, consider that being late to work <strong><em>about</em></strong> 3 times a month isn't helpful if you arrive late only once the first month, then get fired the next month when you arrive late 5 times. Hence, <em>\"Always arrive 10 minutes earlier than you think early is.\"</em>&nbsp; God I hate folk wisdom, especially when it's right.)</p>\n<p>&nbsp;</p>\n<p>The risk level you're acceptable with dictates how much time you need for padding.&nbsp; For job interviews, I'm only willing to arrive late to 1 in 1000, so I prepare 3 standard deviations early now.&nbsp; For first dates, I'm willing to miss about 5%.&nbsp; For dinners with the family, I'm okay with being late half the time.&nbsp; It feels <em>similar</em> to the algorithm I used before, which was a sort of ad-hoc thing where I'd prepared earlier for important things.&nbsp; The main difference is that now I can quantify the risk I'm assuming when I procrastinate.&nbsp; It causes each procrastination to become more concrete for me, and drastically reduces the chance that I'll be willing to make those tradeoffs.&nbsp; Instead of being willing to read lesswrong for 10 more minutes in exchange for \"oh I might have to rush\", I can now see that it would increase my chance of being late from 16% to 50%, which is flatly unacceptable.&nbsp; Viewing procrastination in terms of the latter tradeoff makes it much easier to get myself moving.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The last quote is \"<em>Better three hours too soon than one minute too late</em>.\"&nbsp; I'm glad that at least that one's wrong.&nbsp; I'm sure <a href=\"http://www.scottaaronson.com/blog/?p=40\">Umesh</a> would have some stern words for that saying.&nbsp; My key to arriving on time is locating your acceptable risk threshold and making an informed decision about how much risk you are willing to take.</p>\n<p>&nbsp;</p>\n<p><strong>Summary</strong></p>\n<p>The time it takes for you to complete any task is (usually) described by a bell curve.&nbsp; How much time you think you'll take is a lie, and not just because of the <a href=\"/lw/jg/planning_fallacy/\">planning fallacy</a>.&nbsp; Even if you do the sciency-thing and take the outside view, it's still not enough to keep you from getting fired or showing up to your interview late.&nbsp; To consistently show up on time, you <em>must </em>incorporate padding time.</p>\n<p>&nbsp;</p>\n<p>So I've got a new saying, \"<em>If you wish to be late only 2.3% of the time, you must start getting ready at least two standard deviations before the average prep time you have needed historically.</em>\"&nbsp; I wish my mom would have told me this one.&nbsp; It's so much easier to understand than all those other sayings!</p>\n<p><strong><br /></strong></p>\n<p><sub>(Also my first actual article-thingy, so any comments or suggestions is welcome)</sub></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "fkABsGCJZ6y9qConW": 1, "4R8JYu4QF2FqzJxE5": 1, "KoXbd2HmbdRfqLngk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YuZXRxWSqaCoZHEXr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 100, "baseScore": 132, "extendedScore": null, "score": 0.000274, "legacy": true, "legacyId": "15756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 133, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In hindsight, this post seems incredibly obvious.&nbsp; The meat of it already exists in sayings which we all know we ought to listen to: \"<em>Always arrive 10 minutes earlier than you think early is</em>,\" \"<em>If you arrive on time, then you're late</em>,\" or \"<em>Better three hours too soon than one minute too late</em>.\" Yet even with these sayings, I still never trusted them nor arrived on time.&nbsp; I'd miss deadlines, show up late, and just be generally tardy.&nbsp; The reason is that I never truly understood what it took to arrive on time until I grokked the math of it.&nbsp; So, while this may be remedial reading for most of you, I'm posting this because maybe there's someone out there who missed the same obviousness that I missed.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Statistical_Distributions\">Statistical Distributions</strong></p>\n<p>Everyone here understands that our universe is controlled and explained by math.&nbsp; Math describes how heavenly bodies move.&nbsp; Math describes how our computers run.&nbsp; Math describes how other people act in aggregate.&nbsp; Wait a second, something's not right with that statement... \"<em>other people</em>\".&nbsp; The way it comes out it's natural to think that math controls the way that <em>other people</em> act, and not myself.&nbsp; Intellectually, I am aware that I am not a special snowflake who is exempt from the laws of math.&nbsp; While I had managed to propagate this thought far enough to crush my belief in libertarian free will, I hadn't propagated it fully through my mind.&nbsp; Specifically, I hadn't realized I could also use math to describe my actions and reap the benefit of understanding them mathematically.&nbsp; I was still late to arrive and missing deadlines, and nothing seemed to help.</p>\n<p>&nbsp;</p>\n<p>But wait, I'm a rationalist!&nbsp; I know all about the <a href=\"/lw/jg/planning_fallacy\">planning fallacy</a>; I know to take the <a href=\"/lw/ri/the_outside_views_domain\">outside view</a>!&nbsp; That's enough to save me right?&nbsp; Well, not quite.&nbsp; It seemed I missed one last part of the puzzle... <em>Bell Curves</em>.</p>\n<p>&nbsp;</p>\n<p>When I go to work every day, the time from when I do nothing but getting ready to go to work until the time that I actually arrive there (I'll just call this prep time) usually takes 45 minutes, but sometimes it can take more time or less time.&nbsp; Weirdly and crazily enough, if you plot all the prep times on a graph, the shape would end up looking roughly <em>like a bell</em>.&nbsp; Well that's funny.&nbsp; Math is for <em>other people</em>, but my behavior appears like it can be described statistically.&nbsp; Some days I will have deviations from the normal routine that help me arrive faster while other days will have things that slow me down.&nbsp; Some of them happen more often, some of them happen less often.&nbsp; <em>If</em> I were describable by math, I could almost call these things standard deviations: days where I have almost zero traffic prep time takes 1 standard deviation less, days when I can't find my car keys my prep time takes 1 standard deviation more,&nbsp; days I realize would be late and skip showering take 2 standard deviations less, and days when there is a terrible accident on the freeway end up requiring +2 or +3 standard deviations more in time.&nbsp; To put it in other words, my prep time is a bell curve, and I've got 1-sigma and 2-sigma (and occasionally 3-sigma) events speeding me up and slowing me down.</p>\n<p>&nbsp;</p>\n<p>This holds true for more than just going to work.&nbsp; Everything's time-until-completion can be described this way: project completion times, homework, going to the airport, the duration of foreplay and sex.&nbsp; <em>Everything</em>.&nbsp; It's not always bell curves, but it's a probability distribution with respect to completion times, and that can help give useful insights.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Starting__On_Time__Means_You_Won_t_be_On_Time\">Starting 'On Time' Means You Won't be On Time</strong></p>\n<p>What do we gain by understanding that our actions are described by a probability distribution?&nbsp; The first and most important take away is this: If you only allocate the exact amount of time to do something, you'll be late 50% of the time.&nbsp; I'm going to repeat it and italicize because I think it's that important of a point.&nbsp; <em>If you only allocate the exact amount of time to do something, you'll be late 50% of the time.</em>&nbsp; That's the way bell curves work.</p>\n<p>&nbsp;</p>\n<p>I know I've heard jokes about how 90% of the population has above average children, but it wasn't until I really looked at the math of my behavior that I realized I was doing the exact same thing.&nbsp; I'd say \"oh it takes me 45 minutes on average to go to work every day, so I'll leave at 7:15.\"&nbsp; Yet I never realized that I was completely ignoring that <em>half</em> the time would take longer than average.&nbsp; So half the time, I'd end up be pressed for time and have to skip shaving (or something) or I'd end up late. I was terribly unpunctual until I realized I that I had to arrive early to always arrive on time.&nbsp; \"<em>If you arrive on time, then you are late.\"</em>&nbsp; Hmm.&nbsp; You win this one, folk wisdom.</p>\n<p>&nbsp;</p>\n<p>Still, the question remained.&nbsp; How much early would it take to never be late?&nbsp; The answer lay in bell curves.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Acceptable_Lateness_and_Standard_deviation\">Acceptable Lateness and Standard deviation</strong></p>\n<p>Looking at time requirements as a bell curve implies another thing: One can never completely eliminate all lateness; the only option is to make a choice about what probability of lateness is acceptable.&nbsp; A person must decide what lateness ratio they're willing to take, and then start prepping that many standard deviations beforehand.&nbsp; And, despite what employers say, <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">0% is not a probability</a>.</p>\n<p>If my prep time averages 45 minutes with a standard deviation of 10 minutes then that means...</p>\n<ul>\n<li>Starting 45 minutes beforehand will force me to be late or miss services (eg shaving) around <strong>50%</strong> of the time or <strong>about 10 workdays a month</strong>.</li>\n<li>Starting 55 minutes beforehand will force me to be late or miss services (eg shaving) around <strong>16%</strong> of the time or <strong>about</strong> <strong>3 workdays a month</strong>.</li>\n<li>Starting 65 minutes beforehand will force me to be late or miss services (eg shaving) around <strong>2.3%</strong> of the time or <strong>about 1 day every other month</strong>.</li>\n</ul>\n<p>That's really good risk reduction for a small amount of time spent.&nbsp; (NB, remember that averages are dangerous little things.&nbsp; Taking this to a meta level, consider that being late to work <strong><em>about</em></strong> 3 times a month isn't helpful if you arrive late only once the first month, then get fired the next month when you arrive late 5 times. Hence, <em>\"Always arrive 10 minutes earlier than you think early is.\"</em>&nbsp; God I hate folk wisdom, especially when it's right.)</p>\n<p>&nbsp;</p>\n<p>The risk level you're acceptable with dictates how much time you need for padding.&nbsp; For job interviews, I'm only willing to arrive late to 1 in 1000, so I prepare 3 standard deviations early now.&nbsp; For first dates, I'm willing to miss about 5%.&nbsp; For dinners with the family, I'm okay with being late half the time.&nbsp; It feels <em>similar</em> to the algorithm I used before, which was a sort of ad-hoc thing where I'd prepared earlier for important things.&nbsp; The main difference is that now I can quantify the risk I'm assuming when I procrastinate.&nbsp; It causes each procrastination to become more concrete for me, and drastically reduces the chance that I'll be willing to make those tradeoffs.&nbsp; Instead of being willing to read lesswrong for 10 more minutes in exchange for \"oh I might have to rush\", I can now see that it would increase my chance of being late from 16% to 50%, which is flatly unacceptable.&nbsp; Viewing procrastination in terms of the latter tradeoff makes it much easier to get myself moving.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The last quote is \"<em>Better three hours too soon than one minute too late</em>.\"&nbsp; I'm glad that at least that one's wrong.&nbsp; I'm sure <a href=\"http://www.scottaaronson.com/blog/?p=40\">Umesh</a> would have some stern words for that saying.&nbsp; My key to arriving on time is locating your acceptable risk threshold and making an informed decision about how much risk you are willing to take.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Summary\">Summary</strong></p>\n<p>The time it takes for you to complete any task is (usually) described by a bell curve.&nbsp; How much time you think you'll take is a lie, and not just because of the <a href=\"/lw/jg/planning_fallacy/\">planning fallacy</a>.&nbsp; Even if you do the sciency-thing and take the outside view, it's still not enough to keep you from getting fired or showing up to your interview late.&nbsp; To consistently show up on time, you <em>must </em>incorporate padding time.</p>\n<p>&nbsp;</p>\n<p>So I've got a new saying, \"<em>If you wish to be late only 2.3% of the time, you must start getting ready at least two standard deviations before the average prep time you have needed historically.</em>\"&nbsp; I wish my mom would have told me this one.&nbsp; It's so much easier to understand than all those other sayings!</p>\n<p><strong><br></strong></p>\n<p><sub>(Also my first actual article-thingy, so any comments or suggestions is welcome)</sub></p>", "sections": [{"title": "Statistical Distributions", "anchor": "Statistical_Distributions", "level": 1}, {"title": "Starting 'On Time' Means You Won't be On Time", "anchor": "Starting__On_Time__Means_You_Won_t_be_On_Time", "level": 1}, {"title": "Acceptable Lateness and Standard deviation", "anchor": "Acceptable_Lateness_and_Standard_deviation", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "40 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CPm5LTwHrvBJCa9h5", "pqoxE3AGMbse68dvb", "QGkYCwyC7wTDyt3yT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T02:42:27.005Z", "modifiedAt": null, "url": null, "title": "[POLL] Do You Feel Oppressed? ", "slug": "poll-do-you-feel-oppressed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:36.194Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3WsminxtcheCMM2aP/poll-do-you-feel-oppressed", "pageUrlRelative": "/posts/3WsminxtcheCMM2aP/poll-do-you-feel-oppressed", "linkUrl": "https://www.lesswrong.com/posts/3WsminxtcheCMM2aP/poll-do-you-feel-oppressed", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%5D%20Do%20You%20Feel%20Oppressed%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%5D%20Do%20You%20Feel%20Oppressed%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WsminxtcheCMM2aP%2Fpoll-do-you-feel-oppressed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%5D%20Do%20You%20Feel%20Oppressed%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WsminxtcheCMM2aP%2Fpoll-do-you-feel-oppressed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WsminxtcheCMM2aP%2Fpoll-do-you-feel-oppressed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p><span style=\"color: #333333; font-family: 'Droid Sans', Arial, sans-serif; font-size: 13px; line-height: 20px; white-space: pre-wrap;\">At Reason Rally a couple of months ago, we noticed that a lot of atheists there seemed to be there for mutual support - because their own communities rejected atheists, because they felt outnumbered and threatened by their peers; the rally was a way for them to feel part of an in-group. Reason Rally is definitely an event that selects for people who feel excluded by their communities most of the time. But there may be a different concentration of people who have had this sort of experience on LessWrong, and we wondered what that concentration was.</span></p>\n<p>Hence, this survey: <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHJORmlpTDJDQlc3d0cyNXpHcXJCT3c6MQ#gid=0\">LessWrong Members and their Local Communities</a>.</p>\n<p>If I get a decent sample size, I will post the data for all to enjoy.</p>\n<p>EDIT: I added two questions about current and previous religious views to the poll. If you took it before 11:30PM EST 5/2, I'd appreciate it very much if you would take the time to retake it. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3WsminxtcheCMM2aP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "15764", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T03:30:12.105Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Failures of Eld Science", "slug": "seq-rerun-the-failures-of-eld-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.101Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/anRJKFba6YzmM3Eoc/seq-rerun-the-failures-of-eld-science", "pageUrlRelative": "/posts/anRJKFba6YzmM3Eoc/seq-rerun-the-failures-of-eld-science", "linkUrl": "https://www.lesswrong.com/posts/anRJKFba6YzmM3Eoc/seq-rerun-the-failures-of-eld-science", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Failures%20of%20Eld%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Failures%20of%20Eld%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanRJKFba6YzmM3Eoc%2Fseq-rerun-the-failures-of-eld-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Failures%20of%20Eld%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanRJKFba6YzmM3Eoc%2Fseq-rerun-the-failures-of-eld-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanRJKFba6YzmM3Eoc%2Fseq-rerun-the-failures-of-eld-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/q9/the_failures_of_eld_science/\">The Failures of Eld Science</a> was originally published on 12 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Failures_of_Eld_Science\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A short story set in the same world as \"Initiation Ceremony\". Future physics students look back on the cautionary tale of quantum physics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c4x/seq_rerun_many_worlds_one_best_guess/\">Many Worlds, One Best Guess</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "anRJKFba6YzmM3Eoc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.954148603411487e-07, "legacy": true, "legacyId": "15767", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZxR8P8hBFQ9kC8wMy", "jfdwkkbMfWCBwPxAz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T11:11:17.701Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Sydney - Bayes and Fun Theory", "slug": "meetup-less-wrong-sydney-bayes-and-fun-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:39.509Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u4ZrRWHRdQzM6BGTo/meetup-less-wrong-sydney-bayes-and-fun-theory", "pageUrlRelative": "/posts/u4ZrRWHRdQzM6BGTo/meetup-less-wrong-sydney-bayes-and-fun-theory", "linkUrl": "https://www.lesswrong.com/posts/u4ZrRWHRdQzM6BGTo/meetup-less-wrong-sydney-bayes-and-fun-theory", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Sydney%20-%20Bayes%20and%20Fun%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Sydney%20-%20Bayes%20and%20Fun%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4ZrRWHRdQzM6BGTo%2Fmeetup-less-wrong-sydney-bayes-and-fun-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Sydney%20-%20Bayes%20and%20Fun%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4ZrRWHRdQzM6BGTo%2Fmeetup-less-wrong-sydney-bayes-and-fun-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4ZrRWHRdQzM6BGTo%2Fmeetup-less-wrong-sydney-bayes-and-fun-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9r'>Less Wrong Sydney - Bayes and Fun Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 May 2012 06:00:17PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As suggested via the FB group, this coming meeting's topic will be Bayes and Fun theory. Now looking for Bayes games!</p>\n\n<p>We are making this regular now - So every 3rd week, alternating Mondays/Fridays. As before, we will meet at Cafe Norita for a topic as yet fleshed out. Join us on facebook for more info, or comment here for your suggestions!</p>\n\n<p><a href=\"http://www.facebook.com/groups/219526434802422/\" rel=\"nofollow\">http://www.facebook.com/groups/219526434802422/</a></p>\n\n<p>P.S. Just a congratulations to the \"Post Hocs\" (so classy guys) on their win at last meetings' Fact or Crap game. Their superior use of confidence values to justify wild guessing was sublime!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9r'>Less Wrong Sydney - Bayes and Fun Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u4ZrRWHRdQzM6BGTo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.956141117028028e-07, "legacy": true, "legacyId": "15779", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney___Bayes_and_Fun_Theory\">Discussion article for the meetup : <a href=\"/meetups/9r\">Less Wrong Sydney - Bayes and Fun Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 May 2012 06:00:17PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">77 Liverpool Street, Sydney, Australia, Level 2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As suggested via the FB group, this coming meeting's topic will be Bayes and Fun theory. Now looking for Bayes games!</p>\n\n<p>We are making this regular now - So every 3rd week, alternating Mondays/Fridays. As before, we will meet at Cafe Norita for a topic as yet fleshed out. Join us on facebook for more info, or comment here for your suggestions!</p>\n\n<p><a href=\"http://www.facebook.com/groups/219526434802422/\" rel=\"nofollow\">http://www.facebook.com/groups/219526434802422/</a></p>\n\n<p>P.S. Just a congratulations to the \"Post Hocs\" (so classy guys) on their win at last meetings' Fact or Crap game. Their superior use of confidence values to justify wild guessing was sublime!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney___Bayes_and_Fun_Theory1\">Discussion article for the meetup : <a href=\"/meetups/9r\">Less Wrong Sydney - Bayes and Fun Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Sydney - Bayes and Fun Theory", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney___Bayes_and_Fun_Theory", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Sydney - Bayes and Fun Theory", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney___Bayes_and_Fun_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T11:17:11.885Z", "modifiedAt": null, "url": null, "title": "Another reason why a lot of studies may be wrong", "slug": "another-reason-why-a-lot-of-studies-may-be-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.036Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tARMisKrG8CfF6ivt/another-reason-why-a-lot-of-studies-may-be-wrong", "pageUrlRelative": "/posts/tARMisKrG8CfF6ivt/another-reason-why-a-lot-of-studies-may-be-wrong", "linkUrl": "https://www.lesswrong.com/posts/tARMisKrG8CfF6ivt/another-reason-why-a-lot-of-studies-may-be-wrong", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20reason%20why%20a%20lot%20of%20studies%20may%20be%20wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20reason%20why%20a%20lot%20of%20studies%20may%20be%20wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtARMisKrG8CfF6ivt%2Fanother-reason-why-a-lot-of-studies-may-be-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20reason%20why%20a%20lot%20of%20studies%20may%20be%20wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtARMisKrG8CfF6ivt%2Fanother-reason-why-a-lot-of-studies-may-be-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtARMisKrG8CfF6ivt%2Fanother-reason-why-a-lot-of-studies-may-be-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p>It appears that standard lab rats and mice are all morbidly obese. Using them as model organisms may give misleading results that fail to transfer to humans, or even to healthy rats and mice.</p>\n<p><a href=\"http://www.slate.com/articles/health_and_science/the_mouse_trap/2011/11/lab_mice_are_they_limiting_our_understanding_of_human_disease_.html\">Article in the Slate</a>.</p>\n<p><a href=\"http://www.pnas.org/content/107/14/6127.full\">PNAS version</a>.</p>\n<p>Does this reduce the whole <a href=\"http://en.wikipedia.org/wiki/Calorie_restriction\">calorie-restriction thing</a> to nothing more than the advice to not be a fat slob? Well, maybe not, according to the <a href=\"http://www.nature.com/nm/journal/v10/n4/full/nm0404-324.html\">author</a>, but it has to make one wonder.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tARMisKrG8CfF6ivt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 8.956166630729099e-07, "legacy": true, "legacyId": "15780", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T15:30:26.852Z", "modifiedAt": null, "url": null, "title": "Meetup : First Berlin meetup", "slug": "meetup-first-berlin-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:59.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xBNukbtgHWmkWqNAq/meetup-first-berlin-meetup", "pageUrlRelative": "/posts/xBNukbtgHWmkWqNAq/meetup-first-berlin-meetup", "linkUrl": "https://www.lesswrong.com/posts/xBNukbtgHWmkWqNAq/meetup-first-berlin-meetup", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Berlin%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Berlin%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxBNukbtgHWmkWqNAq%2Fmeetup-first-berlin-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Berlin%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxBNukbtgHWmkWqNAq%2Fmeetup-first-berlin-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxBNukbtgHWmkWqNAq%2Fmeetup-first-berlin-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9s'>First Berlin meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 June 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">52.525856,13.387371</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the first public meetup in Berlin! Some local LW readers have met before, but the meetups were never announced beforehand. We're very interested in getting to know other like-minded people in the area.</p>\n\n<p>We'll be meeting in the Trattoria Peretti, Friedrichstr. 133, in their courtyard if the weather permits it. I'll bring a sign. If you plan to attend, please let me know so I can make sure we get a well-sized table.</p>\n\n<p>Note that google maps gets the location wrong for Friedrichstr. 133. The correct location is approximately 52.525856,13.387371.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9s'>First Berlin meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xBNukbtgHWmkWqNAq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.95726133284396e-07, "legacy": true, "legacyId": "15781", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Berlin_meetup\">Discussion article for the meetup : <a href=\"/meetups/9s\">First Berlin meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 June 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">52.525856,13.387371</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's the first public meetup in Berlin! Some local LW readers have met before, but the meetups were never announced beforehand. We're very interested in getting to know other like-minded people in the area.</p>\n\n<p>We'll be meeting in the Trattoria Peretti, Friedrichstr. 133, in their courtyard if the weather permits it. I'll bring a sign. If you plan to attend, please let me know so I can make sure we get a well-sized table.</p>\n\n<p>Note that google maps gets the location wrong for Friedrichstr. 133. The correct location is approximately 52.525856,13.387371.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Berlin_meetup1\">Discussion article for the meetup : <a href=\"/meetups/9s\">First Berlin meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Berlin meetup", "anchor": "Discussion_article_for_the_meetup___First_Berlin_meetup", "level": 1}, {"title": "Discussion article for the meetup : First Berlin meetup", "anchor": "Discussion_article_for_the_meetup___First_Berlin_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T15:33:10.282Z", "modifiedAt": null, "url": null, "title": "Fiction: A Setting Justifying the Epistemic Aggressiveness Of A Religion Stand-in", "slug": "fiction-a-setting-justifying-the-epistemic-aggressiveness-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:46.424Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dkYvBCQAr6SsTQFc5/fiction-a-setting-justifying-the-epistemic-aggressiveness-of", "pageUrlRelative": "/posts/dkYvBCQAr6SsTQFc5/fiction-a-setting-justifying-the-epistemic-aggressiveness-of", "linkUrl": "https://www.lesswrong.com/posts/dkYvBCQAr6SsTQFc5/fiction-a-setting-justifying-the-epistemic-aggressiveness-of", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fiction%3A%20A%20Setting%20Justifying%20the%20Epistemic%20Aggressiveness%20Of%20A%20Religion%20Stand-in&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFiction%3A%20A%20Setting%20Justifying%20the%20Epistemic%20Aggressiveness%20Of%20A%20Religion%20Stand-in%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkYvBCQAr6SsTQFc5%2Ffiction-a-setting-justifying-the-epistemic-aggressiveness-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fiction%3A%20A%20Setting%20Justifying%20the%20Epistemic%20Aggressiveness%20Of%20A%20Religion%20Stand-in%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkYvBCQAr6SsTQFc5%2Ffiction-a-setting-justifying-the-epistemic-aggressiveness-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkYvBCQAr6SsTQFc5%2Ffiction-a-setting-justifying-the-epistemic-aggressiveness-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 343, "htmlBody": "<p>I've just been thinking (a dangerous pastime, I know) and remembering my religious days, and one of the things that made me so anxious back then, and made adopting atheism so gratifying, what the sheer <em>drought</em> of information. I had so many, so many questions, and there were no satisfying answers to be found.</p>\n<p>How do souls work? When one dies, what does bodiless existence feel like? What happens in Resurrection Day, do cripples come back as such? Do transsexuals come in the body they identify with? Is it a matter of self-image?</p>\n<p>How does God intervene in the gaps? What will Paradise be like? How will society be organized? Will there be further education? Will human nature be radically changed to accomodate eternity? What about Hell? How does it work? What's the point of it?</p>\n<p>And what does <em>that</em> verse mean? And what does <em>that</em> other? And what if the letter of the law, once centuries have passed and context has changed, goes against its spirit? What would God actually want me to do? Why am I supposed to guess? Why is it so important that my faith in Him be groundless and unsubstantiated? Why has He stopped giving orders directly, why has he relied on fallibe intermediaries and easily-tampered-with books?</p>\n<p>So, abundant questions, very few, very vague answers. <em>Important </em>questions, too, an <em>eternity</em> of one's afterlife depends on them!&nbsp;So, I was wondering: is it possible to come up with a fictional setting, resembling the Theist-Abrahamic vision of the world, but mundane (perhaps a game? a computer simulation? a F(?)AI run society?) in which this drought of information is actually justified?</p>\n<p>As an example of what I'm going for, <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Analysis/Warhammer40000\">Warhammer 40K's Imperium of Man gives us an example of a textbook fascist society whose every single trait is perfectly justified by the setting's rules.</a>&nbsp;(See the \"actually has something consistent to say about utilitarianism\" subsection).&nbsp;</p>\n<p>In what kind of world would a Supreme Authority's information-management policies resemble God's, and make <em>sense? </em>I'm not saying \"be good\" or even \"be fair\", just \"make sense\".<a href=\"http://en.wikiquote.org/wiki/Yes,_Minister#Episode_Eight:_The_Tangled_Web\"> Even in a Kafka-ish, \"nonsensical\" way</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dkYvBCQAr6SsTQFc5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 8.957273108095125e-07, "legacy": true, "legacyId": "15782", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T17:28:24.900Z", "modifiedAt": null, "url": null, "title": "Seeking links for the best arguments for economic libertarianism", "slug": "seeking-links-for-the-best-arguments-for-economic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bart119", "createdAt": "2012-04-19T14:46:10.499Z", "isAdmin": false, "displayName": "Bart119"}, "userId": "JQyd5QFQyrRxdo9G4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q7picjbgde8cf5NB4/seeking-links-for-the-best-arguments-for-economic", "pageUrlRelative": "/posts/Q7picjbgde8cf5NB4/seeking-links-for-the-best-arguments-for-economic", "linkUrl": "https://www.lesswrong.com/posts/Q7picjbgde8cf5NB4/seeking-links-for-the-best-arguments-for-economic", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20links%20for%20the%20best%20arguments%20for%20economic%20libertarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20links%20for%20the%20best%20arguments%20for%20economic%20libertarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7picjbgde8cf5NB4%2Fseeking-links-for-the-best-arguments-for-economic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20links%20for%20the%20best%20arguments%20for%20economic%20libertarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7picjbgde8cf5NB4%2Fseeking-links-for-the-best-arguments-for-economic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7picjbgde8cf5NB4%2Fseeking-links-for-the-best-arguments-for-economic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 278, "htmlBody": "<p>[see 'Update' below]</p>\n<p>I know discussions of actual applied politics are to be avoided. I don't want to start one.</p>\n<p>But I thought LessWrong people might be a source for where the best arguments have been made for libertarianism in the economic sense (not why you should stay out of people's bedrooms). Even better, arguments for some degree of socialism in the same place would be nice. It seems there is a natural continuum. To pick one specific realm: anywhere from 0% to 100% of a person's income could be allocated for redistribution to even things out. Where to put that number will inevitably be a matter of grubby politics (won't it?). But still, arguments for why we should have a low number or a high number must involve some basic disagreements which could be (hopefully) separated into different values, different estimated probabilities, and different attempts to apply a rational analysis.</p>\n<p>The world is dripping with partisan analyses along these lines (with \"warfare\" rules). Where are the best ones that avoid that failing?</p>\n<p>I considered posting this under \"dumb questions\" but I judged that it's not really a question about LessWrong per se.</p>\n<p>Update: Thank you to all who took the time to reply. Perhaps I'm learning about how some would start applying consequentialism to a real-life problem. I expected people to point me to discussions about what's right and what's fair -- which is what I'd expect in most other forums. But I guess here my responders so far are taking this to be a sort of question for technocrats who can work out the utility. So my next question will be about consequentialism once I've thought about it a little more.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hMXoyTAKxvCcsQBKf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q7picjbgde8cf5NB4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -1, "extendedScore": null, "score": 8.957771342431695e-07, "legacy": true, "legacyId": "15783", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T17:48:21.815Z", "modifiedAt": null, "url": null, "title": "No independence of irrelevant alternatives (picture proof)", "slug": "no-independence-of-irrelevant-alternatives-picture-proof", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.025Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7BJ6utqA7PbvQ45ne/no-independence-of-irrelevant-alternatives-picture-proof", "pageUrlRelative": "/posts/7BJ6utqA7PbvQ45ne/no-independence-of-irrelevant-alternatives-picture-proof", "linkUrl": "https://www.lesswrong.com/posts/7BJ6utqA7PbvQ45ne/no-independence-of-irrelevant-alternatives-picture-proof", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20independence%20of%20irrelevant%20alternatives%20(picture%20proof)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20independence%20of%20irrelevant%20alternatives%20(picture%20proof)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7BJ6utqA7PbvQ45ne%2Fno-independence-of-irrelevant-alternatives-picture-proof%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20independence%20of%20irrelevant%20alternatives%20(picture%20proof)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7BJ6utqA7PbvQ45ne%2Fno-independence-of-irrelevant-alternatives-picture-proof", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7BJ6utqA7PbvQ45ne%2Fno-independence-of-irrelevant-alternatives-picture-proof", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 381, "htmlBody": "<p>Back in the old days, when people were wise and the government was just, I did a <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">post</a> on the <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem#Nash_bargaining_solution\">Nash bargaining solution</a>&nbsp;for two player games. Here each player has their own utility function and they're choosing amongst joint options, and trying to bargain to find the best one. What was nice about this solution is that it is independent of irrelevant alternatives (IIA): once you've found the best solution, you can erase any other option, and it remains&nbsp;<a href=\"http://www.youtube.com/watch?v=aIrCFrFpHvw\">the best</a>.</p>\n<p>In order to do that, the Nash bargaining solution makes use of a \"disagreement point\", a special point that provides a zero to both utilities. This seems - and is - ugly. Can we preserve IIA without this clunky disagreement point?</p>\n<p>By the title of the this post, you may have guessed that we can't. Specifically, assume the outcome is symmetric across both players (i.e. permuting the two utility functions preserves the outcome choice), the outcome is <a href=\"http://en.wikipedia.org/wiki/Pareto-optimal\">Pareto-optimal</a> (any change will reduce the utility of at least one player) and there is no outside canonical choices for the utility functions (no special scales, no zeroes, no disagreement points). Then IIA must fail. It fails under weaker conditions as well, but the above lead to an easy picture-proof. And <a href=\"/lw/8qv/in_the_pareto_world_liars_prosper/\">picture proofs</a> are nice.<a id=\"more\"></a></p>\n<p>So assume there are five possible choices, whose utility values for the two players are (0, 3), (1.2, 2.6), (2, 2), (2.6, 1.2), (3, 0). These are graphed here:</p>\n<p><img src=\"http://images.lesswrong.com/t3_c6g_0.png?v=4327306a83f4650fac364a21755d6476\" alt=\"\" width=\"360\" height=\"359\" /></p>\n<p>The choice set is symmetric and the green point (2, 2) is Pareto-optimal and on the axis of symmetry. Hence by the assumptions, the green point must be the outcome chosen. Now further assume IIA, and we will derive a contradiction.</p>\n<p>First, by IIA, we can erase the losing points&nbsp;(2.6, 1.2) and (3, 0). Then we can rescale the utility functions: the utility function graphed on the x axis is divided by two, while the utility graphed on the y axis has 2 subtracted from it. These changes are illustrated here:</p>\n<p><img src=\"http://images.lesswrong.com/t3_c6g_1.png?v=8bef7738b47b451b4ff32bf46e9b7fde\" alt=\"\" width=\"360\" height=\"359\" /></p>\n<p>This results in a final setup of (0, 1), (0.6, 0.6) and (1, 0):</p>\n<p><img src=\"http://images.lesswrong.com/t3_c6g_2.png?v=93daca71958757522420385a62f54a03\" alt=\"\" width=\"360\" height=\"359\" /></p>\n<p>But this is obviously wrong: symmetry implies the correct outcome should be the blue point (0.6, 0.6), not the green (1, 0) which was the outcome before we removed the \"irrelevant\" extra points. We have derived a contradiction, and IIA must fall.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7BJ6utqA7PbvQ45ne", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "15784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hCwFxBai3oNnxrM9v", "vSgaExrundWJJBDmZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T19:38:46.371Z", "modifiedAt": null, "url": null, "title": "Urgent: Subjects needed for rationality measure development", "slug": "urgent-subjects-needed-for-rationality-measure-development", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.756Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Drewsmithyman", "createdAt": "2012-05-03T18:32:58.423Z", "isAdmin": false, "displayName": "Drewsmithyman"}, "userId": "4qvjvfWBgBf4FcffP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zqdKnXGJBaYoHgnu7/urgent-subjects-needed-for-rationality-measure-development", "pageUrlRelative": "/posts/zqdKnXGJBaYoHgnu7/urgent-subjects-needed-for-rationality-measure-development", "linkUrl": "https://www.lesswrong.com/posts/zqdKnXGJBaYoHgnu7/urgent-subjects-needed-for-rationality-measure-development", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Urgent%3A%20Subjects%20needed%20for%20rationality%20measure%20development&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUrgent%3A%20Subjects%20needed%20for%20rationality%20measure%20development%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqdKnXGJBaYoHgnu7%2Furgent-subjects-needed-for-rationality-measure-development%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Urgent%3A%20Subjects%20needed%20for%20rationality%20measure%20development%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqdKnXGJBaYoHgnu7%2Furgent-subjects-needed-for-rationality-measure-development", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqdKnXGJBaYoHgnu7%2Furgent-subjects-needed-for-rationality-measure-development", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Hello everyone,</p>\n<p>We at the Center For Applied Rationality (formerly known as the Center for Modern Rationality) need to measure the impact of our minicamps and develop rationality measures, and for this we need a group of test subjects. The interview portion will take about 5-10 minutes and the total time required will be about 30 minutes. All of it will be conducted online. If you are interested we also need you to be willing to fill out some follow-up surveys, questionnaires, etc. in one year, when it will also take about half an hour. The interviews and follow-up surveys will be interesting; you will be helping us conduct proper science; and you will get some information back which should provide you with some new self-knowledge, or at least entertainment.</p>\n<p><br />The interviews will be fairly anonymous; we won't share your personal information with anyone. But, you MUST NOT discuss anything related to the interview with other people, ESPECIALLY not with other less wrongers, as that could mess up our entire process. We'd prefer for that not to happen, understandably.</p>\n<p>If you are interested in doing this, please fill out <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHlmZ2tTTGJMdjlHa21wbnE4cTZab2c6MQ\">this form</a>. Thank you!</p>\n<p>&nbsp;</p>\n<p>If anything in this process does not seem to be working, please send me an email at <a href=\"mailto:Drewsmithyman@gmail.com\">Drewsmithyman@gmail.com</a> and let me know. Thank you for your time, and we hope to be interviewing many of you quite soon!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zqdKnXGJBaYoHgnu7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 22, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "15785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T19:48:58.126Z", "modifiedAt": null, "url": null, "title": "Meetup : SLC Meetup: Social Hacking Presentation", "slug": "meetup-slc-meetup-social-hacking-presentation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.589Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EW3xr6vqn8Jcpcihf/meetup-slc-meetup-social-hacking-presentation", "pageUrlRelative": "/posts/EW3xr6vqn8Jcpcihf/meetup-slc-meetup-social-hacking-presentation", "linkUrl": "https://www.lesswrong.com/posts/EW3xr6vqn8Jcpcihf/meetup-slc-meetup-social-hacking-presentation", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20SLC%20Meetup%3A%20Social%20Hacking%20Presentation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20SLC%20Meetup%3A%20Social%20Hacking%20Presentation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW3xr6vqn8Jcpcihf%2Fmeetup-slc-meetup-social-hacking-presentation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20SLC%20Meetup%3A%20Social%20Hacking%20Presentation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW3xr6vqn8Jcpcihf%2Fmeetup-slc-meetup-social-hacking-presentation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEW3xr6vqn8Jcpcihf%2Fmeetup-slc-meetup-social-hacking-presentation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9t'>SLC Meetup: Social Hacking Presentation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 May 2012 01:49:28PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">810 East 3300 South, Salt Lake City, UT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting in the Calvin E. Smith Library</p>\n\n<p>Exactly what it says on the tin.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9t'>SLC Meetup: Social Hacking Presentation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EW3xr6vqn8Jcpcihf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "15786", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___SLC_Meetup__Social_Hacking_Presentation\">Discussion article for the meetup : <a href=\"/meetups/9t\">SLC Meetup: Social Hacking Presentation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 May 2012 01:49:28PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">810 East 3300 South, Salt Lake City, UT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting in the Calvin E. Smith Library</p>\n\n<p>Exactly what it says on the tin.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___SLC_Meetup__Social_Hacking_Presentation1\">Discussion article for the meetup : <a href=\"/meetups/9t\">SLC Meetup: Social Hacking Presentation</a></h2>", "sections": [{"title": "Discussion article for the meetup : SLC Meetup: Social Hacking Presentation", "anchor": "Discussion_article_for_the_meetup___SLC_Meetup__Social_Hacking_Presentation", "level": 1}, {"title": "Discussion article for the meetup : SLC Meetup: Social Hacking Presentation", "anchor": "Discussion_article_for_the_meetup___SLC_Meetup__Social_Hacking_Presentation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T20:24:46.161Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC meetup", "slug": "meetup-washington-dc-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/biDkFNZpP3bq6bJQE/meetup-washington-dc-meetup-1", "pageUrlRelative": "/posts/biDkFNZpP3bq6bJQE/meetup-washington-dc-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/biDkFNZpP3bq6bJQE/meetup-washington-dc-meetup-1", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbiDkFNZpP3bq6bJQE%2Fmeetup-washington-dc-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbiDkFNZpP3bq6bJQE%2Fmeetup-washington-dc-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbiDkFNZpP3bq6bJQE%2Fmeetup-washington-dc-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9u'>Washington DC meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 May 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Portrait Gallery, central plaza</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please note the date has changed; sorry for the inconvenience. Topic will be the recent critique of SI by givewell. <a href=\"http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/\" rel=\"nofollow\">http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9u'>Washington DC meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "biDkFNZpP3bq6bJQE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.958533872480711e-07, "legacy": true, "legacyId": "15787", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup\">Discussion article for the meetup : <a href=\"/meetups/9u\">Washington DC meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 May 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Portrait Gallery, central plaza</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please note the date has changed; sorry for the inconvenience. Topic will be the recent critique of SI by givewell. <a href=\"http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/\" rel=\"nofollow\">http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup1\">Discussion article for the meetup : <a href=\"/meetups/9u\">Washington DC meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-03T21:23:28.151Z", "modifiedAt": null, "url": null, "title": "Comments on Pascal's Mugging", "slug": "comments-on-pascal-s-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:33.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Smq78wqcuocFHK5pd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5y65oJGeXvCvggYoz/comments-on-pascal-s-mugging", "pageUrlRelative": "/posts/5y65oJGeXvCvggYoz/comments-on-pascal-s-mugging", "linkUrl": "https://www.lesswrong.com/posts/5y65oJGeXvCvggYoz/comments-on-pascal-s-mugging", "postedAtFormatted": "Thursday, May 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Comments%20on%20Pascal's%20Mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComments%20on%20Pascal's%20Mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5y65oJGeXvCvggYoz%2Fcomments-on-pascal-s-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Comments%20on%20Pascal's%20Mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5y65oJGeXvCvggYoz%2Fcomments-on-pascal-s-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5y65oJGeXvCvggYoz%2Fcomments-on-pascal-s-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 635, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">There seems to be some continuing debate about whether or not it is rational to appease a Pascal Mugger. Some are saying that due to scope insensitivity and other biases, we really should just trust what decision theory + Solomonoff induction tells us. I have been thinking about this a lot and I'm at the point where I think I have something to contribute to the discussion.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Consider the Pascal Mugging \"Immediately begin to work only on increasing my utility, according to my utility function 'X', from now on, or my powers from outside the matrix will make minus 3^^^^3 utilons happen to you and yours.\"</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Any agent can commit this Pascal's mugging (PM) against any other agent, at any time. A naive decision-theoretic expected-utility optimizer will always appease the mugger. Consider what the world would be like if all intelligent beings were this kind of agent.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">When you see an agent, any agent, your only strategy would be to try to PM it before it PMs you. More likely, you will PM each other simultaneously, in which case the agent which finishes the mugging first 'wins'. If you finish mugging at the same time, the mugger that uses a larger integer in its threat 'wins'. (So you'll use the most compact notation possible and things like, \"minus the Busy Beaver function of Graham's number utilons\".)</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This may continue until every agent in the community/world/universe has been PMed. Or maybe there could be one agent, a Pascal Highlander, who manages to escape being mugged and has his utility function come to dominate...</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Except, there is nothing stipulating that the mugging has to be delivered in person. With a powerful radio source, you can PM everyone in your future light-cone unfortunate enough to decode your message, potentially highjacking entire distant civilizations of decision-theory users.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Pascal's mugging doesn't have to be targeted. You can claim to be a Herald of Omega and address your mugging \"to whoever receives this transmission\"</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Another strategy might be to build a self-replicating robot (itself too dumb to be mugged) which has a radio which broadcasts a continuous fully general PM, and send it out into space. Then you commit suicide to avoid the fate of being mugged.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Now consider a hypothetical agent which completely ignores muggers. And mugs them back.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Consider what could happen if we build an AI which is friendly in every possible respect except that it appeases PMers.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">To avoid this, you might implement a heuristic that ignores PMs on account of the prior improbability of being able to decide the fate of so many utilons, as Robin Hanson suggested. But an AI using na&iuml;ve expected utility + SI may well have other failure modes roughly analagous to PM that we won't think of until its too late. You might get agents to agree to pre-commit to ignore muggers, or to kill them, but to me this seems unstable. A bandaid that's not addressing the heart of the issue. <span style=\"font-style: normal;\">I think an AI which can envision itself being PMed repeatedly by every other agent on the planet and still evaluate appeasement as the lesser evil cannot possibly be a Friendly AI, even if it has some heuristic or ad hoc patch that says it can ignore the PM. </span></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><span style=\"font-style: normal;\"><br /></span></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Of course there's the possibility that we <strong>are</strong> in a simulation which is occasionally visited by agents from the mother universe, which really does contain 3^^^^3 utilons/people/dustspecks. I'm not convinced acknowledging this possibility changes anything. <span style=\"font-style: normal;\">There's nothing of value that we, as simulated people, could give our Pascal Mugging simulation overlords. Their only motivation would be as absolute sadistic sociopaths, but if that's the reality of the multiverse, in the long term we're screwed no matter what we do, even with friendly AI. And we certainly wouldn't be in any way morally responsible for their actions. </span></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Edit 1: fixed typos</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HNJiR8Jzafsv8cHrC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5y65oJGeXvCvggYoz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 8.958787707255009e-07, "legacy": true, "legacyId": "15788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-04T04:20:36.854Z", "modifiedAt": null, "url": null, "title": "Why do people ____?", "slug": "why-do-people-____", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:30.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "magfrump", "createdAt": "2009-12-10T20:51:45.065Z", "isAdmin": false, "displayName": "magfrump"}, "userId": "KsYFs5ip5jeiFETJa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mLcAyvWJ96SJjAKN2/why-do-people-____", "pageUrlRelative": "/posts/mLcAyvWJ96SJjAKN2/why-do-people-____", "linkUrl": "https://www.lesswrong.com/posts/mLcAyvWJ96SJjAKN2/why-do-people-____", "postedAtFormatted": "Friday, May 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20do%20people%20____%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20do%20people%20____%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmLcAyvWJ96SJjAKN2%2Fwhy-do-people-____%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20do%20people%20____%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmLcAyvWJ96SJjAKN2%2Fwhy-do-people-____", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmLcAyvWJ96SJjAKN2%2Fwhy-do-people-____", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<p>The other day, someone did something I didn't expect. &nbsp;It was something many people have done before; something that I thought of as very normal, but that I in no way understood and had not predicted.</p>\n<p>As I said, this had happened many time before, so I wrote it off as \"me not understanding people\" or \"people are weird\" for a second, like I usually do, before realizing that <a href=\"/lw/c4h/rationality_quotes_may_2012/6hsb\">\"bad at\"</a> really means <a href=\"/lw/453/procedural_knowledge_gaps/\">\"lacking basic knowledge\"</a>, which I had never realized before.</p>\n<p>And then I thought \"I should ask someone who is different from me why people do that, and eventually someone will have an answer.\"</p>\n<p>But many people will have many more questions like this. &nbsp;So, what have you observed people doing time and time again, but never understood? &nbsp;Or something that you only understood after a long time or asking someone about it?</p>\n<p>And can Less Wrong tell us, not necessarily why (I for one can make up evolutionary psychology fairy tales all day if I want) but what&nbsp;conscious&nbsp;thought process occurs behind these events?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 1, "YQW2DxpZFTrqrxHBJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mLcAyvWJ96SJjAKN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 36, "extendedScore": null, "score": 8.9605919266087e-07, "legacy": true, "legacyId": "15802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 256, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ka8eveZpT7hXLhRTM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-04T06:05:51.602Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Dilemma: Science or Bayes?", "slug": "seq-rerun-the-dilemma-science-or-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dspwjD74JXfnMm6Sz/seq-rerun-the-dilemma-science-or-bayes", "pageUrlRelative": "/posts/dspwjD74JXfnMm6Sz/seq-rerun-the-dilemma-science-or-bayes", "linkUrl": "https://www.lesswrong.com/posts/dspwjD74JXfnMm6Sz/seq-rerun-the-dilemma-science-or-bayes", "postedAtFormatted": "Friday, May 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Dilemma%3A%20Science%20or%20Bayes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Dilemma%3A%20Science%20or%20Bayes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdspwjD74JXfnMm6Sz%2Fseq-rerun-the-dilemma-science-or-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Dilemma%3A%20Science%20or%20Bayes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdspwjD74JXfnMm6Sz%2Fseq-rerun-the-dilemma-science-or-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdspwjD74JXfnMm6Sz%2Fseq-rerun-the-dilemma-science-or-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/qa/the_dilemma_science_or_bayes/\">The Dilemma: Science or Bayes?</a> was originally published on 13 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The failure of first-half-of-20th-century-physics was not due to <em>straying </em>from the scientific method. Science and rationality - that is, Science and Bayesianism - aren't the same thing, and sometimes they give different answers.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c5z/seq_rerun_the_failures_of_eld_science/\">The Failures of Eld Science</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dspwjD74JXfnMm6Sz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.961047233255737e-07, "legacy": true, "legacyId": "15805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["viPPjojmChxLGPE2v", "anRJKFba6YzmM3Eoc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-04T12:13:46.482Z", "modifiedAt": null, "url": null, "title": "Zach Weiner on video game violence", "slug": "zach-weiner-on-video-game-violence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:28.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Lqzor7tEPc3YJ4zgs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nfoi7FgWYdkjF8NCr/zach-weiner-on-video-game-violence", "pageUrlRelative": "/posts/nfoi7FgWYdkjF8NCr/zach-weiner-on-video-game-violence", "linkUrl": "https://www.lesswrong.com/posts/nfoi7FgWYdkjF8NCr/zach-weiner-on-video-game-violence", "postedAtFormatted": "Friday, May 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zach%20Weiner%20on%20video%20game%20violence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZach%20Weiner%20on%20video%20game%20violence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnfoi7FgWYdkjF8NCr%2Fzach-weiner-on-video-game-violence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zach%20Weiner%20on%20video%20game%20violence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnfoi7FgWYdkjF8NCr%2Fzach-weiner-on-video-game-violence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnfoi7FgWYdkjF8NCr%2Fzach-weiner-on-video-game-violence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<h1><a href=\"http://www.smbc-comics.com/?id=2600\">See the comic here.</a></h1>\n<p>And the red button below:</p>\n<p><img src=\"http://www.smbc-comics.com/comics/20120504after.gif\" alt=\"\" width=\"293\" height=\"560\" /></p>\n<p>&nbsp;</p>\n<p>What are your thoughts on this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nfoi7FgWYdkjF8NCr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -3, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "15818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-04T16:41:45.739Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Copenhagen, Dallas, Graz, London, Longmont, Melbourne, Pittsburgh, Sydney, Vancouver, Washington", "slug": "weekly-lw-meetups-copenhagen-dallas-graz-london-longmont", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nCzz75zAfiqzA6gMN/weekly-lw-meetups-copenhagen-dallas-graz-london-longmont", "pageUrlRelative": "/posts/nCzz75zAfiqzA6gMN/weekly-lw-meetups-copenhagen-dallas-graz-london-longmont", "linkUrl": "https://www.lesswrong.com/posts/nCzz75zAfiqzA6gMN/weekly-lw-meetups-copenhagen-dallas-graz-london-longmont", "postedAtFormatted": "Friday, May 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Copenhagen%2C%20Dallas%2C%20Graz%2C%20London%2C%20Longmont%2C%20Melbourne%2C%20Pittsburgh%2C%20Sydney%2C%20Vancouver%2C%20Washington&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Copenhagen%2C%20Dallas%2C%20Graz%2C%20London%2C%20Longmont%2C%20Melbourne%2C%20Pittsburgh%2C%20Sydney%2C%20Vancouver%2C%20Washington%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCzz75zAfiqzA6gMN%2Fweekly-lw-meetups-copenhagen-dallas-graz-london-longmont%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Copenhagen%2C%20Dallas%2C%20Graz%2C%20London%2C%20Longmont%2C%20Melbourne%2C%20Pittsburgh%2C%20Sydney%2C%20Vancouver%2C%20Washington%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCzz75zAfiqzA6gMN%2Fweekly-lw-meetups-copenhagen-dallas-graz-london-longmont", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCzz75zAfiqzA6gMN%2Fweekly-lw-meetups-copenhagen-dallas-graz-london-longmont", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 501, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/8r\">Longmont Sparkfun Soldering Competition Field Trip:&nbsp;<span class=\"date\">28 April 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/8i\">Graz Meetup:&nbsp;<span class=\"date\">28 April 2012 11:21PM</span></a></li>\n<li><a href=\"/meetups/97\">First Copenhagen meetup:&nbsp;<span class=\"date\">29 April 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/9g\">Pittsburgh Meetup: Big Gaming Fun 5!:&nbsp;<span class=\"date\">29 April 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/9k\">Dallas - Fort Worth Less Wrong Meetup 4/29/12:&nbsp;<span class=\"date\">29 April 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/9d\">Washington DC meetup:&nbsp;<span class=\"date\">29 April 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/9c\">Vancouver Cafe Meetup!:&nbsp;<span class=\"date\">29 April 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/9i\">Less Wrong sydney - Prediction gambles and location testing:&nbsp;<span class=\"date\">03 May 2012 06:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/9j\">London:&nbsp;<span class=\"date\">01 May 2012 06:30PM</span></a></li>\n<li class=\"last-child\"><a href=\"/meetups/9e\">Melbourne, practical rationality:&nbsp;<span class=\"date\">04 May 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/93\">Cambridge, MA First Sunday Meetup:&nbsp;<span class=\"date\">06 May 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/94\">Cambridge, MA Third Sunday Meetup:&nbsp;<span class=\"date\">20 May 2012 02:20PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nCzz75zAfiqzA6gMN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.963799098812032e-07, "legacy": true, "legacyId": "15633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-04T18:31:25.586Z", "modifiedAt": null, "url": null, "title": "Rationality and Winning", "slug": "rationality-and-winning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.790Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nEgXQkewAnvX6mYd7/rationality-and-winning", "pageUrlRelative": "/posts/nEgXQkewAnvX6mYd7/rationality-and-winning", "linkUrl": "https://www.lesswrong.com/posts/nEgXQkewAnvX6mYd7/rationality-and-winning", "postedAtFormatted": "Friday, May 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20and%20Winning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20and%20Winning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEgXQkewAnvX6mYd7%2Frationality-and-winning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20and%20Winning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEgXQkewAnvX6mYd7%2Frationality-and-winning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEgXQkewAnvX6mYd7%2Frationality-and-winning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 525, "htmlBody": "<p>Someone who claims to have read \"the vast majority\" of the Sequences recently misinterpreted me to be saying that I \"accept 'life success' as an important metric for rationality.\" This may be a common confusion among LessWrongers due to statements like \"<a href=\"/lw/7i/rationality_is_systematized_winning/\">rationality is systematized winning</a>\" and \"<a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">be careful&hellip; any time you find yourself defining the [rationalist] as someone <em>other</em> than the agent who is currently smiling from on top of a giant heap of utility</a>.\"</p>\n<p>So, let me explain why Actual Winning <em>isn't</em> a strong measure of rationality.</p>\n<p>In cognitive science, the \"Standard Picture\" (<a href=\"http://www.amazon.com/Without-Good-Reason-Rationality-Philosophy/dp/0198235747/\">Stein 1996</a>) of rationality is that rationality is a normative concept defined by logic, Bayesian probability theory, and Bayesian decision theory (aka \"rational choice theory\"). (Also see the standard textbooks on judgment and decision-making, e.g. <em><a href=\"http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433/\">Thinking and Deciding</a></em>&nbsp;and <em><a href=\"http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/\">Rational Choice in an Uncertain World</a></em>.) <a href=\"http://books.google.com/books?id=S1-K4AT3zXYC&amp;lpg=PP1&amp;dq=oxford%20handbook%20of%20thinking%20and%20reasoning&amp;pg=PA11#v=onepage&amp;q&amp;f=false\">Oaksford &amp; Chater (2012</a>) explain:</p>\n<p style=\"padding-left: 30px;\">Is it meaningful to attempt to develop a general theory of rationality <em>at all</em>? We might tentatively suggest that it is a prima facie sign of irrationality to believe in alien abduction, or to will a sports team to win in order to increase their chance of victory. But these views or actions might be entirely rational, given suitably nonstandard background beliefs about other alien activity and the general efficacy of psychic powers. Irrationality may, though, be ascribed if there is a <em>clash</em> between a particular belief or behavior and such background assumptions. Thus, a thorough-going physicalist may, perhaps, be accused of irrationality if she simultaneously believes in psychic powers. A theory of rationality cannot, therefore, be viewed as clarifying either what people should believe or how people should act&mdash;but it can determine whether beliefs and behaviors are compatible. Similarly, a theory of rational choice cannot determine whether it is rational to smoke or to exercise daily; but it might clarify whether a particular choice is compatible with other beliefs and choices.</p>\n<p style=\"padding-left: 30px;\">From this viewpoint, normative theories can be viewed as clarifying conditions of consistency&hellip; <em>Logic</em> can be viewed as studying the notion of consistency over <em>beliefs</em>. <em>Probability</em>&hellip; studies consistency over <em>degrees of belief</em>. <em>Rational choice</em> theory studies the consistency of beliefs and values with <em>choices</em>.</p>\n<p>Thus, one could have highly rational beliefs and make highly rational choices and still fail to win due to <a href=\"/lw/h7/selfdeception_hypocrisy_or_akrasia/\">akrasia</a>, lack of resources, lack of intelligence, and so on. <strong>Like intelligence and money, rationality is only a <em>ceteris paribus</em>&nbsp;predictor of success</strong>.</p>\n<p>So while it's empirically true (<a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/\">Stanovich 2010</a>) that rationality is a predictor of life success, it's a weak one. (At least, it's a weak predictor of success at the levels of human rationality we are capable of <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">training</a> <em>today</em>.) If you want to more reliably achieve life success, I recommend inheriting a billion dollars or, failing that, being born+raised to have an excellent work ethic and low akrasia.</p>\n<p>The reason you should \"be careful&hellip; any time you find yourself defining the [rationalist] as someone other than the agent who is currently smiling from on top of a giant heap of utility\" is because you should \"<a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">never end up envying someone else's mere <em>choices</em></a>.\" You are still allowed to envy their resources, intelligence, work ethic, mastery over akrasia, and other predictors of success.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nEgXQkewAnvX6mYd7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 30, "extendedScore": null, "score": 8.9642738213295e-07, "legacy": true, "legacyId": "15820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARtkT3EYox3THYjF", "6ddcsdA2c2XpNpE5x", "JoERzF8ePGr4zP9vv", "fkhbBE2ZTSytvsy9x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T00:21:39.693Z", "modifiedAt": null, "url": null, "title": "Muehlhauser-Goertzel Dialogue, Part 2", "slug": "muehlhauser-goertzel-dialogue-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:53.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qF6hvXi2ytBsyzttp/muehlhauser-goertzel-dialogue-part-2", "pageUrlRelative": "/posts/qF6hvXi2ytBsyzttp/muehlhauser-goertzel-dialogue-part-2", "linkUrl": "https://www.lesswrong.com/posts/qF6hvXi2ytBsyzttp/muehlhauser-goertzel-dialogue-part-2", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Muehlhauser-Goertzel%20Dialogue%2C%20Part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMuehlhauser-Goertzel%20Dialogue%2C%20Part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqF6hvXi2ytBsyzttp%2Fmuehlhauser-goertzel-dialogue-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Muehlhauser-Goertzel%20Dialogue%2C%20Part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqF6hvXi2ytBsyzttp%2Fmuehlhauser-goertzel-dialogue-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqF6hvXi2ytBsyzttp%2Fmuehlhauser-goertzel-dialogue-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2022, "htmlBody": "<p><small> </small></p>\n<p><small>Part of the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser interview series on AGI</a>.</small></p>\n<p><small> </small></p>\n<p>&nbsp;</p>\n<p><small> <a href=\"http://lukeprog.com/\" target=\"_blank\">Luke Muehlhauser</a>&nbsp;is Executive Director of the <a href=\"http://intelligence.org/\" target=\"_blank\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</small></p>\n<p><small> </small></p>\n<p><small><a href=\"http://wp.goertzel.org/?page_id=24\" target=\"_blank\">Ben Goertzel</a>&nbsp;is the Chairman at the AGI company <a href=\"http://wp.novamente.net/\" target=\"_blank\">Novamente</a>&nbsp;and founder of the <a href=\"http://agi-conf.org/\" target=\"_blank\">AGI conference series</a>.</small></p>\n<p><small> </small></p>\n<p><small><br /></small></p>\n<p><small> </small></p>\n<p><strong>Continued from <a href=\"/r/discussion/lw/aw7/muehlhausergoertzel_dialogue_part_1/\">part 1</a>...</strong></p>\n<p>&nbsp;</p>\n<h3>Luke:</h3>\n<p>[Apr 11th, 2012]</p>\n<p>I agree the future is unlikely to consist of a population of fairly distinct AGIs competing for resources, but I never thought that the arguments for Basic AI drives or \"convergent instrumenta l goals\" required that scenario to hold.</p>\n<p>Anyway, I prefer the argument for convergent instrumental goals in Nick Bostrom 's more recent paper \" <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\" target=\"_blank\">The Superintelligent Will</a>.\" Which parts of Nick's argument fail to persuade you?</p>\n<p>&nbsp;</p>\n<h3>Ben:</h3>\n<p>[Apr 12th, 2012]</p>\n<p>Well, for one thing, I think his&nbsp;</p>\n<blockquote>\n<p><strong>Orthogonality Thesis</strong></p>\n<p>Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.</p>\n</blockquote>\n<p>is misguided. It may be true, but who cares about possibility &ldquo;in principle&rdquo;? The question is whether any level of intelligence is PLAUSIBLY LIKELY to be combined with more or less any final goal in practice. And I really doubt it. I guess I could posit the alternative</p>\n<blockquote>\n<p><strong>Interdependency Thesis&nbsp;</strong></p>\n<p>Intelligence and final goals are in practice highly and subtly interdependent. In other words, in the actual world, various levels of intelligence are going to be highly correlated with various probability distributions over the space of final goals.</p>\n</blockquote>\n<p>This just gets back to the issue we discussed already, of me thinking it&rsquo;s really unlikely that a superintelligence would ever really have a really stupid goal like say, tiling the Cosmos with Mickey Mice.</p>\n<p>Bostrom says&nbsp;</p>\n<blockquote>It might be possible through deliberate effort to construct a superintelligence that values ... human welfare, moral goodness, or any other complex purpose that its designers might want it to serve. But it is no less possible&mdash;and probably technically easier&mdash;to build a superintelligence that places final value on nothing but calculating the decimals of pi.</blockquote>\n<p>but he gives no evidence for this assertion. Calculating the decimals of pi may be a fairly simple mathematical operation that doesn&rsquo;t have any need for superintelligence, and thus may be a really unlikely goal for a superintelligence -- so that if you tried to build a superintelligence with this goal and connected it to the real world, it would very likely get its initial goal subverted and wind up pursuing some different, less idiotic goal.</p>\n<p>One basic error Bostrom seems to be making in this paper, is to think about intelligence as something occurring in a sort of mathematical vacuum, divorced from the frustratingly messy and hard-to-quantify probability distributions characterizing actual reality....</p>\n<p>Regarding his</p>\n<blockquote>\n<p><strong>The Instrumental Convergence Thesis</strong></p>\n<p>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent&rsquo;s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</p>\n</blockquote>\n<p>the first clause makes sense to me,</p>\n<blockquote>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent&rsquo;s goal being realized for a wide range of final goals and a wide range of situations</blockquote>\n<p>but it doesn&rsquo;t seem to me to justify the second clause</p>\n<blockquote>implying that these instrumental values are likely to be pursued by many intelligent agents.</blockquote>\n<p>The step from the first to the second clause seems to me to assume that the intelligent agents in question are being created and selected by some sort of process similar to evolution by natural selection, rather than being engineered carefully, or created via some other process beyond current human ken.</p>\n<p>In short, I think the Bostrom paper is an admirably crisp statement of its perspective, and I agree that its conclusions seem to follow from its clearly stated assumptions -- but the assumptions are not justified in the paper, and I don&rsquo;t buy them at all.</p>\n<p>&nbsp;</p>\n<h3>Luke:</h3>\n<p>[Apr. 19, 2012]</p>\n<p>Ben,</p>\n<p>Let me explain why I think that:</p>\n<blockquote>(1) The fact that we can identify convergent instrumental goals (of the sort described by Bostrom) implies that many agents will pursue those instrumental goals.</blockquote>\n<p>Intelligent systems are intelligent because rather than simply executing hard-wired situation-action rules, they figure out how to construct plans that will lead to the probabilistic fulfillment of their final goals. That is why intelligent systems will pursue the convergent instrumental goals described by Bostrom. We might try to hard-wire a collection of rules into an AGI which restrict the pursuit of some of these convergent instrumental goals, but a superhuman AGI would realize that it could better achieve its final goals if it could invent a way around those hard-wired rules and have no ad-hoc obstacles to its ability to execute intelligent plans for achieving its goals.</p>\n<p>Next: I remain confused about why an intelligent system will decide that a particular final goal it has been given is \"stupid,\" and then change its final goals &mdash; especially given the convergent instrumental goal to preserve its final goals.</p>\n<p>Perhaps the word \"intelligence\" is getting in our way. Let's define a notion of \" <a href=\"/lw/va/measuring_optimization_power/\" target=\"_blank\">optimization power</a>,\" which measures (roughly) an agent's ability to optimize the world according to its preference ordering, across a very broad range of possible preference orderings and environments. I think we agree that AGIs with vastly greater-than-human optimization power will arrive in the next century or two. The problem, then, is that this superhuman AGI will almost certainly be optimizing the world for something other than what humans want, because what humans want is complex and fragile, and indeed we remain confused about what exactly it is that we want. A machine superoptimizer with a final goal of solving the Riemann hypothesis will simply be very good at solving the Riemann hypothesis (by whatever means necessary).</p>\n<p>Which parts of this analysis do you think are wrong?</p>\n<p>&nbsp;</p>\n<h3>Ben:</h3>\n<p>[Apr. 20, 2012]</p>\n<p>It seems to me that in your reply you are implicitly assuming a much stronger definition of &ldquo;convergent&rdquo; than the one Bostrom actually gives in his paper. He says</p>\n<blockquote>instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent&rsquo;s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</blockquote>\n<p>Note the somewhat weaselly reference to a &ldquo;wide range&rdquo; of goals and situations -- not, say, &ldquo;nearly all feasible&rdquo; goals and situations. Just because some values are convergent in the weak sense of his definition, doesn&rsquo;t imply that AGIs we create will be likely to adopt these instrumental values. I think that his weak definition of &ldquo;convergent&rdquo; doesn&rsquo;t actually imply convergence in any useful sense. On the other hand, if he&rsquo;d made a stronger statement like</p>\n<blockquote>\n<p>instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent&rsquo;s goal being realized for nearly all feasible final goals and nearly all feasible situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</p>\n</blockquote>\n<p>then I would disagree with the first clause of his statement (&ldquo;instrumental values can be identified which...&rdquo;), but I would be more willing to accept that the second clause (after the &ldquo;implying&rdquo;) followed from the first.</p>\n<p>About optimization -- I think it&rsquo;s rather naive and narrow-minded to view hypothetical superhuman superminds as &ldquo;optimization powers.&rdquo; It&rsquo;s a bit like a dog viewing a human as an &ldquo;eating and mating power.&rdquo; Sure, there&rsquo;s some accuracy to that perspective -- we do eat and mate, and some of our behaviors may be understood based on this. On the other hand, a lot of our behaviors are not very well understood in terms of these, or any dog-level concepts. Similarly, I would bet that the bulk of a superhuman supermind&rsquo;s behaviors and internal structures and dynamics will not be explicable in terms of the concepts that are important to humans, such as &ldquo;optimization.&rdquo;</p>\n<p>So when you say &ldquo;this superhuman AGI will almost certainly be optimizing the world for something other than what humans want,\" I don&rsquo;t feel confident that what a superhuman AGI will be doing, will be usefully describable as optimizing anything ....</p>\n<p>&nbsp;</p>\n<h3>Luke:</h3>\n<p>[May 1, 2012]</p>\n<p>I think our dialogue has reached the point of diminishing marginal returns, so I'll conclude with just a few points and let you have the last word.</p>\n<p>On convergent instrumental goals, I encourage readers to read \" <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\" target=\"_blank\">The Superintelligent Will</a>\" and make up their own minds.</p>\n<p>On the convergence of advanced intelligent systems toward optimization behavior, I'll point you to <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\" target=\"_blank\">Omohundro (2007)</a>.</p>\n<p>&nbsp;</p>\n<h3>Ben:</h3>\n<p>Well, it's been a fun chat. Although it hasn't really covered much new ground, there have been some new phrasings and minor new twists.</p>\n<p>One thing I'm repeatedly struck by in discussions on these matters with you and other SIAI folks, is the way the strings of reason are pulled by the puppet-master of intuition. With so many of these topics on which we disagree -- for example: the Scary Idea, the importance of optimization for intelligence, the existence of strongly convergent goals for intelligences -- you and the other core SIAI folks share a certain set of intuitions, which seem quite strongly held. Then you formulate rational arguments in favor of these intuitions -- but the conclusions that result from these rational arguments are very weak. For instance, the Scary Idea intuition corresponds to a rational argument that \"superhuman AGI might plausibly kill everyone.\" The intuition about strongly convergent goals for intelligences, corresponds to a rational argument about goals that are convergent for a \"wide range\" of intelligences. Etc.</p>\n<p>On my side, I have a strong intuition that OpenCog can be made into a human-level general intelligence, and that if this intelligence is raised properly it will turn out benevolent and help us launch a positive Singularity. However, I can't fully rationally substantiate this intuition either -- all I can really fully rationally argue for is something weaker like \"It seems plausible that a fully implemented OpenCog system might display human-level or greater intelligence on feasible computational resources, and might turn out benevolent if raised properly.\" In my case just like yours, reason is far weaker than intuition.</p>\n<p>Another thing that strikes me, reflecting on our conversation, is the difference between the degrees of confidence required, in modern democratic society, to TRY something versus to STOP others from trying something. A rough intuition is often enough to initiate a project, even a large one. On the other hand, to get someone else's work banned based on a rough intuition is pretty hard. To ban someone else's work, you either need a really thoroughly ironclad logical argument, or you need to stir up a lot of hysteria.</p>\n<p>What this suggests to me is that, while my intuitions regarding OpenCog seem to be sufficient to motivate others to help me to build OpenCog (via making them interested enough in it that they develop their own intuitions about it), your intuitions regarding the dangers of AGI are not going to be sufficient to get work on AGI systems like OpenCog stopped. To halt AGI development, if you wanted to (and you haven't said that you do, I realize), you'd either need to fan hysteria very successfully, or come up with much stronger logical arguments, ones that match the force of your intuition on the subject.</p>\n<p>Anyway, even though I have very different intuitions than you and your SIAI colleagues about a lot of things, I do think you guys are performing some valuable services -- not just through the excellent Singularity Summit conferences, but also by raising some difficult and important issues in the public eye. Humanity spends a lot of its attention on some really unimportant things, so it's good to have folks like SIAI nudging the world to think about critical issues regarding our future. In the end, whether SIAI's views are actually correct may be peripheral to the organization's main value and impact.</p>\n<p>I look forward to future conversations, and especially look forward to resuming this conversation one day with a human-level AGI as the mediator ;-)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qF6hvXi2ytBsyzttp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "15821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small> </small></p>\n<p><small>Part of the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser interview series on AGI</a>.</small></p>\n<p><small> </small></p>\n<p>&nbsp;</p>\n<p><small> <a href=\"http://lukeprog.com/\" target=\"_blank\">Luke Muehlhauser</a>&nbsp;is Executive Director of the <a href=\"http://intelligence.org/\" target=\"_blank\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</small></p>\n<p><small> </small></p>\n<p><small><a href=\"http://wp.goertzel.org/?page_id=24\" target=\"_blank\">Ben Goertzel</a>&nbsp;is the Chairman at the AGI company <a href=\"http://wp.novamente.net/\" target=\"_blank\">Novamente</a>&nbsp;and founder of the <a href=\"http://agi-conf.org/\" target=\"_blank\">AGI conference series</a>.</small></p>\n<p><small> </small></p>\n<p><small><br></small></p>\n<p><small> </small></p>\n<p><strong id=\"Continued_from_part_1___\">Continued from <a href=\"/r/discussion/lw/aw7/muehlhausergoertzel_dialogue_part_1/\">part 1</a>...</strong></p>\n<p>&nbsp;</p>\n<h3 id=\"Luke_\">Luke:</h3>\n<p>[Apr 11th, 2012]</p>\n<p>I agree the future is unlikely to consist of a population of fairly distinct AGIs competing for resources, but I never thought that the arguments for Basic AI drives or \"convergent instrumenta l goals\" required that scenario to hold.</p>\n<p>Anyway, I prefer the argument for convergent instrumental goals in Nick Bostrom 's more recent paper \" <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\" target=\"_blank\">The Superintelligent Will</a>.\" Which parts of Nick's argument fail to persuade you?</p>\n<p>&nbsp;</p>\n<h3 id=\"Ben_\">Ben:</h3>\n<p>[Apr 12th, 2012]</p>\n<p>Well, for one thing, I think his&nbsp;</p>\n<blockquote>\n<p><strong id=\"Orthogonality_Thesis\">Orthogonality Thesis</strong></p>\n<p>Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.</p>\n</blockquote>\n<p>is misguided. It may be true, but who cares about possibility \u201cin principle\u201d? The question is whether any level of intelligence is PLAUSIBLY LIKELY to be combined with more or less any final goal in practice. And I really doubt it. I guess I could posit the alternative</p>\n<blockquote>\n<p><strong id=\"Interdependency_Thesis_\">Interdependency Thesis&nbsp;</strong></p>\n<p>Intelligence and final goals are in practice highly and subtly interdependent. In other words, in the actual world, various levels of intelligence are going to be highly correlated with various probability distributions over the space of final goals.</p>\n</blockquote>\n<p>This just gets back to the issue we discussed already, of me thinking it\u2019s really unlikely that a superintelligence would ever really have a really stupid goal like say, tiling the Cosmos with Mickey Mice.</p>\n<p>Bostrom says&nbsp;</p>\n<blockquote>It might be possible through deliberate effort to construct a superintelligence that values ... human welfare, moral goodness, or any other complex purpose that its designers might want it to serve. But it is no less possible\u2014and probably technically easier\u2014to build a superintelligence that places final value on nothing but calculating the decimals of pi.</blockquote>\n<p>but he gives no evidence for this assertion. Calculating the decimals of pi may be a fairly simple mathematical operation that doesn\u2019t have any need for superintelligence, and thus may be a really unlikely goal for a superintelligence -- so that if you tried to build a superintelligence with this goal and connected it to the real world, it would very likely get its initial goal subverted and wind up pursuing some different, less idiotic goal.</p>\n<p>One basic error Bostrom seems to be making in this paper, is to think about intelligence as something occurring in a sort of mathematical vacuum, divorced from the frustratingly messy and hard-to-quantify probability distributions characterizing actual reality....</p>\n<p>Regarding his</p>\n<blockquote>\n<p><strong id=\"The_Instrumental_Convergence_Thesis\">The Instrumental Convergence Thesis</strong></p>\n<p>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent\u2019s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</p>\n</blockquote>\n<p>the first clause makes sense to me,</p>\n<blockquote>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent\u2019s goal being realized for a wide range of final goals and a wide range of situations</blockquote>\n<p>but it doesn\u2019t seem to me to justify the second clause</p>\n<blockquote>implying that these instrumental values are likely to be pursued by many intelligent agents.</blockquote>\n<p>The step from the first to the second clause seems to me to assume that the intelligent agents in question are being created and selected by some sort of process similar to evolution by natural selection, rather than being engineered carefully, or created via some other process beyond current human ken.</p>\n<p>In short, I think the Bostrom paper is an admirably crisp statement of its perspective, and I agree that its conclusions seem to follow from its clearly stated assumptions -- but the assumptions are not justified in the paper, and I don\u2019t buy them at all.</p>\n<p>&nbsp;</p>\n<h3 id=\"Luke_1\">Luke:</h3>\n<p>[Apr. 19, 2012]</p>\n<p>Ben,</p>\n<p>Let me explain why I think that:</p>\n<blockquote>(1) The fact that we can identify convergent instrumental goals (of the sort described by Bostrom) implies that many agents will pursue those instrumental goals.</blockquote>\n<p>Intelligent systems are intelligent because rather than simply executing hard-wired situation-action rules, they figure out how to construct plans that will lead to the probabilistic fulfillment of their final goals. That is why intelligent systems will pursue the convergent instrumental goals described by Bostrom. We might try to hard-wire a collection of rules into an AGI which restrict the pursuit of some of these convergent instrumental goals, but a superhuman AGI would realize that it could better achieve its final goals if it could invent a way around those hard-wired rules and have no ad-hoc obstacles to its ability to execute intelligent plans for achieving its goals.</p>\n<p>Next: I remain confused about why an intelligent system will decide that a particular final goal it has been given is \"stupid,\" and then change its final goals \u2014 especially given the convergent instrumental goal to preserve its final goals.</p>\n<p>Perhaps the word \"intelligence\" is getting in our way. Let's define a notion of \" <a href=\"/lw/va/measuring_optimization_power/\" target=\"_blank\">optimization power</a>,\" which measures (roughly) an agent's ability to optimize the world according to its preference ordering, across a very broad range of possible preference orderings and environments. I think we agree that AGIs with vastly greater-than-human optimization power will arrive in the next century or two. The problem, then, is that this superhuman AGI will almost certainly be optimizing the world for something other than what humans want, because what humans want is complex and fragile, and indeed we remain confused about what exactly it is that we want. A machine superoptimizer with a final goal of solving the Riemann hypothesis will simply be very good at solving the Riemann hypothesis (by whatever means necessary).</p>\n<p>Which parts of this analysis do you think are wrong?</p>\n<p>&nbsp;</p>\n<h3 id=\"Ben_1\">Ben:</h3>\n<p>[Apr. 20, 2012]</p>\n<p>It seems to me that in your reply you are implicitly assuming a much stronger definition of \u201cconvergent\u201d than the one Bostrom actually gives in his paper. He says</p>\n<blockquote>instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent\u2019s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</blockquote>\n<p>Note the somewhat weaselly reference to a \u201cwide range\u201d of goals and situations -- not, say, \u201cnearly all feasible\u201d goals and situations. Just because some values are convergent in the weak sense of his definition, doesn\u2019t imply that AGIs we create will be likely to adopt these instrumental values. I think that his weak definition of \u201cconvergent\u201d doesn\u2019t actually imply convergence in any useful sense. On the other hand, if he\u2019d made a stronger statement like</p>\n<blockquote>\n<p>instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent\u2019s goal being realized for nearly all feasible final goals and nearly all feasible situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</p>\n</blockquote>\n<p>then I would disagree with the first clause of his statement (\u201cinstrumental values can be identified which...\u201d), but I would be more willing to accept that the second clause (after the \u201cimplying\u201d) followed from the first.</p>\n<p>About optimization -- I think it\u2019s rather naive and narrow-minded to view hypothetical superhuman superminds as \u201coptimization powers.\u201d It\u2019s a bit like a dog viewing a human as an \u201ceating and mating power.\u201d Sure, there\u2019s some accuracy to that perspective -- we do eat and mate, and some of our behaviors may be understood based on this. On the other hand, a lot of our behaviors are not very well understood in terms of these, or any dog-level concepts. Similarly, I would bet that the bulk of a superhuman supermind\u2019s behaviors and internal structures and dynamics will not be explicable in terms of the concepts that are important to humans, such as \u201coptimization.\u201d</p>\n<p>So when you say \u201cthis superhuman AGI will almost certainly be optimizing the world for something other than what humans want,\" I don\u2019t feel confident that what a superhuman AGI will be doing, will be usefully describable as optimizing anything ....</p>\n<p>&nbsp;</p>\n<h3 id=\"Luke_2\">Luke:</h3>\n<p>[May 1, 2012]</p>\n<p>I think our dialogue has reached the point of diminishing marginal returns, so I'll conclude with just a few points and let you have the last word.</p>\n<p>On convergent instrumental goals, I encourage readers to read \" <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\" target=\"_blank\">The Superintelligent Will</a>\" and make up their own minds.</p>\n<p>On the convergence of advanced intelligent systems toward optimization behavior, I'll point you to <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\" target=\"_blank\">Omohundro (2007)</a>.</p>\n<p>&nbsp;</p>\n<h3 id=\"Ben_2\">Ben:</h3>\n<p>Well, it's been a fun chat. Although it hasn't really covered much new ground, there have been some new phrasings and minor new twists.</p>\n<p>One thing I'm repeatedly struck by in discussions on these matters with you and other SIAI folks, is the way the strings of reason are pulled by the puppet-master of intuition. With so many of these topics on which we disagree -- for example: the Scary Idea, the importance of optimization for intelligence, the existence of strongly convergent goals for intelligences -- you and the other core SIAI folks share a certain set of intuitions, which seem quite strongly held. Then you formulate rational arguments in favor of these intuitions -- but the conclusions that result from these rational arguments are very weak. For instance, the Scary Idea intuition corresponds to a rational argument that \"superhuman AGI might plausibly kill everyone.\" The intuition about strongly convergent goals for intelligences, corresponds to a rational argument about goals that are convergent for a \"wide range\" of intelligences. Etc.</p>\n<p>On my side, I have a strong intuition that OpenCog can be made into a human-level general intelligence, and that if this intelligence is raised properly it will turn out benevolent and help us launch a positive Singularity. However, I can't fully rationally substantiate this intuition either -- all I can really fully rationally argue for is something weaker like \"It seems plausible that a fully implemented OpenCog system might display human-level or greater intelligence on feasible computational resources, and might turn out benevolent if raised properly.\" In my case just like yours, reason is far weaker than intuition.</p>\n<p>Another thing that strikes me, reflecting on our conversation, is the difference between the degrees of confidence required, in modern democratic society, to TRY something versus to STOP others from trying something. A rough intuition is often enough to initiate a project, even a large one. On the other hand, to get someone else's work banned based on a rough intuition is pretty hard. To ban someone else's work, you either need a really thoroughly ironclad logical argument, or you need to stir up a lot of hysteria.</p>\n<p>What this suggests to me is that, while my intuitions regarding OpenCog seem to be sufficient to motivate others to help me to build OpenCog (via making them interested enough in it that they develop their own intuitions about it), your intuitions regarding the dangers of AGI are not going to be sufficient to get work on AGI systems like OpenCog stopped. To halt AGI development, if you wanted to (and you haven't said that you do, I realize), you'd either need to fan hysteria very successfully, or come up with much stronger logical arguments, ones that match the force of your intuition on the subject.</p>\n<p>Anyway, even though I have very different intuitions than you and your SIAI colleagues about a lot of things, I do think you guys are performing some valuable services -- not just through the excellent Singularity Summit conferences, but also by raising some difficult and important issues in the public eye. Humanity spends a lot of its attention on some really unimportant things, so it's good to have folks like SIAI nudging the world to think about critical issues regarding our future. In the end, whether SIAI's views are actually correct may be peripheral to the organization's main value and impact.</p>\n<p>I look forward to future conversations, and especially look forward to resuming this conversation one day with a human-level AGI as the mediator ;-)</p>", "sections": [{"title": "Continued from part 1...", "anchor": "Continued_from_part_1___", "level": 2}, {"title": "Luke:", "anchor": "Luke_", "level": 1}, {"title": "Ben:", "anchor": "Ben_", "level": 1}, {"title": "Orthogonality Thesis", "anchor": "Orthogonality_Thesis", "level": 2}, {"title": "Interdependency Thesis\u00a0", "anchor": "Interdependency_Thesis_", "level": 2}, {"title": "The Instrumental Convergence Thesis", "anchor": "The_Instrumental_Convergence_Thesis", "level": 2}, {"title": "Luke:", "anchor": "Luke_1", "level": 1}, {"title": "Ben:", "anchor": "Ben_1", "level": 1}, {"title": "Luke:", "anchor": "Luke_2", "level": 1}, {"title": "Ben:", "anchor": "Ben_2", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TpNRpncLBAzddBnRB", "Q4hLMDrFd8fbteeZ8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T01:53:00.161Z", "modifiedAt": null, "url": null, "title": "Meetup : Dallas - Fort Worth Less Wrong Meetup 5/6/12", "slug": "meetup-dallas-fort-worth-less-wrong-meetup-5-6-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jumandtonic", "createdAt": "2012-04-26T03:09:35.594Z", "isAdmin": false, "displayName": "jumandtonic"}, "userId": "2EmJ3AN5jKXHnvYpP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nq9ZDL55bxqnBGPYD/meetup-dallas-fort-worth-less-wrong-meetup-5-6-12", "pageUrlRelative": "/posts/Nq9ZDL55bxqnBGPYD/meetup-dallas-fort-worth-less-wrong-meetup-5-6-12", "linkUrl": "https://www.lesswrong.com/posts/Nq9ZDL55bxqnBGPYD/meetup-dallas-fort-worth-less-wrong-meetup-5-6-12", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F6%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F6%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNq9ZDL55bxqnBGPYD%2Fmeetup-dallas-fort-worth-less-wrong-meetup-5-6-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Dallas%20-%20Fort%20Worth%20Less%20Wrong%20Meetup%205%2F6%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNq9ZDL55bxqnBGPYD%2Fmeetup-dallas-fort-worth-less-wrong-meetup-5-6-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNq9ZDL55bxqnBGPYD%2Fmeetup-dallas-fort-worth-less-wrong-meetup-5-6-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/9v'>Dallas - Fort Worth Less Wrong Meetup 5/6/12</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We had a handful of people show up last week, and it was quite enjoyable.  Not bad for our first meetup!  I'll remember to bring a sign this time!</p>\n\n<p>Agenda:</p>\n\n<p>1:00 Introductions and call to order</p>\n\n<p>1:15 Discussion of: meetup group management (roles and agenda setting), mailing list options (meetup.com, facebook, google groups, etc.)</p>\n\n<p>1:30 Discussion of core sequences.  Recommended that you have read through the \"Map and Territory\" sequence (this is a short one), and start reading the \"Mysterious Answers to Mysterious Questions\" sequence.  (this is a long sequence, so may take a few meetings to get through)</p>\n\n<p>2:00 Some rationality-improving games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/9v'>Dallas - Fort Worth Less Wrong Meetup 5/6/12</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nq9ZDL55bxqnBGPYD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.966185799286965e-07, "legacy": true, "legacyId": "15833", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_6_12\">Discussion article for the meetup : <a href=\"/meetups/9v\">Dallas - Fort Worth Less Wrong Meetup 5/6/12</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 May 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">America's Best Coffee, Arlington</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We had a handful of people show up last week, and it was quite enjoyable.  Not bad for our first meetup!  I'll remember to bring a sign this time!</p>\n\n<p>Agenda:</p>\n\n<p>1:00 Introductions and call to order</p>\n\n<p>1:15 Discussion of: meetup group management (roles and agenda setting), mailing list options (meetup.com, facebook, google groups, etc.)</p>\n\n<p>1:30 Discussion of core sequences.  Recommended that you have read through the \"Map and Territory\" sequence (this is a short one), and start reading the \"Mysterious Answers to Mysterious Questions\" sequence.  (this is a long sequence, so may take a few meetings to get through)</p>\n\n<p>2:00 Some rationality-improving games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_6_121\">Discussion article for the meetup : <a href=\"/meetups/9v\">Dallas - Fort Worth Less Wrong Meetup 5/6/12</a></h2>", "sections": [{"title": "Discussion article for the meetup : Dallas - Fort Worth Less Wrong Meetup 5/6/12", "anchor": "Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_6_12", "level": 1}, {"title": "Discussion article for the meetup : Dallas - Fort Worth Less Wrong Meetup 5/6/12", "anchor": "Discussion_article_for_the_meetup___Dallas___Fort_Worth_Less_Wrong_Meetup_5_6_121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T05:20:14.372Z", "modifiedAt": null, "url": null, "title": "Low hanging fruit: analyzing your nutrition", "slug": "low-hanging-fruit-analyzing-your-nutrition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MXwyMPM4BHQnZAReN/low-hanging-fruit-analyzing-your-nutrition", "pageUrlRelative": "/posts/MXwyMPM4BHQnZAReN/low-hanging-fruit-analyzing-your-nutrition", "linkUrl": "https://www.lesswrong.com/posts/MXwyMPM4BHQnZAReN/low-hanging-fruit-analyzing-your-nutrition", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Low%20hanging%20fruit%3A%20analyzing%20your%20nutrition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALow%20hanging%20fruit%3A%20analyzing%20your%20nutrition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXwyMPM4BHQnZAReN%2Flow-hanging-fruit-analyzing-your-nutrition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Low%20hanging%20fruit%3A%20analyzing%20your%20nutrition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXwyMPM4BHQnZAReN%2Flow-hanging-fruit-analyzing-your-nutrition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXwyMPM4BHQnZAReN%2Flow-hanging-fruit-analyzing-your-nutrition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>Recently I decided to try an intermittent fasting diet. To do so, I had to figure out how much I could eat on my off/down days. I realized I didn't have a very good idea about how much calories my meals have, and as I was thinking about it, I started to get curious about my diet overall. How many calories do I get a day? How much of it is from fat? Do I get enough vitamins? Etc, etc, etc. All very basic questions, and since my meals are very regular, there was an easy way to find out!</p>\n<p>(Note: anytime you feel curious and are about to find something out, make some predictions. I didn't, but I really wish I did, because I was very surprised by my findings.)</p>\n<p>It took only a couple of hours, and <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AmMbFVjewsaRdGJRZnVDSU9sQVVMRlptMzl4ekFXS1E\">here is the result</a>.</p>\n<p>If you scroll down, you can see that my usual Breakfast+Lunch+Dinner only nets about 1000 calories and gives 30% daily value of fat. No wonder I crave cookies and chocolate so much!</p>\n<p>There are many surprising results that I got from this. And knowing that I've been eating like this for the past few years... Wow. This is the epitome of a low hanging fruit. I can't believe I didn't do this analysis earlier!</p>\n<p>&nbsp;</p>\n<p>Edit: I was not trying to say that I only get 1000 calories a day. Of course I get more than that, but the rest is from cookies and post-meal sweets. I always thought I just have a sweet tooth, but the fact that I wasn't getting enough calories from my main meals can also explain this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MXwyMPM4BHQnZAReN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 15, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "15839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T06:07:27.916Z", "modifiedAt": null, "url": null, "title": "A simple web app to create aversion to unhealthy food", "slug": "a-simple-web-app-to-create-aversion-to-unhealthy-food", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.582Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9x9Fueopv5bNebfXE/a-simple-web-app-to-create-aversion-to-unhealthy-food", "pageUrlRelative": "/posts/9x9Fueopv5bNebfXE/a-simple-web-app-to-create-aversion-to-unhealthy-food", "linkUrl": "https://www.lesswrong.com/posts/9x9Fueopv5bNebfXE/a-simple-web-app-to-create-aversion-to-unhealthy-food", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20simple%20web%20app%20to%20create%20aversion%20to%20unhealthy%20food&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20simple%20web%20app%20to%20create%20aversion%20to%20unhealthy%20food%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9x9Fueopv5bNebfXE%2Fa-simple-web-app-to-create-aversion-to-unhealthy-food%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20simple%20web%20app%20to%20create%20aversion%20to%20unhealthy%20food%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9x9Fueopv5bNebfXE%2Fa-simple-web-app-to-create-aversion-to-unhealthy-food", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9x9Fueopv5bNebfXE%2Fa-simple-web-app-to-create-aversion-to-unhealthy-food", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>At a startup camp weekend, I've <a href=\"http://turnoffconditioner.appspot.com/\">made this (incredibly sketchy, amateurish) app</a> (NSFW) which implements classical conditioning to create a negative response to candy, smoking, or meat.</p>\n<p>After using it intensively for about a day while writing it, I certainly feel disgusted at the sight of candy or chocolate. There's a little bit of evidence that something like this could actually have an effect on people: I'm reminded of the penny jar fetish experiment and <a href=\"http://findarticles.com/p/articles/mi_6884/is_2_3/ai_n28132941/\">this chapter on \"curing\" homosexuality.</a></p>\n<p>I post this here for two reasons: to get advice on how to improve the app, and to get your opinions on whether this type of program might actually be useful. I'm reminded a bit of Anki, how the science led to the effective app. In this case, I'm not sure if it will turn out to be actually useful: in the experiments which have shown an effect from aversion type stimulus like this, the unpleasant image has been causally connected to the stimulus we're conditioning against, eg unhealthy food and diseased tissue.</p>\n<p>Go easy on the web design, I'm just an amateur. :)</p>\n<p>Thanks</p>\n<p>EDIT: Yeah, it should be marked NSFW. Sorry.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9x9Fueopv5bNebfXE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 8.967287922378663e-07, "legacy": true, "legacyId": "15845", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T06:37:31.042Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Science Doesn't Trust Your Rationality", "slug": "seq-rerun-science-doesn-t-trust-your-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CBSuSjJg8YJgi2mCE/seq-rerun-science-doesn-t-trust-your-rationality", "pageUrlRelative": "/posts/CBSuSjJg8YJgi2mCE/seq-rerun-science-doesn-t-trust-your-rationality", "linkUrl": "https://www.lesswrong.com/posts/CBSuSjJg8YJgi2mCE/seq-rerun-science-doesn-t-trust-your-rationality", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Science%20Doesn't%20Trust%20Your%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Science%20Doesn't%20Trust%20Your%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBSuSjJg8YJgi2mCE%2Fseq-rerun-science-doesn-t-trust-your-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Science%20Doesn't%20Trust%20Your%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBSuSjJg8YJgi2mCE%2Fseq-rerun-science-doesn-t-trust-your-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBSuSjJg8YJgi2mCE%2Fseq-rerun-science-doesn-t-trust-your-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/qb/science_doesnt_trust_your_rationality/\">Science Doesn't Trust Your Rationality</a> was originally published on 14 May 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The reason Science doesn't always agree with the exact, Bayesian, rational answer, is that Science doesn't <em>trust </em>you to be rational. It wants you to go out and gather overwhelming experimental evidence.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/c71/seq_rerun_the_dilemma_science_or_bayes/\">The Dilemma: Science or Bayes?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CBSuSjJg8YJgi2mCE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 8.967418099082758e-07, "legacy": true, "legacyId": "15848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5bJyRMZzwMov5u3hW", "dspwjD74JXfnMm6Sz", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T15:07:45.655Z", "modifiedAt": null, "url": null, "title": "Test your forecasting ability, contribute to the science of human judgment", "slug": "test-your-forecasting-ability-contribute-to-the-science-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.142Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelBishop", "createdAt": "2009-02-27T21:29:30.329Z", "isAdmin": false, "displayName": "MichaelBishop"}, "userId": "HEJJhygH2QnAxDddx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/37YSQe3oCBvqWpCNR/test-your-forecasting-ability-contribute-to-the-science-of", "pageUrlRelative": "/posts/37YSQe3oCBvqWpCNR/test-your-forecasting-ability-contribute-to-the-science-of", "linkUrl": "https://www.lesswrong.com/posts/37YSQe3oCBvqWpCNR/test-your-forecasting-ability-contribute-to-the-science-of", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20your%20forecasting%20ability%2C%20contribute%20to%20the%20science%20of%20human%20judgment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20your%20forecasting%20ability%2C%20contribute%20to%20the%20science%20of%20human%20judgment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37YSQe3oCBvqWpCNR%2Ftest-your-forecasting-ability-contribute-to-the-science-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20your%20forecasting%20ability%2C%20contribute%20to%20the%20science%20of%20human%20judgment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37YSQe3oCBvqWpCNR%2Ftest-your-forecasting-ability-contribute-to-the-science-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37YSQe3oCBvqWpCNR%2Ftest-your-forecasting-ability-contribute-to-the-science-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1007, "htmlBody": "<p>As XFrequentist mentioned <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\" target=\"_self\">last August</a>, \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">Intelligence Advanced Research Project Activity (IARPA) with the goal of improving forecasting methods for global events of national (US) interest. One of the teams (The Good Judgement Team) is recruiting volunteers to have their forecasts tracked. Volunteers will receive an annual honorarium ($150), and it appears there will be ongoing training to improve one's forecast accuracy (not sure exactly what form this will take).\"</span></p>\n<p>You can pre-register <a href=\"http://www.goodjudgmentproject.com/register\">here</a>.</p>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 16px;\">Last year, approximately 2400 forecasters were assigned to one of eight experimental conditions. &nbsp;I was the #1 forecaster in my condition. &nbsp;It was fun, and I learned a lot, and eventually they are going to give me a public link so that I can brag about this until the end of time. &nbsp;I'm participating again this year, though I plan to regress towards the mean.</span></span></p>\n<p>I'll share the same info XFrequentist did last year below the fold because I think it's all still relevant.</p>\n<p><a id=\"more\"></a></p>\n<p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Despite its importance in modern life, forecasting remains (ironically) unpredictable. Who is a good forecaster? How do you make people better forecasters? Are there processes or technologies that can improve the ability of governments, companies, and other institutions to perceive and act on trends and threats? Nobody really knows.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">The goal of the Good Judgment Project is to answer these questions. We will systematically compare the effectiveness of different training methods (general education, probabilistic-reasoning training, divergent-thinking training) and forecasting tools (low- and high-information opinion-polls, prediction market, and process-focused tools) in accurately forecasting future events. We also will investigate how different combinations of training and forecasting work together. Finally, we will explore how to more effectively communicate forecasts in ways that avoid overwhelming audiences with technical detail or oversimplifying difficult decisions.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Over the course of each year, forecasters will have an opportunity to respond to 100 questions, each requiring a separate prediction, such as &ldquo;How many countries in the Euro zone will default on bonds in 2011?&rdquo; or &ldquo;Will Southern Sudan become an independent country in 2011?&rdquo; Researchers from the Good Judgment Project will look for the best ways to combine these individual forecasts to yield the most accurate &ldquo;collective wisdom&rdquo; results.&nbsp; Participants also will receive feedback on their individual results.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">All training and forecasting will be done online. Forecasters&rsquo; identities will not be made public; however, successful forecasters will have the option to publicize their own track records.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\"><strong>Who We Are</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">The Good Judgment research team is based in the University of Pennsylvania and the University of California Berkeley. The project is led by psychologists Philip Tetlock, author of the award-winning&nbsp;<em>Expert Political Judgment</em>, Barbara Mellers, an expert on judgment and decision-making, and Don Moore, an expert on overconfidence. Other team members are experts in psychology, economics, statistics, interface design, futures, and computer science.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">We are one of five teams competing in the Aggregative Contingent Estimation (ACE) Program, sponsored by IARPA (the U.S. Intelligence Advanced Research Projects Activity). The ACE Program aims \"to dramatically enhance the accuracy, precision, and timeliness of forecasts for a broad range of event types, through the development of advanced techniques that elicit, weight, and combine the judgments of many intelligence analysts.\" The project is unclassified: our results will be published in traditional scholarly and scientific journals, and will be available to the general public.</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">A general description of the expected benefits for volunteers:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px; line-height: 18px;\"><span style=\"font-family: inherit;\">All decisions involve forecasts, and we all make forecasts all the&nbsp;time.&nbsp;&nbsp;When we decide to change&nbsp;jobs, we perform an analysis of potential futures for&nbsp;each of our options.&nbsp;&nbsp;When a business decides to invest or&nbsp;disinvest in a project, it moves in the direction it believes to present the&nbsp;best opportunity.&nbsp;&nbsp;The&nbsp;same applies&nbsp;when a government decides to launch or abandon a policy.</span><br /><br />But we virtually never keep score. Very few forecasters know what&nbsp;their forecasting batting average is&nbsp;&mdash;&nbsp;or even how to go about estimating what it is.<br /><br />If you want to discover what your forecasting batting average is&nbsp;&mdash;&nbsp;and how to think&nbsp;about the very concept&nbsp;&mdash;&nbsp;you should seriously consider joining The Good Judgment Project. Self-knowledge is its own&nbsp;reward. But with self-knowledge, you have a baseline against which you can&nbsp;measure improvement over time. If you&nbsp;want to explore how high your forecasting&nbsp;batting average could go, and are prepared to put in some work at&nbsp;self-improvement, this is definitely the&nbsp;project for you.</span></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">Could that be any more LessWrong-esque?</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif, sans-serif; line-height: 15px;\">Prediction markets can harness the \"wisdom of crowds\" to solve problems, develop products, and make forecasts. These systems typically treat collective intelligence as a commodity to be mined, not a resource that can be grown and improved. That&rsquo;s about to change.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Starting in mid-2011, five teams will compete in a U.S.-government-sponsored forecasting tournament. Each team will develop its own tools for harnessing and improving collective intelligence and will be judged on how well its forecasters predict major trends and events around the world over the next four years.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">The&nbsp;<em>Good Judgment Team</em>, based in the University of Pennsylvania and the University of California Berkeley, will be one of the five teams competing &ndash; and we&rsquo;d like you to consider joining our team as a forecaster. If you're willing to experiment with ways to improve your forecasting ability and if being part of cutting-edge scientific research appeals to you, then we want your help.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">We can promise you the chance to: (1) learn about yourself (your skill in predicting &ndash; and your skill in becoming more accurate over time as you learn from feedback and/or special training exercises); (2) contribute to cutting-edge scientific work on both individual-level factors that promote or inhibit accuracy and group- or team-level factors that contribute to accuracy; and (3) help us distinguish better from worse approaches to generating forecasts of importance to national security, global affairs, and economics.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\"><strong>Who Can Participate</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Requirements for participation include the following:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">(1) A baccalaureate, bachelors, or undergraduate degree from an accredited college or university (more advanced degrees are welcome);</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">(2) A curiosity about how well you make predictions about world events &ndash; and an interest in exploring techniques for improvement.<a style=\"color: #8a8a8b;\" href=\"http://goodjudgmentproject.blogspot.com/\"></a></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">More info:&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://goodjudgmentproject.blogspot.com/\">http://goodjudgmentproject.blogspot.com/</a></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "37YSQe3oCBvqWpCNR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 8.969628837691924e-07, "legacy": true, "legacyId": "15850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As XFrequentist mentioned <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\" target=\"_self\">last August</a>, \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">Intelligence Advanced Research Project Activity (IARPA) with the goal of improving forecasting methods for global events of national (US) interest. One of the teams (The Good Judgement Team) is recruiting volunteers to have their forecasts tracked. Volunteers will receive an annual honorarium ($150), and it appears there will be ongoing training to improve one's forecast accuracy (not sure exactly what form this will take).\"</span></p>\n<p>You can pre-register <a href=\"http://www.goodjudgmentproject.com/register\">here</a>.</p>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 16px;\">Last year, approximately 2400 forecasters were assigned to one of eight experimental conditions. &nbsp;I was the #1 forecaster in my condition. &nbsp;It was fun, and I learned a lot, and eventually they are going to give me a public link so that I can brag about this until the end of time. &nbsp;I'm participating again this year, though I plan to regress towards the mean.</span></span></p>\n<p>I'll share the same info XFrequentist did last year below the fold because I think it's all still relevant.</p>\n<p><a id=\"more\"></a></p>\n<p>\n</p><blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Despite its importance in modern life, forecasting remains (ironically) unpredictable. Who is a good forecaster? How do you make people better forecasters? Are there processes or technologies that can improve the ability of governments, companies, and other institutions to perceive and act on trends and threats? Nobody really knows.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">The goal of the Good Judgment Project is to answer these questions. We will systematically compare the effectiveness of different training methods (general education, probabilistic-reasoning training, divergent-thinking training) and forecasting tools (low- and high-information opinion-polls, prediction market, and process-focused tools) in accurately forecasting future events. We also will investigate how different combinations of training and forecasting work together. Finally, we will explore how to more effectively communicate forecasts in ways that avoid overwhelming audiences with technical detail or oversimplifying difficult decisions.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Over the course of each year, forecasters will have an opportunity to respond to 100 questions, each requiring a separate prediction, such as \u201cHow many countries in the Euro zone will default on bonds in 2011?\u201d or \u201cWill Southern Sudan become an independent country in 2011?\u201d Researchers from the Good Judgment Project will look for the best ways to combine these individual forecasts to yield the most accurate \u201ccollective wisdom\u201d results.&nbsp; Participants also will receive feedback on their individual results.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">All training and forecasting will be done online. Forecasters\u2019 identities will not be made public; however, successful forecasters will have the option to publicize their own track records.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\"><strong id=\"Who_We_Are\">Who We Are</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">The Good Judgment research team is based in the University of Pennsylvania and the University of California Berkeley. The project is led by psychologists Philip Tetlock, author of the award-winning&nbsp;<em>Expert Political Judgment</em>, Barbara Mellers, an expert on judgment and decision-making, and Don Moore, an expert on overconfidence. Other team members are experts in psychology, economics, statistics, interface design, futures, and computer science.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">We are one of five teams competing in the Aggregative Contingent Estimation (ACE) Program, sponsored by IARPA (the U.S. Intelligence Advanced Research Projects Activity). The ACE Program aims \"to dramatically enhance the accuracy, precision, and timeliness of forecasts for a broad range of event types, through the development of advanced techniques that elicit, weight, and combine the judgments of many intelligence analysts.\" The project is unclassified: our results will be published in traditional scholarly and scientific journals, and will be available to the general public.</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">A general description of the expected benefits for volunteers:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Tahoma, Helvetica, FreeSans, sans-serif; font-size: 13px; line-height: 18px;\"><span style=\"font-family: inherit;\">All decisions involve forecasts, and we all make forecasts all the&nbsp;time.&nbsp;&nbsp;When we decide to change&nbsp;jobs, we perform an analysis of potential futures for&nbsp;each of our options.&nbsp;&nbsp;When a business decides to invest or&nbsp;disinvest in a project, it moves in the direction it believes to present the&nbsp;best opportunity.&nbsp;&nbsp;The&nbsp;same applies&nbsp;when a government decides to launch or abandon a policy.</span><br><br>But we virtually never keep score. Very few forecasters know what&nbsp;their forecasting batting average is&nbsp;\u2014&nbsp;or even how to go about estimating what it is.<br><br>If you want to discover what your forecasting batting average is&nbsp;\u2014&nbsp;and how to think&nbsp;about the very concept&nbsp;\u2014&nbsp;you should seriously consider joining The Good Judgment Project. Self-knowledge is its own&nbsp;reward. But with self-knowledge, you have a baseline against which you can&nbsp;measure improvement over time. If you&nbsp;want to explore how high your forecasting&nbsp;batting average could go, and are prepared to put in some work at&nbsp;self-improvement, this is definitely the&nbsp;project for you.</span></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">Could that be any more LessWrong-esque?</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif, sans-serif; line-height: 15px;\">Prediction markets can harness the \"wisdom of crowds\" to solve problems, develop products, and make forecasts. These systems typically treat collective intelligence as a commodity to be mined, not a resource that can be grown and improved. That\u2019s about to change.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Starting in mid-2011, five teams will compete in a U.S.-government-sponsored forecasting tournament. Each team will develop its own tools for harnessing and improving collective intelligence and will be judged on how well its forecasters predict major trends and events around the world over the next four years.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">The&nbsp;<em>Good Judgment Team</em>, based in the University of Pennsylvania and the University of California Berkeley, will be one of the five teams competing \u2013 and we\u2019d like you to consider joining our team as a forecaster. If you're willing to experiment with ways to improve your forecasting ability and if being part of cutting-edge scientific research appeals to you, then we want your help.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">We can promise you the chance to: (1) learn about yourself (your skill in predicting \u2013 and your skill in becoming more accurate over time as you learn from feedback and/or special training exercises); (2) contribute to cutting-edge scientific work on both individual-level factors that promote or inhibit accuracy and group- or team-level factors that contribute to accuracy; and (3) help us distinguish better from worse approaches to generating forecasts of importance to national security, global affairs, and economics.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\"><strong id=\"Who_Can_Participate\">Who Can Participate</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">Requirements for participation include the following:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif, sans-serif;\">(1) A baccalaureate, bachelors, or undergraduate degree from an accredited college or university (more advanced degrees are welcome);</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">(2) A curiosity about how well you make predictions about world events \u2013 and an interest in exploring techniques for improvement.<a style=\"color: #8a8a8b;\" href=\"http://goodjudgmentproject.blogspot.com/\"></a></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">More info:&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://goodjudgmentproject.blogspot.com/\">http://goodjudgmentproject.blogspot.com/</a></p>\n<p></p>", "sections": [{"title": "Who We Are", "anchor": "Who_We_Are", "level": 1}, {"title": "Who Can Participate", "anchor": "Who_Can_Participate", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PyuRcbHvxXNdCHoG3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-05T22:38:31.741Z", "modifiedAt": null, "url": null, "title": "No Value", "slug": "no-value", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:30.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raiden", "createdAt": "2012-02-24T03:49:11.087Z", "isAdmin": false, "displayName": "Raiden"}, "userId": "kHHiycCw65gSsBj6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/diYL9Fx9ycqb8nE8D/no-value", "pageUrlRelative": "/posts/diYL9Fx9ycqb8nE8D/no-value", "linkUrl": "https://www.lesswrong.com/posts/diYL9Fx9ycqb8nE8D/no-value", "postedAtFormatted": "Saturday, May 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20Value&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20Value%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdiYL9Fx9ycqb8nE8D%2Fno-value%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20Value%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdiYL9Fx9ycqb8nE8D%2Fno-value", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdiYL9Fx9ycqb8nE8D%2Fno-value", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>I am still quite new to LW, so I apologize if this is something that has been discussed before (I did try and search).</p>\n<p>I would't normally post such a thing, as I try not to make a habit of complaining my problems to others, but a solution to this would likely benefit other rationalists (at least that's the excuse I made to myself).</p>\n<p>Essentially, I am currently in a psychological state in which I simply have no strong values. There is no state of the world that I can imagine the world being in that generates a strong emotional reaction. Ever. In fact, I rarely experience strong emotions at all. When I do, I savor them whether they're positive or negative. I do have some preferences; I would somewhat prefer the world to be some ways than others, but never strongly. I prefer to feel pleasure rather than pain; I prefer the world to be a good place than a bad one, but not by much. Even my desire to have values seems to be a mere preference in much the same way. I have nothing to protect.</p>\n<p>Is there any good solution to this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "diYL9Fx9ycqb8nE8D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 20, "extendedScore": null, "score": 8.971582688729711e-07, "legacy": true, "legacyId": "15851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-05-06T00:32:12.376Z", "modifiedAt": null, "url": null, "title": "Delayed Gratification vs. a Time-Dependent Utility Function", "slug": "delayed-gratification-vs-a-time-dependent-utility-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.529Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "momothefiddler", "createdAt": "2011-10-22T23:44:11.681Z", "isAdmin": false, "displayName": "momothefiddler"}, "userId": "TzrLvGFNhmtaadquK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LtXKkfk2ZBqp2XNsu/delayed-gratification-vs-a-time-dependent-utility-function", "pageUrlRelative": "/posts/LtXKkfk2ZBqp2XNsu/delayed-gratification-vs-a-time-dependent-utility-function", "linkUrl": "https://www.lesswrong.com/posts/LtXKkfk2ZBqp2XNsu/delayed-gratification-vs-a-time-dependent-utility-function", "postedAtFormatted": "Sunday, May 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Delayed%20Gratification%20vs.%20a%20Time-Dependent%20Utility%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADelayed%20Gratification%20vs.%20a%20Time-Dependent%20Utility%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtXKkfk2ZBqp2XNsu%2Fdelayed-gratification-vs-a-time-dependent-utility-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Delayed%20Gratification%20vs.%20a%20Time-Dependent%20Utility%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtXKkfk2ZBqp2XNsu%2Fdelayed-gratification-vs-a-time-dependent-utility-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtXKkfk2ZBqp2XNsu%2Fdelayed-gratification-vs-a-time-dependent-utility-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 945, "htmlBody": "<p>Ideally, a utility function would be a rational, perfect, constant entity that accounted for all possible variables, but mine certainly isn't. In fact, I'd feel quite comfortable claiming that no humans at the time of writing do.</p>\n<p>When confronted with the fact that my utility function is non-ideal or - since there's no universal ideal to compare it to - internally inconsistent, I do my best to figure out what to change and do so. The problem with a non-constant utility function, though, is that it makes it hard to maximise total utility. For instance, I am willing to undergo -50 units of utility today in return for +1 utility on each following day indefinitely. What if I accept the -50, but then my utility function changes tomorrow such that I now consider the change to be neutral, or worse, negative per day?</p>\n<p>Just as plausible is the idea that I be offered a trade that, while not of positive utility according to my function now, will be according to a future function. Just as I would think it a good investment to buy gold if I expected the price to go up but bad if I expected the price to go down, so I have to base my long-term utility trades on what I expect my future functions to be. (Not that dollars don't correlate with units of utility, just that they don't correlate <em>strongly</em>.)</p>\n<p>How can I know what I will want to do, much less what I will want to have done? If I obtain the outcome I prefer now, but spend more time not preferring it, does that make it a negative choice? Is it a reasonable decision, in order to maximise utility, to purposefully change your definition of utility such that your expected future would maximise it?</p>\n<p>&nbsp;</p>\n<p>What brings this all to mind is a choice I have to make soon. Technically, I've already made it, but I'm now uncertain of that choice and it has to be made final soon. This fall I transfer from my community college to a university, where I will focus a significant amount of energy studying Something 1 in order to become trained (and certified) to do Something 2 for a long period of time. I had thought until today that it was reasonable for Something 1 to be math and Something 2 to be teaching math. I enjoy the beauty of mathematics. I love how things fit together, barely anything can excite me as much as the definition of a derivative and its meaning, and I've shown myself to be rather good at it (which, to be fair, is by comparison to those around me, so I don't know how I'd fare in a larger or more specialized pool). In addition, I've spent some time as a tutor and I seem to be good at explaining mathematics to other people and I enjoy seeing their faces light up as they see how things fit together.</p>\n<p>Today, though, I don't know if that's really a wise decision. I was rereading <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Eliezer's paper on AI in Global Risk</a> and was struck by a line: \"If we want people who can make progress on&nbsp;Friendly AI, then they have to start training themselves, full-time, years before they are&nbsp;urgently needed.\" It occurred to me that I think FAI is possible and that I expect <em>some</em>&nbsp;sort of AI within my lifetime (though I don't expect that to be short). Perhaps I'd be happier studying topology than I would cognitive science and I'd definitely be happier studying topology than I would evolutionary psychology, but I'm not sure that even matters. Studying mathematics would provide positive utility to me personally and allow me to teach it. Teaching mathematics would be valued positively by me both because of my direct enjoyment <em>and</em>&nbsp;because I value a universe where a given person knows and appreciates math more than an otherwise-identical universe where that person doesn't. The appearance of an FAI would by far outclass the former and likely negate the significance of the latter. A uFAI has such a low utility that it would cancel out any positive utility from studying math. In fact, even if I focus purely on the increase of logical processes and mathematical understanding in Homo Sapiens <em>and</em>&nbsp;neglect the negative effects of a uFAI, moving the creation of an FAI forward by even a matter of days could easily be of more end value than being a professor for twenty years.</p>\n<p>&nbsp;</p>\n<p>I don't want to give up my unrealistic, idealized dream of math professorship to study a subject that makes me less happy, but if I shut up and multiply the numbers tell me that my happiness doesn't matter except as it affects my efficacy. In fact, shutting up and multiplying indicates that, if large amounts of labour were of significant use (and I doubt that would be any more use than large amounts of computing power) then it'd be plausible to at least consider subjugating the entire species and putting all effort to creating an FAI. I'm nearly certain this result comes from having missed something, but I can't see what and I'm scared that near-certainty is merely an expression of my negative anticipation regarding giving up my pretty little plans.</p>\n<p>&nbsp;</p>\n<p>Eliezer routinely puts forward examples such as an AI that tiles the universe with molecular smiley faces as negative. My basic dilemma is this: Does the utility function <em>at the time of the choice</em>&nbsp;have some sort of preferred status in the calculation, or would it be highly positive to create an AI that rewrites brains to value above all else a universe tiled with molecular smiley faces and <em>then</em>&nbsp;tiles the universe with molecular smiley faces?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LtXKkfk2ZBqp2XNsu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 8.972075540799279e-07, "legacy": true, "legacyId": "15852", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}